{
  "torchao_lib": [],
  "__all__": [],
  "_assert_and_get_unique_device": [
    "module"
  ],
  "benchmark_model": [
    "model",
    "num_runs",
    "args",
    "kwargs",
    "device_type"
  ],
  "profiler_runner": [
    "path",
    "fn"
  ],
  "get_available_devices": [],
  "get_current_accelerator_device": [],
  "get_compute_capability": [],
  "compute_max_diff": [
    "output",
    "output_ref"
  ],
  "benchmark_torch_function_in_microseconds": [
    "f"
  ],
  "find_multiple": [
    "n"
  ],
  "_register_custom_op": [
    "lib",
    "inductor_decomposed"
  ],
  "_register_meta_op": [
    "lib",
    "op_name"
  ],
  "get_model_size_in_bytes": [
    "model",
    "ignore_embeddings"
  ],
  "UnwrapTensorSubclass": {
    "forward": [
      "self"
    ],
    "right_inverse": [
      "self",
      "tensor"
    ]
  },
  "unwrap_tensor_subclass": [
    "model",
    "filter_fn"
  ],
  "_is_float8_type": [
    "dtype"
  ],
  "parse_version": [
    "version_string"
  ],
  "is_fbcode": [],
  "torch_version_at_least": [
    "min_version"
  ],
  "_implements": [
    "cls",
    "aten_ops"
  ],
  "_implements_torch_function": [
    "cls",
    "torch_fns"
  ],
  "_implements_common_tensor_ops": [
    "cls"
  ],
  "_torchao_base_tensor__setstate__": [
    "self",
    "state"
  ],
  "_dispatch__torch_function__": [
    "cls",
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "_dispatch__torch_dispatch__": [
    "cls",
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "_register_layout": [
    "tensor_class",
    "layout_class"
  ],
  "_get_tensor_impl_constructor": [
    "tensor_class",
    "layout_class"
  ],
  "_get_to_kwargs": [
    "self"
  ],
  "TorchAOBaseTensor": {
    "__init_subclass__": [
      "cls"
    ],
    "implements": [],
    "implements_torch_function": [],
    "_implements_common_tensor_ops": [],
    "__torch_dispatch__": [],
    "__torch_function__": [],
    "_get_to_kwargs": [],
    "register_layout": [],
    "get_tensor_impl_constructor": [],
    "__init__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__repr__": [
      "self"
    ],
    "get_layout": [
      "self"
    ]
  },
  "fill_defaults": [
    "args",
    "n",
    "defaults_tail"
  ],
  "is_ROCM": [],
  "is_MI300": [],
  "is_MI350": [],
  "is_Navi4": [],
  "is_sm_version": [
    "major",
    "minor"
  ],
  "is_sm_at_least_89": [],
  "is_sm_at_least_90": [],
  "is_sm_at_least_100": [],
  "is_cuda_version_at_least": [
    "major",
    "minor"
  ],
  "check_cpu_version": [
    "device",
    "version"
  ],
  "check_xpu_version": [
    "device",
    "version"
  ],
  "ceil_div": [
    "a",
    "b"
  ],
  "round_up": [
    "a",
    "b"
  ],
  "is_package_at_least": [
    "package_name",
    "min_version"
  ],
  "_is_mslk_available": [],
  "DummyModule": {
    "__init__": [
      "self",
      "weight",
      "bias"
    ]
  },
  "logger": [],
  "_parse_version": [
    "version_string"
  ],
  "skip_loading_so_files": [],
  "force_skip_loading_so_files": [],
  "_lazy_imports": [],
  "__getattr__": [
    "name"
  ],
  "_quantized_decomposed_quantize_per_channel_group_wrapper": [],
  "_quantized_decomposed_choose_qparams_per_token_asymmetric_wrapper": [],
  "_quantized_decomposed_dequantize_per_channel_group_wrapper": [],
  "_quantized_decomposed_quantize_per_token_wrapper": [],
  "_quantized_decomposed_dequantize_per_token_wrapper": [],
  "lib": [],
  "register_custom_op": [
    "name"
  ],
  "register_custom_op_impl": [
    "name"
  ],
  "cached_compute_capability": [],
  "qscaled_dot_product": [
    "query",
    "key",
    "value",
    "attn_mask",
    "dropout_p",
    "is_causal",
    "scale",
    "q_scale",
    "q_zp",
    "k_scale",
    "k_zp",
    "v_scale",
    "v_zp",
    "a_scale",
    "a_zp",
    "o_scale",
    "o_zp"
  ],
  "_": [
    "input",
    "input_scales",
    "weight",
    "weight_scales",
    "bias",
    "out_dtype"
  ],
  "rowwise_scaled_linear_sparse_cutlass_f8f8": [
    "input",
    "input_scale",
    "weight",
    "weight_meta",
    "weight_scale",
    "bias",
    "out_dtype"
  ],
  "to_sparse_semi_structured_cutlass_sm9x_f8": [
    "weight"
  ],
  "swizzle_mm": [
    "mat1",
    "mat2",
    "mat1_is_swizzled",
    "mat2_is_swizzled"
  ],
  "swizzle_scaled_mm": [
    "mat1",
    "mat2",
    "mat1_is_swizzled",
    "mat2_is_swizzled",
    "scale_a",
    "scale_b",
    "bias",
    "scale_result",
    "out_dtype"
  ],
  "_get_dtypes": [],
  "_check_scale_dtypes": [
    "A_scale",
    "B_scale"
  ],
  "meta_mx_fp8_bf16": [
    "A",
    "B",
    "A_scale",
    "B_scale"
  ],
  "da8w4_linear_prepack_cpu": [
    "weight",
    "scales",
    "qzeros"
  ],
  "da8w4_linear_cpu": [
    "input",
    "input_scales",
    "input_qzeros",
    "weight",
    "weight_scales",
    "weight_qzeros",
    "compensation",
    "bias",
    "out_dtype"
  ],
  "float8_linear_prepack_cpu": [
    "weight",
    "scales"
  ],
  "float8_linear_cpu": [
    "input",
    "input_scales",
    "weight",
    "weight_scales",
    "bias",
    "out_dtype"
  ],
  "AUTOTUNE": [],
  "tune_bsr_dense_addmm": [
    "input",
    "bsr",
    "dense"
  ],
  "bsr_dense_addmm_meta": [
    "M",
    "K",
    "N",
    "Ms",
    "Ks",
    "beta",
    "alpha",
    "SPLIT_N",
    "GROUP_SIZE_ROW",
    "num_warps",
    "num_stages",
    "sparsity",
    "dtype",
    "out_dtype",
    "_version"
  ],
  "bsr_dense_addmm": [
    "input",
    "bsr",
    "dense"
  ],
  "_triton_initialized": [],
  "_bsr_strided_addmm_kernel": [],
  "_lazy_init_triton": [],
  "AUTOTUNER_ENABLE": [],
  "safe_int_mm": [
    "input",
    "mat2"
  ],
  "int_matmul": [
    "a",
    "b"
  ],
  "int_scaled_matmul": [
    "a",
    "b",
    "scales1"
  ],
  "_triton_available": [],
  "_blockwise_fp8_gemm_impl": [],
  "_fp8_blockwise_act_quant_kernel": [],
  "_fp8_blockwise_weight_quant_kernel": [],
  "_fp8_blockwise_weight_dequant_kernel": [],
  "blockwise_fp8_gemm": [
    "a",
    "a_s",
    "b",
    "b_s",
    "block_size"
  ],
  "fp8_blockwise_act_quant": [
    "x",
    "block_size",
    "dtype"
  ],
  "fp8_blockwise_weight_quant": [
    "x",
    "block_size",
    "dtype"
  ],
  "fp8_blockwise_weight_dequant": [
    "x",
    "s",
    "block_size"
  ],
  "AUTOTUNER_DATA_PATH": [],
  "do_bench_triton": [
    "fn",
    "warmup",
    "rep",
    "grad_to_none",
    "quantiles",
    "fast_flush",
    "return_mode"
  ],
  "BEST_CONFIGS": [],
  "_save_best_configs": [
    "best_configs"
  ],
  "_load_best_configs": [],
  "get_arg_key": [
    "a"
  ],
  "get_args_key": [
    "args"
  ],
  "do_bench_basic": [
    "fn",
    "rep"
  ],
  "do_bench": [
    "fn",
    "args",
    "config",
    "best_time"
  ],
  "get_best_config_by_key": [
    "key"
  ],
  "get_best_config_fn": [
    "fn",
    "args",
    "configs"
  ],
  "int8_mm_kernel_configs": [],
  "matmul_kernel_with_block_pointers": [
    "a_ptr",
    "b_ptr",
    "c_ptr",
    "M",
    "N",
    "K",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "GROUP_M"
  ],
  "scaled_matmul_kernel_with_block_pointers": [
    "a_ptr",
    "b_ptr",
    "c_ptr",
    "s1_ptr",
    "M",
    "N",
    "K",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "stride_s1m",
    "stride_s1n",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "GROUP_M",
    "EVEN_K",
    "ACC_TYPE"
  ],
  "int_matmul_kernel": [
    "a",
    "b",
    "c",
    "config"
  ],
  "int_scaled_matmul_kernel": [
    "a",
    "b",
    "scales1",
    "c",
    "config"
  ],
  "int_matmul_meta": [
    "a",
    "b"
  ],
  "int_matmul_cuda": [
    "a",
    "b"
  ],
  "int_scaled_matmul_meta": [
    "a",
    "b",
    "scales1"
  ],
  "int_scaled_matmul_cuda": [
    "a",
    "b",
    "scales1"
  ],
  "get_arch_name": [],
  "write_json_result_ossci": [
    "output_json_path",
    "headers",
    "row"
  ],
  "write_json_result_local": [
    "output_json_path",
    "headers",
    "row"
  ],
  "HF_MODEL_ID_TO_FILENAMES": [],
  "build_sam2": [
    "config_file",
    "ckpt_path",
    "device",
    "mode",
    "hydra_overrides_extra",
    "apply_postprocessing"
  ],
  "build_sam2_video_predictor": [
    "config_file",
    "ckpt_path",
    "device",
    "mode",
    "hydra_overrides_extra",
    "apply_postprocessing"
  ],
  "_hf_download": [
    "model_id"
  ],
  "build_sam2_hf": [
    "model_id"
  ],
  "build_sam2_video_predictor_hf": [
    "model_id"
  ],
  "_load_checkpoint": [
    "model",
    "ckpt_path"
  ],
  "SAM2ImagePredictor": {
    "__init__": [
      "self",
      "sam_model",
      "mask_threshold",
      "max_hole_area",
      "max_sprinkle_area"
    ],
    "from_pretrained": [
      "cls",
      "model_id"
    ],
    "set_image": [
      "self",
      "image"
    ],
    "set_image_batch": [
      "self",
      "image_list"
    ],
    "predict_batch": [
      "self",
      "point_coords_batch",
      "point_labels_batch",
      "box_batch",
      "mask_input_batch",
      "multimask_output",
      "return_logits",
      "normalize_coords",
      "return_type"
    ],
    "predict": [
      "self",
      "point_coords",
      "point_labels",
      "box",
      "mask_input",
      "multimask_output",
      "return_logits",
      "normalize_coords",
      "return_type"
    ],
    "_prep_prompts": [
      "self",
      "point_coords",
      "point_labels",
      "box",
      "mask_logits",
      "normalize_coords",
      "img_idx"
    ],
    "_predict": [
      "self",
      "point_coords",
      "point_labels",
      "boxes",
      "mask_input",
      "multimask_output",
      "return_logits",
      "img_idx"
    ],
    "_predict_masks": [
      "self",
      "high_res_feats_input",
      "image_embed",
      "image_pe",
      "point_coords",
      "point_labels",
      "boxes",
      "mask_input",
      "multimask_output"
    ],
    "_predict_masks_postprocess": [
      "self",
      "low_res_masks",
      "img_idx",
      "return_logits",
      "channel_1"
    ],
    "get_image_embedding": [
      "self"
    ],
    "device": [
      "self"
    ],
    "reset_predictor": [
      "self"
    ]
  },
  "MAP_TENSOR_ATEN_OP_TABLE": [],
  "implements": [
    "aten_ops_or_torch_fns"
  ],
  "no_dispatch": [],
  "wrap_dim": [
    "i",
    "dim"
  ],
  "unwrap": [
    "t"
  ],
  "unwrap_i": [
    "t",
    "i"
  ],
  "unwrap_fn": [
    "t",
    "fn"
  ],
  "wrap": [
    "t"
  ],
  "layer_norm_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "add_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "cat_ops_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "select_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "slice_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "reductions_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "view_ops_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "view_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mm_ops_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "unsqueeze_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "squeeze_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "addmm_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "convolution_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "upsample_bilinear2d_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "transpose_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "unbind_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "permute_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "_scaled_dot_product_efficient_attention_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "_scaled_dot_product_flash_attention_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "index_ops_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "dim_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "sym_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "is_contiguous_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "forwardables_impl": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "run_invariant_test": [
    "res",
    "func",
    "args",
    "kwargs"
  ],
  "MapTensor": {
    "__new__": [
      "cls",
      "elems"
    ],
    "__init__": [
      "self",
      "elems"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__repr__": [
      "self"
    ],
    "pin_memory": [
      "self"
    ]
  },
  "to_map_tensor": [
    "ts"
  ],
  "SAM2AutomaticMaskGenerator": {
    "__init__": [
      "self",
      "model",
      "points_per_side",
      "points_per_batch",
      "pred_iou_thresh",
      "stability_score_thresh",
      "stability_score_offset",
      "mask_threshold",
      "box_nms_thresh",
      "crop_n_layers",
      "crop_nms_thresh",
      "crop_overlap_ratio",
      "crop_n_points_downscale_factor",
      "point_grids",
      "min_mask_region_area",
      "output_mode",
      "use_m2m",
      "multimask_output"
    ],
    "from_pretrained": [
      "cls",
      "model_id"
    ],
    "generate": [
      "self",
      "image"
    ],
    "_encode_masks": [
      "self",
      "mask_data"
    ],
    "generate_batch": [
      "self",
      "images"
    ],
    "_generate_masks": [
      "self",
      "image"
    ],
    "_deduplicate_masks": [
      "self",
      "crop_boxes",
      "data"
    ],
    "_generate_masks_batch": [
      "self",
      "images"
    ],
    "_process_crop": [
      "self",
      "image",
      "crop_box",
      "crop_layer_idx",
      "orig_size"
    ],
    "_process_crop_points": [
      "self",
      "cropped_im_size",
      "crop_layer_idx",
      "crop_box",
      "orig_size"
    ],
    "_process_crop_points_dedup": [
      "self",
      "data",
      "crop_box"
    ],
    "_process_crop_batch": [
      "self",
      "images",
      "all_crop_boxes",
      "all_layer_idxs",
      "all_orig_size_compact"
    ],
    "_process_batch_fullgraph": [
      "self",
      "points",
      "im_size",
      "crop_box",
      "crop_box_torch",
      "orig_size",
      "normalize",
      "orig_box_torch"
    ],
    "_process_batch": [
      "self",
      "points",
      "im_size",
      "crop_box",
      "orig_size",
      "normalize"
    ],
    "postprocess_small_regions": [
      "mask_data",
      "min_area",
      "nms_thresh"
    ],
    "refine_with_m2m": [
      "self",
      "points",
      "point_labels",
      "low_res_masks",
      "points_per_batch"
    ]
  },
  "SAM2VideoPredictor": {
    "__init__": [
      "self",
      "fill_hole_area",
      "non_overlap_masks",
      "clear_non_cond_mem_around_input",
      "clear_non_cond_mem_for_multi_obj",
      "add_all_frames_to_correct_as_cond"
    ],
    "batch_inference_states": [
      "inference_states"
    ],
    "init_state": [
      "self",
      "video_path",
      "offload_video_to_cpu",
      "offload_state_to_cpu",
      "async_loading_frames"
    ],
    "from_pretrained": [
      "cls",
      "model_id"
    ],
    "_obj_id_to_idx": [
      "self",
      "inference_state",
      "obj_id"
    ],
    "_obj_idx_to_id": [
      "self",
      "inference_state",
      "obj_idx"
    ],
    "_get_obj_num": [
      "self",
      "inference_state"
    ],
    "add_new_points_or_box": [
      "self",
      "inference_state",
      "frame_idx",
      "obj_id",
      "points",
      "labels",
      "clear_old_points",
      "normalize_coords",
      "box"
    ],
    "add_new_points": [
      "self"
    ],
    "add_new_mask": [
      "self",
      "inference_state",
      "frame_idx",
      "obj_id",
      "mask"
    ],
    "_get_orig_video_res_output": [
      "self",
      "inference_state",
      "any_res_masks"
    ],
    "_consolidate_temp_output_across_obj": [
      "self",
      "inference_state",
      "frame_idx",
      "is_cond",
      "run_mem_encoder",
      "consolidate_at_video_res"
    ],
    "_get_empty_mask_ptr": [
      "self",
      "inference_state",
      "frame_idx"
    ],
    "propagate_in_video_preflight": [
      "self",
      "inference_state"
    ],
    "propagate_in_video": [
      "self",
      "inference_state",
      "start_frame_idx",
      "max_frame_num_to_track",
      "reverse"
    ],
    "_add_output_per_object": [
      "self",
      "inference_state",
      "frame_idx",
      "current_out",
      "storage_key"
    ],
    "clear_all_prompts_in_frame": [
      "self",
      "inference_state",
      "frame_idx",
      "obj_id",
      "need_output"
    ],
    "reset_state": [
      "self",
      "inference_state"
    ],
    "_reset_tracking_results": [
      "self",
      "inference_state"
    ],
    "_get_image_feature": [
      "self",
      "inference_state",
      "frame_idx",
      "batch_size"
    ],
    "_run_single_frame_inference": [
      "self",
      "inference_state",
      "output_dict",
      "frame_idx",
      "batch_size",
      "is_init_cond_frame",
      "point_inputs",
      "mask_inputs",
      "reverse",
      "run_mem_encoder",
      "prev_sam_mask_logits"
    ],
    "_run_memory_encoder": [
      "self",
      "inference_state",
      "frame_idx",
      "batch_size",
      "high_res_masks",
      "object_score_logits",
      "is_mask_from_pts"
    ],
    "_get_maskmem_pos_enc": [
      "self",
      "inference_state",
      "current_out"
    ],
    "remove_object": [
      "self",
      "inference_state",
      "obj_id",
      "strict",
      "need_output"
    ],
    "_clear_non_cond_mem_around_input": [
      "self",
      "inference_state",
      "frame_idx"
    ]
  },
  "PositionEmbeddingSine": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "_encode_xy": [
      "self",
      "x",
      "y"
    ],
    "encode_boxes": [
      "self",
      "x",
      "y",
      "w",
      "h"
    ],
    "encode": [],
    "encode_points": [
      "self",
      "x",
      "y",
      "labels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PositionEmbeddingRandom": {
    "__init__": [
      "self",
      "num_pos_feats",
      "scale"
    ],
    "_pe_encoding": [
      "self",
      "coords"
    ],
    "forward": [
      "self",
      "size"
    ],
    "forward_with_coords": [
      "self",
      "coords_input",
      "image_size"
    ]
  },
  "init_t_xy": [
    "end_x",
    "end_y",
    "device"
  ],
  "compute_axial_cis": [
    "dim",
    "end_x",
    "end_y",
    "theta",
    "device"
  ],
  "reshape_for_broadcast": [
    "freqs_cis",
    "x"
  ],
  "apply_rotary_enc": [
    "xq",
    "xk",
    "freqs_cis",
    "repeat_freqs_k"
  ],
  "select_closest_cond_frames": [
    "frame_idx",
    "cond_frame_outputs",
    "max_cond_frame_num"
  ],
  "get_1d_sine_pe": [
    "pos_inds",
    "dim",
    "temperature"
  ],
  "get_activation_fn": [
    "activation"
  ],
  "get_clones": [
    "module",
    "N"
  ],
  "DropPath": {
    "__init__": [
      "self",
      "drop_prob",
      "scale_by_keep"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "activation",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LayerNorm2d": {
    "__init__": [
      "self",
      "num_channels",
      "eps"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "sample_box_points": [
    "masks",
    "noise",
    "noise_bound",
    "top_left_label",
    "bottom_right_label"
  ],
  "sample_random_points_from_errors": [
    "gt_masks",
    "pred_masks",
    "num_pt"
  ],
  "sample_one_point_from_error_center": [
    "gt_masks",
    "pred_masks",
    "padding"
  ],
  "get_next_point": [
    "gt_masks",
    "pred_masks",
    "method"
  ],
  "MemoryAttentionLayer": {
    "__init__": [
      "self",
      "activation",
      "cross_attention",
      "d_model",
      "dim_feedforward",
      "dropout",
      "pos_enc_at_attn",
      "pos_enc_at_cross_attn_keys",
      "pos_enc_at_cross_attn_queries",
      "self_attention"
    ],
    "_forward_sa": [
      "self",
      "tgt",
      "query_pos"
    ],
    "_forward_ca": [
      "self",
      "tgt",
      "memory",
      "query_pos",
      "pos",
      "num_k_exclude_rope"
    ],
    "forward": [
      "self",
      "tgt",
      "memory",
      "pos",
      "query_pos",
      "num_k_exclude_rope"
    ]
  },
  "MemoryAttention": {
    "__init__": [
      "self",
      "d_model",
      "pos_enc_at_input",
      "layer",
      "num_layers",
      "batch_first"
    ],
    "forward": [
      "self",
      "curr",
      "memory",
      "curr_pos",
      "memory_pos",
      "num_obj_ptr_tokens"
    ]
  },
  "MaskDownSampler": {
    "__init__": [
      "self",
      "embed_dim",
      "kernel_size",
      "stride",
      "padding",
      "total_stride",
      "activation"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CXBlock": {
    "__init__": [
      "self",
      "dim",
      "kernel_size",
      "padding",
      "drop_path",
      "layer_scale_init_value",
      "use_dwconv"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Fuser": {
    "__init__": [
      "self",
      "layer",
      "num_layers",
      "dim",
      "input_projection"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MemoryEncoder": {
    "__init__": [
      "self",
      "out_dim",
      "mask_downsampler",
      "fuser",
      "position_encoding",
      "in_dim"
    ],
    "forward": [
      "self",
      "pix_feat",
      "masks",
      "skip_mask_sigmoid"
    ]
  },
  "NO_OBJ_SCORE": [],
  "SAM2Base": {
    "__init__": [
      "self",
      "image_encoder",
      "memory_attention",
      "memory_encoder",
      "num_maskmem",
      "image_size",
      "backbone_stride",
      "sigmoid_scale_for_mem_enc",
      "sigmoid_bias_for_mem_enc",
      "binarize_mask_from_pts_for_mem_enc",
      "use_mask_input_as_output_without_sam",
      "max_cond_frames_in_attn",
      "directly_add_no_mem_embed",
      "use_high_res_features_in_sam",
      "multimask_output_in_sam",
      "multimask_min_pt_num",
      "multimask_max_pt_num",
      "multimask_output_for_tracking",
      "use_multimask_token_for_obj_ptr",
      "iou_prediction_use_sigmoid",
      "memory_temporal_stride_for_eval",
      "non_overlap_masks_for_mem_enc",
      "use_obj_ptrs_in_encoder",
      "max_obj_ptrs_in_encoder",
      "add_tpos_enc_to_obj_ptrs",
      "proj_tpos_enc_in_obj_ptrs",
      "use_signed_tpos_enc_to_obj_ptrs",
      "only_obj_ptrs_in_the_past_for_eval",
      "pred_obj_scores",
      "pred_obj_scores_mlp",
      "fixed_no_obj_ptr",
      "soft_no_obj_ptr",
      "use_mlp_for_obj_ptr_proj",
      "no_obj_embed_spatial",
      "sam_mask_decoder_extra_args",
      "compile_image_encoder"
    ],
    "device": [
      "self"
    ],
    "forward": [
      "self"
    ],
    "_build_sam_heads": [
      "self"
    ],
    "_forward_sam_heads": [
      "self",
      "backbone_features",
      "point_inputs",
      "mask_inputs",
      "high_res_features",
      "multimask_output"
    ],
    "_use_mask_as_output": [
      "self",
      "backbone_features",
      "high_res_features",
      "mask_inputs"
    ],
    "forward_image": [
      "self",
      "img_batch"
    ],
    "_prepare_backbone_features": [
      "self",
      "backbone_out"
    ],
    "_prepare_memory_conditioned_features": [
      "self",
      "frame_idx",
      "is_init_cond_frame",
      "current_vision_feats",
      "current_vision_pos_embeds",
      "feat_sizes",
      "output_dict",
      "num_frames",
      "track_in_reverse"
    ],
    "_encode_new_memory": [
      "self",
      "current_vision_feats",
      "feat_sizes",
      "pred_masks_high_res",
      "object_score_logits",
      "is_mask_from_pts"
    ],
    "_track_step": [
      "self",
      "frame_idx",
      "is_init_cond_frame",
      "current_vision_feats",
      "current_vision_pos_embeds",
      "feat_sizes",
      "point_inputs",
      "mask_inputs",
      "output_dict",
      "num_frames",
      "track_in_reverse",
      "prev_sam_mask_logits"
    ],
    "_encode_memory_in_output": [
      "self",
      "current_vision_feats",
      "feat_sizes",
      "point_inputs",
      "run_mem_encoder",
      "high_res_masks",
      "object_score_logits",
      "current_out"
    ],
    "track_step": [
      "self",
      "frame_idx",
      "is_init_cond_frame",
      "current_vision_feats",
      "current_vision_pos_embeds",
      "feat_sizes",
      "point_inputs",
      "mask_inputs",
      "output_dict",
      "num_frames",
      "track_in_reverse",
      "run_mem_encoder",
      "prev_sam_mask_logits"
    ],
    "_use_multimask": [
      "self",
      "is_init_cond_frame",
      "point_inputs"
    ],
    "_apply_non_overlapping_constraints": [
      "self",
      "pred_masks"
    ]
  },
  "window_partition": [
    "x",
    "window_size"
  ],
  "window_unpartition": [
    "windows",
    "window_size",
    "pad_hw",
    "hw"
  ],
  "PatchEmbed": {
    "__init__": [
      "self",
      "kernel_size",
      "stride",
      "padding",
      "in_chans",
      "embed_dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "do_pool": [
    "x",
    "pool",
    "norm"
  ],
  "MultiScaleAttention": {
    "__init__": [
      "self",
      "dim",
      "dim_out",
      "num_heads",
      "q_pool"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MultiScaleBlock": {
    "__init__": [
      "self",
      "dim",
      "dim_out",
      "num_heads",
      "mlp_ratio",
      "drop_path",
      "norm_layer",
      "q_stride",
      "act_layer",
      "window_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Hiera": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "drop_path_rate",
      "q_pool",
      "q_stride",
      "stages",
      "dim_mul",
      "head_mul",
      "window_pos_embed_bkg_spatial_size",
      "window_spec",
      "global_att_blocks",
      "weights_path",
      "return_interm_layers"
    ],
    "_get_pos_embed": [
      "self",
      "hw"
    ],
    "forward": [
      "self",
      "x"
    ],
    "get_layer_id": [
      "self",
      "layer_name"
    ],
    "get_num_layers": [
      "self"
    ]
  },
  "ImageEncoder": {
    "__init__": [
      "self",
      "trunk",
      "neck",
      "scalp"
    ],
    "forward": [
      "self",
      "sample"
    ]
  },
  "FpnNeck": {
    "__init__": [
      "self",
      "position_encoding",
      "d_model",
      "backbone_channel_list",
      "kernel_size",
      "stride",
      "padding",
      "fpn_interp_model",
      "fuse_type",
      "fpn_top_down_levels"
    ],
    "forward": [
      "self",
      "xs"
    ]
  },
  "PromptEncoder": {
    "__init__": [
      "self",
      "embed_dim",
      "image_embedding_size",
      "input_image_size",
      "mask_in_chans",
      "activation"
    ],
    "get_dense_pe": [
      "self"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ],
    "_embed_masks": [
      "self",
      "masks"
    ],
    "_get_batch_size": [
      "self",
      "points",
      "boxes",
      "masks"
    ],
    "_get_device": [
      "self"
    ],
    "forward": [
      "self",
      "points",
      "boxes",
      "masks"
    ]
  },
  "MaskDecoder": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_pe",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "repeat_image",
      "high_res_features"
    ],
    "predict_masks": [
      "self",
      "image_embeddings",
      "image_pe",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "repeat_image",
      "high_res_features"
    ],
    "_get_stability_scores": [
      "self",
      "mask_logits"
    ],
    "_dynamic_multimask_via_stability": [
      "self",
      "all_mask_logits",
      "all_iou_scores"
    ]
  },
  "ALLOW_ALL_KERNELS": [],
  "sdp_kernel_context": [
    "dropout_p"
  ],
  "TwoWayTransformer": {
    "__init__": [
      "self",
      "depth",
      "embedding_dim",
      "num_heads",
      "mlp_dim",
      "activation",
      "attention_downsample_rate"
    ],
    "forward": [
      "self",
      "image_embedding",
      "image_pe",
      "point_embedding"
    ]
  },
  "TwoWayAttentionBlock": {
    "__init__": [
      "self",
      "embedding_dim",
      "num_heads",
      "mlp_dim",
      "activation",
      "attention_downsample_rate",
      "skip_first_layer_pe"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "query_pe",
      "key_pe"
    ]
  },
  "Attention": {
    "__init__": [
      "self",
      "embedding_dim",
      "num_heads",
      "downsample_rate",
      "dropout",
      "kv_in_dim"
    ],
    "_separate_heads": [
      "self",
      "x",
      "num_heads"
    ],
    "_recombine_heads": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "q",
      "k",
      "v"
    ]
  },
  "RoPEAttention": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "q",
      "k",
      "v",
      "num_k_exclude_rope"
    ]
  },
  "SAM2Transforms": {
    "__init__": [
      "self",
      "resolution",
      "mask_threshold",
      "max_hole_area",
      "max_sprinkle_area"
    ],
    "__call__": [
      "self",
      "x"
    ],
    "forward_batch": [
      "self",
      "img_list"
    ],
    "transform_coords": [
      "self",
      "coords",
      "normalize",
      "orig_hw"
    ],
    "transform_boxes": [
      "self",
      "boxes",
      "normalize",
      "orig_hw"
    ],
    "postprocess_masks": [
      "self",
      "masks",
      "orig_hw",
      "output_dtype"
    ],
    "postprocess_masks_1_channel": [
      "self",
      "masks",
      "orig_hw",
      "output_dtype"
    ]
  },
  "get_sdpa_settings": [],
  "get_connected_components": [
    "mask"
  ],
  "mask_to_box": [
    "masks"
  ],
  "_load_img_as_tensor": [
    "img_path",
    "image_size"
  ],
  "AsyncVideoFrameLoader": {
    "__init__": [
      "self",
      "img_paths",
      "image_size",
      "offload_video_to_cpu",
      "img_mean",
      "img_std",
      "compute_device"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "__len__": [
      "self"
    ]
  },
  "load_video_frames": [
    "video_path",
    "image_size",
    "offload_video_to_cpu",
    "img_mean",
    "img_std",
    "async_loading_frames",
    "compute_device"
  ],
  "load_video_frames_from_jpg_images": [
    "video_path",
    "image_size",
    "offload_video_to_cpu",
    "img_mean",
    "img_std",
    "async_loading_frames",
    "compute_device"
  ],
  "load_video_frames_from_video_file": [
    "video_path",
    "image_size",
    "offload_video_to_cpu",
    "img_mean",
    "img_std",
    "compute_device"
  ],
  "fill_holes_in_mask_scores": [
    "mask",
    "max_area"
  ],
  "concat_points": [
    "old_point_inputs",
    "new_points",
    "new_labels"
  ],
  "get_image_size": [
    "image"
  ],
  "crop_image": [
    "image",
    "crop_box"
  ],
  "RLEData": {
    "__len__": [
      "self"
    ]
  },
  "nt_index_select_dim_0": [
    "nt",
    "index"
  ],
  "nt_cat_dim_0": [
    "nts"
  ],
  "MaskData": {
    "__init__": [
      "self"
    ],
    "__setitem__": [
      "self",
      "key",
      "item"
    ],
    "__delitem__": [
      "self",
      "key"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "items": [
      "self"
    ],
    "filter": [
      "self",
      "keep"
    ],
    "cat": [
      "self",
      "new_stats"
    ],
    "to_numpy": [
      "self"
    ]
  },
  "is_box_near_crop_edge_torch": [
    "boxes",
    "crop_box",
    "crop_box_torch",
    "orig_box_torch",
    "atol"
  ],
  "is_box_near_crop_edge": [
    "boxes",
    "crop_box",
    "orig_box",
    "atol"
  ],
  "box_xyxy_to_xywh": [
    "box_xyxy"
  ],
  "batch_iterator": [
    "batch_size"
  ],
  "mask_to_rle_pytorch": [
    "tensor"
  ],
  "rle_to_mask": [
    "rle"
  ],
  "_mask_to_rle_pytorch_2_0_0": [
    "tensor"
  ],
  "_mask_to_rle_pytorch_2_0_1": [
    "tensor",
    "diff",
    "change_indices"
  ],
  "_mask_to_rle_pytorch_2_0": [
    "tensor"
  ],
  "_mask_to_rle_pytorch_2_1": [
    "rle_data"
  ],
  "mask_to_rle_pytorch_2": [
    "tensor"
  ],
  "area_from_rle": [
    "rle"
  ],
  "calculate_stability_score": [
    "masks",
    "mask_threshold",
    "threshold_offset"
  ],
  "build_point_grid": [
    "n_per_side"
  ],
  "build_all_layer_point_grids": [
    "n_per_side",
    "n_layers",
    "scale_per_layer"
  ],
  "generate_crop_boxes": [
    "im_size",
    "n_layers",
    "overlap_ratio"
  ],
  "uncrop_boxes_xyxy": [
    "boxes",
    "crop_box"
  ],
  "uncrop_points": [
    "points",
    "crop_box"
  ],
  "uncrop_masks": [
    "masks",
    "crop_box",
    "orig_h",
    "orig_w"
  ],
  "remove_small_regions": [
    "mask",
    "area_thresh",
    "mode"
  ],
  "coco_encode_rle": [
    "uncompressed_rle"
  ],
  "batched_mask_to_box": [
    "masks"
  ],
  "ToySingleLinearModel": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "dtype",
      "device",
      "has_bias"
    ],
    "example_inputs": [
      "self",
      "batch_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ToyTwoLinearModel": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "dtype",
      "device",
      "has_bias"
    ],
    "example_inputs": [
      "self",
      "batch_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConvWithSharedWeightInExportedModel": {
    "__init__": [
      "self",
      "n_chunks",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "bias"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LNLinearActivationModel": {
    "__init__": [
      "self",
      "fc_dim1",
      "fc_dim2",
      "dtype",
      "activation"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "RMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "TransformerBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "mlp_ratio",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "create_model_and_input_data": [
    "model_type",
    "m",
    "k",
    "n",
    "high_precision_dtype",
    "device",
    "activation"
  ],
  "LlamaModelsLlama4Experts": {
    "__init__": [
      "self",
      "num_local_experts",
      "dim",
      "hidden_dim",
      "dtype",
      "device"
    ],
    "forward": [
      "self",
      "routed_in_egD"
    ]
  },
  "skip_if_compute_capability_less_than": [
    "min_capability"
  ],
  "skip_if_rocm": [
    "message"
  ],
  "skip_if_no_xpu": [],
  "skip_if_xpu": [
    "message"
  ],
  "skip_if_no_cuda": [],
  "skip_if_no_gemlite": [],
  "copy_tests": [
    "my_cls",
    "other_cls",
    "suffix",
    "test_failures",
    "xfail_prop"
  ],
  "TorchAOBasicTestCase": {
    "COMMON_DEVICES": [],
    "COMMON_DTYPES": [],
    "TENSOR_SUBCLASS": [],
    "FACTORY_FN": [],
    "kwargs": [],
    "LINEAR_MIN_SQNR": [],
    "test_flatten_unflatten": [
      "self"
    ],
    "test_hp_tensor_device_dtype": [
      "self",
      "device",
      "dtype"
    ],
    "test_device1_to_device2": [
      "self",
      "device1",
      "device2"
    ],
    "test_transpose": [
      "self",
      "device",
      "dtype"
    ],
    "test_linear": [
      "self",
      "device",
      "dtype"
    ]
  },
  "TorchAOCompileTestCase": {
    "COMMON_DEVICES": [],
    "COMMON_DTYPES": [],
    "TENSOR_SUBCLASS": [],
    "FACTORY_FN": [],
    "kwargs": [],
    "LINEAR_MIN_SQNR": [],
    "COMPILE_MIN_SQNR": [],
    "test_input_output_tensor_subclass": [
      "self",
      "device",
      "dtype"
    ],
    "test_input_tensor_subclass": [
      "self",
      "device",
      "dtype"
    ],
    "test_output_tensor_subclass": [
      "self",
      "device",
      "dtype"
    ],
    "test_linear_compile": [
      "self",
      "device",
      "dtype"
    ]
  },
  "TorchAOTensorParallelTestCase": {
    "COMMON_DTYPES": [],
    "TENSOR_SUBCLASS": [],
    "QUANT_METHOD_FN": [],
    "QUANT_METHOD_KWARGS": [],
    "colwise_shard": [
      "m",
      "mesh"
    ],
    "rowwise_shard": [
      "m",
      "mesh"
    ],
    "quantize": [
      "self",
      "m"
    ],
    "test_tp": [
      "self",
      "dtype"
    ]
  },
  "TorchAOIntegrationTestCase": {
    "_test_slice_and_copy_similar_to_vllm": [
      "self",
      "config"
    ],
    "_test_moe_weight_reshape_ops": [
      "self",
      "config"
    ],
    "_test_narrow_similar_to_vllm": [
      "self",
      "config"
    ],
    "_test_quantize_3d_param_similar_to_vllm": [
      "self",
      "config"
    ],
    "_test_chunk_similar_to_vllm_llama4": [
      "self",
      "ao_tensor",
      "dim"
    ]
  },
  "check_parity_no_mp": [
    "test_cls",
    "ref_model",
    "ref_optim",
    "fsdp_model",
    "fsdp_optim",
    "local_inp",
    "config",
    "precompute",
    "compile_transformer_block"
  ],
  "check_parity_bf16_mp": [
    "test_cls",
    "ref_model",
    "ref_model_bf16",
    "ref_optim",
    "fsdp_model",
    "fsdp_optim",
    "local_inp"
  ],
  "get_test_float8_linear_config": [
    "scaling_type_input",
    "scaling_type_weight",
    "scaling_type_grad_output",
    "emulate"
  ],
  "BYTES_PER_EL_FLOAT4": [],
  "BYTES_PER_EL_FLOAT8": [],
  "BYTES_PER_EL_BF16": [],
  "BYTES_PER_EL_FLOAT32": [],
  "gpu_name_to_specs": [],
  "get_specs": [
    "gpu_name"
  ],
  "KERNEL_LAUNCH_OVERHEAD_SEC": [],
  "get_tensor_memory_traffic_ovhd_s": [
    "specs",
    "dim0",
    "dim1",
    "tensor_role",
    "float8_recipe_name",
    "mx_recipe_name",
    "fuse_with_prev"
  ],
  "get_individual_gemm_time_sympy": [
    "M",
    "K",
    "N",
    "dtype",
    "mx_recipe_name",
    "gpu_name"
  ],
  "get_gemm_time_sympy": [
    "M",
    "K",
    "N",
    "dtype",
    "float8_recipe_name",
    "mx_recipe_name",
    "gpu_name"
  ],
  "get_float8_mem_sympy": [
    "M",
    "K",
    "N",
    "float8_recipe_name",
    "mx_recipe_name",
    "enable_fusion_modeling",
    "gpu_name"
  ],
  "get_inference_tensor_memory_traffic_ovhd_s": [
    "specs",
    "dim0",
    "dim1",
    "tensor_role",
    "recipe_name",
    "fuse_with_prev"
  ],
  "get_inference_float8_mem_sympy": [
    "M",
    "K",
    "N",
    "recipe_name",
    "gpu_name"
  ],
  "get_inference_gemm_time_sympy": [
    "M",
    "K",
    "N",
    "dtype",
    "recipe_name",
    "gpu_name"
  ],
  "FeedForward": {
    "__init__": [
      "self",
      "size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ToyModel": {
    "__init__": [
      "self",
      "size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_test_lowp_mlp_tensor_parallelism_base": [
    "mesh",
    "config",
    "size",
    "compile",
    "allgather_in_lowp"
  ],
  "_get_dynamo_graph": [
    "function",
    "inputs"
  ],
  "_get_linear_patterns": [
    "input_size"
  ],
  "_supported_symmetric_quantized_operators": [],
  "_get_supported_symmetric_config_and_operators": [],
  "get_symmetric_quantization_config": [
    "is_per_channel",
    "is_qat",
    "is_dynamic",
    "act_qmin",
    "act_qmax",
    "weight_qmin",
    "weight_qmax"
  ],
  "_get_supported_config_and_operators": [],
  "_get_module_type_filter": [
    "tp"
  ],
  "_get_not_module_type_or_name_filter": [
    "tp_list",
    "module_name_list"
  ],
  "XNNPACKQuantizer": {
    "supported_config_and_operators": [],
    "STATIC_QAT_ONLY_OPS": [],
    "STATIC_OPS": [],
    "DYNAMIC_OPS": [],
    "__init__": [
      "self"
    ],
    "get_supported_quantization_configs": [
      "cls"
    ],
    "get_supported_operator_for_quantization_config": [
      "cls",
      "quantization_config"
    ],
    "set_global": [
      "self",
      "quantization_config"
    ],
    "set_operator_type": [
      "self",
      "operator_type",
      "quantization_config"
    ],
    "set_module_type": [
      "self",
      "module_type",
      "quantization_config"
    ],
    "set_module_name": [
      "self",
      "module_name",
      "quantization_config"
    ],
    "transform_for_annotation": [
      "self",
      "model"
    ],
    "annotate": [
      "self",
      "model"
    ],
    "_annotate_all_static_patterns": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_all_dynamic_patterns": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_for_static_quantization_config": [
      "self",
      "model"
    ],
    "_annotate_for_dynamic_quantization_config": [
      "self",
      "model"
    ],
    "validate": [
      "self",
      "model"
    ],
    "get_supported_operators": [
      "cls"
    ]
  },
  "PT2EQuantizationTestCase": {
    "_MAP_TO_FX_TRACED_OPS": [],
    "_test_quantizer": [
      "self",
      "model",
      "example_inputs",
      "quantizer",
      "expected_node_occurrence",
      "expected_node_list",
      "check_against_fx_quant",
      "fx_qconfig_mapping",
      "export_with_dynamic_shape",
      "is_qat",
      "is_debug_mode",
      "training_ir_node_occurrence"
    ]
  },
  "PT2ENumericDebuggerTestCase": {
    "_assert_each_node_has_from_node_source": [
      "self",
      "model"
    ],
    "_extract_from_node_source": [
      "self",
      "model"
    ],
    "_extract_from_node_source_with_prev_decomp_op": [
      "self",
      "model"
    ],
    "assertNodeSourcesEqual": [
      "self",
      "node_source_1",
      "node_source_2"
    ]
  },
  "QuantizationConfig": {},
  "OperatorPatternType": [],
  "AnnotatorType": [],
  "register_annotator": [
    "op"
  ],
  "OperatorConfig": {},
  "_is_annotated": [
    "nodes"
  ],
  "_mark_nodes_as_annotated": [
    "nodes"
  ],
  "get_input_act_qspec": [
    "quantization_config"
  ],
  "get_output_act_qspec": [
    "quantization_config"
  ],
  "get_weight_qspec": [
    "quantization_config"
  ],
  "get_bias_qspec": [
    "quantization_config"
  ],
  "_annotate_linear": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_linear_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_do_annotate_conv_relu": [
    "gm",
    "quantization_config",
    "filter_fn",
    "is_conv_transpose"
  ],
  "_annotate_conv_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_transpose_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_bn": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_bn_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_transpose_bn": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_transpose_bn_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_do_annotate_conv_bn": [
    "gm",
    "quantization_config",
    "filter_fn",
    "has_relu",
    "is_conv_transpose"
  ],
  "_annotate_gru_io_only": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_adaptive_avg_pool2d": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_is_input_large_scalar": [
    "node",
    "gm"
  ],
  "_is_input_non_float_tensor": [
    "node"
  ],
  "_annotate_add_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_add": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_mul_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_mul": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_cat": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_is_share_obs_or_fq_op": [
    "op"
  ],
  "propagate_annotation": [
    "model"
  ],
  "_convert_scalars_to_attrs": [
    "model"
  ],
  "aten": [],
  "ScaledMMConfig": {},
  "LinearMMConfig": {},
  "GemmInputRole": {
    "INPUT": [],
    "WEIGHT": [],
    "GRAD_OUTPUT": []
  },
  "choose_scaled_mm_config": [
    "a_role",
    "a_linear_mm_config",
    "b_role",
    "b_linear_mm_config"
  ],
  "_ToFloat8ConstrFunc": {
    "forward": [
      "ctx",
      "tensor",
      "scale",
      "float8_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "axiswise_dim"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "_FromFloat8ConstrFunc": {
    "forward": [
      "ctx",
      "tensor"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "hp_tensor_and_scale_to_float8": [
    "hp_tensor",
    "s",
    "float8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "axiswise_dim"
  ],
  "Float8TrainingTensor": {
    "__slots__": [],
    "__new__": [
      "cls",
      "data",
      "scale",
      "orig_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "axiswise_dim"
    ],
    "__repr__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "metadata",
      "outer_size",
      "outer_stride"
    ],
    "to_original_precision": [
      "self"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": []
  },
  "_float8_linear_supports_float8_allgather": [
    "m"
  ],
  "Float8ColwiseParallel": {
    "_prepare_input_fn": [
      "input_layouts",
      "desired_input_layouts",
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "output_layouts",
      "use_local_output",
      "mod",
      "outputs",
      "device_mesh"
    ],
    "_apply": [
      "self",
      "module",
      "device_mesh"
    ]
  },
  "Float8RowwiseParallel": {
    "_prepare_input_fn": [
      "input_layouts",
      "desired_input_layouts",
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "output_layouts",
      "use_local_output",
      "mod",
      "outputs",
      "device_mesh"
    ],
    "_apply": [
      "self",
      "module",
      "device_mesh"
    ]
  },
  "PrepareFloat8ModuleInput": {
    "__init__": [
      "self"
    ],
    "_prepare_input_arg": [
      "self",
      "input",
      "mesh",
      "input_layout",
      "desired_layout"
    ],
    "_apply": [
      "self",
      "module",
      "device_mesh"
    ]
  },
  "c10d_functional": [],
  "_c10d_functional": [],
  "addmm_float8_unwrapped": [
    "a_data",
    "a_scale",
    "b_data",
    "b_scale",
    "output_dtype",
    "output_scale",
    "bias",
    "use_fast_accum"
  ],
  "_assert_tensorwise_scale": [
    "aten_op",
    "scale"
  ],
  "float8_desugar_op": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_desugar_data_and_scale_op": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_transpose": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_view": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_split": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_cat": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_cast_up_op": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "preprocess_addmm": [
    "a",
    "b"
  ],
  "float8_mm": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_addmm": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "float8_is_same_size": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "autocast_to_copy": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "allgather_fp8": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "wait_tensor_fp8": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "index_put_fp8": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "copy_fp8": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "precompute_float8_dynamic_scale_for_fsdp": [
    "module"
  ],
  "_ops_to_preserve_subclass": [],
  "WeightWithDynamicFloat8CastTensor": {
    "__new__": [
      "cls",
      "tensor",
      "linear_mm_config",
      "dtype",
      "precomputed_scale"
    ],
    "__init__": [
      "self",
      "tensor",
      "linear_mm_config",
      "dtype",
      "precomputed_scale"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "flatten_spec",
      "outer_size",
      "outer_stride"
    ],
    "__repr__": [
      "self"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ]
  },
  "matmul_with_hp_or_float8_args": {
    "forward": [
      "ctx",
      "input_hp",
      "weight_hp_t",
      "linear_mm_config",
      "config"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "Float8Linear": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ],
    "extra_repr": [
      "self"
    ],
    "from_float": [
      "cls",
      "mod",
      "config"
    ]
  },
  "ScalingType": {
    "DYNAMIC": [],
    "DISABLED": [],
    "short_str": [
      "self"
    ]
  },
  "ScalingGranularity": {
    "TENSORWISE": [],
    "AXISWISE": [],
    "short_str": [
      "self"
    ]
  },
  "Float8TypeConfig": {
    "e4m3_dtype": [],
    "e5m2_dtype": [],
    "__post_init__": [
      "self"
    ]
  },
  "type_config": [],
  "e4m3_dtype": [],
  "e5m2_dtype": [],
  "CastConfig": {
    "short_str": [
      "self"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "Float8GemmConfig": {},
  "Float8LinearRecipeName": {
    "TENSORWISE": [],
    "ROWWISE": [],
    "ROWWISE_WITH_GW_HP": []
  },
  "Float8LinearConfig": {
    "__post_init__": [
      "self"
    ],
    "from_recipe_name": [
      "recipe_name"
    ]
  },
  "log": [],
  "swap_linear_layers": [
    "module",
    "from_float_func"
  ],
  "convert_to_float8_training": [
    "module"
  ],
  "_auto_filter_for_recipe": [
    "recipe",
    "filter_fqns"
  ],
  "_auto_filter_for_rowwise": [
    "mod",
    "fqn",
    "filter_fqns"
  ],
  "_auto_filter_for_tensorwise": [
    "mod",
    "fqn",
    "filter_fqns"
  ],
  "FP8Granularity": [],
  "tensor_already_casted_to_fp8": [
    "tensor"
  ],
  "hp_tensor_to_float8_dynamic": [
    "hp_tensor",
    "float8_dtype",
    "linear_mm_config",
    "reduce_amax",
    "gemm_input_role",
    "device_mesh",
    "scaling_granularity",
    "axiswise_dim",
    "round_scales_to_power_of_2"
  ],
  "get_maybe_axiswise_dim": [
    "axiswise_dim",
    "scaling_granularity"
  ],
  "NoopFwToFloat8BwDynamic": {
    "forward": [
      "ctx",
      "tensor",
      "linear_mm_config",
      "target_dtype"
    ],
    "backward": [
      "ctx",
      "gradY"
    ]
  },
  "Tensor": [],
  "Float8MMConfig": {},
  "preprocess_data": [
    "a_data",
    "b_data",
    "scaled_mm_config"
  ],
  "preprocess_scale": [
    "input_scale",
    "input_shape"
  ],
  "addmm_float8_unwrapped_inference": [
    "a_data",
    "a_scale",
    "b_data",
    "b_scale",
    "output_dtype",
    "output_scale",
    "bias",
    "use_fast_accum"
  ],
  "_slice_scale_for_dimension": [
    "scale",
    "data_shape",
    "dim",
    "start",
    "end",
    "step"
  ],
  "_is_rowwise_scaled": [
    "x"
  ],
  "_is_tensorwise_scaled": [
    "x"
  ],
  "_is_1_128_scaled": [
    "x"
  ],
  "_is_128_128_scaled": [
    "x"
  ],
  "_granularity_is_a_1_128_w_128_128": [
    "g"
  ],
  "_normalize_granularity": [
    "granularity"
  ],
  "_check_hardware_support": [
    "granularities"
  ],
  "EPS": [],
  "IS_ROCM": [],
  "FP8_TYPES": [],
  "amax_to_scale": [
    "amax",
    "float8_dtype",
    "round_scales_to_power_of_2"
  ],
  "tensor_to_amax": [
    "x",
    "reduce_amax",
    "device_mesh",
    "scaling_granularity",
    "axiswise_dim"
  ],
  "tensor_to_scale": [
    "hp_tensor",
    "float8_dtype",
    "reduce_amax",
    "device_mesh",
    "scaling_granularity",
    "axiswise_dim",
    "round_scales_to_power_of_2"
  ],
  "to_fp8_saturated": [
    "x",
    "float8_dtype"
  ],
  "compute_error": [
    "x",
    "y"
  ],
  "fp8_tensor_statistics": [
    "tensor",
    "float8_dtype"
  ],
  "is_row_major": [
    "stride"
  ],
  "_get_min_alignment": [
    "size",
    "alignment_value"
  ],
  "pad_tensor_for_matmul": [
    "tensor",
    "dims"
  ],
  "_round_scale_down_to_power_of_2": [
    "scale"
  ],
  "handler": [],
  "formatter": [],
  "UIntxChooseQParamsAlgorithm": {
    "MIN_MAX": [],
    "HQQ": []
  },
  "_quantize": [
    "vals",
    "group_size",
    "nbit",
    "has_weight_zeros",
    "signed"
  ],
  "UIntxWeightOnlyQuantizedLinear": {
    "__init__": [
      "self",
      "pack_weight_op",
      "linear_op",
      "bias"
    ],
    "quantize_and_pack_weights": [
      "self",
      "weights",
      "nbit",
      "group_size",
      "uintx_choose_qparams_algorithm"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_replace_linear_with_quantized_linear_mps": [
    "module",
    "kwargs"
  ],
  "UIntxWeightOnlyLinearQuantizer": {
    "__init__": [
      "self"
    ],
    "quantize": [
      "self",
      "model"
    ]
  },
  "UIntxWeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_uintx_weight_only_mps_transform": [
    "module",
    "config"
  ],
  "_get_torchao_mps_lib_path": [],
  "_load_torchao_mps_lib": [],
  "_n_ones": [
    "n"
  ],
  "F32_EXP_BIAS": [],
  "_f32_to_floatx_unpacked": [
    "x",
    "ebits",
    "mbits"
  ],
  "_floatx_unpacked_to_f32": [
    "x",
    "ebits",
    "mbits"
  ],
  "NF4WeightOnlyConfig": {},
  "nf4_weight_only": [],
  "_nf4_weight_only_transform": [
    "module",
    "config"
  ],
  "get_act_scale": [
    "x"
  ],
  "AWQObserver": {
    "__init__": [
      "self",
      "weight",
      "bias",
      "base_config",
      "scale_search_space_size"
    ],
    "forward": [
      "self",
      "input",
      "output"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "AWQObservedLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "act_obs",
      "bias",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "input"
    ],
    "from_float": [
      "cls",
      "float_linear",
      "act_obs"
    ]
  },
  "AWQConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_awq_transform": [
    "module",
    "config"
  ],
  "get_calib_dataset": [
    "tokenizer",
    "n_samples",
    "block_size"
  ],
  "wiki2_eval": [
    "model",
    "tokenizer",
    "sequence_length",
    "stride",
    "verbose",
    "device"
  ],
  "benchmark": [
    "model",
    "tokenizer",
    "max_length",
    "tasks",
    "evaluation_limit",
    "device"
  ],
  "quantize_and_eval": [
    "repo_id",
    "quant",
    "tasks",
    "max_seq_length",
    "calibration_limit",
    "evaluation_limit",
    "device",
    "precision",
    "compile",
    "model_save_path",
    "model_save_hf_hub_path"
  ],
  "PROFILE_DIR": [],
  "simple_bench": [
    "fn"
  ],
  "check": [
    "expected",
    "actual",
    "atol"
  ],
  "benchmark_mm": [
    "test_fn",
    "xs",
    "weight",
    "ref_fn",
    "headers"
  ],
  "run_bench": [
    "xs",
    "weight"
  ],
  "CudaProfilerCtx": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "exc_traceback"
    ],
    "step": [
      "self"
    ]
  },
  "trace_handler": [
    "prof",
    "group_by_stack",
    "group_by_input_shapes",
    "prefix",
    "out_dir",
    "export_events",
    "export_trace",
    "export_memory_timeline"
  ],
  "get_torch_profiler": [
    "name",
    "with_stack",
    "with_flops",
    "with_modules",
    "record_shapes",
    "export_events",
    "export_trace",
    "export_memory_timeline",
    "out_dir",
    "warmup",
    "active"
  ],
  "TorchProfilerCtx": {
    "profiler": [
      "name",
      "out_dir",
      "warmup",
      "active",
      "record_shapes",
      "with_stack",
      "export_events",
      "export_trace",
      "export_memory_timeline"
    ]
  },
  "get_annotation_ctx": [
    "profiler_type"
  ],
  "_PERF_COLUMNS": [],
  "PERF_COLS_SELECT": [],
  "is_function": [
    "obj"
  ],
  "is_method": [
    "obj"
  ],
  "is_private": [
    "prop"
  ],
  "should_exclude": [
    "obj",
    "prop"
  ],
  "_get_event_props": [
    "event"
  ],
  "get_events_df": [
    "events"
  ],
  "get_perf_df": [
    "events",
    "sort"
  ],
  "pivot_df": [
    "df",
    "id_cols",
    "columns",
    "values",
    "column_order",
    "show"
  ],
  "get_clock_rate_in_khz": [],
  "get_tensorcore_tflops": [
    "device",
    "num_ctas",
    "num_warps",
    "dtype"
  ],
  "get_simd_tflops": [
    "device",
    "num_ctas",
    "num_warps",
    "dtype"
  ],
  "get_tflops": [
    "device",
    "num_ctas",
    "num_warps",
    "dtype"
  ],
  "estimate_matmul_time": [
    "num_warps",
    "num_stages",
    "A",
    "B",
    "C",
    "M",
    "N",
    "K",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "SPLIT_K",
    "debug"
  ],
  "early_config_prune": [
    "configs",
    "named_args"
  ],
  "_ordered_datatypes": [],
  "upcast_if_fp8": [
    "a"
  ],
  "get_higher_dtype": [
    "a",
    "b"
  ],
  "init_to_zero": [
    "name"
  ],
  "get_configs_io_bound": [],
  "_kernel": [
    "A",
    "B",
    "C",
    "M",
    "N",
    "K",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "acc_dtype",
    "input_precision",
    "fp8_fast_accum",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "GROUP_M",
    "SPLIT_K",
    "EVEN_K",
    "AB_DTYPE"
  ],
  "_matmul": {
    "kernel": [],
    "_locks": [],
    "_call": [
      "a",
      "b",
      "acc_dtype",
      "input_precision",
      "fp8_fast_accum",
      "output_dtype"
    ],
    "forward": [
      "ctx",
      "a",
      "b",
      "acc_dtype",
      "input_precision",
      "fp8_fast_accum",
      "output_dtype"
    ]
  },
  "matmul": [],
  "Int8MixedPrecisionTrainingConfig": {},
  "int8_mixed_precision_training": [],
  "_DEFAULT_CONFIG": [],
  "Int8MixedPrecisionTrainingLinearWeight": {
    "__new__": [
      "cls",
      "data",
      "config"
    ],
    "__init__": [
      "self",
      "data",
      "config"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "__repr__": [
      "self"
    ],
    "to_original": [
      "self"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh",
      "outer_size",
      "outer_stride",
      "module",
      "mp_policy"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ]
  },
  "Int8MixedPrecisionTrainingLinear": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "_dynamic_int8_mm": [
    "A",
    "B"
  ],
  "_Int8MixedPrecisionTrainingLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "weight",
      "bias",
      "config"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_int8_mixed_precision_training_transform": [
    "module",
    "config"
  ],
  "quantize_int8_rowwise": [
    "tensor",
    "stochastic_rounding",
    "eps"
  ],
  "Int8QuantizedTrainingLinearWeight": {
    "__new__": [
      "cls",
      "int_data",
      "scale"
    ],
    "__init__": [
      "self",
      "int_data",
      "scale"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_float": [
      "cls",
      "tensor"
    ],
    "dequantize": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh",
      "outer_size",
      "outer_stride",
      "module",
      "mp_policy"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ]
  },
  "_Int8WeightOnlyLinear": {
    "forward": [
      "ctx",
      "input",
      "weight",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "implements_torch_function": [],
  "Int8WeightOnlyQuantizedTrainingConfig": {},
  "int8_weight_only_quantized_training": [],
  "_int8_weight_only_quantized_training_transform": [
    "module",
    "config"
  ],
  "configs": [],
  "_scaled_int8_mm_kernel": [
    "A_ptr",
    "B_ptr",
    "C_ptr",
    "row_scale_ptr",
    "col_scale_ptr",
    "M",
    "N",
    "K",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "GROUP_M",
    "EVEN_K",
    "COL_SCALE_SCALAR"
  ],
  "scaled_int8_mm": [
    "A",
    "B",
    "row_scale",
    "col_scale"
  ],
  "scaled_int8_mm_cuda": [
    "A",
    "B",
    "row_scale",
    "col_scale"
  ],
  "BitNetTrainingLinearWeight": {
    "__new__": [
      "cls",
      "data",
      "precomputed_scale"
    ],
    "__init__": [
      "self",
      "data",
      "precomputed_scale"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "__repr__": [
      "self"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ]
  },
  "get_bitnet_scale": [
    "x"
  ],
  "quantize_bitnet_weight": [
    "w",
    "scale",
    "eps"
  ],
  "precompute_bitnet_scale_for_fsdp": [
    "module"
  ],
  "_BitNetTrainingLinear": {
    "forward": [
      "ctx",
      "input",
      "weight",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "BitNetTrainingConfig": {},
  "bitnet_training": [],
  "_bitnet_training_transform": [
    "module",
    "config"
  ],
  "_pack_i2_in_i8": [
    "x"
  ],
  "_unpack_i2_in_i8": [
    "x"
  ],
  "BitNetPacked2bitLinearWeight": {
    "__new__": [
      "cls",
      "int_data",
      "scale"
    ],
    "__init__": [
      "self",
      "int_data",
      "scale"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "__repr__": [
      "self"
    ],
    "dequantize": [
      "self",
      "out_dtype"
    ]
  },
  "_BitNetPacked2bitLinear": {
    "forward": [
      "ctx",
      "input",
      "weight",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "E4M3_EPS": [],
  "QuantizeTensorToNVFP4Kwargs": {},
  "NVFP4Tensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_data_names": [],
    "optional_tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "block_size",
      "orig_dtype",
      "per_tensor_scale",
      "act_per_tensor_scale",
      "is_swizzled_scales",
      "use_triton_kernel",
      "act_quant_kwargs"
    ],
    "__repr__": [
      "self"
    ],
    "_quantization_type": [
      "self"
    ],
    "to_nvfp4": [
      "data_hp",
      "block_size",
      "per_tensor_scale",
      "act_per_tensor_scale",
      "is_swizzled_scales",
      "use_triton_kernel",
      "act_quant_kwargs"
    ],
    "__torch_function__": [],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "get_hp_scales": [
      "self"
    ],
    "_same_metadata": [
      "cls",
      "self",
      "src"
    ]
  },
  "nvfp4_to_copy": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "nvfp4_slice": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "nvfp4_t": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "nvfp4_transpose": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "nvfp4_view_op": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "nvfp4_select": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "_addmm_nvfp4_dispatch": [
    "a",
    "b",
    "aten_op",
    "bias"
  ],
  "nvfp4_linear": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "nvfp4_mm": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "nvfp4_addmm": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "per_tensor_amax_to_scale": [
    "amax"
  ],
  "nvfp4_quantize": [
    "data_hp",
    "block_size",
    "per_tensor_scale"
  ],
  "MXDynamicActivationMXWeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_linear_extra_repr": [
    "self"
  ],
  "_mx_inference_linear_transform": [
    "module",
    "config"
  ],
  "NVFP4DynamicActivationNVFP4WeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_nvfp4_inference_linear_transform": [
    "module",
    "config"
  ],
  "NVFP4WeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_nvfp4_weight_only_linear_transform": [
    "module",
    "config"
  ],
  "_auto_filter_for_nfp4": [
    "mod",
    "fqn"
  ],
  "implements_func": [
    "torch_ops"
  ],
  "mx_linear": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "to_blocked": [
    "input_matrix",
    "use_triton_kernel"
  ],
  "from_blocked": [
    "blocked_tensor",
    "original_rows",
    "original_cols"
  ],
  "hp_data_dims_to_swizzled_scale_dims_nvfp4": [
    "hp_data_M",
    "hp_data_K"
  ],
  "hp_data_dims_to_swizzled_scale_dims_mx": [
    "hp_data_M",
    "hp_data_K"
  ],
  "_to_blocked_single": [
    "scales"
  ],
  "_to_mxfp8_dim1_kernel_wrapper": [
    "a",
    "block_size",
    "elem_dtype",
    "hp_dtype",
    "kernel_preference",
    "cast_kernel_choice",
    "scale_calculation_mode"
  ],
  "_swizzle_aware_slice": [
    "x",
    "dim",
    "start",
    "end",
    "step"
  ],
  "dtype_to_bitwidth": [],
  "dtype_to_sem_len": [],
  "dtype_to_exp_bias": [],
  "dtype_to_int_dtype": [],
  "dtype_to_interesting_values": [],
  "float4_e2m1_interesting_values": [],
  "float4_e2m1_neg": [],
  "float6_e3m2_interesting_values": [],
  "float6_e3m2_neg": [],
  "float6_e2m3_interesting_values": [],
  "float6_e2m3_neg": [],
  "_assert_equals": [
    "fp_ref",
    "s_enc_ref",
    "e_enc_ref",
    "m_enc_ref",
    "dtype"
  ],
  "get_sem_bits": [
    "x",
    "bitwidth"
  ],
  "exp_encoding_to_exp": [
    "exp_bit_str",
    "dtype"
  ],
  "sem_bits_to_sem_vals": [
    "s_enc",
    "e_enc",
    "m_enc",
    "dtype"
  ],
  "sem_vals_to_f32": [
    "s_i",
    "e_i",
    "m_f",
    "special_value"
  ],
  "sem_vals_to_formula": [
    "s_i",
    "e_i",
    "m_f",
    "special_value"
  ],
  "assert_same": [
    "fp1",
    "fp2"
  ],
  "run": [
    "dtype"
  ],
  "get_bits": [
    "x"
  ],
  "SIGN_MASK_F4": [],
  "MANTISSA_MASK_F4": [],
  "SIGN_MASK_F6_E2M3": [],
  "MANTISSA_MASK_F6_E2M3": [],
  "SIGN_MASK_F6_E3M2": [],
  "MANTISSA_MASK_F6_E3M2": [],
  "ZERO_BITS_F32": [],
  "ZERO_POINT_FIVE_BITS_F32": [],
  "f32_to_f4_unpacked": [
    "x"
  ],
  "f32_to_f6_e2m3_unpacked": [
    "x"
  ],
  "f32_to_f6_e3m2_unpacked": [
    "x"
  ],
  "f4_unpacked_to_f32": [
    "x"
  ],
  "f6_e2m3_unpacked_to_f32": [
    "x"
  ],
  "f6_e3m2_unpacked_to_f32": [
    "x"
  ],
  "down_size": [
    "size"
  ],
  "up_size": [
    "size"
  ],
  "unpack_uint4": [
    "uint8_data"
  ],
  "pack_uint4": [
    "uint8_data"
  ],
  "_triton_kernels_available": [],
  "_mxfp8_cuda_kernels_available": [],
  "MXFP8Dim0CastKernelChoice": {
    "TRITON": [],
    "TORCH": []
  },
  "MXFP8Dim1CastKernelChoice": {
    "TRITON": [],
    "CUDA": [],
    "TORCH": []
  },
  "MXLinearRecipeName": {
    "MXFP8_EMULATED": [],
    "MXFP8_CUBLAS": [],
    "MXFP8_CUBLAS_RCEIL": [],
    "MXFP4_EMULATED": [],
    "MXFP4_CUTLASS": []
  },
  "ScaleCalculationMode": {
    "FLOOR": [],
    "RCEIL": [],
    "CEIL": [],
    "EVEN": [],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "_validate_elem_dtype": [
    "elem_dtype"
  ],
  "_validate_kernel_preference": [
    "kernel_preference",
    "block_size",
    "elem_dtype"
  ],
  "_validate_mxfp8_dim0_cast_kernel_choice": [
    "mxfp8_dim0_cast_kernel_choice",
    "scale_calculation_mode"
  ],
  "_validate_mxfp8_dim1_cast_kernel_choice": [
    "mxfp8_dim1_cast_kernel_choice",
    "scale_calculation_mode"
  ],
  "MXLinearConfig": {
    "__post_init__": [
      "self"
    ],
    "from_recipe_name": [
      "recipe_name"
    ],
    "short_str": [
      "self"
    ]
  },
  "mx_mm": {
    "forward": [
      "ctx",
      "input_hp",
      "weight_hp",
      "in_elem_dtype",
      "w_elem_dtype",
      "grad_elem_dtype",
      "block_size",
      "kernel_preference",
      "mxfp8_dim0_cast_kernel_choice",
      "mxfp8_dim1_cast_kernel_choice",
      "scale_calculation_mode"
    ],
    "backward": [
      "ctx",
      "grad_output_hp"
    ]
  },
  "MXLinear": {
    "from_float": [
      "cls",
      "mod",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "_mx_linear_transform": [
    "module",
    "config"
  ],
  "QuantizeTensorToMXKwargs": {},
  "_to_mx_rceil": [
    "data_hp",
    "max_abs",
    "max_pos"
  ],
  "to_mx": [
    "data_hp",
    "elem_dtype",
    "block_size",
    "scaling_mode",
    "is_swizzled_scales"
  ],
  "get_fp_scale": [
    "scale_e8m0"
  ],
  "to_dtype": [
    "data_lp",
    "scale_e8m0",
    "elem_dtype",
    "block_size",
    "target_dtype"
  ],
  "tensor_size_hp_to_fp4x2": [
    "orig_size",
    "is_contiguous"
  ],
  "tensor_size_fp4x2_to_hp": [
    "orig_size",
    "is_contiguous"
  ],
  "MXTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "elem_dtype",
      "block_size",
      "orig_dtype",
      "kernel_preference",
      "act_quant_kwargs",
      "is_swizzled_scales"
    ],
    "__repr__": [
      "self"
    ],
    "_quantization_type": [
      "self"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "to_mx": [
      "data_hp",
      "elem_dtype",
      "block_size",
      "scaling_mode",
      "kernel_preference",
      "act_quant_kwargs",
      "is_swizzled_scales",
      "mxfp8_dim0_cast_kernel_choice"
    ],
    "__torch_function__": []
  },
  "_get_gemm_choice": [
    "choice_a",
    "choice_b"
  ],
  "_addmm_mx_dispatch": [
    "a",
    "b",
    "aten_op",
    "bias"
  ],
  "mx_addmm": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_t": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_cast_up_op": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_view_op": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_slice": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_clone": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_select": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_all_gather": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "mx_wait_tensor": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "DTYPE_FP6_E3M2": [],
  "DTYPE_FP6_E2M3": [],
  "SUPPORTED_ELEM_DTYPES": [],
  "DTYPE_TO_SHORT_STR": [],
  "F8E4M3_MAX": [],
  "F8E5M2_MAX": [],
  "F8E4M3_MAX_POW2": [],
  "F8E5M2_MAX_POW2": [],
  "F6_E2M3_MAX_POW2": [],
  "F6_E3M2_MAX_POW2": [],
  "F4_E2M1_MAX_POW2": [],
  "E8M0_EXPONENT_BIAS": [],
  "E8M0_EXPONENT_NAN_VAL": [],
  "BF16_EXP_BIAS": [],
  "F6_E2M3_EXP_BIAS": [],
  "F6_E3M2_EXP_BIAS": [],
  "F4_E2M1_EXP_BIAS": [],
  "F32_MIN_NORMAL": [],
  "F6_E2M3_MAX": [],
  "F6_E2M3_MIN_NORMAL": [],
  "F6_E2M3_MAX_INT": [],
  "F6_E3M2_MAX": [],
  "F6_E3M2_MIN_NORMAL": [],
  "F6_E3M2_MAX_INT": [],
  "F4_E2M1_MAX": [],
  "F4_E2M1_MIN_NORMAL": [],
  "F4_E2M1_MAX_INT": [],
  "BLOCK_SIZE_DEFAULT": [],
  "Float8DynamicActivationFloat8WeightOpaqueTensorConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_float8_dynamic_activation_float8_weight_opaque_tensor_quantize": [
    "weight",
    "config"
  ],
  "_float8_dynamic_activation_float8_weight_opaque_tensor_transform": [
    "module",
    "config"
  ],
  "ValidGranularity": [],
  "Float8OpaqueTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "block_size",
      "act_quant_kwargs"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale",
      "block_size",
      "act_quant_kwargs"
    ],
    "_quantization_type": [
      "self"
    ],
    "_normalize_and_check_granularity": [
      "cls",
      "granularity"
    ],
    "from_hp": [
      "cls",
      "hp_tensor",
      "block_size",
      "act_quant_kwargs"
    ]
  },
  "ALLOWED_CLASSES": [],
  "ALLOWED_TENSORS_SUBCLASSES": [],
  "TensorSubclassAttributeJSONEncoder": {
    "default": [
      "self",
      "o"
    ],
    "encode_value": [
      "self",
      "value"
    ]
  },
  "object_from_dict": [
    "data"
  ],
  "is_metadata_torchao": [
    "metadata"
  ],
  "unflatten_tensor_state_dict": [
    "tensors_data_dict",
    "metadata"
  ],
  "flatten_tensor_state_dict": [
    "tensors_dict"
  ],
  "_AutoRoundConfig": {},
  "_auto_round_config": [],
  "_OptimizationTracker": {
    "reset": [
      "self"
    ]
  },
  "_optimization_tracker": [],
  "_replace_model_buffers_and_params": [
    "model",
    "replacement_fn"
  ],
  "_tensor_to_multi_tensor": [
    "model"
  ],
  "_multi_tensor_to_tensor": [
    "model"
  ],
  "prepare_model_for_applying_auto_round_": [
    "model",
    "is_target_module",
    "bits",
    "group_size",
    "iters",
    "use_optimized_layer_output",
    "gradient_accumulate_steps",
    "compile_optimization_process",
    "device"
  ],
  "apply_auto_round": [],
  "_apply_auto_round_optimization": [
    "block",
    "block_inputs",
    "block_outputs",
    "config"
  ],
  "apply_auto_round_optimization": [
    "module",
    "args",
    "kwargs",
    "output",
    "config"
  ],
  "_is_package_available": [
    "pkg_name",
    "metadata_name"
  ],
  "is_auto_round_available": [],
  "import_dataloader": [],
  "singleton": [
    "cls"
  ],
  "freeze_random": [
    "seed"
  ],
  "count_tensor_of_type": [
    "mod",
    "cls"
  ],
  "see_memory_usage": [
    "message",
    "force"
  ],
  "gen_text": [
    "model",
    "tokenizer",
    "msg",
    "device",
    "prompt",
    "max_length"
  ],
  "gen_example_inputs": [
    "tokenizer",
    "device",
    "max_length"
  ],
  "_auto_detect_decoder_cls": [
    "model"
  ],
  "get_float_model_info": [
    "model_name_or_path",
    "dtype"
  ],
  "execution_records": [],
  "dump_elapsed_time": [
    "customized_msg",
    "record"
  ],
  "_MultiTensorConfig": {},
  "_multi_tensor_config": [],
  "MultiTensor": {
    "__new__": [
      "cls",
      "input"
    ],
    "__init__": [
      "self",
      "input"
    ],
    "__repr__": [
      "self"
    ],
    "add_tensors": [
      "self",
      "input"
    ],
    "get_value": [
      "self",
      "i"
    ],
    "flat_to_grouped": [
      "cls",
      "flat"
    ],
    "grouped_to_flat": [
      "cls",
      "grouped"
    ],
    "revert_to_tensor_pairs": [
      "cls",
      "args",
      "kwargs"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ]
  },
  "quantize_model_with_autoround_": [
    "model",
    "tokenizer",
    "is_target_module",
    "bits",
    "group_size",
    "iters",
    "seqlen",
    "dataset_name",
    "batch_size",
    "nsamples",
    "use_optimized_layer_output",
    "gradient_accumulate_steps",
    "compile_optimization_process"
  ],
  "main": [
    "args"
  ],
  "_use_deterministic": [],
  "AO_USE_DETERMINISTIC_ALGORITHMS": [],
  "run_evaluation": [
    "model",
    "tokenizer",
    "tasks",
    "compile",
    "batch_size"
  ],
  "bench_accuracy": [
    "model",
    "tokenizer",
    "tasks",
    "msg"
  ],
  "_is_linear_but_not_lm_head": [
    "mod",
    "fqn"
  ],
  "Float8LinearNoCompile": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ],
    "from_float": [
      "cls",
      "mod",
      "config",
      "kernel_algo",
      "no_precompute_for_backward"
    ]
  },
  "matmul_with_args_in_hp": {
    "forward": [
      "ctx",
      "input_hp",
      "weight_hp",
      "config",
      "linear_mm_config",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "matmul_with_args_in_hp_no_precompute_for_backward": {
    "forward": [
      "ctx",
      "input_hp",
      "weight_hp",
      "config",
      "linear_mm_config",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "test_matmul_with_args_in_hp": [
    "input_shape"
  ],
  "ToFP8RowAndColumnMajor": {
    "forward": [
      "ctx",
      "tensor",
      "float8_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "ToFP8RowMajor": {
    "forward": [
      "ctx",
      "tensor",
      "float8_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "ToFP8RowMajorT": {
    "forward": [
      "ctx",
      "tensor",
      "float8_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "ToFP8ColumnMajor": {
    "forward": [
      "ctx",
      "tensor",
      "float8_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "ToFP8ColumnMajorT": {
    "forward": [
      "ctx",
      "tensor",
      "float8_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "ToFP8RowMajorTAndNonT": {
    "forward": [
      "ctx",
      "tensor",
      "float8_dtype",
      "linear_mm_config",
      "gemm_input_role",
      "kernel_algo"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "convert_to_float8_nocompile_training": [
    "module"
  ],
  "FP8_DTYPE_MAP": [],
  "kernel_configs_1D": [],
  "kernel_configs_2D": [],
  "KernelAlgorithm": {
    "ATOMIC_MAX": [],
    "REDUCTION": []
  },
  "_to_fp8_row_major": [
    "input_ptr",
    "out_ptr",
    "scale_ptr",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE",
    "EPS"
  ],
  "_to_fp8_row_major_t": [
    "input_ptr",
    "out_ptr",
    "scale_ptr",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_num_rows",
    "input_num_cols",
    "output_num_rows",
    "output_num_cols",
    "input_stride_row",
    "input_stride_col",
    "output_stride_row",
    "output_stride_col",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE_ROWS",
    "BLOCK_SIZE_COLS",
    "EPS"
  ],
  "_to_fp8_col_major": [
    "input_ptr",
    "out_ptr",
    "scale_ptr",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "num_rows",
    "num_cols",
    "out_stride_row",
    "out_stride_col",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE_ROWS",
    "BLOCK_SIZE_COLS",
    "EPS"
  ],
  "to_fp8_col_major_t": [
    "input_ptr",
    "out_ptr",
    "scale_ptr",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_num_rows",
    "input_num_cols",
    "output_num_rows",
    "output_num_cols",
    "input_stride_row",
    "input_stride_col",
    "output_stride_row",
    "output_stride_col",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE_ROWS",
    "BLOCK_SIZE_COLS",
    "EPS"
  ],
  "_to_fp8_row_and_col_major": [
    "input_ptr",
    "row_major_out_ptr",
    "col_major_out_ptr",
    "scale_ptr",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "num_rows",
    "num_cols",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE_ROWS",
    "BLOCK_SIZE_COLS",
    "EPS"
  ],
  "_to_fp8_row_major_t_and_non_t": [
    "input_ptr",
    "row_major_out_ptr",
    "row_major_t_out_ptr",
    "scale_ptr",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_num_rows",
    "input_num_cols",
    "input_stride_row",
    "input_stride_col",
    "row_major_out_stride_row",
    "row_major_out_stride_col",
    "row_major_t_out_stride_row",
    "row_major_t_out_stride_col",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE_ROWS",
    "BLOCK_SIZE_COLS",
    "EPS"
  ],
  "_to_fp8_col_major_t_and_non_t": [
    "input_ptr",
    "col_major_out_ptr",
    "col_major_t_out_ptr",
    "scale_ptr",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_num_rows",
    "input_num_cols",
    "input_stride_row",
    "input_stride_col",
    "col_major_out_stride_row",
    "col_major_out_stride_col",
    "col_major_t_out_stride_row",
    "col_major_t_out_stride_col",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE_ROWS",
    "BLOCK_SIZE_COLS",
    "EPS"
  ],
  "_amax_atomic": [
    "input_ptr",
    "amax_ptr",
    "num_elements",
    "input_dtype",
    "BLOCK_SIZE",
    "EPS"
  ],
  "_scale_atomic": [
    "amax_ptr",
    "scale_out_ptr",
    "fp8_dtype_max",
    "EPS"
  ],
  "_amax_reduction": [
    "input_ptr",
    "block_amaxes_ptr",
    "num_elements",
    "input_dtype",
    "BLOCK_SIZE",
    "EPS"
  ],
  "_scale_reduction": [
    "block_amaxes_ptr",
    "scale_out_ptr",
    "num_elements",
    "fp8_dtype_max",
    "BLOCK_SIZE",
    "EPS"
  ],
  "hp_to_fp8_row_major": [
    "hp_tensor",
    "fp8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "algo"
  ],
  "hp_to_fp8_row_major_t": [
    "hp_tensor",
    "fp8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "algo"
  ],
  "hp_to_fp8_col_major": [
    "hp_tensor",
    "fp8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "algo"
  ],
  "hp_to_fp8_col_major_t": [
    "hp_tensor",
    "fp8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "algo"
  ],
  "hp_to_fp8_row_and_col_major": [
    "hp_tensor",
    "fp8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "algo"
  ],
  "hp_to_fp8_row_major_t_and_non_t": [
    "hp_tensor",
    "fp8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "algo"
  ],
  "hp_to_fp8_col_major_t_and_non_t": [
    "hp_tensor",
    "fp8_dtype",
    "linear_mm_config",
    "gemm_input_role",
    "algo"
  ],
  "_hp_tensor_to_scale": [
    "hp_tensor",
    "tl_input_dtype",
    "fp8_dtype_max",
    "algo"
  ],
  "test_fp8_hp_to_fp8_row_major": [
    "input_shape",
    "algo"
  ],
  "test_fp8_hp_to_fp8_row_major_t": [
    "input_shape",
    "algo"
  ],
  "test_fp8_hp_to_fp8_col_major": [
    "input_shape",
    "algo"
  ],
  "test_fp8_hp_to_fp8_col_major_t": [
    "input_shape",
    "algo"
  ],
  "test_fp8_hp_to_fp8_row_and_col_major": [
    "input_shape",
    "algo"
  ],
  "test_fp8_hp_to_fp8_row_major_t_and_non_t": [
    "input_shape",
    "algo"
  ],
  "test_fp8_hp_to_fp8_col_major_t_and_non_t": [
    "input_shape",
    "algo"
  ],
  "sequential_quantize": [
    "model",
    "calibration_data",
    "config"
  ],
  "prepare_dataset": [
    "tokenizer",
    "max_sequence_length",
    "num_calibration_samples",
    "dataset_id",
    "dataset_split",
    "seed"
  ],
  "parse_args": [],
  "CONFIG_TO_TORCHAO_BASE_TENSOR": [],
  "GPTQConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_gptq_config_transform": [
    "module",
    "config"
  ],
  "_int4_row_quantize_zp_precomputed_qparams": [
    "x",
    "scales",
    "zeros",
    "group_size"
  ],
  "_int4_row_dequantize_zp": [
    "x",
    "scales",
    "zeros",
    "group_size"
  ],
  "gptq_quantize": [
    "H",
    "W",
    "config"
  ],
  "GPTQObserverTensor": {
    "tensor_data_names": [],
    "optional_tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "hp_data",
      "total_batches",
      "hessian"
    ],
    "__init__": [
      "self",
      "hp_data",
      "total_batches",
      "hessian"
    ],
    "update": [
      "self",
      "input"
    ],
    "from_hp": [
      "cls",
      "hp_tensor"
    ]
  },
  "op_qsdpa": [],
  "quantize_dtypes": [],
  "register_qsdpa": [],
  "expand": [
    "node",
    "sizes"
  ],
  "USEFUL_FUNCTIONS": [],
  "ALLOCATE_BUFFER": [],
  "INT8_SDPA_ONE_LOOP_TEMPLATE": [],
  "INT8_SDPA_SEVERAL_LOOPS_TEMPLATE": [],
  "CppInt8SdpaTemplate": {
    "__init__": [
      "self",
      "input_nodes",
      "layout",
      "scale",
      "q_scale",
      "q_zp",
      "k_scale",
      "k_zp",
      "v_scale",
      "v_zp",
      "a_scale",
      "a_zp",
      "o_scale",
      "o_zp"
    ],
    "add_choices": [
      "choices",
      "input_nodes",
      "layout",
      "scale",
      "q_scale",
      "q_zp",
      "k_scale",
      "k_zp",
      "v_scale",
      "v_zp",
      "a_scale",
      "a_zp",
      "o_scale",
      "o_zp"
    ],
    "reshape_attn_mask_to_4d": [
      "self",
      "kernel",
      "attn_mask",
      "batchSize",
      "num_head",
      "qSize",
      "kvSize"
    ],
    "get_options": [
      "self",
      "query",
      "key",
      "value",
      "qSize",
      "kvSize",
      "headSize",
      "batchSize",
      "num_head",
      "num_threads"
    ],
    "render": [
      "self",
      "kernel",
      "template_buffer_node",
      "epilogue_nodes"
    ],
    "codegen_useful_function": [
      "self",
      "kernel_name"
    ],
    "codegen_allocate_buffer": [
      "self",
      "buffer_name",
      "buffer_dtype",
      "buffer_size"
    ]
  },
  "_is_valid_concat_linear_da8w4_fusion": [
    "computation_nodes"
  ],
  "_concat_linear_dq8w4_cpu": [
    "graph"
  ],
  "register_da8w4_concat_linear_cpu_pass": [],
  "_is_valid_qsdpa_pattern": [],
  "_register_qsdpa_pattern": [
    "pattern",
    "custom_pass_dict"
  ],
  "_generate_dequant_pattern": [
    "input_pattern",
    "qtype",
    "is_reduced_type",
    "scale",
    "zp"
  ],
  "_generate_quant_pattern": [
    "input_pattern",
    "qtype",
    "scale",
    "zp"
  ],
  "_get_qsdpa_qkv_pattern": [
    "qtype",
    "is_batch_size_1",
    "is_reduced_type",
    "has_convert",
    "input_name"
  ],
  "_get_qsdpa_score_pattern": [
    "qtype",
    "has_mask",
    "is_batch_size_1",
    "is_reduced_type",
    "has_convert",
    "is_inv_scale"
  ],
  "_get_qsdpa_exp_pattern": [
    "qtype",
    "has_mask",
    "is_batch_size_1",
    "is_reduced_type",
    "has_convert",
    "is_inv_scale"
  ],
  "_get_qsdpa_attn_pattern": [
    "qtype",
    "has_mask",
    "is_batch_size_1",
    "is_reduced_type",
    "has_convert",
    "is_inv_scale"
  ],
  "_get_qsdpa_final_pattern": [
    "qtype",
    "has_mask",
    "is_batch_size_1",
    "is_reduced_type",
    "has_convert",
    "is_inv_scale"
  ],
  "_register_qsdpa_lowerings": [
    "custom_pass_dict"
  ],
  "custom_pass": [],
  "_qsdpa_init": [],
  "BlockwiseQuantLinear": {
    "dtype": [],
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "block_size",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "fp8_gemm_configs_max_autotune": [],
  "triton_fp8_gemm_1x128_128x128_kernel": [
    "a_ptr",
    "a_stride_dim_0",
    "a_stride_dim_1",
    "b_ptr",
    "b_stride_dim_0",
    "b_stride_dim_1",
    "c_ptr",
    "c_stride_dim_0",
    "c_stride_dim_1",
    "a_s_ptr",
    "a_s_stride_dim_0",
    "a_s_stride_dim_1",
    "b_s_ptr",
    "b_s_stride_dim_0",
    "b_s_stride_dim_1",
    "M",
    "N",
    "K",
    "out_dtype",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K"
  ],
  "triton_fp8_gemm_1x128_128x128": [
    "a",
    "b",
    "a_s",
    "b_s",
    "block_size",
    "out_dtype"
  ],
  "triton_fp8_gemm_1x128_128x1_kernel": [
    "a_ptr",
    "a_stride_dim_0",
    "a_stride_dim_1",
    "b_ptr",
    "b_stride_dim_0",
    "b_stride_dim_1",
    "c_ptr",
    "a_s_ptr",
    "a_s_stride_dim_0",
    "a_s_stride_dim_1",
    "b_s_ptr",
    "b_s_stride_dim_0",
    "b_s_stride_dim_1",
    "M",
    "N",
    "K",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K"
  ],
  "triton_fp8_gemm_1x128_128x1": [
    "a",
    "b",
    "a_s",
    "b_s",
    "block_size",
    "out_dtype"
  ],
  "quant_kernel_configs": [],
  "quant_kernel_configs_with_groups": [],
  "triton_fp8_blockwise_act_quant_lhs_kernel": [
    "x_ptr",
    "x_stride_dim_0",
    "x_stride_dim_1",
    "y_ptr",
    "y_stride_dim_0",
    "y_stride_dim_1",
    "s_ptr",
    "s_stride_dim_0",
    "s_stride_dim_1",
    "M",
    "K",
    "BLOCK_SIZE",
    "NUM_GROUPS",
    "EPS"
  ],
  "triton_fp8_blockwise_act_quant_lhs": [
    "x",
    "block_size",
    "dtype"
  ],
  "triton_fp8_blockwise_act_quant_rhs_kernel": [
    "x_ptr",
    "x_stride_dim_0",
    "x_stride_dim_1",
    "y_ptr",
    "y_stride_dim_0",
    "y_stride_dim_1",
    "s_ptr",
    "s_stride_dim_0",
    "s_stride_dim_1",
    "M",
    "K",
    "BLOCK_SIZE",
    "NUM_GROUPS",
    "EPS"
  ],
  "triton_fp8_blockwise_act_quant_rhs": [
    "x",
    "block_size",
    "dtype"
  ],
  "triton_fp8_blockwise_act_quant_transposed_lhs_kernel": [
    "x_ptr",
    "x_stride_dim_0",
    "x_stride_dim_1",
    "y_ptr",
    "y_stride_dim_0",
    "y_stride_dim_1",
    "s_ptr",
    "s_stride_dim_0",
    "s_stride_dim_1",
    "M",
    "K",
    "BLOCK_SIZE",
    "NUM_GROUPS",
    "EPS"
  ],
  "triton_fp8_blockwise_act_quant_transposed_lhs": [
    "x",
    "block_size",
    "dtype"
  ],
  "triton_fp8_blockwise_weight_quant_rhs_kernel": [
    "x_ptr",
    "x_stride_dim_0",
    "x_stride_dim_1",
    "y_ptr",
    "y_stride_dim_0",
    "y_stride_dim_1",
    "s_ptr",
    "s_stride_dim_0",
    "s_stride_dim_1",
    "M",
    "N",
    "BLOCK_SIZE",
    "EPS"
  ],
  "triton_fp8_blockwise_weight_quant_rhs": [
    "x",
    "block_size",
    "dtype"
  ],
  "triton_fp8_blockwise_weight_quant_transposed_rhs_kernel": [
    "x_ptr",
    "x_stride_dim_0",
    "x_stride_dim_1",
    "y_ptr",
    "y_stride_dim_0",
    "y_stride_dim_1",
    "s_ptr",
    "s_stride_dim_0",
    "s_stride_dim_1",
    "M",
    "N",
    "BLOCK_SIZE",
    "EPS"
  ],
  "triton_fp8_blockwise_weight_quant_transposed_rhs": [
    "x",
    "block_size",
    "dtype"
  ],
  "torch_blockwise_scale_act_quant_lhs": [
    "x",
    "tile_size"
  ],
  "torch_blockwise_scale_act_quant_rhs": [
    "x",
    "block_size",
    "dtype",
    "eps"
  ],
  "torch_blockwise_scale_weight_quant": [
    "x",
    "tile_size"
  ],
  "fp8_blockwise_mm": {
    "forward": [
      "ctx",
      "x",
      "weight",
      "block_size",
      "out_dtype",
      "use_triton"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "Float8BlockwiseLinear": {
    "supported_dtypes": [],
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "from_float": [
      "cls",
      "mod"
    ]
  },
  "Float8BlockwiseLinearConfig": {},
  "_float8_blockwise_transform": [
    "module",
    "config"
  ],
  "MoEScalingType": {
    "FP8_ROWWISE": [],
    "MXFP8": [],
    "MXFP8_WGRAD_WITH_HP": []
  },
  "MoETrainingConfig": {
    "__init__": [
      "self",
      "scaling_type",
      "kernel_preference"
    ]
  },
  "_moe_training_transform": [
    "module",
    "config"
  ],
  "_swap_params": [
    "module"
  ],
  "torch_to_float8_per_group_colwise": [
    "A_col_major",
    "offs",
    "target_dtype",
    "round_scales_to_power_of_2"
  ],
  "torch_to_float8_per_group_rowwise": [
    "x",
    "offs",
    "target_dtype",
    "round_scales_to_power_of_2"
  ],
  "torch_to_3d_rowwise_float8_transpose_rhs": [
    "input_hp_t",
    "target_dtype",
    "round_scales_to_power_of_2"
  ],
  "_to_mxfp8_per_group_rowwise": [
    "x",
    "offs",
    "block_size"
  ],
  "_to_mxfp8_per_group_colwise": [
    "A_col_major",
    "offs",
    "block_size"
  ],
  "_is_column_major": [
    "x"
  ],
  "_is_row_major": [
    "x"
  ],
  "generate_jagged_offs": [
    "E",
    "M",
    "multiple_of",
    "dtype",
    "device"
  ],
  "conditional_nostrict_trace": [
    "fn"
  ],
  "_SM100_KERNELS_AVAILABLE": [],
  "_quantize_then_scaled_grouped_mm": [
    "A",
    "B_t",
    "offs",
    "out_dtype",
    "scaling_type",
    "kernel_preference"
  ],
  "_Float8GroupedMM": {
    "forward": [
      "ctx",
      "A",
      "B_t",
      "offs",
      "out_dtype"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_MXFP8GroupedMM": {
    "forward": [
      "ctx",
      "input_act",
      "weight_t",
      "group_offsets",
      "block_size",
      "out_dtype",
      "kernel_preference",
      "wgrad_with_hp",
      "scale_calculation_mode"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_compute_dgrad": [
    "grad_output",
    "weight_t",
    "group_offsets",
    "block_size",
    "out_dtype",
    "scale_calculation_mode",
    "kernel_preference"
  ],
  "_compute_wgrad": [
    "grad_output",
    "input_act",
    "group_offsets",
    "block_size",
    "out_dtype",
    "scale_calculation_mode",
    "wgrad_with_hp",
    "kernel_preference"
  ],
  "_quantize_3d_along_dim1_native": [
    "x",
    "block_size",
    "scale_calculation_mode"
  ],
  "_extract_or_quantize_dim0": [
    "tensor",
    "block_size",
    "kernel_preference",
    "scale_calculation_mode"
  ],
  "_dequantize_if_mxtensor": [
    "tensor",
    "block_size"
  ],
  "_to_mxfp8_dim1_3d": [
    "B",
    "block_size",
    "scaling_mode"
  ],
  "_emulated_mxfp8_scaled_grouped_mm_2d_3d": [
    "A_data",
    "A_scale",
    "B_data",
    "B_scale",
    "offs",
    "out_dtype",
    "block_size"
  ],
  "_emulated_mxfp8_scaled_grouped_mm_2d_2d": [
    "A_data",
    "A_scale",
    "B_data",
    "B_scale",
    "offs",
    "out_dtype",
    "block_size"
  ],
  "_to_mxfp8_then_scaled_grouped_mm": [
    "A",
    "B_t",
    "offs",
    "block_size",
    "out_dtype",
    "kernel_preference",
    "wgrad_with_hp",
    "scale_calculation_mode"
  ],
  "_to_fp8_rowwise_then_scaled_grouped_mm": [],
  "ScaledGroupedMMTensor": {
    "grouped_mm_func_name": [],
    "offs_arg_name": [],
    "__new__": [
      "cls",
      "tensor",
      "scaling_type",
      "kernel_preference"
    ],
    "__init__": [
      "self",
      "tensor",
      "scaling_type",
      "kernel_preference"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__repr__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "flatten_spec",
      "outer_size",
      "outer_stride"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh",
      "outer_size",
      "outer_stride",
      "module",
      "mp_policy"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ]
  },
  "block_sizes_n": [],
  "block_sizes_k": [],
  "num_warps": [],
  "num_stages": [],
  "atomic_kernel_configs_2D": [],
  "triton_fp8_rowwise_3d_transpose_rhs": [
    "hp_tensor",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "_fake_triton_fp8_rowwise_3d_transpose_rhs": [
    "hp_tensor",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "_triton_fp8_rowwise_3d_transpose_scales_rhs_kernel": [
    "input_ptr",
    "stride_input_dim0",
    "stride_input_dim1",
    "stride_input_dim2",
    "scales_ptr",
    "stride_scales_dim0",
    "stride_scales_dim1",
    "E",
    "N",
    "K",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_dtype",
    "round_scales_to_power_of_2",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "EPS"
  ],
  "_triton_fp8_rowwise_3d_transpose_cast_rhs_kernel": [
    "input_ptr",
    "stride_input_dim0",
    "stride_input_dim1",
    "stride_input_dim2",
    "output_ptr",
    "stride_output_dim0",
    "stride_output_dim1",
    "stride_output_dim2",
    "scales_ptr",
    "stride_scales_dim0",
    "stride_scales_dim1",
    "E",
    "N",
    "K",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_dtype",
    "output_dtype",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K"
  ],
  "reduction_kernel_configs_2D": [],
  "_triton_fp8_rowwise_3d_transpose_rhs_fused_reduction_kernel": [
    "input_ptr",
    "stride_input_dim0",
    "stride_input_dim1",
    "stride_input_dim2",
    "output_ptr",
    "stride_output_dim0",
    "stride_output_dim1",
    "stride_output_dim2",
    "scales_ptr",
    "stride_scales_dim0",
    "stride_scales_dim1",
    "E",
    "N",
    "K",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_dtype",
    "output_dtype",
    "round_scales_to_power_of_2",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "EPS"
  ],
  "triton_fp8_rowwise_3d_transpose_rhs_fused_reduction": [
    "hp_tensor",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "_fake_triton_fp8_rowwise_3d_transpose_rhs_fused_reduction": [
    "hp_tensor",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "block_sizes": [],
  "block_sizes_iter": [],
  "triton_fp8_per_group_rowwise_scales": [
    "hp_tensor",
    "offsets",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "_fake_triton_fp8_per_group_rowwise_scales_kernel": [
    "hp_tensor",
    "offsets",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "_triton_fp8_per_group_rowwise_scales_kernel": [
    "input_ptr",
    "offsets_ptr",
    "out_ptr",
    "scales_ptr",
    "M",
    "K",
    "stride_input_row",
    "stride_input_col",
    "stride_output_row",
    "stride_output_col",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_dtype",
    "output_dtype",
    "round_scales_to_power_of_2",
    "BLOCK_SIZE",
    "BLOCK_SIZE_ITER",
    "EPS"
  ],
  "triton_fp8_per_group_colwise_scales": [
    "hp_tensor",
    "offsets",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "_fake_triton_fp8_per_group_colwise_scales": [
    "hp_tensor",
    "offsets",
    "output_dtype",
    "round_scales_to_power_of_2"
  ],
  "_triton_fp8_per_group_colwise_scales_kernel": [
    "input_ptr",
    "offsets_ptr",
    "out_ptr",
    "scales_ptr",
    "K",
    "N",
    "stride_input_row",
    "stride_input_col",
    "stride_output_row",
    "stride_output_col",
    "num_elements",
    "fp8_dtype_min",
    "fp8_dtype_max",
    "input_dtype",
    "output_dtype",
    "round_scales_to_power_of_2",
    "BLOCK_SIZE",
    "BLOCK_SIZE_ITER",
    "EPS"
  ],
  "sync_threads": [],
  "send_signal": [
    "addrs",
    "sem"
  ],
  "wait_signal": [
    "addrs",
    "sem"
  ],
  "blockwise_barrier": [
    "signal_pad_ptrs",
    "block_id",
    "rank",
    "world_size",
    "sem"
  ],
  "get_flat_bid": [],
  "get_tid": [],
  "get_ntid": [],
  "get_flat_tid": [],
  "torch_to_blocked_2d_M_groups": [
    "x_scales",
    "group_offs",
    "block_size"
  ],
  "torch_to_blocked_2d_K_groups": [
    "x_scales",
    "group_offs",
    "block_size"
  ],
  "torch_to_blocked_per_group_3d": [
    "weight_scales"
  ],
  "compute_blocked_scale_offsets_for_M_groups": [
    "offsets"
  ],
  "compute_blocked_scale_offsets_for_K_groups": [
    "scale_group_offsets",
    "block_size"
  ],
  "triton_mx_block_rearrange_2d_M_groups": [
    "scales_tensor",
    "input_group_end_offsets"
  ],
  "triton_scale_swizzle_M_groups": [
    "scales_ptr",
    "scales_stride_dim0",
    "scales_stride_dim1",
    "scale_rows",
    "scale_cols",
    "orig_offsets",
    "output_scales_ptr",
    "output_scales_stride_dim0",
    "output_stride_per_block",
    "output_stride_per_row_of_blocks",
    "num_groups",
    "BLOCK_ROWS",
    "BLOCK_COLS"
  ],
  "triton_mx_block_rearrange_per_group_3d": [
    "scale_tensor"
  ],
  "triton_scale_swizzle_per_group_3d": [
    "input_ptr",
    "input_stride_dim0",
    "input_stride_dim1",
    "input_stride_dim2",
    "output_ptr",
    "output_stride_dim0",
    "output_block_stride",
    "scale_rows",
    "scale_cols",
    "BLOCK_ROWS",
    "BLOCK_COLS"
  ],
  "triton_mx_block_rearrange_2d_K_groups": [
    "scales_tensor",
    "input_group_end_offsets"
  ],
  "triton_scale_swizzle_2d_K_groups": [
    "scales_ptr",
    "scales_stride_dim0",
    "scales_stride_dim1",
    "scale_rows",
    "scale_cols",
    "padded_rows",
    "orig_offsets",
    "output_scales_ptr",
    "output_stride_per_block",
    "num_groups",
    "BLOCK_ROWS",
    "BLOCK_COLS",
    "DEBUG"
  ],
  "_dest_indices_for_block": [
    "row_offs",
    "col_offs",
    "BLOCK_ROWS",
    "BLOCK_COLS"
  ],
  "_blocked_group_start_idx": [
    "group_pid",
    "orig_offsets",
    "num_groups",
    "padding_size"
  ],
  "MXFP8OnDeviceAllToAllV": {
    "input_sym_mem_buf": [],
    "scales_sym_mem_buf": [],
    "input_splits_sym_mem_buf": [],
    "grad_out_sym_mem_buf": [],
    "max_output_rows_per_rank": [],
    "grad_input_buf": [],
    "grad_input_scales_buf": [],
    "grad_input_splits_buf": [],
    "forward": [
      "ctx",
      "input",
      "input_splits",
      "max_output_rows_per_rank",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output",
      "grad_splits"
    ]
  },
  "mxfp8_on_device_all_to_all_v": [],
  "_mxfp8_on_device_all_to_all_v": [
    "input",
    "input_scales",
    "input_splits",
    "output",
    "output_scales",
    "output_splits",
    "group",
    "BLOCKS_PER_REMOTE_RANK",
    "BLOCK_SIZE"
  ],
  "_mxfp8_all_to_all_v_kernel": [
    "input_ptrs",
    "input_scales_ptrs",
    "input_splits_ptr",
    "output_ptr",
    "output_scales_ptr",
    "output_splits_ptr",
    "signal_pad_ptrs",
    "dim",
    "dim_scaling_groups",
    "rank",
    "world_size",
    "BLOCKS_PER_REMOTE_RANK",
    "BLOCK_SIZE"
  ],
  "_exchange_row_offsets": [
    "split_sizes_ptrs",
    "local_rank",
    "remote_rank",
    "world_size"
  ],
  "ToMXFP8AllToAllVDequant": {
    "forward": [
      "ctx",
      "input",
      "output_splits",
      "input_splits",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output_hp"
    ]
  },
  "to_mxfp8_a2a_dequant": [],
  "_round_up": [
    "x",
    "y"
  ],
  "_PermuteMXFP8FwdHPBwd": {
    "forward": [
      "ctx",
      "mx_tensor",
      "num_tokens_per_expert",
      "ep_degree",
      "num_local_experts",
      "group_size_multiple_of",
      "use_triton_for_bwd"
    ],
    "backward": [
      "ctx",
      "grad_padded_shape",
      "grad_output",
      "grad_permuted_indices",
      "grad_num_tokens_per_expert_padded",
      "grad_group_offsets"
    ]
  },
  "_permute_bf16": [
    "x",
    "num_tokens_per_expert",
    "ep_degree",
    "num_local_experts",
    "alignment"
  ],
  "permute_mxfp8_fwd_hp_bwd": [
    "mx_tensor",
    "num_tokens_per_expert",
    "ep_degree",
    "num_local_experts",
    "group_size_multiple_of",
    "use_mxfp8",
    "use_triton_for_bwd"
  ],
  "_triton_permute_bwd": [
    "grad_output",
    "permuted_indices",
    "original_rows",
    "original_cols"
  ],
  "_triton_permute_bwd_kernel": [
    "grad_ptr",
    "permuted_indices_ptr",
    "output_buffer_ptr",
    "grad_rows",
    "grad_cols",
    "original_rows",
    "original_cols",
    "BLOCK_ROWS",
    "BLOCK_COLS",
    "PADDING_VALUE"
  ],
  "_fill_indices_kernel": [
    "tokens_per_expert_group_ptr",
    "start_index_values_ptr",
    "write_offsets_ptr",
    "output_ptr",
    "experts_per_rank",
    "num_ranks",
    "BLOCK_SIZE"
  ],
  "fill_indices_wrapper": [
    "tokens_per_expert_group",
    "start_index_values",
    "write_offsets",
    "experts_per_rank",
    "num_ranks",
    "max_len",
    "block_size",
    "max_blocks"
  ],
  "fill_indices_cpu": [
    "tokens_per_expert_group",
    "start_index_values",
    "write_offsets",
    "experts_per_rank",
    "num_ranks",
    "max_len"
  ],
  "generate_permute_indices": [
    "tokens_per_expert_group",
    "experts_per_rank",
    "num_ranks",
    "max_len",
    "alignment",
    "use_cpu"
  ],
  "_A2ACombineHPFwdMXFP8Bwd": {
    "forward": [
      "ctx",
      "input",
      "output_splits",
      "input_splits",
      "group",
      "scaling_mode",
      "block_size",
      "mxfp8_bwd"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "a2a_combine_hp_fwd_mxfp8_bwd": [
    "input",
    "output_splits",
    "input_splits",
    "group_name",
    "scaling_mode",
    "block_size",
    "mxfp8_bwd"
  ],
  "_A2ADispatchMXFP8FwdHPBwd": {
    "forward": [
      "ctx",
      "input",
      "output_splits",
      "input_splits",
      "group",
      "scaling_mode",
      "block_size"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "a2a_dispatch_mxfp8_fwd_hp_bwd": [
    "input",
    "output_splits",
    "input_splits",
    "group_name",
    "scaling_mode",
    "block_size"
  ],
  "_UnpermuteHPFwdMXFP8Bwd": {
    "forward": [
      "ctx",
      "input",
      "permuted_indices",
      "padded_shape"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "unpermute_hp_fwd_mxfp8_bwd": [
    "input",
    "permuted_indices",
    "padded_shape"
  ],
  "_unpermute_bf16": [
    "out",
    "permuted_indices",
    "input_shape"
  ],
  "_convert_linear_weight_to_int8_lut_tensor": [
    "module"
  ],
  "_convert_module_weight_to_intx_opaque_tensor": [
    "module",
    "intx_packing_format"
  ],
  "_find_tied_module_names_for_embedding": [
    "embedding_weight",
    "model"
  ],
  "_find_tied_params": [
    "model"
  ],
  "_convert_model_for_aarch64": [
    "model"
  ],
  "convert_to_packed_tensor_based_on_current_hardware": [
    "tensor"
  ],
  "train": [],
  "LsqBinaryTernaryExtension": {
    "forward": [
      "ctx",
      "input",
      "alpha",
      "num_bits",
      "layerwise"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "StretchedElasticQuant": {
    "forward": [
      "ctx",
      "input",
      "alpha",
      "num_bits",
      "layerwise"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "QuantizeLinear": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "input_"
    ]
  },
  "_CHECKPOINT_FOR_DOC": [],
  "_CONFIG_FOR_DOC": [],
  "LlamaRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "LlamaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "_dynamic_frequency_update": [
      "self",
      "position_ids",
      "device"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "rotate_half": [
    "x"
  ],
  "apply_rotary_pos_emb": [
    "q",
    "k",
    "cos",
    "sin",
    "position_ids",
    "unsqueeze_dim"
  ],
  "LlamaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "repeat_kv": [
    "hidden_states",
    "n_rep"
  ],
  "eager_attention_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "scaling",
    "dropout"
  ],
  "LlamaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_value",
      "cache_position"
    ]
  },
  "LlamaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "LLAMA_START_DOCSTRING": [],
  "LlamaPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn_2": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_cache_class": [],
    "_supports_quantized_cache": [],
    "_supports_static_cache": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LLAMA_INPUTS_DOCSTRING": [],
  "LlamaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "device",
      "cache_position",
      "batch_size"
    ]
  },
  "KwargsForCausalLM": {},
  "LlamaForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "LlamaForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LlamaForQuestionAnswering": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LlamaForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LlamaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "pretraining_tp",
      "tie_word_embeddings",
      "rope_theta",
      "rope_scaling",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "head_dim",
      "w_bits"
    ]
  },
  "HadamardMultiplier": {
    "__init__": [
      "self",
      "had_K",
      "K",
      "use_fp32"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "apply_spinquant": [
    "model",
    "use_r1",
    "use_r2",
    "use_r4",
    "pretrained_rotation_path",
    "qkv_split"
  ],
  "apply_spinquant_r1": [
    "model",
    "device",
    "pretrained_rotation_path",
    "qkv_split"
  ],
  "apply_spinquant_r2": [
    "model",
    "device",
    "pretrained_rotation_path",
    "qkv_split"
  ],
  "apply_spinquant_r4": [
    "model",
    "device"
  ],
  "_fuse_layernorm_into_linear": [
    "layernorm",
    "linear_layers"
  ],
  "_rotate_model_r1": [
    "model",
    "R1",
    "qkv_split"
  ],
  "_rotate_model_r2": [
    "model",
    "R2s",
    "qkv_split"
  ],
  "_rotate_model_r4": [
    "model"
  ],
  "_add_activation_wrappers_r4": [
    "model"
  ],
  "fuse_layernorm_into_linear": [
    "model",
    "qkv_split"
  ],
  "_rotate_mlp_output": [
    "layer",
    "R1"
  ],
  "_rotate_mlp_input": [
    "layer",
    "R1"
  ],
  "_rotate_attention_output": [
    "layer",
    "R1"
  ],
  "_rotate_attention_inputs": [
    "layer",
    "R1",
    "qkv_split"
  ],
  "_rotate_head": [
    "model",
    "R1"
  ],
  "_rotate_embeddings": [
    "model",
    "R1"
  ],
  "_rotate_mod_weight_right": [
    "mod",
    "R"
  ],
  "_rotate_mod_weight_left": [
    "mod",
    "R"
  ],
  "register_custom_op_abstract": [
    "name"
  ],
  "hadamard_transform": [
    "x",
    "scale"
  ],
  "HadamardTransform": {
    "forward": [
      "ctx",
      "u"
    ],
    "backward": [
      "ctx",
      "grad"
    ]
  },
  "is_pow2": [
    "n"
  ],
  "get_hadK": [
    "n",
    "transpose"
  ],
  "matmul_hadU_slow": [
    "X",
    "hadK",
    "K"
  ],
  "matmul_hadU_fast": [
    "X",
    "hadK",
    "K"
  ],
  "random_hadamard_matrix": [
    "size",
    "device",
    "seed"
  ],
  "hadamard_matrix": [
    "size",
    "device"
  ],
  "apply_exact_had_to_linear": [
    "module",
    "had_dim",
    "output",
    "R2"
  ],
  "_DATA_FILENAME": [],
  "_JSON_FILENAME": [],
  "_write_pickle": [
    "raw_matrices",
    "pickle_path"
  ],
  "_load_raw_matrices": [],
  "_load_hadamard_matrices": [],
  "_get_hadamard_matrix": [
    "name"
  ],
  "get_had12": [],
  "get_had20": [],
  "get_had28": [],
  "get_had36": [],
  "get_had40": [],
  "get_had44": [],
  "get_had52": [],
  "get_had60": [],
  "get_had108": [],
  "get_had140": [],
  "get_had156": [],
  "get_had172": [],
  "Int4WeightOnlyOpaqueTensorConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_int4_weight_only_opaque_tensor_quantize": [
    "weight",
    "config"
  ],
  "_int4_weight_only_transform": [
    "module",
    "config"
  ],
  "Int4OpaqueTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_data_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale_and_zero",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale_and_zero",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "_quantization_type": [
      "self"
    ],
    "from_hp": [
      "cls",
      "w",
      "block_size",
      "int4_choose_qparams_algorithm"
    ]
  },
  "pack_2xint4": [
    "t"
  ],
  "triton_mixed_mm": [
    "a",
    "b",
    "scales",
    "zeros",
    "group_size",
    "transposed",
    "acc_dtype",
    "input_precision",
    "fp8_fast_accum",
    "kernel_type",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K"
  ],
  "device": [],
  "HQQLinearTorchWeightOnlyInt4": {
    "__init__": [
      "self",
      "linear_layer",
      "quant_config",
      "del_orig",
      "compute_dtype",
      "device",
      "initialize",
      "inner_k_tiles",
      "padding"
    ],
    "initialize_with_hqq_quants": [
      "self",
      "W_q",
      "meta",
      "bias"
    ],
    "initialize": [
      "self"
    ],
    "quantize": [
      "self",
      "W",
      "weight_quant_params",
      "scale_quant_params",
      "zero_quant_params",
      "offload_meta"
    ],
    "reshape_meta_axis1": [
      "self",
      "meta_tensor",
      "new_group_size",
      "shape"
    ],
    "find_multiple": [
      "self",
      "n",
      "k"
    ],
    "set_shape": [
      "self",
      "shape"
    ],
    "process_hqq_quants": [
      "self",
      "W_q",
      "meta"
    ],
    "hqq_quants_to_torch_quants": [
      "self",
      "W_q",
      "scales",
      "zeros",
      "shape",
      "nbits"
    ],
    "pack_scales_and_zeros": [
      "self",
      "scales",
      "zeros"
    ],
    "matmul": [
      "self",
      "x"
    ],
    "dequantize": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "get_configs_compute_bound": [],
  "MIXED_MM_HEURISTICS": [],
  "_mixed_mm_kernel": [
    "A",
    "B",
    "scales_ptr",
    "zeros_ptr",
    "C",
    "M",
    "N",
    "K",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "stride_scale_k",
    "stride_scale_n",
    "IS_BFLOAT16",
    "QGROUP_SIZE",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "SPLIT_K",
    "EVEN_K",
    "TRANSPOSED",
    "GROUP_M",
    "acc_dtype",
    "input_precision",
    "fp8_fast_accum",
    "DEBUG"
  ],
  "_mixed_mm": [],
  "mixed_mm_kernel_max_autotune": [],
  "mixed_mm_kernel_compute_bound": [],
  "_mixed_mm_debug": [],
  "NVFP4FakeQuantizeConfig": {},
  "_NVFP4QuantizedForwardFakeQuantizedBackward": {
    "forward": [
      "ctx",
      "_input",
      "weight",
      "bias",
      "activation_config",
      "weight_config"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "NVFP4FakeQuantizedLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "activation_config",
      "weight_config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "to_linear": [
      "self"
    ],
    "from_linear": [
      "cls",
      "mod",
      "activation_config",
      "weight_config"
    ]
  },
  "_DEFAULT_MX_DTYPE": [],
  "MXFakeQuantizeConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_MXQuantizedForwardFakeQuantizedBackward": {
    "forward": [
      "ctx",
      "_input",
      "weight",
      "bias",
      "activation_config",
      "weight_config"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "MXFakeQuantizedLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "activation_config",
      "weight_config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "to_linear": [
      "self"
    ],
    "from_linear": [
      "cls",
      "mod",
      "activation_config",
      "weight_config"
    ]
  },
  "Int8DynamicActivationInt4WeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_int8_dynamic_activation_int4_weight_transform": [
    "module",
    "config"
  ],
  "GemliteUIntXWeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_gemlite_uintx_weight_only_transform": [
    "module",
    "config"
  ],
  "Float8StaticActivationFloat8WeightConfig": {
    "__post_init__": [
      "self"
    ],
    "get_act_quant_kwargs": [
      "self"
    ]
  },
  "Float8ObservedLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "input_act_obs",
      "bias",
      "device",
      "dtype",
      "output_act_obs"
    ],
    "forward": [
      "self",
      "input"
    ],
    "from_float": [
      "cls",
      "float_linear",
      "input_act_obs",
      "output_act_obs"
    ]
  },
  "Float8ObservedSoftmax": {
    "__init__": [
      "self",
      "dim",
      "output_act_obs"
    ],
    "forward": [
      "self",
      "input"
    ],
    "from_float": [
      "cls",
      "float_softmax",
      "output_act_obs"
    ]
  },
  "Float8QuantizedSoftmax": {
    "__init__": [
      "self",
      "dim",
      "output_act_quant_scale",
      "output_act_quant_kwargs"
    ],
    "dim": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ],
    "from_observed": [
      "cls",
      "observed_softmax",
      "output_act_quant_scale",
      "output_act_quant_kwargs"
    ]
  },
  "_float8_static_activation_float8_weight_transform": [
    "module",
    "config"
  ],
  "UIntXWeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_uintx_weight_only_transform": [
    "module",
    "config"
  ],
  "target_folder": [],
  "_is_linear": [
    "mod"
  ],
  "_graph_equals": [
    "g1",
    "g2"
  ],
  "_AUTOQUANT_CACHE": [],
  "LLAMA": [],
  "check_cache": [
    "gm",
    "cls",
    "shapes_and_dtype"
  ],
  "update_cache": [
    "gm",
    "cls",
    "shapes_and_dtype",
    "res"
  ],
  "resize_input": [
    "t",
    "extracted_bsz",
    "target_bsz"
  ],
  "maybe_adjust_model_bsz": [
    "m",
    "extracted_bsz",
    "target_bsz"
  ],
  "AutoQuantizableLinearWeight": {
    "__new__": [
      "cls",
      "weight",
      "qtensor_class_list"
    ],
    "__init__": [
      "self",
      "weight",
      "qtensor_class_list"
    ],
    "__repr__": [
      "self"
    ],
    "log_shape": [
      "act_mat",
      "w_autoquant",
      "bias"
    ],
    "tune_autoquant2": [
      "self",
      "fqn",
      "m",
      "batch_size",
      "inputs",
      "q_cls",
      "shapes_and_dtype",
      "time_for_best_shape"
    ],
    "to_quantized": [
      "self",
      "error_on_unseen"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_float": [
      "cls",
      "weight",
      "qtensor_class_list"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ]
  },
  "do_autoquant_bench": [
    "op"
  ],
  "_is_interpolate_mode": [
    "mode"
  ],
  "AQMixin": {
    "_autoquant_test": [
      "cls",
      "act_mat",
      "weight",
      "bias",
      "best_time",
      "mode"
    ]
  },
  "AQInt8DynamicallyQuantizedLinearWeight": {
    "from_float": [
      "cls",
      "weight"
    ],
    "_autoquant_test": [
      "cls",
      "act_mat",
      "weight",
      "bias",
      "best_time",
      "mode"
    ]
  },
  "AQInt8WeightOnlyQuantizedLinearWeight": {
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQInt8WeightOnlyQuantizedLinearWeight2": {
    "_quantized_linear_op": [
      "act_mat",
      "w_qtensor",
      "bias"
    ],
    "_autoquant_test": [
      "cls",
      "act_mat"
    ]
  },
  "AQInt8WeightOnlyQuantizedLinearWeight3": {
    "_quantized_linear_op": [
      "act_mat",
      "w_qtensor",
      "bias"
    ]
  },
  "AQInt4G32WeightOnlyQuantizedLinearWeight": {
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQInt4G64WeightOnlyQuantizedLinearWeight": {},
  "AQInt4G128WeightOnlyQuantizedLinearWeight": {},
  "AQInt4G256WeightOnlyQuantizedLinearWeight": {},
  "AQDefaultLinearWeight": {
    "__init__": [
      "self"
    ],
    "_quantized_linear_op": [
      "act_mat",
      "w_qtensor",
      "bias"
    ],
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "Float32Tensor": {
    "__init__": [
      "self",
      "weight"
    ],
    "_quantized_linear_op": [
      "act_mat",
      "w_qtensor",
      "bias"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "BFloat16Tensor": {
    "__init__": [
      "self",
      "weight"
    ],
    "_quantized_linear_op": [
      "act_mat",
      "w_qtensor",
      "bias"
    ]
  },
  "Float16Tensor": {
    "__init__": [
      "self",
      "weight"
    ],
    "_quantized_linear_op": [
      "act_mat",
      "w_qtensor",
      "bias"
    ]
  },
  "AQFloat32LinearWeight": {
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQBFloat16LinearWeight": {
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQFloat16LinearWeight": {
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQFloat8WeightOnlyQuantizedLinearWeight": {
    "_quantized_linear_op": [
      "act_mat",
      "w_qtensor",
      "bias"
    ],
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQFloat8PerRowScalingDynamicallyQuantizedLinearWeight": {
    "activation_granularity": [],
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQFloat8PerTensorScalingDynamicallyQuantizedLinearWeight": {
    "activation_granularity": [],
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "DEFAULT_AUTOQUANT_CLASS_LIST": [],
  "DEFAULT_INT4_AUTOQUANT_CLASS_LIST": [],
  "DEFAULT_FLOAT_AUTOQUANT_CLASS_LIST": [],
  "OTHER_AUTOQUANT_CLASS_LIST": [],
  "ALL_AUTOQUANT_CLASS_LIST": [],
  "_replace_with_custom_fn_if_matches_filter": [
    "model",
    "replacement_fn",
    "filter_fn",
    "cur_fqn",
    "device"
  ],
  "dict_union": [],
  "_change_linears_to_autoquantizable": [
    "model",
    "example_input",
    "fqn_to_submodule",
    "batch_size"
  ],
  "_change_autoquantizable_to_quantized": [
    "model",
    "supress_autoquant_errors"
  ],
  "autoquant_v2": [
    "model",
    "example_input",
    "qtensor_class_list",
    "filter_fn",
    "mode",
    "manual",
    "set_inductor_config",
    "supress_autoquant_errors",
    "batch_size"
  ],
  "GGUFWeightOnlyConfig": {},
  "_gguf_weight_only_transform": [
    "module",
    "config"
  ],
  "_QK_K": [],
  "GGUFQuantizedTensor": {
    "__new__": [
      "cls",
      "n_blocks_per_superblock",
      "super_block_scale_scale",
      "super_block_min_scale",
      "quantized_block_scale",
      "quantized_block_min",
      "int_data",
      "shape"
    ],
    "__init__": [
      "self",
      "n_blocks_per_superblock",
      "super_block_scale_scale",
      "super_block_min_scale",
      "quantized_block_scale",
      "quantized_block_min",
      "int_data",
      "shape"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "attributes",
      "outer_size",
      "outer_stride"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "to": [
      "self"
    ],
    "requires_grad_": [
      "self",
      "requires_grad"
    ],
    "from_float": [
      "cls",
      "input_float",
      "n_blocks_per_superblock",
      "target_dtype"
    ]
  },
  "block_shape_to_group_size": [
    "block_shape",
    "tensor_shape"
  ],
  "group_size_to_block_shapes": [
    "lut_group_size",
    "tensor_shape"
  ],
  "_quantize_row_wise_group_with_scales": [
    "input_tensor",
    "rows_per_group",
    "scale_block_shape",
    "code_dtype"
  ],
  "_dequantize_row_wise_group_with_scales": [
    "codes",
    "luts",
    "scales",
    "rows_per_group",
    "scale_group_size",
    "output_dtype"
  ],
  "quantize_flexible_grouping": [
    "input_tensor",
    "lut_block_shape",
    "code_dtype"
  ],
  "dequantize_with_flexible_grouping": [
    "codes",
    "luts",
    "lut_block_shape",
    "code_dtype",
    "output_dtype"
  ],
  "quantize_dispatch": [
    "input_tensor",
    "lut_block_shape",
    "code_dtype",
    "scale_block_shape",
    "backend"
  ],
  "dequantize_dispatch": [
    "codes",
    "luts",
    "scales",
    "lut_block_shape",
    "scale_block_shape",
    "backend",
    "code_dtype",
    "output_dtype"
  ],
  "save_quantized_data": [
    "data",
    "filepath"
  ],
  "load_quantized_data": [
    "filepath"
  ],
  "QuantizedEmbedding": {
    "__init__": [
      "self",
      "bit_width"
    ],
    "quantize_and_pack_weights": [
      "self",
      "weights",
      "group_size",
      "mapping_type"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "QuantizedEmbeddingFallback": {
    "__init__": [
      "self",
      "bit_width"
    ],
    "quantize_and_pack_weights": [
      "self",
      "weights",
      "group_size",
      "mapping_type"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "QuantizedTiedEmbedding": {
    "__init__": [
      "self",
      "bit_width",
      "unembedding_packed_weights",
      "group_size",
      "n",
      "k"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_replace_embedding_with_quantized_embedding": [
    "module",
    "kwargs",
    "fqn"
  ],
  "EmbeddingQuantizer": {
    "__init__": [
      "self",
      "weight_dtype",
      "granularity",
      "mapping_type",
      "use_fallback"
    ],
    "quantize": [
      "self",
      "model"
    ]
  },
  "_get_fqns_with_filter": [
    "module",
    "filter_fn",
    "fqn",
    "fqns"
  ],
  "get_fqns_with_filter": [
    "module",
    "filter_fn"
  ],
  "QuantizedLinear": {
    "__init__": [
      "self",
      "packed_weight",
      "n",
      "k",
      "group_size",
      "bit_width",
      "bias"
    ],
    "_forward_2d": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "get_parent_by_fqn": [
    "root",
    "fqn"
  ],
  "TiedEmbeddingQuantizer": {
    "__init__": [
      "self",
      "weight_dtype",
      "granularity",
      "mapping_type"
    ],
    "quantize": [
      "self",
      "model",
      "embedding_to_unembedding"
    ]
  },
  "quantize_codebook": [
    "input",
    "codebook",
    "scales",
    "chunk_size",
    "code_dtype"
  ],
  "dequantize_codebook": [
    "codes",
    "codebook",
    "scales",
    "output_dtype"
  ],
  "choose_qparams_codebook": [
    "input_tensor",
    "block_size",
    "scale_block_size",
    "code_dtype",
    "max_iter",
    "devices"
  ],
  "_kmeans_greedy_init": [
    "data",
    "k"
  ],
  "fit_kmeans": [
    "data",
    "k",
    "max_iter",
    "check_every",
    "rtol",
    "atol",
    "greedy_init",
    "block_size_vals",
    "devices"
  ],
  "_reshape_into_blocks": [
    "input",
    "block_size"
  ],
  "_reshape_from_blocks": [
    "blocks",
    "block_size",
    "original_shape"
  ],
  "CodebookQuantizedTensor": {
    "__new__": [
      "cls",
      "codes",
      "codebook",
      "block_size",
      "scales",
      "shape",
      "dtype",
      "strides"
    ],
    "__init__": [
      "self",
      "codes",
      "codebook",
      "block_size",
      "scales",
      "shape",
      "dtype",
      "strides"
    ],
    "__repr__": [
      "self"
    ],
    "_quantization_type": [
      "self"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_float": [
      "cls",
      "input_tensor",
      "block_size",
      "code_dtype",
      "scale_block_size",
      "chunk_size"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "detach": [
      "self"
    ],
    "requires_grad_": [
      "self",
      "requires_grad"
    ],
    "dtype": [
      "self"
    ]
  },
  "CODEBOOK_TORCH_FUNCTIONS": [],
  "function_detach": [
    "tensor"
  ],
  "function_requires_grad_": [
    "tensor"
  ],
  "CodebookWeightOnlyConfig": {},
  "codebook_weight_only": [],
  "_codebook_weight_only_transform": [
    "module",
    "config"
  ],
  "QuantizationRecipe": {},
  "get_layer_parent_by_name": [
    "model",
    "input_name"
  ],
  "quantize_module_swap": [
    "model",
    "recipe",
    "dtype"
  ],
  "replace_all_embedding_with_quantized": [
    "model",
    "recipe"
  ],
  "replace_all_linear_with_quantized_linear": [
    "model",
    "recipe"
  ],
  "initialize_model_parameters": [
    "model",
    "recipe",
    "dtype"
  ],
  "get_layer_by_name": [
    "model",
    "query_name"
  ],
  "all_quantizers_off": [
    "module"
  ],
  "all_quantizers_on": [
    "module"
  ],
  "all_activation_quantizers_off": [
    "module"
  ],
  "all_activation_quantizers_on": [
    "module"
  ],
  "all_weight_quantizers_on": [
    "module"
  ],
  "set_bit_widths_by_name": [
    "model",
    "bit_width_dict"
  ],
  "set_weight_min_max": [
    "model"
  ],
  "set_weight_mse": [
    "model",
    "num_points",
    "max_shrink",
    "norm"
  ],
  "get_batched_output": [
    "module",
    "input_data",
    "batch_size"
  ],
  "set_weight_range_activation_loss": [
    "model",
    "data",
    "batch_size",
    "num_points",
    "progressive",
    "data_getter"
  ],
  "set_activation_min_max": [
    "model",
    "data",
    "batch_size"
  ],
  "find_optimal_scales_with_loss": [
    "module",
    "loss_fn",
    "num_points",
    "max_shrink"
  ],
  "quantize_per_group_scales": [
    "model",
    "bit_width"
  ],
  "__supported_group_size_strings__": [],
  "RoundStraightThrough": {
    "forward": [
      "ctx",
      "x"
    ],
    "backward": [
      "ctx",
      "output_grad"
    ]
  },
  "IntQuantizer": {
    "__init__": [
      "self",
      "num_bits",
      "group_size",
      "dynamic",
      "quantization_mode",
      "range_learning",
      "scale_eps"
    ],
    "forward": [
      "self",
      "x"
    ],
    "q_min": [
      "self"
    ],
    "q_max": [
      "self"
    ],
    "quant_mode_to_signed": [
      "quant_mode"
    ],
    "get_scale_param_size": [
      "x",
      "group_size"
    ],
    "get_qmin_qmax": [
      "n_bits",
      "signed"
    ],
    "get_scale_offset": [
      "x",
      "group_size",
      "quantization_mode",
      "q_min",
      "q_max",
      "scale_eps"
    ],
    "quantize_forward": [
      "x",
      "scale",
      "offset",
      "q_min",
      "q_max",
      "group_size"
    ],
    "process_group_size_to_int": [
      "group_size",
      "x"
    ],
    "get_scale_from_min_max": [
      "x_min",
      "x_max",
      "q_min",
      "q_max",
      "eps"
    ],
    "get_scale_offset_from_min_max": [
      "x_min",
      "x_max",
      "q_min",
      "q_max",
      "eps"
    ],
    "set_scale_offset_to_min_max": [
      "self",
      "x"
    ]
  },
  "CodeBookQuantizer": {
    "__init__": [
      "self",
      "n_bits",
      "features",
      "codebook_dim",
      "seed"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VectorQuantizerFunction": {
    "forward": [
      "ctx",
      "inputs",
      "codebook"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "SupportedQuantizers": [],
  "WeightModuleQuantizerBase": {
    "set_weight_scale_to_min_max": [
      "self"
    ],
    "weight_scale": [
      "self"
    ],
    "quantized_weight": [
      "self"
    ]
  },
  "ExpectedError": {},
  "DataGetter": {
    "__init__": [
      "self"
    ],
    "pop": [
      "self",
      "model",
      "name"
    ],
    "get_base_name": [
      "self",
      "name"
    ],
    "initialize": [
      "self",
      "model",
      "data",
      "batch_size"
    ]
  },
  "get_module_input_data": [
    "model",
    "data",
    "module",
    "batch_size",
    "layer_kwargs"
  ],
  "LLMPTQDataGetter": {
    "__init__": [
      "self",
      "model",
      "data",
      "batch_size"
    ],
    "initialize": [
      "self",
      "model",
      "data",
      "batch_size"
    ],
    "pop": [
      "self",
      "model",
      "name"
    ],
    "get_layer_kwargs": [
      "self",
      "model",
      "data"
    ]
  },
  "kmeans_codebook": [
    "model",
    "niter",
    "nredo",
    "dtype"
  ],
  "Int8LutTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "packed_weights",
      "bit_width",
      "block_size",
      "shape",
      "dtype",
      "packed_weights_has_bias"
    ],
    "__init__": [
      "self",
      "packed_weights",
      "bit_width",
      "block_size",
      "shape",
      "dtype",
      "packed_weights_has_bias"
    ],
    "_quantization_type": [
      "self"
    ],
    "to": [
      "self"
    ],
    "_get_lut_params": [
      "cls",
      "tensor"
    ],
    "from_intx_unpacked_to_int8_tensor": [
      "cls",
      "tensor"
    ]
  },
  "_linear_impl_2d": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_get_linear_extra_repr_for_lut": [
    "self"
  ],
  "GroupwiseLutWeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_groupwise_lut_weight_transform": [
    "module",
    "config"
  ],
  "get_pack_op": [
    "weight_nbit"
  ],
  "get_linear_op": [
    "weight_nbit"
  ],
  "CodebookQuantizedPackedTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "packed_weight",
      "bit_width",
      "lut_block_size",
      "scale_block_size",
      "shape",
      "dtype"
    ],
    "__init__": [
      "self",
      "packed_weight",
      "bit_width",
      "lut_block_size",
      "scale_block_size",
      "shape",
      "dtype"
    ],
    "from_unpacked": [
      "cls",
      "int_data",
      "luts",
      "scales",
      "bit_width",
      "lut_block_size",
      "scale_block_size",
      "original_shape",
      "bias"
    ],
    "from_codebook_quantized_tensor": [
      "cls",
      "tensor"
    ]
  },
  "quant_lib": [],
  "choose_qparams_and_quantize_codebook_coreml": [
    "input_tensor",
    "code_dtype",
    "block_size",
    "force_kmeans1d",
    "cluster_dim",
    "vector_axis"
  ],
  "graph_tabular_log": [],
  "summary_headers": [],
  "DEBUG_LINEARS_CALL_COUNTER": [],
  "maybe_short_name": [
    "torch_fn"
  ],
  "get_meta_val": [
    "n"
  ],
  "get_stack_summary": [
    "n"
  ],
  "is_first_node_of_dual_linear": [
    "gm",
    "n"
  ],
  "debug_single_linear": [
    "gm",
    "linear_node",
    "linear_mod",
    "debug_logs_filename",
    "subgraph_idx"
  ],
  "extract_linear_subgraph": [
    "old_gm",
    "old_linear_node",
    "old_linear_mod",
    "subgraph_save_filename"
  ],
  "print_and_append_to_logs": [
    "logger",
    "filename",
    "s"
  ],
  "prepare_target_folder": [
    "target_folder"
  ],
  "debug_linears_for_float8": [
    "g",
    "target_folder",
    "linear_mod_filter_fn",
    "linear_node_filter_fn"
  ],
  "eval": [
    "model",
    "tokenizer",
    "num_PPL_eval_samples",
    "fqn_to_config"
  ],
  "get_initial_samples": [
    "num_BO_initial_samples"
  ],
  "run_sequential_BO": [
    "device",
    "checkpoint",
    "num_PPL_eval_samples",
    "num_trials",
    "model_size_constraint",
    "history_output",
    "parameters_list",
    "initial_samples"
  ],
  "eval_in_parallel": [
    "gpu_id",
    "checkpoint",
    "num_PPL_eval_samples",
    "config",
    "return_dict",
    "proc_id",
    "trial_id"
  ],
  "run_parallel_BO": [
    "device",
    "checkpoint",
    "num_PPL_eval_samples",
    "num_trials",
    "model_size_constraint",
    "gpu_list",
    "history_output",
    "parameters_list",
    "initial_samples"
  ],
  "group_product": [
    "xs",
    "ys"
  ],
  "get_wikitext2": [
    "nsamples",
    "seed",
    "seqlen",
    "tokenizer"
  ],
  "dataloader_hv_product": [
    "layerid",
    "params",
    "device",
    "v",
    "data",
    "nsamples",
    "model",
    "max_seqlen",
    "criterion"
  ],
  "cal_trace": [
    "layerid",
    "params",
    "device",
    "data",
    "nsamples",
    "model",
    "max_iter",
    "max_seqlen",
    "criterion"
  ],
  "IntNWeightOnlyConfig": {},
  "intN_weight_only": [],
  "_intN_weight_only_transform": [
    "module",
    "config"
  ],
  "write_history_to_csv": [
    "history",
    "output_file",
    "keyword"
  ],
  "quantize_by_fqn_to_config": [
    "model",
    "device",
    "fqn_to_config"
  ],
  "cal_wikitext_ppl": [
    "model",
    "tokenizer",
    "limit"
  ],
  "cal_model_size": [
    "model",
    "fqn_to_config"
  ],
  "load_model": [
    "repo_id",
    "device"
  ],
  "load_parameters_from_json": [
    "json_path"
  ],
  "load_initial_samples": [
    "json_path"
  ],
  "default_device": [],
  "decode_n_tokens": [
    "model",
    "cur_token",
    "input_pos",
    "num_new_tokens",
    "callback"
  ],
  "generate": [
    "model",
    "prompt",
    "max_new_tokens"
  ],
  "cal_throughput": [
    "model",
    "tokenizer",
    "device",
    "prompt",
    "interactive",
    "num_samples",
    "max_new_tokens",
    "top_k",
    "temperature",
    "checkpoint_path",
    "quantization",
    "kv_cache_quantization",
    "save",
    "compile",
    "compile_prefill",
    "profile",
    "precision",
    "write_result"
  ],
  "define_parameter_list": [],
  "cal_FIT": [
    "device",
    "data",
    "nsamples",
    "model",
    "max_iter",
    "max_seqlen",
    "criterion",
    "num_layers"
  ],
  "del_attr": [
    "obj",
    "names"
  ],
  "set_attr": [
    "obj",
    "names",
    "val"
  ],
  "make_functional": [
    "mod",
    "layer_id"
  ],
  "PrototypeFloat8Tensor": {
    "tensor_data_names": [],
    "optional_tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "act_quant_scale",
      "output_act_quant_scale",
      "block_size",
      "mm_config",
      "act_quant_kwargs",
      "kernel_preference",
      "dtype",
      "output_act_quant_kwargs"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale",
      "act_quant_scale",
      "output_act_quant_scale",
      "block_size",
      "mm_config",
      "act_quant_kwargs",
      "kernel_preference",
      "dtype",
      "output_act_quant_kwargs"
    ],
    "__repr__": [
      "self"
    ],
    "_quantization_type": [
      "self"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "from_hp": [
      "cls",
      "hp_tensor",
      "float8_dtype",
      "granularity",
      "mm_config",
      "hp_value_lb",
      "hp_value_ub",
      "kernel_preference",
      "act_quant_kwargs",
      "scale",
      "act_quant_scale",
      "output_act_quant_scale",
      "output_act_quant_kwargs"
    ]
  },
  "_float8_addmm_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_quantize_and_scaled_conv3d": [
    "input_tensor",
    "weight_tensor",
    "bias",
    "stride",
    "padding",
    "dilation"
  ],
  "_choose_quant_func_and_quantize_tensor": [
    "tensor",
    "quant_kwargs",
    "act_quant_scale"
  ],
  "_same_metadata": [
    "self",
    "src"
  ],
  "get_gemlite_quant_kwargs": [
    "bit_width",
    "group_size",
    "dtype"
  ],
  "get_gemlite_aqt_kwargs": [
    "weight",
    "group_size",
    "bit_width",
    "packing_bitwidth",
    "mode",
    "use_hqq"
  ],
  "GemlitePackedLayout": {},
  "GemliteAQTTensorImpl": {
    "__new__": [
      "cls",
      "packed_weight",
      "scale",
      "zero_point",
      "gemlite_kwargs",
      "_layout"
    ],
    "__init__": [
      "self",
      "packed_weight",
      "scale",
      "zero_point",
      "gemlite_kwargs",
      "_layout"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "get_plain": [
      "self"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "get_layout": [
      "self"
    ],
    "block_size": [
      "self"
    ]
  },
  "_linear_fp_act_int4_weight_gemlite_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp_act_int4_weight_gemlite_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_DTYPE_TO_BIT_WIDTH": [],
  "_BIT_WIDTH_TO_DTYPE": [],
  "UintxTensor": {
    "bits_to_shard": [],
    "__new__": [
      "cls",
      "shards",
      "packed_shape",
      "bit_width",
      "pack_dim"
    ],
    "__init__": [
      "self",
      "shards",
      "packed_shape",
      "bit_width",
      "pack_dim"
    ],
    "get_shards": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "get_plain": [
      "self"
    ],
    "apply_transformation": [
      "self",
      "fn"
    ],
    "apply_fn_to_shards": [
      "self",
      "fn"
    ],
    "from_uint8": [
      "cls",
      "int_data",
      "dtype",
      "pack_dim"
    ],
    "_get_to_kwargs": [
      "self"
    ],
    "to": [
      "self"
    ]
  },
  "to_uintx": [],
  "UintxLayout": {
    "post_process": [
      "self",
      "input",
      "scale",
      "zero_point",
      "block_size"
    ]
  },
  "UintxAQTTensorImpl": {
    "get_plain": [
      "self"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ]
  },
  "BlockSparseLayout": {},
  "BlockSparseAQTTensorImpl": {
    "__slots__": [],
    "__new__": [
      "cls",
      "shape",
      "bsr_crow_indices",
      "bsr_col_indices",
      "bsr_values",
      "scale",
      "zero_point",
      "_layout",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "shape",
      "bsr_crow_indices",
      "bsr_col_indices",
      "bsr_values",
      "scale",
      "zero_point",
      "_layout",
      "requires_grad"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "inner_tensors",
      "tensor_meta",
      "outer_size",
      "outer_stride"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "get_plain": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "func"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ]
  },
  "_linear_int8_act_int8_weight_block_sparse_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_int8_act_int8_weight_block_sparse_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "Int8DynamicActInt4WeightCPULayout": {},
  "DA8W4CPUAQTTensorImpl": {
    "__new__": [
      "cls",
      "packed_weight",
      "scales",
      "qzeros",
      "compensation",
      "transposed",
      "_layout"
    ],
    "__init__": [
      "self",
      "packed_weight",
      "scales",
      "qzeros",
      "compensation",
      "transposed",
      "_layout"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "block_size": [
      "self"
    ],
    "get_plain": [
      "self"
    ]
  },
  "_aqt_is_uint8": [
    "aqt"
  ],
  "_aqt_is_int8": [
    "aqt"
  ],
  "_aqt_is_uint4": [
    "aqt"
  ],
  "_linear_int8_act_int4_weight_cpu_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_int8_act_int4_weight_cpu_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "NearlyDiagonalSparsifier": {
    "__init__": [
      "self",
      "nearliness"
    ],
    "update_mask": [
      "self",
      "module",
      "tensor_name",
      "nearliness"
    ]
  },
  "module_contains_param": [
    "module",
    "parametrization"
  ],
  "swap_module": [
    "mod",
    "mapping"
  ],
  "module_to_fqn": [
    "model",
    "module",
    "prefix"
  ],
  "fqn_to_module": [
    "model",
    "path"
  ],
  "get_arg_info_from_tensor_fqn": [
    "model",
    "tensor_fqn"
  ],
  "FakeSparsity": {
    "__init__": [
      "self",
      "mask"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SUPPORTED_MODULES": [],
  "KEYS_NOT_IN_STATE_DICT": [],
  "BaseSparsifier": {
    "__init__": [
      "self",
      "defaults"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "__repr__": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict",
      "strict"
    ],
    "make_config_from_model": [
      "self",
      "model",
      "SUPPORTED_MODULES"
    ],
    "prepare": [
      "self",
      "model",
      "config"
    ],
    "_prepare": [
      "self"
    ],
    "squash_mask": [
      "self",
      "params_to_keep",
      "params_to_keep_per_layer"
    ],
    "convert": [
      "self",
      "module",
      "mapping",
      "inplace",
      "parameterization"
    ],
    "step": [
      "self",
      "use_path"
    ],
    "update_mask": [
      "self",
      "module",
      "tensor_name"
    ]
  },
  "_flat_idx_to_2d": [
    "idx",
    "shape"
  ],
  "WeightNormSparsifier": {
    "__init__": [
      "self",
      "sparsity_level",
      "sparse_block_shape",
      "zeros_per_block",
      "norm"
    ],
    "_scatter_fold_block_mask": [
      "self",
      "output_shape",
      "dim",
      "indices",
      "block_shape",
      "mask",
      "input_shape",
      "device"
    ],
    "_make_tensor_mask": [
      "self",
      "data",
      "input_shape",
      "sparsity_level",
      "sparse_block_shape",
      "mask"
    ],
    "_make_block_mask": [
      "self",
      "data",
      "sparse_block_shape",
      "zeros_per_block",
      "mask"
    ],
    "update_mask": [
      "self",
      "module",
      "tensor_name",
      "sparsity_level",
      "sparse_block_shape",
      "zeros_per_block"
    ]
  },
  "SaliencyPruner": {
    "update_mask": [
      "self",
      "module",
      "tensor_name"
    ]
  },
  "_remove_bias_handles": [
    "module"
  ],
  "_get_adjusted_next_layer_bias": [
    "next_layer",
    "pruned_biases",
    "mask"
  ],
  "_prune_module_bias": [
    "module",
    "mask"
  ],
  "_propogate_module_bias": [
    "module",
    "mask"
  ],
  "_prune_linear_helper": [
    "linear"
  ],
  "prune_linear": [
    "linear"
  ],
  "prune_linear_linear": [
    "linear1",
    "linear2"
  ],
  "prune_linear_activation_linear": [
    "linear1",
    "activation",
    "linear2"
  ],
  "_prune_conv2d_helper": [
    "conv2d"
  ],
  "prune_conv2d_padded": [
    "conv2d_1"
  ],
  "prune_conv2d": [
    "conv2d"
  ],
  "prune_conv2d_conv2d": [
    "conv2d_1",
    "conv2d_2"
  ],
  "prune_conv2d_activation_conv2d": [
    "conv2d_1",
    "activation",
    "conv2d_2"
  ],
  "prune_conv2d_pool_activation_conv2d": [
    "c1",
    "pool",
    "activation",
    "c2"
  ],
  "prune_conv2d_activation_pool_conv2d": [
    "c1",
    "activation",
    "pool",
    "c2"
  ],
  "prune_conv2d_pool_flatten_linear": [
    "conv2d",
    "pool",
    "flatten",
    "linear"
  ],
  "prune_lstm_output_linear": [
    "lstm",
    "getitem",
    "linear"
  ],
  "prune_lstm_output_layernorm_linear": [
    "lstm",
    "getitem",
    "layernorm",
    "linear"
  ],
  "_get_supported_structured_pruning_modules": [],
  "_get_supported_activation_functions": [],
  "_get_supported_activation_modules": [],
  "_get_default_structured_pruning_patterns": [],
  "BaseStructuredSparsifier": {
    "__init__": [
      "self",
      "defaults",
      "patterns"
    ],
    "make_config_from_model": [
      "self",
      "model",
      "SUPPORTED_MODULES"
    ],
    "_prepare": [
      "self"
    ],
    "prune": [
      "self"
    ]
  },
  "FakeStructuredSparsity": {
    "__init__": [
      "self",
      "mask"
    ],
    "forward": [
      "self",
      "x"
    ],
    "state_dict": [
      "self"
    ]
  },
  "BiasHook": {
    "__init__": [
      "self",
      "parametrization",
      "prune_bias"
    ],
    "__call__": [
      "self",
      "module",
      "input",
      "output"
    ]
  },
  "_match": [
    "modules",
    "node",
    "current"
  ],
  "apply_match": [
    "modules",
    "pattern",
    "node",
    "matched_node_pattern"
  ],
  "FPGMPruner": {
    "__init__": [
      "self",
      "sparsity_level",
      "dist"
    ],
    "_compute_distance": [
      "self",
      "t"
    ],
    "update_mask": [
      "self",
      "module",
      "tensor_name",
      "sparsity_level"
    ]
  },
  "LSTMSaliencyPruner": {
    "update_mask": [
      "self",
      "module",
      "tensor_name"
    ]
  },
  "BaseScheduler": {
    "__init__": [
      "self",
      "sparsifier",
      "last_epoch",
      "verbose"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "get_last_sl": [
      "self"
    ],
    "get_sl": [
      "self"
    ],
    "print_sl": [
      "self",
      "is_verbose",
      "group",
      "sl",
      "epoch"
    ],
    "__repr__": [
      "self"
    ],
    "step": [
      "self",
      "epoch"
    ],
    "_make_sure_a_list": [
      "self",
      "var"
    ]
  },
  "LambdaSL": {
    "__init__": [
      "self",
      "sparsifier",
      "sl_lambda",
      "last_epoch",
      "verbose"
    ],
    "get_sl": [
      "self"
    ]
  },
  "_clamp": [
    "x",
    "lo",
    "hi"
  ],
  "CubicSL": {
    "__init__": [
      "self",
      "sparsifier",
      "init_sl",
      "init_t",
      "delta_t",
      "total_t",
      "initially_zero",
      "last_epoch",
      "verbose"
    ],
    "sparsity_compute_fn": [
      "s_0",
      "s_f",
      "t",
      "t_0",
      "dt",
      "n",
      "initially_zero"
    ],
    "get_sl": [
      "self"
    ]
  },
  "fp8_sdpa_parallel": [
    "query",
    "key",
    "value",
    "attn_mask",
    "dropout_p",
    "is_causal",
    "scale",
    "num_chunks"
  ],
  "test_stable_diffusion_regular_sdpa": [],
  "test_stable_diffusion_fp8_sdpa": [],
  "test_fp8_sdpa_numerical_accuracy": [],
  "test_fp8_sdpa_benchmark": [],
  "test_fp8_sdpa_profiler": [],
  "_original_sdpa": [],
  "fp8_sdpa_context": [],
  "wrap_module_with_fp8_sdpa": [
    "module"
  ],
  "_compute_num_chunks": [
    "tensor",
    "S"
  ],
  "qkv_phase1_kernel": [
    "q_ptr",
    "k_ptr",
    "v_ptr",
    "partial_max_ptr",
    "stride_b",
    "stride_h",
    "stride_s",
    "stride_d",
    "S",
    "D",
    "chunk_size",
    "num_chunks",
    "H",
    "BLOCK_SIZE"
  ],
  "qkv_reduce_kernel": [
    "partial_max_ptr",
    "q_scale_ptr",
    "k_scale_ptr",
    "v_scale_ptr",
    "q_descale_ptr",
    "k_descale_ptr",
    "v_descale_ptr",
    "H",
    "num_chunks"
  ],
  "qkv_phase2_kernel": [
    "q_ptr",
    "k_ptr",
    "v_ptr",
    "q_out_ptr",
    "k_out_ptr",
    "v_out_ptr",
    "q_scale_ptr",
    "k_scale_ptr",
    "v_scale_ptr",
    "stride_b",
    "stride_h",
    "stride_s",
    "stride_d",
    "S",
    "D",
    "H",
    "chunk_size",
    "BLOCK_SIZE"
  ],
  "qkv_quantize_func": [
    "q",
    "k",
    "v",
    "num_chunks"
  ],
  "q_phase1_kernel": [
    "x_ptr",
    "partial_max_ptr",
    "stride_b",
    "stride_h",
    "stride_s",
    "stride_d",
    "S",
    "D",
    "chunk_size",
    "num_chunks",
    "H",
    "BLOCK_SIZE"
  ],
  "q_reduce_kernel": [
    "partial_max_ptr",
    "scale_ptr",
    "descale_ptr",
    "H",
    "num_chunks"
  ],
  "q_phase2_kernel": [
    "x_ptr",
    "x_out_ptr",
    "scale_ptr",
    "stride_b",
    "stride_h",
    "stride_s",
    "stride_d",
    "S",
    "D",
    "H",
    "chunk_size",
    "BLOCK_SIZE"
  ],
  "q_quantize_func": [
    "x",
    "num_chunks"
  ],
  "kv_phase1_kernel": [
    "k_ptr",
    "v_ptr",
    "partial_max_ptr",
    "stride_b",
    "stride_h",
    "stride_s",
    "stride_d",
    "S",
    "D",
    "chunk_size",
    "num_chunks",
    "H",
    "BLOCK_SIZE"
  ],
  "kv_reduce_kernel": [
    "partial_max_ptr",
    "k_scale_ptr",
    "v_scale_ptr",
    "k_descale_ptr",
    "v_descale_ptr",
    "H",
    "num_chunks"
  ],
  "kv_phase2_kernel": [
    "k_ptr",
    "v_ptr",
    "k_out_ptr",
    "v_out_ptr",
    "k_scale_ptr",
    "v_scale_ptr",
    "stride_b",
    "stride_h",
    "stride_s",
    "stride_d",
    "S",
    "D",
    "H",
    "chunk_size",
    "BLOCK_SIZE"
  ],
  "kv_quantize_func": [
    "k",
    "v",
    "num_chunks"
  ],
  "fp8_sdpa_quantize_func": [
    "q",
    "k",
    "v",
    "num_chunks"
  ],
  "SmoothQuantObserver": {
    "__init__": [
      "self",
      "weight",
      "alpha"
    ],
    "forward": [
      "self",
      "input"
    ],
    "calculate_qparams": [
      "self",
      "weight_quant_kwargs"
    ]
  },
  "SmoothQuantObservedLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "obs",
      "is_bias",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "input"
    ],
    "from_float": [
      "cls",
      "float_linear",
      "obs"
    ]
  },
  "SmoothQuantConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_smooth_quant_transform": [
    "module",
    "config"
  ],
  "compare_models": [
    "model_id",
    "alpha",
    "tasks",
    "max_seq_length",
    "calibration_limit",
    "device",
    "model_save_path",
    "model_save_hf_hub_path"
  ],
  "create_parser": [],
  "instantiate_module": [
    "module_path",
    "module_suffix"
  ],
  "is_dtensor": [
    "x"
  ],
  "channel_bucketize": [
    "input",
    "boundaries",
    "right"
  ],
  "QuantConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "create_param_groups_and_group_quantizer_map": [
    "model",
    "quant_configs_and_filter_fns"
  ],
  "create_optimizer": [
    "model",
    "quant_configs_and_filter_fns",
    "base_optimizer_cls",
    "base_optimizer_kwargs"
  ],
  "UnifTorchaoQuantizer": {
    "__init__": [
      "self",
      "mapping_type",
      "target_dtype",
      "quant_min",
      "quant_max",
      "eps",
      "preserve_zero",
      "zero_point_domain"
    ],
    "_init_quant_min_max": [
      "self",
      "b"
    ],
    "get_quant_size": [
      "self",
      "b"
    ],
    "quantize": [
      "self",
      "p",
      "b",
      "dim"
    ]
  },
  "StretchedUnifTorchaoQuantizer": {
    "__init__": [
      "self",
      "b",
      "int_shift"
    ],
    "get_quant_size": [
      "self",
      "b"
    ]
  },
  "Int4UnifTorchaoQuantizer": {
    "__init__": [
      "self"
    ]
  },
  "choose_qparams_stretched_affine": [
    "input_float",
    "mapping_type",
    "block_size",
    "target_dtype",
    "b",
    "quant_min",
    "quant_max",
    "eps",
    "scale_dtype",
    "zero_point_dtype"
  ],
  "quantize_stretched_affine": [
    "input_float",
    "block_size",
    "scale",
    "zero_point",
    "target_dtype",
    "quant_min",
    "quant_max"
  ],
  "Quantizer": {
    "__init__": [
      "self",
      "center"
    ],
    "get_quant_size": [
      "self",
      "b"
    ],
    "quantize": [
      "self",
      "p",
      "b"
    ],
    "remove_mean": [
      "p",
      "dim"
    ]
  },
  "StretchedIntxWeightConfig": {},
  "_int8_dynamic_activation_stretched_intx_transform": [
    "module",
    "config"
  ],
  "_get_config_from_quantizer": [
    "quantizer",
    "weight_only",
    "device",
    "b",
    "block_size",
    "version"
  ],
  "_is_hf_model": [
    "model"
  ],
  "_attach_hf_quantization_config": [
    "model",
    "filter_fns",
    "configs",
    "module_to_config"
  ],
  "binary_sign": [
    "input"
  ],
  "binary_quant_residue": [
    "u",
    "vs"
  ],
  "compute_v_per_channel": [
    "p",
    "dim",
    "ternary"
  ],
  "LSBQuantizer": {
    "__init__": [
      "self",
      "center",
      "optimal",
      "ternary_multiplier"
    ],
    "get_quant_size": [
      "self",
      "b"
    ],
    "quantize": [
      "self",
      "p",
      "b",
      "dim"
    ],
    "quantize_greedy": [
      "p",
      "b",
      "dim"
    ],
    "quantize_optimal_2bits": [
      "p",
      "dim"
    ],
    "quantize_optimal_ternary": [
      "p",
      "dim"
    ],
    "quantize_simple_ternary": [
      "p",
      "multiplier",
      "dim"
    ]
  },
  "get_q_max": [
    "q",
    "b",
    "dim",
    "scale_method"
  ],
  "UnifQuantizer": {
    "__init__": [
      "self",
      "center",
      "scale_method",
      "int_shift",
      "zero_point"
    ],
    "get_quant_size": [
      "self",
      "b"
    ],
    "quantize": [
      "self",
      "p",
      "b",
      "dim"
    ]
  },
  "MaxUnifQuantizer": {
    "__init__": [
      "self",
      "center",
      "scale_method",
      "int_shift",
      "zero_point"
    ]
  },
  "AsymUnifQuantizer": {
    "get_quant_size": [
      "self",
      "b"
    ],
    "quantize": [
      "self",
      "p",
      "b",
      "dim"
    ]
  },
  "TernaryUnifQuantizer": {
    "get_quant_size": [
      "self",
      "b"
    ],
    "quantize": [
      "self",
      "p",
      "b",
      "dim"
    ]
  },
  "ProxMap": {
    "apply_": [
      "self",
      "p",
      "q",
      "Q",
      "step_count"
    ]
  },
  "ProxHardQuant": {
    "apply_": [
      "self",
      "p",
      "q",
      "Q",
      "step_count",
      "dim"
    ]
  },
  "amp_custom_fwd": [
    "cast_inputs"
  ],
  "normalized_mirror_sigmoid": [
    "t",
    "t1",
    "t2",
    "s",
    "c"
  ],
  "ProxPARQ": {
    "__init__": [
      "self",
      "anneal_start",
      "anneal_end",
      "steepness",
      "anneal_center"
    ],
    "apply_": [
      "self",
      "p",
      "q",
      "Q",
      "step_count",
      "dim"
    ]
  },
  "ProxBinaryRelax": {
    "__init__": [
      "self",
      "anneal_start",
      "anneal_end"
    ],
    "apply_": [
      "self",
      "p",
      "q",
      "Q",
      "step_count",
      "dim"
    ]
  },
  "QuantOptimizer": {
    "__init__": [
      "self",
      "base_optimizer",
      "quantizer",
      "prox_map",
      "warmup_steps",
      "quant_period",
      "quant_per_channel",
      "quant_shrink",
      "anneal_wd_frac",
      "group_quantizer_map"
    ],
    "__getattribute__": [
      "self",
      "name"
    ],
    "__repr__": [
      "self"
    ],
    "state": [
      "self"
    ],
    "num_steps": [
      "self"
    ],
    "quantize_": [
      "p",
      "quants",
      "quantizer",
      "b",
      "dim"
    ],
    "regularized_param_groups": [
      "self"
    ],
    "_param_sets": [
      "self"
    ],
    "get_filter_fns": [
      "self",
      "module"
    ],
    "_get_quantizer": [
      "self",
      "group_idx"
    ],
    "torchao_convert": [
      "self",
      "model",
      "weight_only",
      "embed_weight_only"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "step": [
      "self",
      "closure"
    ],
    "restore_latent_params": [
      "self"
    ],
    "save_latent_params": [
      "self"
    ]
  },
  "_AdamBase": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "add_param_group": [
      "self",
      "param_group"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "_subclass_zeros": [
      "p",
      "signed",
      "block_size"
    ],
    "_new_buffer": [
      "self",
      "p",
      "signed"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "single_param_adam": [
    "p",
    "grad",
    "step",
    "exp_avg",
    "exp_avg_sq",
    "max_exp_avg_sq",
    "lr",
    "beta1",
    "beta2",
    "weight_decay",
    "eps",
    "IS_ADAMW",
    "BF16_STOCHASTIC_ROUND"
  ],
  "Adam8bit": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "_subclass_zeros": [
      "p",
      "signed",
      "block_size"
    ]
  },
  "Adam4bit": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "_subclass_zeros": [
      "p",
      "signed",
      "block_size"
    ]
  },
  "AdamFp8": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "_subclass_zeros": [
      "p",
      "signed",
      "block_size"
    ]
  },
  "AdamW8bit": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "_subclass_zeros": [
      "p",
      "signed",
      "block_size"
    ]
  },
  "AdamW4bit": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "_subclass_zeros": [
      "p",
      "signed",
      "block_size"
    ]
  },
  "AdamWFp8": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "_subclass_zeros": [
      "p",
      "signed",
      "block_size"
    ]
  },
  "_AdamW": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ]
  },
  "DTYPE": [],
  "quantize_fp8": [
    "input",
    "block_size"
  ],
  "OptimStateFp8": {
    "tensor_attrs": [],
    "__new__": [
      "cls",
      "codes",
      "scale"
    ],
    "__init__": [
      "self",
      "codes",
      "scale"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "zeros": [
      "cls",
      "shape",
      "block_size",
      "device"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_optim_state_fp8_c10d_ops": [],
  "get_qmap_signed": [],
  "get_qmap_unsigned": [],
  "OptimState8bit": {
    "tensor_attrs": [],
    "__new__": [
      "cls",
      "codes",
      "scale",
      "qmap",
      "signed"
    ],
    "__init__": [
      "self",
      "codes",
      "scale",
      "qmap",
      "signed"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "zeros": [
      "cls",
      "shape",
      "signed",
      "block_size",
      "device"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_optim_state_8bit_c10d_ops": [],
  "OptimState4bit": {
    "tensor_attrs": [],
    "__new__": [
      "cls",
      "codes",
      "scale",
      "qmap",
      "signed",
      "shape"
    ],
    "__init__": [
      "self",
      "codes",
      "scale",
      "qmap",
      "signed",
      "shape"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "zeros": [
      "cls",
      "shape",
      "signed",
      "block_size",
      "device"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_optim_state_4bit_c10d_ops": [],
  "create_dynamic_map": [
    "signed",
    "max_exponent_bits",
    "total_bits"
  ],
  "scale_tensor": [
    "input",
    "block_size"
  ],
  "quantize_8bit_with_qmap": [
    "input",
    "qmap"
  ],
  "quantize_4bit_with_qmap": [
    "input",
    "qmap"
  ],
  "dequant_with_qmap": [
    "codes",
    "qmap",
    "scale"
  ],
  "_fp32_to_bf16_sr": [
    "_x_f32"
  ],
  "CPUOffloadOptimizer": {
    "__init__": [
      "self",
      "params",
      "optimizer_class"
    ],
    "step": [
      "self",
      "closure"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "param_groups": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "LinearActivationQuantizedTensor": {
    "__new__": [
      "cls",
      "original_weight_tensor",
      "input_quant_func",
      "quant_kwargs"
    ],
    "__init__": [
      "self",
      "original_weight_tensor",
      "input_quant_func",
      "quant_kwargs"
    ],
    "__repr__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "_quantized_linear_op": [
      "input_tensor",
      "weight_tensor",
      "bias"
    ],
    "from_float": [
      "cls",
      "input_float",
      "input_quant_func",
      "quant_kwargs"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "to": [
      "self"
    ]
  },
  "to_linear_activation_quantized": [],
  "WeightTensorWithLinearActivationQuantizationMetadata": {
    "__new__": [
      "cls",
      "original_weight_tensor",
      "input_quant_func_static",
      "scale",
      "zero_point",
      "quant_kwargs"
    ],
    "__init__": [
      "self",
      "original_weight_tensor",
      "input_quant_func_static",
      "scale",
      "zero_point",
      "quant_kwargs"
    ],
    "__repr__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "_quantized_linear_op": [
      "input_tensor",
      "weight_tensor",
      "bias"
    ],
    "from_float": [
      "cls",
      "input_float",
      "input_quant_func",
      "scale",
      "zero_point",
      "quant_kwargs"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "to": [
      "self"
    ]
  },
  "to_weight_tensor_with_linear_activation_quantization_metadata": [],
  "LAYOUT_TO_ZERO_POINT_DOMAIN": [],
  "LAYOUT_TO_PRESERVE_ZEROS": [],
  "_get_subclass_inserter": [
    "cls",
    "enable_parametrization"
  ],
  "swap_conv2d_1x1_to_linear": [
    "model",
    "filter_fn"
  ],
  "insert_observers_": [
    "model",
    "input_observer",
    "weight_observer"
  ],
  "_embedding_extra_repr": [
    "self"
  ],
  "_module_extra_repr": [
    "self",
    "original_extra_repr",
    "parameter_name"
  ],
  "_get_linear_subclass_inserter": [
    "constructor"
  ],
  "quantize_": [
    "model",
    "config",
    "filter_fn",
    "device"
  ],
  "_int8_asymm_per_token_quant": [
    "x"
  ],
  "_uint8_asymm_per_token_quant": [
    "x"
  ],
  "_int8_symm_per_token_quant": [
    "x"
  ],
  "Int8DynamicActivationIntxWeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_int8_dynamic_activation_intx_weight_quantize_tensor": [
    "weight",
    "bias",
    "config"
  ],
  "_int8_dynamic_activation_intx_weight_transform": [
    "module",
    "config"
  ],
  "Int4WeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_int4_weight_only_quantize_tensor": [
    "weight",
    "config"
  ],
  "Float8DynamicActivationInt4WeightConfig": {},
  "_float8_dynamic_activation_int4_weight_transform": [
    "module",
    "config"
  ],
  "Int8WeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_int8_weight_only_quantize_tensor": [
    "weight",
    "config"
  ],
  "_int8_weight_only_transform": [
    "module",
    "config"
  ],
  "_int8_symm_per_token_reduced_range_quant": [
    "x"
  ],
  "_int8_symm_per_token_reduced_range_quant_noop_decode": [
    "x"
  ],
  "_float8_cutlass_quant": [
    "x",
    "target_dtype"
  ],
  "_float8_cutlass_quant_sparse": [
    "x",
    "target_dtype"
  ],
  "Int8DynamicActivationInt8WeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_int8_dynamic_activation_int8_weight_quantize_tensor": [
    "weight",
    "config"
  ],
  "_int8_dynamic_activation_int8_weight_transform": [
    "module",
    "config"
  ],
  "Int8StaticActivationInt8WeightConfig": {
    "__post_init__": [
      "self"
    ],
    "get_act_quant_kwargs": [
      "self"
    ]
  },
  "_int8_static_activation_int8_weight_transform": [
    "module",
    "config"
  ],
  "int8_dynamic_activation_int8_semi_sparse_weight": [],
  "Float8WeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_float8_weight_only_quant_tensor": [
    "weight",
    "config"
  ],
  "_float8_weight_only_transform": [
    "module",
    "config"
  ],
  "_input_activation_quant_func_fp8": [
    "x",
    "activation_granularity",
    "activation_dtype",
    "scale",
    "zero_point"
  ],
  "Float8DynamicActivationFloat8WeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_float8_dynamic_activation_float8_weight_quantize_tensor": [
    "weight",
    "config"
  ],
  "_float8_dynamic_activation_float8_weight_transform": [
    "module",
    "config"
  ],
  "Float8DynamicActivationFloat8SemiSparseWeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_float8_dynamic_activation_float8_semi_sparse_weight_transform": [
    "module",
    "config"
  ],
  "_adjust_scale_dtype_in_intx_unpacked_tensor": [
    "intx_unpacked_tensor",
    "hp_tensor",
    "scale_dtype"
  ],
  "IntxWeightOnlyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_intx_weight_only_quantize_tensor": [
    "weight",
    "config"
  ],
  "_intx_weight_only_transform": [
    "module",
    "config"
  ],
  "FqnToConfig": {
    "__post_init__": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "ModuleFqnToConfig": [],
  "CUSTOM_PARAM_QUANTIZATION_SUPPORTED_CONFIGS": [],
  "_fqn_to_config_handler": [
    "module",
    "fqn",
    "config"
  ],
  "fqn_matches_fqn_config": [
    "fqn",
    "config"
  ],
  "_module_param_matches_fqn_config": [
    "module",
    "fqn",
    "config"
  ],
  "_unwrap_float8_linear": [
    "module"
  ],
  "WeightTensorWithLinearActivationScaleMetadata": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "original_weight_tensor",
      "scale"
    ],
    "__init__": [
      "self",
      "original_weight_tensor",
      "scale"
    ],
    "_quantization_type": [
      "self"
    ],
    "_quantized_linear_op": [
      "input_tensor",
      "weight_tensor",
      "bias"
    ],
    "from_float": [
      "cls",
      "input_float",
      "scale"
    ]
  },
  "to_weight_tensor_with_linear_activation_scale_metadata": [],
  "TwoStepQuantizer": {
    "prepare": [
      "self",
      "model"
    ],
    "convert": [
      "self",
      "model"
    ]
  },
  "_check_linear_int4_k": [
    "k",
    "groupsize",
    "inner_k_tiles"
  ],
  "linear_forward_int4": [
    "x",
    "weight_int4pack",
    "scales_and_zeros",
    "out_features",
    "groupsize",
    "precision",
    "scales_precision"
  ],
  "WeightOnlyInt4Linear": {
    "__constants__": [],
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "device",
      "dtype",
      "groupsize",
      "inner_k_tiles",
      "precision",
      "scales_precision"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "_replace_linear_int4": [
    "module",
    "groupsize",
    "inner_k_tiles",
    "padding_allowed",
    "skip_layer_func",
    "precision",
    "scales_precision",
    "linear_class",
    "copy_weights"
  ],
  "replace_linear_int4": [
    "module",
    "groupsize",
    "inner_k_tiles",
    "padding_allowed",
    "skip_layer_func"
  ],
  "Int4WeightOnlyQuantizer": {
    "__init__": [
      "self",
      "groupsize",
      "padding_allowed",
      "inner_k_tiles",
      "device",
      "precision"
    ],
    "_create_quantized_state_dict": [
      "self",
      "model"
    ],
    "_convert_for_runtime": [
      "self",
      "model"
    ],
    "quantize": [
      "self",
      "model"
    ]
  },
  "linear_forward_8da4w": [
    "x",
    "weight_int8",
    "bias",
    "scales",
    "zeros",
    "out_features",
    "groupsize",
    "output_precision"
  ],
  "Int8DynActInt4WeightLinear": {
    "__constants__": [],
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "device",
      "dtype",
      "groupsize",
      "precision",
      "scales_precision"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "_replace_linear_8da4w": [
    "module",
    "groupsize",
    "padding_allowed",
    "precision",
    "scales_precision",
    "linear_class",
    "copy_weights"
  ],
  "replace_linear_8da4w": [
    "module",
    "groupsize",
    "padding_allowed",
    "precision",
    "scales_precision"
  ],
  "Int8DynActInt4WeightQuantizer": {
    "__init__": [
      "self",
      "groupsize",
      "padding_allowed",
      "precision",
      "scales_precision",
      "device",
      "mapping_type"
    ],
    "_create_quantized_state_dict": [
      "self",
      "model"
    ],
    "_convert_for_runtime": [
      "self",
      "model"
    ],
    "quantize": [
      "self",
      "model"
    ]
  },
  "_get_logging_hook": [
    "fqn"
  ],
  "_apply_logging_hook": [
    "model"
  ],
  "LoggingTensorMode": {
    "__torch_dispatch__": [
      "self",
      "func",
      "types",
      "args",
      "kwargs"
    ]
  },
  "_MultiInput": {
    "__init__": [
      "self",
      "inputs"
    ],
    "add_input": [
      "self",
      "input"
    ],
    "__getitem__": [
      "self",
      "slice"
    ],
    "cuda": [
      "self"
    ],
    "xpu": [
      "self"
    ]
  },
  "_guard_dtype_size": [
    "tensor_arg",
    "arg_name",
    "dtype",
    "size"
  ],
  "_get_per_token_block_size": [
    "x"
  ],
  "_quantize_activation_per_token_absmax": [
    "t"
  ],
  "_quant_int8_dynamic_per_token_linear": [
    "x",
    "w_vals_int8_t",
    "w_scales",
    "bias",
    "out_dtype"
  ],
  "_quant_int8_per_token_matmul": [
    "x_vals_int8",
    "x_scales",
    "w_vals_int8_t",
    "w_scales",
    "output_dtype"
  ],
  "dynamically_quantize_per_channel": [
    "x",
    "quant_min",
    "quant_max",
    "target_dtype"
  ],
  "dequantize_per_tensor": [
    "int_repr",
    "scale",
    "zero_point",
    "out_dtype"
  ],
  "dequantize_per_channel": [
    "int_repr",
    "scales",
    "zero_points",
    "out_dtype"
  ],
  "get_groupwise_affine_qparams": [
    "w",
    "n_bit",
    "groupsize",
    "dtype",
    "zero_point_domain",
    "preserve_zero",
    "eps"
  ],
  "pack_tinygemm_scales_and_zeros": [
    "scales",
    "zeros",
    "dtype"
  ],
  "unpack_tinygemm_scales_and_zeros": [
    "scales_and_zeros"
  ],
  "groupwise_affine_quantize_tensor_from_qparams": [
    "w",
    "scales",
    "zeros",
    "n_bit",
    "groupsize",
    "zero_point_domain"
  ],
  "groupwise_affine_dequantize_tensor_from_qparams": [
    "w_int4x8",
    "scales",
    "zeros",
    "n_bit",
    "groupsize",
    "zero_point_domain"
  ],
  "groupwise_affine_quantize_tensor": [
    "w",
    "n_bit",
    "groupsize",
    "dtype",
    "zero_point_domain",
    "preserve_zero"
  ],
  "groupwise_affine_dequantize_tensor": [
    "w_int4x8",
    "scales_and_zeros",
    "n_bit",
    "groupsize"
  ],
  "get_group_qparams_symmetric": [
    "w",
    "n_bit",
    "groupsize",
    "precision",
    "mapping_type",
    "eps"
  ],
  "group_quantize_tensor_symmetric": [
    "w",
    "n_bit",
    "group_size",
    "precision",
    "mapping_type"
  ],
  "per_token_dynamic_quant": [
    "input",
    "scale_dtype",
    "zero_point_dtype",
    "eps"
  ],
  "recommended_inductor_config_setter": [],
  "get_block_size": [
    "input_shape",
    "granularity"
  ],
  "_quantization_type": [
    "weight"
  ],
  "_fp8_mm_compat": [
    "weight"
  ],
  "MappingType": {
    "SYMMETRIC": [],
    "SYMMETRIC_NO_CLIPPING_ERR": [],
    "ASYMMETRIC": []
  },
  "ZeroPointDomain": {
    "INT": [],
    "FLOAT": [],
    "NONE": []
  },
  "TorchAODType": {
    "INT1": [],
    "INT2": [],
    "INT3": [],
    "INT4": [],
    "INT5": [],
    "INT6": [],
    "INT7": []
  },
  "_SUB_BYTE_UINT_BOUNDS": [],
  "_GGUF_QK_K": [],
  "_ONES_TABLE": [],
  "_Round": {
    "forward": [
      "ctx",
      "x"
    ],
    "backward": [
      "ctx",
      "gy"
    ]
  },
  "_RoundToFloat8": {
    "forward": [
      "ctx",
      "x",
      "float8_dtype"
    ],
    "backward": [
      "ctx",
      "gy"
    ]
  },
  "_get_and_check_qmin_qmax": [
    "dtype",
    "quant_min",
    "quant_max"
  ],
  "_get_reduction_params": [
    "block_size",
    "input_size"
  ],
  "quantize_affine": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "output_dtype",
    "quant_min",
    "quant_max"
  ],
  "_quantize_affine": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "output_dtype",
    "quant_min",
    "quant_max"
  ],
  "_quantize_affine_no_dtype_cast": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max"
  ],
  "_quantize_affine_tinygemm": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "output_dtype",
    "quant_min",
    "quant_max"
  ],
  "_quantize_affine_tinygemm_no_dtype_cast": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max"
  ],
  "_quantize_affine_no_zero_point": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "output_dtype",
    "quant_min",
    "quant_max"
  ],
  "_quantize_affine_no_zero_point_no_dtype_cast": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max"
  ],
  "dequantize_affine": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "input_dtype",
    "quant_min",
    "quant_max"
  ],
  "_dequantize_affine": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "input_dtype",
    "quant_min",
    "quant_max",
    "output_dtype"
  ],
  "_dequantize_affine_no_dtype_check": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max",
    "output_dtype"
  ],
  "_dequantize_affine_no_zero_point_no_dtype_check": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max",
    "output_dtype"
  ],
  "_dequantize_affine_no_zero_point": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "input_dtype",
    "quant_min",
    "quant_max"
  ],
  "_dequantize_affine_tinygemm_no_dtype_check": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max",
    "output_dtype"
  ],
  "_dequantize_affine_tinygemm": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "input_dtype",
    "quant_min",
    "quant_max"
  ],
  "_fake_quantize_affine": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_dtype",
    "quant_min",
    "quant_max",
    "zero_point_domain"
  ],
  "_fake_quantize_affine_cachemask": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_dtype",
    "quant_min",
    "quant_max",
    "zero_point_domain"
  ],
  "_do_fake_quantize_affine": [
    "input",
    "block_size",
    "scale",
    "zero_point",
    "quant_dtype",
    "quant_min",
    "quant_max",
    "zero_point_domain"
  ],
  "choose_qparams_affine": [
    "input",
    "mapping_type",
    "block_size",
    "target_dtype",
    "quant_min",
    "quant_max",
    "eps",
    "scale_dtype",
    "zero_point_dtype",
    "keepdim"
  ],
  "_choose_qparams_affine_tinygemm": [
    "input",
    "mapping_type",
    "block_size",
    "target_dtype",
    "quant_min",
    "quant_max",
    "eps",
    "scale_dtype",
    "zero_point_dtype"
  ],
  "_choose_qparams_affine_dont_preserve_zero": [
    "input",
    "mapping_type",
    "block_size",
    "target_dtype",
    "quant_min",
    "quant_max",
    "eps",
    "scale_dtype",
    "zero_point_dtype"
  ],
  "choose_qparams_affine_with_min_max": [
    "min_val",
    "max_val",
    "mapping_type",
    "block_size",
    "target_dtype",
    "quant_min",
    "quant_max",
    "eps",
    "scale_dtype",
    "zero_point_dtype",
    "preserve_zero",
    "zero_point_domain"
  ],
  "_choose_qparams_affine": [
    "input",
    "mapping_type",
    "block_size",
    "target_dtype",
    "quant_min",
    "quant_max",
    "eps",
    "scale_dtype",
    "zero_point_dtype",
    "keepdim"
  ],
  "_choose_qparams_gguf": [
    "input",
    "block_size",
    "target_dtype"
  ],
  "_quantize_gguf": [
    "input",
    "block_size",
    "target_dtype",
    "super_block_scale_scale",
    "super_block_min_scale",
    "quantized_block_scale",
    "quantized_block_min"
  ],
  "_dequantize_gguf": [
    "input",
    "block_size",
    "target_dtype",
    "super_block_scale_scale",
    "super_block_min_scale",
    "quantized_block_scale",
    "quantized_block_min",
    "output_dtype"
  ],
  "_shrink_lp_op": [
    "x",
    "beta",
    "lp_norm"
  ],
  "optimize_weights_proximal_legacy": [
    "tensor",
    "scale",
    "zero",
    "min_max",
    "axis",
    "dtype",
    "device",
    "verbose",
    "opt_params"
  ],
  "_is_divisible": [
    "val1",
    "val2"
  ],
  "_convert_to_affinequantized_format": [
    "W_q",
    "scale",
    "zero",
    "nbits",
    "shape"
  ],
  "_choose_qparams_and_quantize_affine_hqq": [
    "tensor",
    "nbits",
    "group_size",
    "optimize",
    "axis",
    "compute_dtype",
    "device",
    "verbose",
    "raw_output",
    "optimize_weights"
  ],
  "_choose_qparams_and_quantize_scale_only_hqq": [
    "hp_tensor",
    "block_size",
    "qmin",
    "qmax"
  ],
  "_choose_qparams_and_quantize_scale_only_sinq": [
    "tensor",
    "qmin",
    "qmax",
    "group_size",
    "niter",
    "compute_dtype"
  ],
  "_choose_qparams_affine_floatx": [
    "tensor",
    "ebits",
    "mbits"
  ],
  "_quantize_affine_floatx": [
    "tensor",
    "scale",
    "ebits",
    "mbits"
  ],
  "_dequantize_affine_floatx": [
    "tensor",
    "scale",
    "ebits",
    "mbits",
    "output_dtype"
  ],
  "_choose_scale_float8": [
    "tensor",
    "block_size",
    "float8_dtype",
    "scale_dtype",
    "hp_value_lb",
    "hp_value_ub"
  ],
  "_maybe_expand_scale_to_tensor_shape": [
    "scale",
    "target_shape"
  ],
  "_quantize_affine_float8": [
    "tensor",
    "scale",
    "float8_dtype"
  ],
  "_dequantize_affine_float8": [
    "tensor",
    "scale",
    "output_dtype"
  ],
  "_quantize_affine_float8_non_decomposed": [
    "tensor",
    "scale",
    "float8_dtype"
  ],
  "_quantize_affine_float8_meta": [
    "tensor",
    "scale",
    "float8_dtype"
  ],
  "_dequantize_affine_float8_non_decomposed": [
    "tensor",
    "scale",
    "output_dtype"
  ],
  "_dequantize_affine_float8_meta": [
    "tensor",
    "scale",
    "output_dtype"
  ],
  "AOPerModuleConfig": [],
  "_check_cache": [
    "cls",
    "shapes_and_dtype"
  ],
  "_update_cache": [
    "cls",
    "shapes_and_dtype",
    "res"
  ],
  "_to_float16": [
    "x"
  ],
  "_to_bfloat16": [
    "x"
  ],
  "_identity": [
    "x"
  ],
  "AQInt8DynamicallyQuantizedSemiSparseLinearWeight": {
    "_autoquant_test": [
      "cls",
      "act_mat",
      "weight",
      "bias",
      "best_time",
      "mode"
    ]
  },
  "AQGemliteInt4G32WeightOnlyQuantizedLinearWeight": {
    "from_float": [
      "cls",
      "weight"
    ]
  },
  "AQGemliteInt4G64WeightOnlyQuantizedLinearWeight": {},
  "AQGemliteInt4G128WeightOnlyQuantizedLinearWeight": {},
  "AQGemliteInt4G256WeightOnlyQuantizedLinearWeight": {},
  "GEMLITE_INT4_AUTOQUANT_CLASS_LIST": [],
  "DEFAULT_SPARSE_AUTOQUANT_CLASS_LIST": [],
  "autoquant": [
    "model",
    "example_input",
    "qtensor_class_list",
    "filter_fn",
    "mode",
    "manual",
    "set_inductor_config",
    "supress_autoquant_errors",
    "min_sqnr"
  ],
  "LinearActivationWeightObservedTensor": {
    "__new__": [
      "cls",
      "original_weight_tensor",
      "input_observer",
      "weight_observer"
    ],
    "__init__": [
      "self",
      "original_weight_tensor",
      "input_observer",
      "weight_observer"
    ],
    "__repr__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_float": [
      "cls",
      "original_weight_tensor",
      "input_observer",
      "weight_observer"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "to": [
      "self"
    ]
  },
  "Granularity": {},
  "PerTensor": {},
  "PerAxis": {},
  "PerGroup": {},
  "PerRow": {},
  "PerToken": {},
  "PerBlock": {},
  "_PartialWrapper": {
    "__init__": [
      "self",
      "p"
    ],
    "__call__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "with_args": [
      "self"
    ]
  },
  "_with_args": [
    "cls_or_self"
  ],
  "AffineQuantizedObserverBase": {
    "with_args": [],
    "__init__": [
      "self",
      "mapping_type",
      "target_dtype",
      "granularity",
      "quant_min",
      "quant_max",
      "eps",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain",
      "keepdim"
    ],
    "forward": [
      "self",
      "input"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "AffineQuantizedMinMaxObserver": {
    "forward": [
      "self",
      "input"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "AffineQuantizedFixedQParamObserver": {
    "__init__": [
      "self",
      "mapping_type",
      "target_dtype",
      "granularity",
      "quant_min",
      "quant_max",
      "eps",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain",
      "scale",
      "zero_point"
    ],
    "set_qparams": [
      "self",
      "scale",
      "zero_point"
    ],
    "forward": [
      "self",
      "input"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "AffineQuantizedMSEObserver": {
    "__init__": [
      "self",
      "mapping_type",
      "target_dtype",
      "granularity",
      "quant_min",
      "quant_max",
      "eps",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain",
      "steps",
      "run_once"
    ],
    "mse": [
      "self",
      "pred",
      "expect",
      "block_size"
    ],
    "loss_fn": [
      "self",
      "x",
      "new_min",
      "new_max"
    ],
    "line_search": [
      "self",
      "input"
    ],
    "forward": [
      "self",
      "input"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "register_quantize_module_handler": [
    "config_type"
  ],
  "prepare_pt2e": [
    "model",
    "quantizer"
  ],
  "prepare_qat_pt2e": [
    "model",
    "quantizer"
  ],
  "_QUANT_OPS": [],
  "_quant_node_constraint": [
    "n"
  ],
  "_is_torchao_prepared_do_not_use_outside_this_file": [
    "model"
  ],
  "convert_pt2e": [
    "model",
    "use_reference_representation",
    "fold_quantize"
  ],
  "_EXPORTED_TRAINING_ATTR": [],
  "WrapperModule": {
    "__init__": [
      "self",
      "fn"
    ],
    "forward": [
      "self"
    ]
  },
  "model_is_exported": [
    "m"
  ],
  "_replace_dropout": [
    "m",
    "train_to_eval"
  ],
  "_replace_batchnorm": [
    "m",
    "train_to_eval"
  ],
  "_move_exported_model_to_eval": [
    "model"
  ],
  "_move_exported_model_to_train": [
    "model"
  ],
  "_allow_exported_model_train_eval": [
    "model"
  ],
  "is_per_tensor": [
    "qscheme"
  ],
  "is_per_channel": [
    "qscheme"
  ],
  "getattr_from_fqn": [
    "obj",
    "fqn"
  ],
  "to_underlying_dtype": [
    "qdtype"
  ],
  "get_qparam_dict": [
    "observer_or_fake_quant"
  ],
  "check_min_max_valid": [
    "min_val",
    "max_val"
  ],
  "calculate_qmin_qmax": [
    "quant_min",
    "quant_max",
    "has_customized_qrange",
    "dtype",
    "reduce_range"
  ],
  "_parent_name": [
    "target"
  ],
  "has_no_children_ignoring_parametrizations": [
    "module"
  ],
  "_get_path_of_module": [
    "root",
    "submodule"
  ],
  "_get_signature_locals": [
    "f",
    "loc"
  ],
  "_get_default_kwargs": [
    "f"
  ],
  "_normalize_kwargs": [
    "func",
    "loc"
  ],
  "validate_qmin_qmax": [
    "quant_min",
    "quant_max"
  ],
  "determine_qparams": [
    "min_val",
    "max_val",
    "quant_min",
    "quant_max",
    "dtype",
    "eps",
    "has_customized_qrange",
    "qscheme"
  ],
  "_get_num_pos_args": [
    "f"
  ],
  "get_fqn_to_example_inputs": [
    "model",
    "example_inputs"
  ],
  "get_new_attr_name_with_prefix": [
    "prefix"
  ],
  "create_getattr_from_value": [
    "module",
    "graph",
    "prefix",
    "value",
    "device"
  ],
  "_QUANTIZE_OPS": [],
  "_DEQUANTIZE_OPS": [],
  "_is_connected": [
    "source",
    "dest"
  ],
  "_get_tensor_constant_from_node": [
    "node",
    "m"
  ],
  "_get_all_arguments": [
    "orig_args",
    "orig_kwargs",
    "args_schema"
  ],
  "_is_supported_batch_norm_for_training": [
    "node"
  ],
  "_is_conv_node": [
    "n"
  ],
  "_is_conv_transpose_node": [
    "n"
  ],
  "_is_conv_or_conv_transpose_node": [
    "n"
  ],
  "_is_conv_transpose_fn": [
    "conv_fn"
  ],
  "_is_bn_node": [
    "n"
  ],
  "fold_bn_weights_into_conv_node": [
    "conv_node",
    "conv_weight_node",
    "conv_bias_node",
    "bn_node",
    "m",
    "fake_fuse"
  ],
  "_fuse_conv_bn_": [
    "m"
  ],
  "_get_node_name_to_scope": [
    "model"
  ],
  "_get_aten_graph_module_for_pattern": [
    "pattern",
    "example_inputs",
    "is_cuda"
  ],
  "remove_tensor_overload_for_qdq_ops": [
    "match_pattern"
  ],
  "_is_literal": [
    "arg"
  ],
  "_replace_literals_with_new_placeholders": [
    "gm",
    "merge_dup",
    "exclude_literals"
  ],
  "_replace_literals_with_existing_placeholders": [
    "gm",
    "exclude_literals",
    "literal_to_ph_idx"
  ],
  "_disallow_eval_train": [
    "model"
  ],
  "_is_sym_size_node": [
    "node"
  ],
  "_filter_sym_size_users": [
    "node"
  ],
  "NUMERIC_DEBUG_HANDLE_KEY": [],
  "CUSTOM_KEY": [],
  "FROM_NODE_KEY": [],
  "NodeSourceDebugInfo": {},
  "generate_numeric_debug_handle": [
    "ep"
  ],
  "_extract_node_source_debug_info": [
    "node"
  ],
  "_detach": [
    "x"
  ],
  "_tensor_shape_equals": [
    "x",
    "y"
  ],
  "_loss_fn": [
    "loss",
    "x",
    "y"
  ],
  "OutputLogger": {
    "_is_impure": [],
    "__init__": [
      "self",
      "debug_info",
      "node_name",
      "nn_module_stack"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__extra_repr__": [
      "self"
    ]
  },
  "_insert_logger": [
    "model",
    "node",
    "debug_info"
  ],
  "prepare_for_propagation_comparison": [
    "model"
  ],
  "QuantizationComparisonResult": {
    "mse_loss": [
      "self"
    ],
    "sqnr": [
      "self"
    ],
    "loss": [
      "self",
      "loss_function"
    ],
    "__repr__": [
      "self"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "NodeAccuracySummary": {},
  "_module_stack_to_str": [
    "module_stack"
  ],
  "extract_results_from_loggers": [
    "model"
  ],
  "compare_results": [
    "ref_results",
    "actual_results"
  ],
  "lower_pt2e_quantized_to_x86": [
    "model",
    "example_inputs"
  ],
  "META_TAG": [],
  "MODULE_TAG": [],
  "CONST_MODULE_TAG": [],
  "replace_node_with_constant": [
    "gm",
    "node",
    "constant",
    "name"
  ],
  "is_const_source": [
    "node",
    "lifted_constant_names"
  ],
  "ConstantFolder": {
    "__init__": [
      "self",
      "gm",
      "skip_constructors",
      "lifted_constant_names",
      "skip_folding_node_fn"
    ],
    "_find_mutable_buffers": [
      "self"
    ],
    "_support_dynamic_shape": [
      "self"
    ],
    "_deduce_value": [
      "self",
      "node"
    ],
    "is_impure": [
      "self",
      "node"
    ],
    "node_to_last_non_output_use": [
      "self"
    ],
    "run_node": [
      "self",
      "node"
    ],
    "insertable_tensor_check": [
      "self",
      "tensor"
    ],
    "add_node_replacement": [
      "self",
      "node",
      "tensor"
    ],
    "run": [
      "self"
    ],
    "insert_placerholder_values": [
      "self",
      "env"
    ]
  },
  "constant_fold": [
    "gm",
    "constraint_fn"
  ],
  "constant_graph_tag": [
    "gm",
    "skip_constructors",
    "lifted_constant_names",
    "skip_folding_node_fn"
  ],
  "run_and_get_constant_graph": [
    "gm",
    "skip_constructors",
    "lifted_constant_names",
    "skip_folding_node_fn"
  ],
  "ObserverOrFakeQuantize": [],
  "ObserverOrFakeQuantizeConstructor": [],
  "DerivedObserverOrFakeQuantize": {
    "__init__": [
      "self",
      "dtype",
      "obs_or_fqs",
      "derive_qparams_fn",
      "quant_min",
      "quant_max",
      "qscheme",
      "ch_axis"
    ],
    "forward": [
      "self",
      "x"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "_qdq_quantized_linear": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "x_quant_min",
    "x_quant_max",
    "weight_i8",
    "weight_scale",
    "weight_zero_point",
    "weight_quant_min",
    "weight_quant_max",
    "bias_fp32",
    "out_scale",
    "out_zero_point",
    "out_quant_min",
    "out_quant_max"
  ],
  "_reference_quantized_linear": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "x_quant_min",
    "x_quant_max",
    "weight_i8",
    "weight_scale",
    "weight_zero_point",
    "weight_quant_min",
    "weight_quant_max",
    "bias_fp32",
    "out_scale",
    "out_zero_point",
    "out_quant_min",
    "out_quant_max"
  ],
  "_qdq_dynamic_quantized_linear": [
    "x_fp32",
    "x_quant_min",
    "x_quant_max",
    "x_eps",
    "weight_i8",
    "weight_scale",
    "weight_zero_point",
    "weight_quant_min",
    "weight_quant_max",
    "bias_fp32"
  ],
  "_reference_dynamic_quantized_linear": [
    "x_fp32",
    "x_quant_min",
    "x_quant_max",
    "x_eps",
    "weight_i8",
    "weight_scale",
    "weight_zero_point",
    "weight_quant_min",
    "weight_quant_max",
    "bias_fp32"
  ],
  "_qdq_dynamic_quantized_linear_4bit_groupwise": [
    "x_fp32",
    "x_eps",
    "weight_i4",
    "weight_scale",
    "weight_zero_point",
    "bias_fp32",
    "group_size"
  ],
  "_reference_dqlinear_int4": [
    "x_fp32",
    "x_eps",
    "weight_i4",
    "weight_scale",
    "weight_zero_point",
    "bias_fp32",
    "group_size"
  ],
  "_reference_dynamic_quantized_linear_4bit_groupwise": [
    "x_fp32",
    "x_eps",
    "weight_i4",
    "weight_scale",
    "weight_zero_point",
    "bias_fp32",
    "group_size"
  ],
  "_filter_fn_for_dynamic_quantized_linear_4bit_groupwise": [
    "match",
    "original_graph",
    "pattern_graph"
  ],
  "_port_metadata_for_dynamic_quantized_linear_4bit_groupwise": [
    "replacement_pattern"
  ],
  "_qdq_quantized_conv2d": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "x_quant_min",
    "x_quant_max",
    "weight_i8",
    "weight_scale",
    "weight_zero_point",
    "weight_quant_min",
    "weight_quant_max",
    "bias_fp32",
    "out_scale",
    "out_zero_point",
    "out_quant_min",
    "out_quant_max"
  ],
  "_reference_quantized_conv2d": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "x_quant_min",
    "x_quant_max",
    "weight_i8",
    "weight_scale",
    "weight_zero_point",
    "weight_quant_min",
    "weight_quant_max",
    "bias_fp32",
    "out_scale",
    "out_zero_point",
    "out_quant_min",
    "out_quant_max"
  ],
  "_qdq_quantized_add_relu": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "y_i8",
    "y_scale",
    "y_zero_point",
    "out_scale",
    "out_zero_point",
    "quant_min",
    "quant_max"
  ],
  "_reference_quantized_add_relu": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "y_i8",
    "y_scale",
    "y_zero_point",
    "out_scale",
    "out_zero_point",
    "quant_min",
    "quant_max"
  ],
  "_qdq_quantized_add": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "y_i8",
    "y_scale",
    "y_zero_point",
    "out_scale",
    "out_zero_point",
    "quant_min",
    "quant_max"
  ],
  "_reference_quantized_add": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "y_i8",
    "y_scale",
    "y_zero_point",
    "out_scale",
    "out_zero_point",
    "quant_min",
    "quant_max"
  ],
  "_qdq_quantized_max_pool2d": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "x_quant_min",
    "x_quant_max",
    "out_scale",
    "out_zero_point",
    "out_quant_min",
    "out_quant_max"
  ],
  "_reference_quantized_max_pool2d": [
    "x_i8",
    "x_scale",
    "x_zero_point",
    "x_quant_min",
    "x_quant_max",
    "out_scale",
    "out_zero_point",
    "out_quant_min",
    "out_quant_max"
  ],
  "_quantize_per_tensor_int8": [
    "x_fp32",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max"
  ],
  "_reference_quantize_per_tensor_int8": [
    "x_fp32",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max"
  ],
  "_dequantize_per_tensor_int8": [
    "x_i8",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max"
  ],
  "_reference_dequantize_per_tensor_int8": [
    "x_i8",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max"
  ],
  "_quantize_per_channel_int8": [
    "x_fp32",
    "scales",
    "zero_points",
    "ch_axis",
    "quant_min",
    "quant_max"
  ],
  "_reference_quantize_per_channel_int8": [
    "x_fp32",
    "scales",
    "zero_points",
    "ch_axis",
    "quant_min",
    "quant_max"
  ],
  "_dequantize_per_channel_int8": [
    "x_i8",
    "scales",
    "zero_points",
    "ch_axis",
    "quant_min",
    "quant_max"
  ],
  "_reference_dequantize_per_channel_int8": [
    "x_i8",
    "scales",
    "zero_points",
    "ch_axis",
    "quant_min",
    "quant_max"
  ],
  "_replace_ph_qdq_per_channel_replacement": [
    "gm"
  ],
  "_RewriteInfo": {},
  "reference_representation_rewrite": [
    "model"
  ],
  "_create_equivalent_types_dict": [],
  "_EQUIVALENT_TYPES_DICT": [],
  "get_equivalent_types": [],
  "update_equivalent_types_dict": [
    "customized_equivalent_types"
  ],
  "_partitions_sequential": [
    "partitions"
  ],
  "_get_matching_types": [
    "partition_type"
  ],
  "_valid_type_sequence": [
    "partition_types"
  ],
  "find_sequential_partitions": [
    "gm",
    "partition_types",
    "include_functional_equivalent",
    "filter_fn"
  ],
  "_get_submodule": [
    "graph_module",
    "node",
    "arg_index"
  ],
  "_get_control_flow_submodules": [
    "graph_module"
  ],
  "bfs_trace_with_node_process": [
    "model",
    "node_op"
  ],
  "AffineQuantizedMovingAverageMinMaxObserver": {
    "__init__": [
      "self",
      "mapping_type",
      "target_dtype",
      "granularity",
      "averaging_constant",
      "quant_min",
      "quant_max",
      "eps",
      "is_dynamic",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain"
    ],
    "forward": [
      "self",
      "input"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "AffineQuantizedPlaceholderObserver": {
    "__init__": [
      "self",
      "mapping_type",
      "target_dtype",
      "granularity",
      "quant_min",
      "quant_max",
      "eps",
      "is_dynamic",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain"
    ],
    "forward": [
      "self",
      "input"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "SUPPORTED_QDTYPES": [],
  "_QSCHEME_TO_CHOOSE_QPARAMS_OP": [],
  "attach_preserved_attrs_to_model": [
    "model",
    "preserved_attrs"
  ],
  "_check_is_graph_module": [
    "model"
  ],
  "_replace_observer_with_quantize_dequantize_node_decomposed": [
    "model",
    "node",
    "modules",
    "node_name_to_scope",
    "node_name_to_qconfig",
    "model_device"
  ],
  "_replace_observer_with_quantize_dequantize_node": [
    "model",
    "node",
    "modules",
    "node_name_to_scope",
    "node_name_to_qconfig",
    "model_device"
  ],
  "_replace_observer_or_dequant_stub_with_dequantize_node": [
    "node",
    "graph"
  ],
  "_is_conversion_supported": [
    "activation_post_process"
  ],
  "_has_none_qconfig": [
    "node",
    "node_name_to_qconfig"
  ],
  "_run_weight_observers": [
    "observed",
    "backend_config"
  ],
  "_maybe_recursive_remove_dequantize": [
    "arg",
    "node",
    "graph"
  ],
  "_get_module_path_and_prefix": [
    "obs_node",
    "node_name_to_scope",
    "node_name_to_qconfig"
  ],
  "_insert_dequantize_node": [
    "node",
    "graph"
  ],
  "_maybe_get_observer_for_node": [
    "node",
    "modules"
  ],
  "convert_standalone_module": [
    "node",
    "modules",
    "model",
    "is_reference",
    "backend_config"
  ],
  "convert_weighted_module": [
    "node",
    "modules",
    "observed_node_names",
    "node_name_to_qconfig",
    "backend_config",
    "is_decomposed",
    "is_reference",
    "model_device"
  ],
  "convert": [
    "model",
    "is_reference",
    "convert_custom_config",
    "is_standalone_module",
    "_remove_qconfig_flag",
    "qconfig_mapping",
    "backend_config",
    "is_decomposed",
    "keep_original_weights"
  ],
  "_convert_fx": [
    "graph_module",
    "is_reference",
    "convert_custom_config",
    "is_standalone_module",
    "_remove_qconfig",
    "qconfig_mapping",
    "backend_config",
    "is_decomposed",
    "keep_original_weights"
  ],
  "_convert_to_reference_decomposed_fx": [
    "graph_module",
    "convert_custom_config",
    "qconfig_mapping",
    "backend_config"
  ],
  "_is_per_channel": [
    "qscheme"
  ],
  "_is_per_tensor": [
    "qscheme"
  ],
  "_is_symmetric_quant": [
    "qscheme"
  ],
  "_is_float_qparams": [
    "qscheme"
  ],
  "FakeQuantizeBase": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "calculate_qparams": [
      "self"
    ],
    "enable_fake_quant": [
      "self",
      "enabled"
    ],
    "disable_fake_quant": [
      "self"
    ],
    "enable_observer": [
      "self",
      "enabled"
    ],
    "disable_observer": [
      "self"
    ],
    "with_args": [
      "cls"
    ]
  },
  "FakeQuantize": {
    "__init__": [
      "self",
      "observer",
      "quant_min",
      "quant_max",
      "is_dynamic"
    ],
    "calculate_qparams": [
      "self"
    ],
    "forward": [
      "self",
      "X"
    ],
    "extra_repr": [
      "self"
    ],
    "_save_to_state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ]
  },
  "FixedQParamsFakeQuantize": {
    "__init__": [
      "self",
      "observer"
    ],
    "calculate_qparams": [
      "self"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "FusedMovingAvgObsFakeQuantize": {
    "__init__": [
      "self",
      "observer",
      "quant_min",
      "quant_max"
    ],
    "calculate_qparams": [
      "self"
    ],
    "extra_repr": [
      "self"
    ],
    "forward": [
      "self",
      "X"
    ]
  },
  "default_fake_quant": [],
  "default_weight_fake_quant": [],
  "default_dynamic_fake_quant": [],
  "default_fixed_qparams_range_neg1to1_fake_quant": [],
  "default_fixed_qparams_range_0to1_fake_quant": [],
  "default_symmetric_fixed_qparams_fake_quant": [],
  "default_affine_fixed_qparams_fake_quant": [],
  "default_per_channel_weight_fake_quant": [],
  "default_embedding_fake_quant": [],
  "default_embedding_fake_quant_4bit": [],
  "default_histogram_fake_quant": [],
  "default_fused_act_fake_quant": [],
  "default_fused_wt_fake_quant": [],
  "default_fused_per_channel_wt_fake_quant": [],
  "fused_wt_fake_quant_range_neg_127_to_127": [],
  "fused_per_channel_wt_fake_quant_range_neg_127_to_127": [],
  "_is_fake_quant_script_module": [
    "mod"
  ],
  "disable_fake_quant": [
    "mod"
  ],
  "enable_fake_quant": [
    "mod"
  ],
  "disable_observer": [
    "mod"
  ],
  "enable_observer": [
    "mod"
  ],
  "LearnableFakeQuantize": {
    "__init__": [
      "self",
      "observer",
      "quant_min",
      "quant_max",
      "use_grad_scaling"
    ],
    "enable_range_learning": [
      "self"
    ],
    "disable_range_learning": [
      "self"
    ],
    "enable_observer": [
      "self",
      "enabled"
    ],
    "disable_observer": [
      "self"
    ],
    "enable_fake_quant": [
      "self",
      "enabled"
    ],
    "disable_fake_quant": [
      "self"
    ],
    "observe_quant_params": [
      "self"
    ],
    "calculate_qparams": [
      "self"
    ],
    "extra_repr": [
      "self"
    ],
    "_initialize_or_update_qparams": [
      "self",
      "scale",
      "zero_point"
    ],
    "forward": [
      "self",
      "X"
    ]
  },
  "enable_range_learning": [
    "mod"
  ],
  "disable_range_learning": [
    "mod"
  ],
  "PartialWrapper": {
    "__init__": [
      "self",
      "p"
    ],
    "__call__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "with_args": [
      "self"
    ],
    "with_callable_args": [
      "self"
    ]
  },
  "_with_callable_args": [
    "cls_or_self"
  ],
  "ObserverBase": {
    "__init__": [
      "self",
      "dtype",
      "is_dynamic"
    ],
    "forward": [
      "self",
      "x"
    ],
    "calculate_qparams": [
      "self"
    ],
    "with_args": [],
    "with_callable_args": []
  },
  "UniformQuantizationObserverBase": {
    "_version": [],
    "__init__": [
      "self",
      "dtype",
      "qscheme",
      "reduce_range",
      "quant_min",
      "quant_max",
      "factory_kwargs",
      "eps",
      "is_dynamic"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "_validate_qmin_qmax": [
      "self",
      "quant_min",
      "quant_max"
    ],
    "_calculate_qparams": [
      "self",
      "min_val",
      "max_val"
    ],
    "reset_min_max_vals": [
      "self"
    ]
  },
  "_ObserverBase": [],
  "MinMaxObserver": {
    "__init__": [
      "self",
      "dtype",
      "qscheme",
      "reduce_range",
      "quant_min",
      "quant_max",
      "factory_kwargs",
      "eps",
      "is_dynamic"
    ],
    "forward": [
      "self",
      "x_orig"
    ],
    "calculate_qparams": [
      "self"
    ],
    "extra_repr": [
      "self"
    ],
    "reset_min_max_vals": [
      "self"
    ]
  },
  "MovingAverageMinMaxObserver": {
    "__init__": [
      "self",
      "averaging_constant",
      "dtype",
      "qscheme",
      "reduce_range",
      "quant_min",
      "quant_max",
      "eps",
      "is_dynamic"
    ],
    "forward": [
      "self",
      "x_orig"
    ]
  },
  "PerChannelMinMaxObserver": {
    "__init__": [
      "self",
      "ch_axis",
      "dtype",
      "qscheme",
      "reduce_range",
      "quant_min",
      "quant_max",
      "factory_kwargs",
      "eps",
      "is_dynamic"
    ],
    "forward": [
      "self",
      "x_orig"
    ],
    "_forward": [
      "self",
      "x_orig"
    ],
    "calculate_qparams": [
      "self"
    ],
    "extra_repr": [
      "self"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "_load_from_state_dict_script": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "reset_min_max_vals": [
      "self"
    ]
  },
  "MovingAveragePerChannelMinMaxObserver": {
    "__init__": [
      "self",
      "averaging_constant",
      "ch_axis",
      "dtype",
      "qscheme",
      "reduce_range",
      "quant_min",
      "quant_max",
      "eps",
      "is_dynamic"
    ],
    "forward": [
      "self",
      "x_orig"
    ]
  },
  "HistogramObserver": {
    "__init__": [
      "self",
      "bins",
      "dtype",
      "qscheme",
      "reduce_range",
      "quant_min",
      "quant_max",
      "factory_kwargs",
      "eps",
      "is_dynamic"
    ],
    "_get_norm": [
      "self",
      "delta_begin",
      "delta_end",
      "density"
    ],
    "_compute_quantization_error": [
      "self",
      "next_start_bin",
      "next_end_bin"
    ],
    "_non_linear_param_search": [
      "self"
    ],
    "_upscale_histogram": [
      "self",
      "histogram",
      "orig_min",
      "orig_max",
      "update_min",
      "update_max"
    ],
    "_combine_histograms": [
      "self",
      "orig_hist",
      "orig_min",
      "orig_max",
      "update_hist",
      "update_min",
      "update_max"
    ],
    "reset_histogram": [
      "self",
      "x",
      "min_val",
      "max_val"
    ],
    "forward": [
      "self",
      "x_orig"
    ],
    "calculate_qparams": [
      "self"
    ],
    "_save_to_state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "FixedQParamsObserver": {
    "__init__": [
      "self",
      "scale",
      "zero_point",
      "dtype",
      "qscheme",
      "quant_min",
      "quant_max",
      "is_dynamic"
    ],
    "forward": [
      "self",
      "X"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "PlaceholderObserver": {
    "__init__": [
      "self",
      "dtype",
      "custom_op_name",
      "compute_dtype",
      "quant_min",
      "quant_max",
      "qscheme",
      "eps",
      "is_dynamic"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "RecordingObserver": {
    "__annotations__": [],
    "__init__": [
      "self",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ],
    "calculate_qparams": [
      "self"
    ],
    "get_tensor_value": [
      "self"
    ]
  },
  "NoopObserver": {
    "__init__": [
      "self",
      "dtype",
      "custom_op_name"
    ],
    "forward": [
      "self",
      "x"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "ReuseInputObserver": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "_is_observer_script_module": [
    "mod",
    "obs_type_name"
  ],
  "_is_activation_post_process": [
    "module"
  ],
  "_is_per_channel_script_obs_instance": [
    "module"
  ],
  "get_observer_state_dict": [
    "mod"
  ],
  "load_observer_state_dict": [
    "mod",
    "obs_dict"
  ],
  "default_observer": [],
  "default_placeholder_observer": [],
  "default_debug_observer": [],
  "default_weight_observer": [],
  "weight_observer_range_neg_127_to_127": [],
  "default_histogram_observer": [],
  "default_per_channel_weight_observer": [],
  "per_channel_weight_observer_range_neg_127_to_127": [],
  "default_dynamic_quant_observer": [],
  "default_float_qparams_observer": [],
  "default_float_qparams_observer_4bit": [],
  "default_fixed_qparams_range_neg1to1_observer": [],
  "default_fixed_qparams_range_0to1_observer": [],
  "default_symmetric_fixed_qparams_observer": [],
  "default_affine_fixed_qparams_observer": [],
  "default_reuse_input_observer": [],
  "_is_activation_post_process_node": [
    "node",
    "named_modules"
  ],
  "_get_observer_kwargs": [
    "quant_spec"
  ],
  "_create_obs_or_fq_from_qspec": [
    "quantization_spec",
    "obs_or_fq_map",
    "is_qat"
  ],
  "_find_root_edge_or_node": [
    "edge_or_node",
    "shared_with_map"
  ],
  "_union": [
    "parent",
    "child",
    "shared_with_map",
    "edge_or_node_to_qspec"
  ],
  "_update_shared_with": [
    "child",
    "qspec",
    "shared_with_map",
    "edge_or_node_to_qspec"
  ],
  "_unwrap_shared_qspec": [
    "qspec",
    "edge_or_node_to_qspec",
    "shared_with_map"
  ],
  "_has_same_attr": [
    "qspec_a",
    "qspec_b",
    "attr_name"
  ],
  "_get_edge_or_node_to_qspec": [
    "model"
  ],
  "_union_input_edge_with": [
    "input_edge",
    "input_edge_root_qspec",
    "edge_or_node",
    "edge_or_node_to_qspec",
    "shared_with_map"
  ],
  "_get_edge_or_node_to_group_id": [
    "edge_or_node_to_qspec"
  ],
  "_get_obs_or_fq_map": [
    "edge_or_node_to_group_id",
    "edge_or_node_to_qspec",
    "is_qat"
  ],
  "_maybe_insert_input_observer_for_arg_or_kwarg": [
    "node",
    "arg",
    "qconfig",
    "model",
    "named_modules",
    "obs_or_fq_map",
    "is_qat",
    "model_device"
  ],
  "_maybe_insert_input_observers_for_node": [
    "node",
    "qconfig",
    "model",
    "named_modules",
    "obs_or_fq_map",
    "is_qat",
    "model_device"
  ],
  "_maybe_insert_output_observer_for_node": [
    "node",
    "model",
    "named_modules",
    "graph",
    "obs_or_fq_map",
    "is_qat",
    "model_device"
  ],
  "_maybe_insert_input_and_output_observers_for_node": [
    "node",
    "model",
    "obs_or_fq_map",
    "named_modules",
    "is_qat",
    "model_device"
  ],
  "prepare": [
    "model",
    "node_name_to_scope",
    "is_qat",
    "obs_or_fq_callback"
  ],
  "_get_quantized_conv_bn_example_inputs_kwargs": [
    "is_per_channel",
    "has_bias",
    "bias_is_quantized",
    "is_cuda"
  ],
  "_get_conv_bn_pattern": [
    "conv_fn"
  ],
  "_get_qat_conv_bn_pattern": [
    "conv_fn"
  ],
  "_get_qat_conv_bn_pattern_no_conv_bias": [
    "conv_fn"
  ],
  "_append_qdq": [
    "x",
    "is_per_channel",
    "is_bias",
    "kwargs"
  ],
  "_get_quantized_qat_conv_bn_pattern": [
    "is_per_channel",
    "has_bias",
    "bias_is_quantized",
    "conv_fn",
    "bn_is_training"
  ],
  "_get_folded_quantized_qat_conv_bn_pattern": [
    "is_per_channel",
    "has_bias",
    "bias_is_quantized",
    "conv_fn",
    "bn_is_training"
  ],
  "_has_conv_bias_filter": [
    "match",
    "original_graph",
    "pattern_graph"
  ],
  "_no_conv_bias_filter": [
    "match",
    "original_graph",
    "pattern_graph"
  ],
  "_is_quantize": [
    "n"
  ],
  "_is_dequantize": [
    "n"
  ],
  "_get_conv_bn_pattern_nodes": [
    "r"
  ],
  "_filter_nodes_map": [
    "nodes_map"
  ],
  "_copy_over_literal_conv_args": [
    "original_node",
    "new_node"
  ],
  "_update_conv_input_qspec_map_after_replacement": [
    "original_node",
    "replacement_node"
  ],
  "_update_special_qspecs_after_replacement": [
    "node",
    "original_to_replacement_node"
  ],
  "_fuse_conv_bn_qat": [
    "m"
  ],
  "_fuse_conv_bn_qat_helper": [
    "m",
    "conv_fn",
    "example_inputs",
    "is_cuda"
  ],
  "_duplicate_dequantize_node": [
    "m"
  ],
  "_remove_extra_dequantize": [
    "m"
  ],
  "_copy_over_q_dq_args": [
    "original_node",
    "replacement_node"
  ],
  "_fold_conv_bn_qat": [
    "m"
  ],
  "_fold_conv_bn_qat_helper": [
    "m",
    "conv_fn",
    "example_inputs",
    "is_cuda"
  ],
  "_METADATA_TO_PORT": [],
  "_CHOOSE_QPARAMS_OPS": [],
  "_add_metadata": [
    "to_node",
    "from_node"
  ],
  "_has_quant_annotation": [
    "node"
  ],
  "_find_choose_qparams_node": [
    "node"
  ],
  "_find_q_dq_node_for_user": [
    "produer",
    "user"
  ],
  "_port_metadata_for_input_quant_nodes": [
    "input_node",
    "node",
    "qspec"
  ],
  "_port_metadata_for_output_quant_nodes": [
    "node",
    "qspec"
  ],
  "PortNodeMetaForQDQ": {
    "call": [
      "self",
      "graph_module"
    ]
  },
  "annotate_input_qspec_map": [
    "node",
    "input_node",
    "qspec"
  ],
  "annotate_output_qspec": [
    "node",
    "qspec"
  ],
  "get_module_name_filter": [
    "module_name"
  ],
  "is_valid_annotation": [
    "annotation"
  ],
  "Q_ANNOTATION_KEY": [],
  "QuantizationSpecBase": {},
  "QuantizationSpec": {
    "__post_init__": [
      "self"
    ]
  },
  "FixedQParamsQuantizationSpec": {},
  "EdgeOrNode": [],
  "SharedQuantizationSpec": {},
  "DerivedQuantizationSpec": {},
  "QuantizationAnnotation": {},
  "ComposableQuantizer": {
    "__init__": [
      "self",
      "quantizers"
    ],
    "_record_and_validate_annotations": [
      "self",
      "gm",
      "quantizer"
    ],
    "annotate": [
      "self",
      "model"
    ],
    "transform_for_annotation": [
      "self",
      "model"
    ],
    "validate": [
      "self",
      "model"
    ]
  },
  "get_default_xpu_inductor_quantization_config": [],
  "XPUInductorQuantizer": {
    "__init__": [
      "self"
    ],
    "_annotate_qat_conv2d_fusion_pattern": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_maxpool2d": [
      "self",
      "node",
      "quantization_config"
    ],
    "_annotate_output_for_int8_in_int8_out_pattern": [
      "self",
      "node"
    ]
  },
  "_maybe_duplicate_dq": [
    "gm",
    "dq_node",
    "user"
  ],
  "DuplicateDQPass": {
    "call": [
      "self",
      "graph_module"
    ]
  },
  "_X86InductorQuantizationAnnotation": {},
  "propagation_quantizable_ops": [],
  "default_quantizable_ops": [],
  "quantizable_ops": [],
  "QUANT_ANNOTATION_KEY": [],
  "_skip_annotate": [
    "nodes",
    "filter_fn"
  ],
  "_create_module_name_filter": [
    "module_name"
  ],
  "_create_operator_type_filter": [
    "operator_type"
  ],
  "_global_config_filter": [
    "nodes"
  ],
  "_map_module_function_to_aten_operator_type": [],
  "_is_node_annotated": [
    "_node"
  ],
  "_is_any_annotated": [
    "nodes"
  ],
  "_is_all_annotated": [
    "nodes"
  ],
  "_is_quantized_op_pt2e": [
    "node"
  ],
  "get_default_x86_inductor_quantization_config": [
    "is_qat",
    "is_dynamic",
    "reduce_range"
  ],
  "get_x86_inductor_linear_dynamic_fp16_config": [],
  "_annotate_nodes_not_quantize": [
    "nodes"
  ],
  "_config_checker": [
    "method"
  ],
  "_CurrentQuantizationMode": {},
  "X86InductorQuantizer": {
    "module_function_to_aten_operator_type": [],
    "__init__": [
      "self"
    ],
    "_get_current_quantization_mode": [
      "self"
    ],
    "_need_skip_config": [
      "self",
      "quantization_config"
    ],
    "set_global": [
      "self",
      "quantization_config"
    ],
    "get_global_quantization_config": [
      "self"
    ],
    "set_function_type_qconfig": [
      "self",
      "function_type",
      "quantization_config"
    ],
    "set_module_type_qconfig": [
      "self",
      "module_type",
      "quantization_config"
    ],
    "set_module_name_qconfig": [
      "self",
      "module_name",
      "quantization_config"
    ],
    "_set_aten_operator_qconfig": [
      "self",
      "operator_type",
      "quantization_config"
    ],
    "_annotate_conv_node_helper": [
      "self",
      "conv_node",
      "annotate_output",
      "quantization_config"
    ],
    "_annotate_linear_node_helper": [
      "self",
      "linear_node",
      "annotate_output",
      "quantization_config"
    ],
    "_get_output_nodes_of_partitions": [
      "self",
      "partition_list"
    ],
    "_get_input_idx_for_binary_node": [
      "self",
      "conv_gemm_node",
      "binary_node"
    ],
    "annotate": [
      "self",
      "model"
    ],
    "_annotate_with_config": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_qat_conv2d_fusion_pattern": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_qat_conv2d_bn_binary_unary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_qat_conv2d_bn_binary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_qat_conv2d_bn_unary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_qat_conv2d_bn": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_conv2d_fusion_pattern": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_linear_fusion_pattern": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_matmul": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_conv2d_binary_unary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_conv2d_binary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_conv2d_unary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_conv2d": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_maxpool2d": [
      "self",
      "node",
      "quantization_config"
    ],
    "_annotate_cat": [
      "self",
      "node",
      "quantization_config"
    ],
    "_annotate_propagation_quantizable_pattern_entry": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_propagation_quantizable_pattern": [
      "self",
      "node",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_output_share_observer_as_input": [
      "self",
      "input_node",
      "source_node"
    ],
    "_annotate_output_for_int8_in_int8_out_pattern_entry": [
      "self",
      "model"
    ],
    "_annotate_output_for_int8_in_int8_out_pattern": [
      "self",
      "node"
    ],
    "_annotate_linear": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_linear_unary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_linear_binary_unary": [
      "self",
      "gm",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_mul_tensor": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "validate": [
      "self",
      "model"
    ]
  },
  "_ArmInductorQuantizationAnnotation": {},
  "get_default_arm_inductor_quantization_config": [
    "is_qat",
    "is_dynamic"
  ],
  "ArmInductorQuantizer": {
    "module_function_to_aten_operator_type": [],
    "get_global_quantization_config": [
      "self"
    ],
    "set_function_type_qconfig": [
      "self",
      "function_type",
      "quantization_config"
    ],
    "set_module_type_qconfig": [
      "self",
      "module_type",
      "quantization_config"
    ],
    "set_module_name_qconfig": [
      "self",
      "module_name",
      "quantization_config"
    ],
    "_set_aten_operator_qconfig": [
      "self",
      "operator_type",
      "quantization_config"
    ],
    "annotate": [
      "self",
      "model"
    ],
    "_annotate_with_config": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_qat_conv2d_fusion_pattern": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_conv2d_fusion_pattern": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_linear_fusion_pattern": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ]
  },
  "get_embedding_operators_config": [],
  "_register_dequantize_fp8_lowering": [],
  "prims": [],
  "quantized_decomposed": [],
  "quantized": [],
  "_PER_TENSOR_QUANTIZE_OPS": [],
  "_VIEW_FUNCTION_OPS": [],
  "_VIEW_METHOD_OPS": [],
  "_get_pattern_output_dtype": [
    "match"
  ],
  "_may_generate_pattern_with_dtype_convert": [
    "pattern",
    "dtype",
    "with_dtype_convert",
    "users"
  ],
  "_may_generate_pattern_with_reshape": [
    "pattern",
    "reshape_size",
    "with_reshape"
  ],
  "_generate_linear_t_pattern": [
    "_dequant_per_channel_pattern",
    "dtype"
  ],
  "_unary_fusion_pattern": [
    "unary_fusion",
    "call_fn",
    "users",
    "is_bf16"
  ],
  "get_dequantize_per_tensor_activation_pattern": [
    "is_tensor_overload",
    "is_fp8"
  ],
  "dequantize_per_channel_weight_pattern": [],
  "dequantize_fp8_weight_pattern": [],
  "get_dequantize_to_bf16_weight_pattern": [
    "dequant_wgt_pattern"
  ],
  "get_dequantize_clone_weight_pattern": [
    "dequant_wgt_pattern"
  ],
  "get_dequantize_to_bf16_clone_weight_pattern": [
    "dequant_wgt_pattern"
  ],
  "get_qconv_pt2e_pattern": [
    "x_scale_zp_are_tensors",
    "users"
  ],
  "get_qlinear_pt2e_pattern": [
    "x_scale_zp_are_tensors",
    "users"
  ],
  "get_qlinear_binary_pt2e_pattern": [
    "x_scale_zp_are_tensors",
    "users"
  ],
  "dequantize_accum_pattern": [],
  "generate_pattern_with_binary": [
    "binary_post_op",
    "computation_call",
    "extra_input_pattern",
    "dtype_convert",
    "swap_inputs"
  ],
  "generate_pattern_with_unary": [
    "computation_call",
    "unary_post_op"
  ],
  "generate_pattern_with_output_quant": [
    "computation_call",
    "with_dtype_convert",
    "is_fp8"
  ],
  "_check_node_kwarg_arg_value": [
    "check_node",
    "kwarg_name",
    "args_index",
    "expected_value"
  ],
  "_is_valid_quantized_conv_optimization_pattern": [],
  "_is_valid_qconv_post_op_fusion_pattern": [
    "has_binary_post_op"
  ],
  "_is_valid_quantized_linear_optimization_pattern": [],
  "_is_valid_qlinear_post_op_fusion_pattern": [
    "has_binary_post_op"
  ],
  "_is_valid_qconv_binary_optimization_pattern": [],
  "_is_valid_qlinear_binary_optimization_pattern": [],
  "_is_valid_quantized_op_binary_optimization_pattern": [
    "qop",
    "extra_input_from_dequant"
  ],
  "_is_valid_dequant_promotion_pattern": [
    "dtype"
  ],
  "_register_dequant_promotion_pass": [
    "pattern",
    "pass_number",
    "dtype"
  ],
  "_is_valid_dequant_conv_pattern": [
    "dtype"
  ],
  "_register_qconv_weight_prepack_pass": [
    "pattern",
    "pass_number",
    "dtype",
    "is_fp8"
  ],
  "_generate_dequant_convolution_node_pattern": [
    "_dequant_pattern",
    "dtype",
    "is_fp8"
  ],
  "_generate_qconv_weight_prepack_patterns": [
    "dtype",
    "is_fp8"
  ],
  "_get_linear_node": [
    "match",
    "input_dim_exceeds_two",
    "input_contiguous"
  ],
  "_get_linear_dq_node": [
    "linear_node",
    "input_index",
    "dtype",
    "input_dim_exceeds_two",
    "input_contiguous"
  ],
  "_is_valid_dequant_linear_pattern": [
    "dtype",
    "input_dim_exceeds_two",
    "input_contiguous"
  ],
  "_register_qlinear_weight_prepack_pass": [
    "pattern",
    "pass_number",
    "dtype",
    "input_dim_exceeds_two",
    "input_contiguous",
    "is_fp8"
  ],
  "_generate_dequant_linear_node_pattern": [
    "_dequant_per_channel_pattern",
    "dtype",
    "input_dim_exceeds_two",
    "is_tensor_overload",
    "is_fp8"
  ],
  "_generate_dequant_bmm_node_pattern": [
    "_dequant_per_channel_pattern",
    "dtype",
    "with_bias",
    "is_tensor_overload",
    "is_fp8"
  ],
  "_generate_qlinear_weight_prepack_patterns": [
    "dtype",
    "input_dim_exceeds_two",
    "input_contiguous",
    "with_bias",
    "is_tensor_overload",
    "is_fp8"
  ],
  "_generate_linear_dynamic_fp16_pattern": [
    "_dequant_weight_pattern",
    "input_dim_exceeds_two",
    "input_contiguous",
    "relu_fused"
  ],
  "_register_dequant_promotion": [],
  "_register_qconv_weight_prepack": [],
  "_register_qlinear_weight_prepack": [],
  "_register_linear_dynamic_fp16_weight_prepack_pass": [
    "pattern",
    "pass_number",
    "input_dim_exceeds_two",
    "input_contiguous",
    "relu_fused"
  ],
  "_register_linear_dynamic_fp16_weight_prepack": [],
  "_register_smooth_quant_int_mm_pattern": [],
  "PostOpAttr": {
    "__init__": [
      "self",
      "binary_op_name",
      "alpha",
      "unary_op_name",
      "scalars_attr",
      "algorithm_attr"
    ]
  },
  "_register_qconv_post_op_fusion_pass": [
    "pattern",
    "pass_number",
    "computation_op",
    "post_op_attr"
  ],
  "_register_qconv_unary_fusion": [],
  "_register_qconv_binary_fusion": [],
  "_register_qlinear_post_op_fusion_pass": [
    "pattern",
    "pass_number",
    "computation_op",
    "post_op_attr"
  ],
  "_register_qlinear_unary_fusion": [],
  "_register_qlinear_binary_fusion": [],
  "_register_scaled_embedding_bag_pass": [
    "pattern",
    "pass_number",
    "dtype"
  ],
  "_generate_scaled_embedding_bag_patterns": [
    "dq_pattern"
  ],
  "_register_quantization_embeddingbag_pass": [],
  "_is_valid_concat_dq_q_pattern": [],
  "_register_concat_dequant_quant_pass": [
    "pattern",
    "pass_number"
  ],
  "_register_concat_dq_q_pattern": [],
  "_register_quantization_weight_pack_pass": [],
  "quant_lift_up": [
    "module_graph"
  ],
  "PackingFormat": {
    "PLAIN": [],
    "OPAQUE": []
  },
  "SupportsActivationPreScaling": {},
  "IsStaticQuantizationConfig": {
    "get_act_quant_kwargs": [
      "self"
    ]
  },
  "QuantizationStep": {
    "PREPARE": [],
    "CONVERT": [],
    "PREPARE_FOR_LOADING": []
  },
  "KernelPreference": {
    "AUTO": [],
    "TORCH": [],
    "MSLK": [],
    "EMULATED": []
  },
  "QuantizeTensorKwargs": {},
  "Int4PlainInt32Tensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_data_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "zero_point",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale",
      "zero_point",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "_quantization_type": [
      "self"
    ],
    "from_hp": [
      "cls",
      "w",
      "block_size"
    ]
  },
  "_from_hp_xpu": [
    "cls",
    "w",
    "block_size"
  ],
  "_from_hp_npu": [
    "cls",
    "w",
    "block_size"
  ],
  "_linear_xpu": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_npu": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "Int4TilePackedTo4dTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_data_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale_and_zero",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale_and_zero",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "_quantization_type": [
      "self"
    ],
    "from_hp": [
      "cls",
      "hp_tensor",
      "block_size",
      "int4_choose_qparams_algorithm"
    ]
  },
  "Int4PackingFormat": {
    "PLAIN": [],
    "PRESHUFFLED": [],
    "PLAIN_INT32": [],
    "TILE_PACKED_TO_4D": []
  },
  "Int4ChooseQParamsAlgorithm": {
    "TINYGEMM": [],
    "HQQ": []
  },
  "Int4Tensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_data_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "zero_point",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale",
      "zero_point",
      "block_size",
      "shape",
      "act_pre_scale"
    ],
    "_quantization_type": [
      "self"
    ],
    "from_hp": [
      "cls",
      "w",
      "block_size"
    ]
  },
  "Int4PreshuffledTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_data_names": [],
    "__new__": [
      "cls",
      "qdata",
      "group_scale",
      "block_size",
      "shape",
      "group_zero",
      "row_scale"
    ],
    "__init__": [
      "self",
      "qdata",
      "group_scale",
      "block_size",
      "shape",
      "group_zero",
      "row_scale"
    ],
    "_quantization_type": [
      "self"
    ],
    "from_hp": [
      "cls",
      "w",
      "block_size",
      "activation_dtype"
    ],
    "from_int4_tensor": [
      "cls",
      "tensor"
    ]
  },
  "Float8PackingFormat": {
    "PLAIN": [],
    "SPARSE_CUTLASS": []
  },
  "QuantizeTensorToFloat8Kwargs": {},
  "Float8Tensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "block_size",
      "mm_config",
      "act_quant_kwargs",
      "kernel_preference",
      "dtype"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale",
      "block_size",
      "mm_config",
      "act_quant_kwargs",
      "kernel_preference",
      "dtype"
    ],
    "__repr__": [
      "self"
    ],
    "_quantization_type": [
      "self"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "from_hp": [
      "cls",
      "hp_tensor",
      "float8_dtype",
      "granularity",
      "mm_config",
      "hp_value_lb",
      "hp_value_ub",
      "kernel_preference",
      "act_quant_kwargs"
    ]
  },
  "Sparse2x4CUTLASSFloat8Tensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "sparse_metadata",
      "scale",
      "block_size",
      "act_quant_kwargs",
      "dtype"
    ],
    "__init__": [
      "self",
      "qdata",
      "sparse_metadata",
      "scale",
      "block_size",
      "act_quant_kwargs",
      "dtype"
    ],
    "__repr__": [
      "self"
    ],
    "_quantization_type": [
      "self"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "from_hp": [
      "cls",
      "hp_tensor",
      "float8_dtype",
      "granularity",
      "hp_value_lb",
      "hp_value_ub",
      "act_quant_kwargs"
    ]
  },
  "_is_kernel_library_loaded": [],
  "IntxOpaqueTensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "packed_weights",
      "bit_width",
      "block_size",
      "shape",
      "dtype",
      "packed_weights_has_zeros",
      "packed_weights_has_bias",
      "intx_packing_format"
    ],
    "__init__": [
      "self",
      "packed_weights",
      "bit_width",
      "block_size",
      "shape",
      "dtype",
      "packed_weights_has_zeros",
      "packed_weights_has_bias",
      "intx_packing_format"
    ],
    "_quantization_type": [
      "self"
    ],
    "to": [
      "self"
    ],
    "from_intx_unpacked_to_int8_tensor": [
      "cls",
      "tensor"
    ]
  },
  "_linear_impl_2d_aten": [
    "input_tensor",
    "weight_tensor"
  ],
  "_linear_impl_2d_torchao": [
    "input_tensor",
    "weight_tensor"
  ],
  "IntxUnpackedToInt8TensorActivationQuantization": {
    "INT8_ASYM_PER_TOKEN": []
  },
  "IntxUnpackedToInt8Tensor": {
    "tensor_data_names": [],
    "tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "zero_point",
      "target_dtype",
      "block_size",
      "dtype",
      "activation_quantization"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale",
      "zero_point",
      "target_dtype",
      "block_size",
      "dtype",
      "activation_quantization"
    ],
    "_quantization_type": [
      "self"
    ],
    "_has_float_zero_point": [
      "self"
    ],
    "to": [
      "self"
    ],
    "from_hp": [
      "cls",
      "hp_tensor",
      "block_size",
      "target_dtype"
    ],
    "dequantize": [
      "self"
    ]
  },
  "_apply_int8_act_asym_per_token_quant_dequant": [
    "hp_tensor"
  ],
  "IntxChooseQParamsAlgorithm": {
    "AFFINE": [],
    "HQQ_SCALE_ONLY": []
  },
  "IntxPackingFormat": {
    "UNPACKED_TO_INT8": [],
    "OPAQUE_ATEN_KLEIDIAI": [],
    "OPAQUE_TORCHAO_AUTO": [],
    "OPAQUE_TORCHAO_KLEIDIAI": [],
    "OPAQUE_TORCHAO_LOWBIT": []
  },
  "QuantizeTensorToInt8Kwargs": {},
  "Int8Tensor": {
    "tensor_data_names": [],
    "optional_tensor_data_names": [],
    "tensor_attribute_names": [],
    "optional_tensor_attribute_names": [],
    "__new__": [
      "cls",
      "qdata",
      "scale",
      "block_size",
      "dtype",
      "act_quant_scale",
      "act_pre_scale",
      "act_quant_kwargs"
    ],
    "__init__": [
      "self",
      "qdata",
      "scale",
      "block_size",
      "dtype",
      "act_quant_scale",
      "act_pre_scale",
      "act_quant_kwargs"
    ],
    "__repr__": [
      "self"
    ],
    "from_hp": [
      "cls",
      "hp_tensor",
      "granularity",
      "mapping_type",
      "scale",
      "act_quant_kwargs",
      "act_quant_scale",
      "act_pre_scale"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ]
  },
  "FakeQuantizeConfigBase": {},
  "Float8FakeQuantizeConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "Int4WeightFakeQuantizeConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "IntxFakeQuantizeConfig": {
    "__init__": [
      "self",
      "dtype",
      "granularity",
      "mapping_type",
      "scale_precision",
      "zero_point_precision",
      "zero_point_domain",
      "is_dynamic",
      "range_learning",
      "eps"
    ],
    "__post_init__": [
      "self"
    ],
    "_get_granularity": [
      "self",
      "granularity",
      "group_size"
    ],
    "_get_mapping_type": [
      "self",
      "mapping_type",
      "is_symmetric"
    ],
    "group_size": [
      "self"
    ],
    "is_symmetric": [
      "self"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ]
  },
  "FakeQuantizeConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_infer_fake_quantize_configs": [
    "base_config"
  ],
  "FakeQuantizedEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "max_norm",
      "norm_type",
      "scale_grad_by_freq",
      "sparse",
      "weight_config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "to_embedding": [
      "self"
    ],
    "from_embedding": [
      "cls",
      "mod",
      "weight_config"
    ]
  },
  "Int4WeightOnlyEmbeddingQATQuantizer": {
    "__init__": [
      "self",
      "group_size",
      "scale_precision",
      "zero_point_precision"
    ],
    "prepare": [
      "self",
      "model"
    ],
    "convert": [
      "self",
      "model"
    ],
    "_convert_helper": [
      "self",
      "module"
    ]
  },
  "Int4WeightOnlyQATEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "max_norm",
      "norm_type",
      "scale_grad_by_freq",
      "sparse",
      "group_size",
      "scale_precision",
      "zero_point_precision"
    ],
    "enable_fake_quant": [
      "self",
      "enabled"
    ],
    "disable_fake_quant": [
      "self"
    ]
  },
  "Int4WeightOnlyEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "max_norm",
      "norm_type",
      "scale_grad_by_freq",
      "sparse",
      "group_size",
      "scale_precision",
      "zero_point_precision",
      "device",
      "output_dtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_fake_quantize_per_channel_group": [
    "input",
    "scales",
    "zero_points",
    "quant_min",
    "quant_max",
    "group_size",
    "zero_point_domain"
  ],
  "_fake_quantize_per_token": [
    "input",
    "scales",
    "zero_points",
    "quant_min",
    "quant_max"
  ],
  "_get_qmin_qmax": [
    "n_bit",
    "symmetric"
  ],
  "_log_deprecation_warning": [
    "old_api_object"
  ],
  "QATStep": {
    "PREPARE": [],
    "CONVERT": []
  },
  "QATConfig": {
    "__init__": [
      "self",
      "base_config",
      "activation_config",
      "weight_config"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "_qat_config_transform": [
    "module",
    "config"
  ],
  "IntXQuantizationAwareTrainingConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "intx_quantization_aware_training": {},
  "_intx_quantization_aware_training_transform": [
    "module",
    "config"
  ],
  "FromIntXQuantizationAwareTrainingConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "from_intx_quantization_aware_training": {},
  "_from_intx_quantization_aware_training_transform": [
    "mod",
    "config"
  ],
  "ComposableQATQuantizer": {
    "__init__": [
      "self",
      "quantizers"
    ],
    "prepare": [
      "self",
      "model"
    ],
    "convert": [
      "self",
      "model"
    ]
  },
  "initialize_fake_quantizers": [
    "model",
    "example_inputs"
  ],
  "FakeQuantizerBase": {
    "__repr__": [
      "self"
    ],
    "from_config": [
      "config"
    ]
  },
  "Float8FakeQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Int4WeightFakeQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "w"
    ],
    "_fp8_activations_forward": [
      "self",
      "w"
    ],
    "_bf16_activations_forward": [
      "self",
      "w"
    ]
  },
  "IntxFakeQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "_per_token_forward": [
      "self",
      "x"
    ],
    "_per_channel_or_group_forward": [
      "self",
      "x"
    ],
    "_should_compute_qparams": [
      "self"
    ],
    "_maybe_update_qparams_for_range_learning": [
      "self"
    ]
  },
  "FakeQuantizer": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "FakeQuantizedLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "activation_config",
      "weight_config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "to_linear": [
      "self"
    ],
    "from_linear": [
      "cls",
      "mod",
      "activation_config",
      "weight_config"
    ]
  },
  "enable_linear_fake_quant": [
    "mod",
    "enabled"
  ],
  "disable_linear_fake_quant": [
    "mod"
  ],
  "_LegacyQATQuantizer": {
    "get_activation_fake_quantize_config": [
      "self"
    ],
    "get_weight_fake_quantize_config": [
      "self"
    ]
  },
  "Int8DynActInt4WeightQATQuantizer": {
    "__init__": [
      "self",
      "groupsize",
      "padding_allowed",
      "precision",
      "scales_precision"
    ],
    "prepare": [
      "self",
      "model"
    ],
    "convert": [
      "self",
      "model"
    ],
    "_convert_qat_linear_8da4w": [
      "self",
      "module"
    ],
    "get_activation_fake_quantize_config": [
      "self"
    ],
    "get_weight_fake_quantize_config": [
      "self"
    ]
  },
  "Int8DynActInt4WeightQATLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "device",
      "groupsize",
      "precision",
      "scales_precision"
    ],
    "enable_fake_quant": [
      "self",
      "enabled"
    ],
    "disable_fake_quant": [
      "self"
    ]
  },
  "enable_8da4w_fake_quant": [
    "mod"
  ],
  "disable_8da4w_fake_quant": [
    "mod"
  ],
  "_get_8da4w_activation_config": [
    "qparams_precision"
  ],
  "_get_8da4w_weight_config": [
    "group_size",
    "qparams_precision"
  ],
  "Int4WeightOnlyQATQuantizer": {
    "__init__": [
      "self",
      "groupsize",
      "inner_k_tiles",
      "precision",
      "scales_precision"
    ],
    "prepare": [
      "self",
      "model"
    ],
    "convert": [
      "self",
      "model"
    ],
    "_convert_qat_linear_4w": [
      "self",
      "module"
    ],
    "get_weight_fake_quantize_config": [
      "self"
    ]
  },
  "Int4WeightOnlyQATLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "device",
      "groupsize",
      "inner_k_tiles",
      "precision",
      "scales_precision"
    ],
    "enable_fake_quant": [
      "self",
      "enabled"
    ],
    "disable_fake_quant": [
      "self"
    ]
  },
  "enable_4w_fake_quant": [
    "mod"
  ],
  "disable_4w_fake_quant": [
    "mod"
  ],
  "_get_4w_weight_config": [
    "group_size",
    "qparams_precision"
  ],
  "Float8ActInt4WeightQATQuantizer": {
    "__init__": [
      "self",
      "group_size",
      "scale_precision"
    ],
    "prepare": [
      "self",
      "model"
    ],
    "convert": [
      "self",
      "model"
    ],
    "get_activation_fake_quantize_config": [
      "self"
    ],
    "get_weight_fake_quantize_config": [
      "self"
    ]
  },
  "_ToAffineFakeQuantized": {
    "forward": [
      "ctx",
      "original_tensor",
      "mapping_type",
      "block_size",
      "target_dtype",
      "quant_min",
      "quant_max",
      "eps",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain"
    ],
    "backward": [
      "ctx",
      "gy"
    ]
  },
  "_AffineFakeQuantizedTensor": {
    "__new__": [
      "cls",
      "original_tensor",
      "apply_fake_quant_fn",
      "fake_quant_enabled"
    ],
    "__init__": [
      "self",
      "original_tensor",
      "apply_fake_quant_fn",
      "fake_quant_enabled"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_float": [
      "cls",
      "original_input",
      "mapping_type",
      "block_size",
      "target_dtype",
      "quant_min",
      "quant_max",
      "eps",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain"
    ],
    "get_value": [
      "self"
    ],
    "_get_to_kwargs": [
      "self"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "_create_new": [
      "self",
      "new_value"
    ]
  },
  "_to_affine_fake_quantized": [],
  "nf4_all_gather_into_tensor": [
    "func"
  ],
  "scatter_nf4tensor": [
    "func"
  ],
  "_INNER_TENSOR_NAMES_FOR_SHARDING": [],
  "CHUNK_SIZE": [],
  "same_metadata": [
    "a",
    "b"
  ],
  "construct_nf4_args": [
    "nf4tensor",
    "kwargs"
  ],
  "apply_to_inner_tensors": [
    "nf4tensor",
    "aten_op",
    "args",
    "kwargs"
  ],
  "call_from_inner_tensors": [
    "nf4tensor",
    "method_name",
    "args",
    "kwargs"
  ],
  "CompareOp": {
    "EQ": [],
    "LT": []
  },
  "expect_num_of_args": [
    "op",
    "num",
    "msg"
  ],
  "expect_arg_value_at_k": [
    "k",
    "op",
    "value",
    "msg"
  ],
  "expect_args_len_at_k": [
    "k",
    "op",
    "value",
    "msg"
  ],
  "noop_detach": [
    "func"
  ],
  "clone": [
    "func"
  ],
  "nf4_detach": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_empty_like": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_split": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_new_zeros": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_slice": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_view": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_as_strided": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "_to_copy": [
    "func"
  ],
  "t_default": [
    "func"
  ],
  "mm_default": [
    "func"
  ],
  "copy_": [
    "func"
  ],
  "nf4_is_pinned": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_pin_memory": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "nf4_cat": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "wait_tensor": [
    "func"
  ],
  "SubclassTensorArgs": {},
  "get_block_absmax": [
    "input_tensor",
    "block_size"
  ],
  "NF4Tensor": {
    "__new__": [
      "cls",
      "tensor_meta",
      "block_size",
      "n_blocks",
      "scaler_block_size",
      "quantized_scalers",
      "quantization_factor",
      "scaler_mean",
      "quantized_data",
      "nf4"
    ],
    "__init__": [
      "self",
      "tensor_meta",
      "block_size",
      "n_blocks",
      "scaler_block_size",
      "quantized_scalers",
      "quantization_factor",
      "scaler_mean",
      "quantized_data",
      "nf4"
    ],
    "from_tensor": [
      "cls",
      "input_tensor",
      "block_size",
      "scaler_block_size"
    ],
    "double_quantize_scalers": [
      "input_tensor",
      "block_size",
      "scaler_block_size"
    ],
    "dequantize_scalers": [
      "self",
      "input_tensor",
      "quantization_factor",
      "scaler_block_size"
    ],
    "convert_to_norm_float_weight": [
      "input_tensor",
      "n_blocks",
      "block_size",
      "nf4"
    ],
    "get_original_weight": [
      "self"
    ],
    "quantize_tensor_nearest": [
      "value",
      "nf4"
    ],
    "dequantize": [
      "value",
      "nf4"
    ],
    "__repr__": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "metadata",
      "outer_size",
      "outer_stride"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ]
  },
  "LinearNF4": {
    "forward": [
      "ctx",
      "input",
      "weight"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "linear_nf4": [
    "input",
    "weight"
  ],
  "to_nf4": [
    "tensor",
    "block_size",
    "scaler_block_size"
  ],
  "NF4_TORCH_FUNCTIONS": [],
  "function_to_dtype": [],
  "function_cpu": [],
  "function_cuda": [],
  "function_view_as": [],
  "nf4_constructor": [
    "tensor_meta",
    "block_size",
    "n_blocks",
    "scaler_block_size",
    "quantized_scalers",
    "quantization_factor",
    "scaler_mean",
    "quantized_data",
    "nf4"
  ],
  "_DEFAULT_ZPD": [],
  "AffineQuantizedTensor": {
    "__new__": [
      "cls",
      "tensor_impl",
      "block_size",
      "shape",
      "quant_min",
      "quant_max",
      "zero_point_domain",
      "dtype",
      "strides"
    ],
    "__init__": [
      "self",
      "tensor_impl",
      "block_size",
      "shape",
      "quant_min",
      "quant_max",
      "zero_point_domain",
      "dtype",
      "strides"
    ],
    "__repr__": [
      "self"
    ],
    "_quantization_type": [
      "self"
    ],
    "dequantize": [
      "self",
      "output_dtype"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_hp_to_intx": [
      "cls",
      "input_float",
      "mapping_type",
      "block_size",
      "target_dtype",
      "quant_min",
      "quant_max",
      "eps",
      "scale_dtype",
      "zero_point_dtype",
      "preserve_zero",
      "zero_point_domain",
      "_layout",
      "use_hqq"
    ],
    "from_hp_to_intx_static": [
      "cls",
      "input_float",
      "scale",
      "zero_point",
      "block_size",
      "target_dtype",
      "quant_min",
      "quant_max",
      "zero_point_domain",
      "_layout"
    ],
    "from_hp_to_floatx": [
      "cls",
      "input_float",
      "block_size",
      "target_dtype",
      "_layout",
      "scale_dtype"
    ],
    "from_hp_to_floatx_static": [
      "cls",
      "input_float",
      "scale",
      "block_size",
      "target_dtype",
      "_layout",
      "scale_dtype"
    ],
    "_layout": [
      "self"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ]
  },
  "register_layout": [],
  "get_tensor_impl_constructor": [],
  "to_affine_quantized_intx": [],
  "to_affine_quantized_intx_static": [],
  "to_affine_quantized_floatx": [],
  "to_affine_quantized_floatx_static": [],
  "Layout": {
    "pre_process": [
      "self",
      "input"
    ],
    "post_process": [
      "self",
      "input",
      "scale",
      "zero_point",
      "block_size"
    ],
    "pre_process_static": [
      "self",
      "input",
      "scale",
      "zero_point",
      "block_size"
    ],
    "__repr__": [
      "self"
    ],
    "extra_repr": [
      "self"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "PlainLayout": {},
  "is_device": [
    "target_device_str",
    "device"
  ],
  "get_out_shape": [
    "input_shape",
    "weight_shape"
  ],
  "AQTTensorImpl": {
    "get_plain": [
      "self"
    ],
    "get_layout": [
      "self"
    ],
    "from_plain": [
      "cls",
      "data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_AQT_QLINEAR_DISPATCH_TABLE": [],
  "register_aqt_quantized_linear_dispatch": [
    "dispatch_condition",
    "impl"
  ],
  "deregister_aqt_quantized_linear_dispatch": [
    "dispatch_condition"
  ],
  "QuantizedLinearNotImplementedError": {},
  "_quantized_linear_op": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_register_aqt_quantized_linear_dispatches": [],
  "CutlassSemiSparseLayout": {
    "pre_process": [
      "self",
      "dense"
    ]
  },
  "CutlassSemiSparseTensorImpl": {
    "__new__": [
      "cls",
      "sparse",
      "meta",
      "scale",
      "_layout"
    ],
    "__init__": [
      "self",
      "sparse",
      "meta",
      "scale",
      "_layout"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "get_plain": [
      "self"
    ],
    "from_plain": [
      "cls",
      "dense",
      "scale",
      "zero_point",
      "_layout"
    ],
    "get_layout": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ]
  },
  "_linear_fp8_act_fp8_weight_sparse_cutlass_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp8_act_fp8_weight_sparse_cutlass_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "Float8Layout": {},
  "_fallback_warning_shown": [],
  "Float8AQTTensorImpl": {
    "__new__": [
      "cls",
      "float8_data",
      "scale",
      "transposed",
      "_layout"
    ],
    "__init__": [
      "self",
      "float8_data",
      "scale",
      "transposed",
      "_layout"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "to": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "get_plain": [
      "self"
    ],
    "get_layout": [
      "self"
    ],
    "from_plain": [
      "cls",
      "data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_linear_fp8_act_fp8_weight_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp8_act_fp8_weight_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp_act_fp8_weight_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp_act_fp8_weight_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "QDQLayout": {},
  "QDQTensorImpl": {
    "__new__": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "__init__": [
      "self",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "get_plain": [
      "self"
    ],
    "get_layout": [
      "self"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ]
  },
  "_linear_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_embedding_check": [
    "args",
    "kwargs"
  ],
  "_embedding_impl": [
    "args",
    "kwargs"
  ],
  "_aqt_is_xpu_layout_uint4": [
    "aqt"
  ],
  "_linear_bf16_act_uint4_weight_float_zero_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_bf16_act_uint4_weight_float_zero_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp_act_uint4_weight_int8_zero_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp_act_uint4_weight_int8_zero_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "Int4XPULayout": {},
  "Int4XPUAQTTensorImpl": {
    "__new__": [
      "cls",
      "packed_weight",
      "scale_and_zero",
      "transposed",
      "_layout",
      "scale",
      "zero"
    ],
    "__init__": [
      "self",
      "packed_weight",
      "scale_and_zero",
      "transposed",
      "_layout",
      "scale",
      "zero"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "get_plain": [
      "self"
    ],
    "get_layout": [
      "self"
    ]
  },
  "_aqt_is_tensor_core_tile_uint4": [
    "aqt"
  ],
  "_linear_bf16_act_uint4_weight_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_bf16_act_uint4_weight_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "TensorCoreTiledLayout": {
    "pre_process": [
      "self",
      "input"
    ],
    "pre_process_static": [
      "self",
      "input",
      "scale",
      "zero_point",
      "block_size"
    ],
    "post_process": [
      "self",
      "input",
      "scale",
      "zero_point",
      "block_size"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "TensorCoreTiledAQTTensorImpl": {
    "__new__": [
      "cls",
      "packed_weight",
      "scale_and_zero",
      "transposed",
      "_layout"
    ],
    "__init__": [
      "self",
      "packed_weight",
      "scale_and_zero",
      "transposed",
      "_layout"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "block_size": [
      "self"
    ],
    "get_plain": [
      "self"
    ],
    "get_layout": [
      "self"
    ]
  },
  "PlainAQTTensorImpl": {
    "__new__": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "__init__": [
      "self",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "get_plain": [
      "self"
    ],
    "get_layout": [
      "self"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ]
  },
  "_aqt_is_int8_reduced_range": [
    "aqt"
  ],
  "_linear_fp_act_int8_weight_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp_act_int8_weight_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_int8_act_int8_weight_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_int8_act_int8_weight_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "Target": {
    "AUTO": [],
    "UNIVERSAL": [],
    "KLEIDIAI": [],
    "ATEN": []
  },
  "_TARGET_AND_STR": [],
  "target_to_str": [
    "target"
  ],
  "target_from_str": [
    "target"
  ],
  "PackedLinearInt8DynamicActivationIntxWeightLayout": {
    "__init__": [
      "self",
      "target"
    ],
    "extra_repr": [
      "self"
    ],
    "has_params_set": [
      "self"
    ],
    "set_params": [
      "self",
      "bit_width",
      "group_size",
      "has_weight_zeros",
      "has_bias"
    ]
  },
  "PackedLinearInt8DynamicActivationIntxWeightAQTTensorImpl": {
    "__new__": [
      "cls",
      "packed_weight",
      "_layout"
    ],
    "__init__": [
      "self",
      "packed_weight",
      "_layout"
    ],
    "__repr__": [
      "self"
    ],
    "get_layout": [
      "self"
    ],
    "get_plain": [
      "self"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "layout",
      "bias"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ]
  },
  "make_packed_linear_int8_dynamic_activation_intx_weight_tensor": [
    "int_data",
    "scale",
    "zero_point",
    "bias",
    "data_dtype",
    "target"
  ],
  "Int4CPULayout": {},
  "Int4CPUAQTTensorImpl": {
    "__new__": [
      "cls",
      "packed_weight",
      "scale_and_zero",
      "transposed",
      "_layout"
    ],
    "__init__": [
      "self",
      "packed_weight",
      "scale_and_zero",
      "transposed",
      "_layout"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "tensor_data_dict",
      "tensor_attributes",
      "outer_size",
      "outer_stride"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ],
    "to": [
      "self"
    ],
    "_apply_fn_to_data": [
      "self",
      "fn"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [],
    "block_size": [
      "self"
    ],
    "get_plain": [
      "self"
    ],
    "get_layout": [
      "self"
    ]
  },
  "_is_float": [
    "dtype"
  ],
  "_linear_fp_act_uint4_weight_cpu_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_fp_act_uint4_weight_cpu_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_int8_act_int8_weight_semi_structured_sparse_check": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "_linear_int8_act_int8_weight_semi_structured_sparse_impl": [
    "input_tensor",
    "weight_tensor",
    "bias"
  ],
  "SemiSparseLayout": {
    "pre_process": [
      "self",
      "input"
    ]
  },
  "SemiSparseAQTTensorImpl": {
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "get_plain": [
      "self"
    ],
    "from_plain": [
      "cls",
      "int_data",
      "scale",
      "zero_point",
      "_layout"
    ]
  },
  "maskbits": [],
  "unpack_mask": [],
  "numbits": [],
  "shifts": [],
  "abs_lsh": [
    "data",
    "shift"
  ],
  "abs_rsh": [
    "data",
    "shift"
  ],
  "pack_cpu": [
    "data",
    "elem_size",
    "dim"
  ],
  "unpack_cpu": [
    "data",
    "elem_size",
    "dim"
  ],
  "_pack": [
    "data",
    "elem_size",
    "scale",
    "dim"
  ],
  "_unpack": [
    "data",
    "element_size",
    "scale",
    "dim"
  ],
  "pack": [
    "data",
    "elem_size",
    "dim"
  ],
  "unpack": [
    "data",
    "elem_size",
    "dim"
  ],
  "swizzle_bmm": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "swizzle_addmm": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "swizzle_permute": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "swizzle_numpy_T": [
    "aten_op",
    "args",
    "kwargs"
  ],
  "SwizzleTensor": {
    "__new__": [
      "cls",
      "original",
      "shallow"
    ],
    "__init__": [
      "self",
      "original",
      "shallow"
    ],
    "__repr__": [
      "self"
    ],
    "unswizzle": [
      "self"
    ],
    "as_tensor": [
      "self"
    ],
    "shallow_transpose": [
      "self"
    ],
    "shape": [
      "self"
    ],
    "stride": [
      "self"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": []
  },
  "WandaSparsifier": {
    "__init__": [
      "self",
      "sparsity_level",
      "semi_structured_block_size"
    ],
    "prepare": [
      "self",
      "model",
      "config"
    ],
    "update_mask": [
      "self",
      "module",
      "tensor_name",
      "sparsity_level"
    ],
    "squash_mask": [
      "self",
      "params_to_keep",
      "params_to_keep_per_layer"
    ]
  },
  "bsr_to_dense": [
    "crow_indices",
    "col_indices",
    "values",
    "M",
    "K"
  ],
  "bsr_to_dense_abstract": [
    "crow_indices",
    "col_indices",
    "values",
    "M",
    "K"
  ],
  "blocksparse_int_addmm": [
    "crow_indices",
    "col_indices",
    "values",
    "A",
    "left_alpha",
    "right_alpha"
  ],
  "blocksparse_int_addmm_abstract": [
    "crow_indices",
    "col_indices",
    "values",
    "A",
    "left_alpha",
    "right_alpha"
  ],
  "blocksparse_addmm": [
    "x_padded",
    "crow_indices",
    "col_indices",
    "values",
    "M",
    "K",
    "bias"
  ],
  "blocksparse_addmm_abstract": [
    "x_padded",
    "crow_indices",
    "col_indices",
    "values",
    "M",
    "K",
    "bias"
  ],
  "BlockSparseTensor": {
    "__slots__": [],
    "__new__": [
      "cls",
      "shape",
      "blocksize",
      "bsr_crow_indices",
      "bsr_col_indices",
      "bsr_values",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "cls",
      "inner_tensors",
      "tensor_meta",
      "outer_size",
      "outer_stride"
    ],
    "from_dense": [
      "cls",
      "dense_tensor",
      "blocksize"
    ],
    "apply_fn_to_shard": [
      "self",
      "func"
    ]
  },
  "block_sparse_detach": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse_unsqueeze": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse_mul": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse_sum": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse_values": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse_crow_indices": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse_col_indices": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse__nnz": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "block_sparse_linear": [
    "func",
    "types",
    "args",
    "kwargs"
  ],
  "create_block_sparse_tensor": [
    "M",
    "N",
    "blocksize",
    "sparsity",
    "dtype"
  ],
  "create_semi_structured_tensor": [
    "r",
    "c",
    "dtype"
  ],
  "PerChannelNormObserver": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x_orig"
    ],
    "calculate_qparams": [
      "self"
    ]
  },
  "mask_creator": [
    "tensor",
    "N",
    "M"
  ],
  "SCORES_MIN": [],
  "SCORES_MAX": [],
  "percentile": [
    "t",
    "q"
  ],
  "GetSubnet": {
    "forward": [
      "ctx",
      "scores",
      "zeros",
      "ones",
      "sparsity"
    ],
    "backward": [
      "ctx",
      "g"
    ]
  },
  "ApplyMask": {
    "forward": [
      "ctx",
      "weight",
      "scores"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "SupermaskLinear": {
    "__init__": [
      "self",
      "sparsity_level",
      "blocksize",
      "fixed_mask",
      "fixed_weight"
    ],
    "get_mask": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "from_linear": [
      "cls",
      "linear",
      "sparsity_level",
      "blocksize"
    ],
    "to_linear": [
      "cls",
      "supermask_linear"
    ]
  },
  "apply_fake_sparsity": [
    "model"
  ],
  "BlockSparseWeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "block_sparse_weight": [],
  "_block_sparse_weight_transform": [
    "module",
    "config"
  ],
  "SemiSparseWeightConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "semi_sparse_weight": [],
  "_semi_sparse_weight_transform": [
    "module",
    "config"
  ],
  "sparsify_": [
    "model",
    "config",
    "filter_fn"
  ],
  "_semi_sparse_pointwise_op": [
    "func",
    "types",
    "args",
    "kwargs",
    "sparsify_like_args_list"
  ],
  "CUTLASS_POINTWISE_OP_DISPATCH_TABLE": [],
  "SemiSparseLinear": {
    "forward": [
      "self",
      "x"
    ],
    "from_dense": [
      "cls",
      "linear"
    ],
    "to_dense": [
      "cls",
      "semi_sparse_linear"
    ]
  },
  "SemiSparseActivationLinear": {
    "forward": [
      "self",
      "x"
    ],
    "from_dense": [
      "cls",
      "linear"
    ],
    "to_dense": [
      "cls",
      "semi_sparse_linear"
    ]
  },
  "swap_linear_with_semi_sparse_linear": [
    "model",
    "config",
    "current"
  ],
  "swap_semi_sparse_linear_with_linear": [
    "model",
    "current"
  ],
  "GRADIENT_TYPE": [],
  "_SparsifyFunc": {
    "forward": [
      "ctx",
      "x",
      "algo",
      "backend"
    ],
    "backward": [
      "ctx",
      "grad_out"
    ]
  },
  "_SparsifyLikeFunc": {
    "forward": [
      "ctx",
      "x",
      "pattern",
      "gradient"
    ],
    "backward": [
      "ctx",
      "grad_out"
    ]
  },
  "semi_structured_sparsify": [
    "x",
    "algo",
    "backend"
  ],
  "semi_structured_sparsify_like": [
    "x",
    "pattern",
    "gradient"
  ],
  "_DEFAULT_VERSION": [],
  "AOBaseConfig": {},
  "ConfigJSONEncoder": {
    "default": [
      "self",
      "o"
    ],
    "encode_value": [
      "self",
      "value"
    ]
  },
  "config_to_dict": [
    "config"
  ],
  "ALLOWED_AO_MODULES": [],
  "config_from_dict": [
    "data"
  ],
  "TestLeanImport": {
    "test_torchao_import_does_not_initialize_cuda": [
      "self"
    ]
  },
  "DTYPES": [],
  "XQ_WQ_DTYPES": [],
  "BATCH_SIZE": [],
  "SIZE_MNK": [],
  "USE_BIAS": [],
  "TEST_PARAMS": [],
  "run_test_for_op": [
    "op",
    "dtype",
    "Xq_dtype",
    "Wq_dtype",
    "batch_size",
    "size_mnk",
    "use_bias"
  ],
  "test_rowwise_scaled_linear_sparse_cutlass_f8f8": [
    "dtype",
    "Xq_Wq_dtypes",
    "batch_size",
    "size_mnk",
    "use_bias"
  ],
  "IS_CUDA": [],
  "TestOps": {
    "_scaled_dot_product_int8_op_ref": [
      "self",
      "q",
      "k",
      "v",
      "attn_mask",
      "dropout_p",
      "is_causal",
      "q_scale",
      "q_zp",
      "k_scale",
      "k_zp",
      "v_scale",
      "v_zp",
      "a_scale",
      "a_zp",
      "o_scale",
      "o_zp"
    ],
    "_scaled_dot_product_fp8_op_ref": [
      "self",
      "q",
      "k",
      "v",
      "attn_mask",
      "dropout_p",
      "is_causal",
      "q_scale",
      "k_scale",
      "v_scale",
      "a_scale",
      "o_scale"
    ],
    "test_quantized_scaled_dot_product_op": [
      "self",
      "input_dtype",
      "batch_size",
      "n_head",
      "q_seq_len",
      "kv_seq_len",
      "head_dim",
      "mask_dtype"
    ]
  },
  "test_swizzle_mm": [],
  "EMBEDINGBAG_MULTIHOT_SIZES": [],
  "EMBEDINGBAG_BAG_SIZES": [],
  "EMBEDINGBAG_VECTOR_SIZES": [],
  "EMBEDINGBAG_INDEX_DTYPES": [],
  "EMBEDINGBAG_TEST_PARAMS": [],
  "_test_scaled_embedding_bag_cpu_helper": [
    "multi_hot",
    "batch_size",
    "vector_size",
    "index_type",
    "qtype",
    "out_dtype"
  ],
  "test_scaled_embedding_bag_int8_cpu": [
    "multi_hot",
    "batch_size",
    "vector_size",
    "index_type",
    "out_dtype"
  ],
  "test_scaled_embedding_bag_fp8_cpu": [
    "multi_hot",
    "batch_size",
    "vector_size",
    "index_type",
    "out_dtype"
  ],
  "test_float8_linear_cpu": [
    "shape",
    "bs",
    "out_dtype",
    "bias",
    "x_granularity",
    "w_granularity"
  ],
  "TestTorchVersion": {
    "test_torch_version_at_least": [
      "self"
    ]
  },
  "TestTorchAOBaseTensor": {
    "test_print_arg_types": [
      "self"
    ],
    "_test_default_impls_helper": [
      "self",
      "lp_tensor",
      "lp_tensor_for_copy"
    ],
    "test_default_impls": [
      "self"
    ],
    "test_default_impls_with_optional_data": [
      "self"
    ],
    "test_default_impls_with_optional_attr": [
      "self"
    ],
    "test_implements_and_torch_function_together": [
      "self"
    ],
    "test_subclassing": [
      "self"
    ],
    "test_subclassing_with_real_op": [
      "self"
    ],
    "test_op_overwrite": [
      "self"
    ],
    "test_multiple_inheritance": [
      "self"
    ],
    "test_multiple_inheritance_with_child_override": [
      "self"
    ]
  },
  "TestModels": {
    "test_toy_linear_model": [
      "self",
      "device"
    ],
    "test_ln_linear_activation_model": [
      "self",
      "device"
    ],
    "test_transformer_block": [
      "self",
      "device"
    ]
  },
  "_DEVICES": [],
  "TestQuantize": {
    "test_quantize_8bit_with_qmap_correctness": [
      "self",
      "device"
    ],
    "test_quantize_8bit_with_qmap_compile": [
      "self",
      "device"
    ],
    "test_quantize_4bit_with_qmap_correctness": [
      "self",
      "device"
    ],
    "test_quantize_4bit_with_qmap_compile": [
      "self",
      "device"
    ],
    "test_bf16_stochastic_round": [
      "self",
      "device",
      "compile"
    ],
    "test_bf16_stochastic_round_dtensor": [
      "self",
      "device",
      "compile"
    ]
  },
  "TestOptim": {
    "test_optim_smoke": [
      "self",
      "optim_name",
      "dtype",
      "device"
    ],
    "test_optim_default_dtype_bf16": [
      "self",
      "optim_name",
      "device"
    ],
    "test_param_groups": [
      "self",
      "optim_name",
      "device"
    ],
    "test_subclass_slice": [
      "self",
      "subclass",
      "shape",
      "device"
    ],
    "test_optim_8bit_correctness": [
      "self",
      "optim_name"
    ],
    "test_optim_4bit_correctness": [
      "self",
      "optim_name"
    ],
    "test_optim_cpu_offload_correctness": [
      "self",
      "offload_grad",
      "grad_accum"
    ],
    "test_optim_cpu_offload_save_load": [
      "self"
    ],
    "test_optim_bf16_stochastic_round_correctness": [
      "self",
      "device"
    ]
  },
  "_FSDP_WORLD_SIZE": [],
  "TestFSDP2": {
    "world_size": [
      "self"
    ],
    "test_fsdp2": [
      "self"
    ],
    "_test_fsdp2": [
      "self",
      "args"
    ],
    "test_uneven_shard": [
      "self"
    ]
  },
  "TestCodebookQuantization": {
    "setUp": [
      "self"
    ],
    "test_choose_qparams_codebook": [
      "self"
    ],
    "test_codebook_quantized_tensor_from_float": [
      "self"
    ],
    "test_codebook_quantized_tensor_from_float2": [
      "self"
    ],
    "test_quantize_api": [
      "self"
    ]
  },
  "DEVICES": [],
  "SimplePruner": {
    "update_mask": [
      "self",
      "module",
      "tensor_name"
    ]
  },
  "ImplementedPruner": {
    "update_mask": [
      "self",
      "module",
      "tensor_name"
    ]
  },
  "BottomHalfLSTMPruner": {
    "update_mask": [
      "self",
      "module",
      "tensor_name"
    ]
  },
  "TestSaliencyPruner": {
    "test_saliency_pruner_update_mask": [
      "self"
    ],
    "test_lstm_saliency_pruner_update_mask": [
      "self"
    ]
  },
  "TestBaseStructuredSparsifier": {
    "_check_pruner_prepared": [
      "self",
      "model",
      "pruner",
      "device"
    ],
    "_check_pruner_valid_before_step": [
      "self",
      "model",
      "pruner",
      "device"
    ],
    "_check_pruner_valid_after_step": [
      "self",
      "model",
      "pruner",
      "mask",
      "device"
    ],
    "_test_constructor_on_device": [
      "self",
      "model",
      "device"
    ],
    "test_constructor": [
      "self"
    ],
    "_test_prepare_linear_on_device": [
      "self",
      "model",
      "device"
    ],
    "test_prepare_linear": [
      "self"
    ],
    "_test_prepare_conv2d_on_device": [
      "self",
      "model",
      "expected_shape",
      "config",
      "device"
    ],
    "test_prepare_conv2d": [
      "self"
    ],
    "_test_step_linear_on_device": [
      "self",
      "model",
      "device"
    ],
    "test_step_linear": [
      "self"
    ],
    "_test_step_conv2d_on_device": [
      "self",
      "model",
      "expected_shape",
      "config",
      "device"
    ],
    "test_step_conv2d": [
      "self"
    ],
    "_check_pruner_pruned": [
      "self",
      "model",
      "pruner",
      "device"
    ],
    "_test_linear_on_device": [
      "self",
      "model",
      "config",
      "expected_shape",
      "device",
      "also_prune_bias"
    ],
    "test_prune_linear_linear": [
      "self"
    ],
    "test_prune_linear_bias_linear": [
      "self"
    ],
    "test_prune_linear_activation_linear": [
      "self"
    ],
    "_test_conv2d_on_device": [
      "self",
      "model",
      "config",
      "x",
      "expected_shape",
      "device",
      "also_prune_bias"
    ],
    "test_prune_conv2d_conv2d": [
      "self"
    ],
    "test_prune_conv2d_bias_conv2d": [
      "self"
    ],
    "test_prune_conv2d_activation_conv2d": [
      "self"
    ],
    "test_prune_conv2d_padding_conv2d": [
      "self"
    ],
    "test_prune_conv2d_pool_conv2d": [
      "self"
    ],
    "test_complex_conv2d": [
      "self"
    ],
    "test_prune_lstm_linear_multiple_layer": [
      "self"
    ],
    "test_prune_lstm_linear_single_layer": [
      "self"
    ],
    "test_prune_lstm_layernorm_linear_multiple_layer": [
      "self"
    ],
    "test_prune_lstm_layernorm_linear_single_layer": [
      "self"
    ]
  },
  "TestFPGMPruner": {
    "test_compute_distance": [
      "self",
      "device"
    ],
    "_test_update_mask_on_single_layer": [
      "self",
      "expected_conv1",
      "device"
    ],
    "_test_update_mask_on_multiple_layer": [
      "self",
      "expected_conv1",
      "expected_conv2",
      "device"
    ],
    "test_update_mask": [
      "self"
    ]
  },
  "model_list": [],
  "TestSparsityUtilFunctions": {
    "test_module_to_fqn": [
      "self"
    ],
    "test_module_to_fqn_fail": [
      "self"
    ],
    "test_module_to_fqn_root": [
      "self"
    ],
    "test_fqn_to_module": [
      "self"
    ],
    "test_fqn_to_module_fail": [
      "self"
    ],
    "test_fqn_to_module_for_tensors": [
      "self"
    ],
    "test_get_arg_info_from_tensor_fqn": [
      "self"
    ],
    "test_get_arg_info_from_tensor_fqn_fail": [
      "self"
    ]
  },
  "_reset": [],
  "TestQuantizedTraining": {
    "test_int8_stochastic_rounding": [
      "self",
      "device"
    ],
    "_forward_and_backward": [
      "module",
      "input",
      "grad"
    ],
    "test_int8_weight_only_correctness": [
      "self",
      "leading_dims",
      "bias",
      "device"
    ],
    "test_int8_weight_only_compile": [
      "self",
      "leading_dims",
      "bias",
      "device"
    ],
    "test_int8_weight_only_training": [
      "self",
      "compile",
      "device"
    ],
    "test_int8_mixed_precision_training": [
      "self",
      "compile",
      "config",
      "module_swap"
    ],
    "test_bitnet_training": [
      "self",
      "compile"
    ]
  },
  "ToyConvModel": {
    "__init__": [
      "self",
      "dim",
      "in_channels",
      "out_channels",
      "kernel_size",
      "bias",
      "padding",
      "dtype",
      "device"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ToyLinearSoftmaxModel": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "dtype",
      "device"
    ],
    "forward": [
      "self",
      "x"
    ],
    "example_inputs": [
      "self",
      "batch_size"
    ]
  },
  "TestFloat8StaticActivation": {
    "setUp": [
      "self"
    ],
    "test_static_activation_float8_weight": [
      "self",
      "granularity"
    ],
    "test_creation_and_attributes": [
      "self",
      "granularity"
    ],
    "test_fp8_conv_variants": [
      "self",
      "dtype",
      "compile",
      "inference_mode",
      "sizes"
    ],
    "test_static_quant_flow_with_observers": [
      "self"
    ],
    "test_config_implements_static_quant_protocol": [
      "self"
    ],
    "test_static_quant_with_output_quantization": [
      "self",
      "input_shape"
    ],
    "test_static_quant_softmax": [
      "self"
    ]
  },
  "get_config": [
    "granularity"
  ],
  "TestFloat8OpaqueTensor": {
    "setUp": [
      "self"
    ],
    "test_dynamic_float8_linear": [
      "self",
      "dtype",
      "x_dim",
      "bias",
      "bs",
      "x_granularity",
      "w_granularity"
    ],
    "test_dynamic_float8_linear_fallback_path": [
      "self",
      "dtype",
      "x_dim",
      "bias",
      "bs"
    ],
    "test_module_path": [
      "self",
      "dtype"
    ]
  },
  "_DEVICE": [],
  "M": {
    "__init__": [
      "self",
      "m",
      "n",
      "k",
      "bias",
      "embedding",
      "tied_weights"
    ],
    "tie_weights": [
      "self"
    ],
    "example_inputs": [
      "self",
      "device"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "split_param_groups": [
    "model"
  ],
  "build_param_groups": [
    "model",
    "b",
    "group_size",
    "embed_b"
  ],
  "get_optim_kwargs": [
    "model",
    "base_optimizer",
    "embedding",
    "quant_cls"
  ],
  "compare_quantized_models": [
    "model",
    "m_ref",
    "quantizer",
    "b",
    "group_size"
  ],
  "compare_parq_convert": [
    "model",
    "m_ref",
    "optimizer",
    "weight_only"
  ],
  "check_torchao_tensor_subclass": [
    "test_case",
    "model",
    "weight_only"
  ],
  "apply_activation_quantization": [
    "model",
    "optimizer",
    "model_dtype"
  ],
  "TestPARQuantization": {
    "setUp": [
      "self"
    ],
    "test_parq_train_loop": [
      "self",
      "b",
      "unif_quant",
      "hard_prox",
      "per_group_quantizer"
    ]
  },
  "TestUnifTorchaoQuantizer": {
    "setUp": [
      "self"
    ],
    "test_int4_weight_only": [
      "self",
      "group_size"
    ],
    "test_intx_weight_only": [
      "self",
      "b",
      "group_size"
    ],
    "test_int4_weight_only_e2e": [
      "self",
      "group_size"
    ],
    "test_intx_weight_only_e2e": [
      "self",
      "b",
      "group_size"
    ]
  },
  "TestStretchedUnifTorchaoQuantizer": {
    "setUp": [
      "self"
    ],
    "test_intx_weight_only_parq_equivalent": [
      "self",
      "b",
      "group_size"
    ],
    "test_intx_weight_only": [
      "self",
      "b",
      "group_size"
    ],
    "test_intx_weight_only_e2e": [
      "self",
      "b",
      "group_size"
    ],
    "test_intx_weight_only_tied_embed_linear": [
      "self",
      "b",
      "model_dtype"
    ]
  },
  "TestInt8DynamicActivationTorchaoQuantizer": {
    "setUp": [
      "self"
    ],
    "test_int8_dynamic_activation_intx_e2e": [
      "self",
      "b",
      "model_dtype",
      "group_size"
    ]
  },
  "TestTorchAoConfigIntegration": {
    "test_tied_weights_quantization": [
      "self",
      "b"
    ]
  },
  "_is_fp8_sdpa_available": [],
  "TestFP8SDPAInference": {
    "test_numerical_accuracy": [
      "self",
      "shape",
      "dtype"
    ],
    "test_different_qkv_shapes": [
      "self"
    ],
    "test_wrap_module_with_fp8_sdpa": [
      "self"
    ],
    "test_unsupported_attn_mask_raises": [
      "self"
    ],
    "test_unsupported_dropout_raises": [
      "self"
    ],
    "test_causal_attention": [
      "self"
    ],
    "test_wrap_module_with_compile": [
      "self"
    ]
  },
  "TestEmbeddingQuantizer": {
    "test_accuracy": [
      "self"
    ],
    "test_export_compile_aoti": [
      "self"
    ],
    "test_shared_embedding": [
      "self"
    ],
    "test_identical_to_IntxWeightOnlyConfig": [
      "self",
      "weight_dtype",
      "granularity",
      "mapping_type",
      "model_dtype"
    ],
    "test_identical_to_IntXQuantizationAwareTrainingConfig": [
      "self",
      "weight_dtype",
      "granularity",
      "mapping_type",
      "scale_dtype",
      "model_dtype"
    ],
    "test_identical_to_Int4WeightOnlyEmbeddingQATQuantizer": [
      "self",
      "granularity",
      "scale_dtype",
      "model_dtype"
    ]
  },
  "ToyLinearModelWithTiedEmbedding": {
    "__init__": [
      "self",
      "d0",
      "d1",
      "d2",
      "d3",
      "d4"
    ],
    "example_inputs": [
      "self",
      "lead_dim",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "run_before_and_after_tests": [],
  "test_aarch64_conversion": [
    "dtype",
    "granularity",
    "bit_width",
    "lead_dim"
  ],
  "test_int4_tensor_conversion": [],
  "ToyLinearModel": {
    "__init__": [
      "self",
      "m",
      "n",
      "k",
      "dtype",
      "device"
    ],
    "example_inputs": [
      "self",
      "batch_size",
      "sequence_length"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "devices": [],
  "device_to_base_configs": [],
  "device_config_pairs": [],
  "TestAWQ": {
    "test_awq_config": [
      "self"
    ],
    "test_awq_functionality": [
      "self",
      "device",
      "base_config"
    ],
    "test_awq_loading": [
      "self",
      "device",
      "base_config"
    ],
    "test_awq_loading_vllm": [
      "self",
      "device",
      "base_config"
    ]
  },
  "TestParetoQ": {
    "test_quantized_linear": [
      "self"
    ],
    "test_quantize_functions": [
      "self"
    ]
  },
  "TestBaseSparsifier": {
    "test_constructor": [
      "self"
    ],
    "test_prepare_config": [
      "self"
    ],
    "test_step": [
      "self"
    ],
    "test_state_dict": [
      "self"
    ],
    "test_convert": [
      "self"
    ],
    "test_mask_squash": [
      "self"
    ],
    "test_mask_squash_with_params1": [
      "self"
    ],
    "test_mask_squash_with_params2": [
      "self"
    ],
    "test_mask_squash_with_params3": [
      "self"
    ]
  },
  "TestWeightNormSparsifier": {
    "test_constructor": [
      "self"
    ],
    "test_step": [
      "self"
    ],
    "test_step_2_of_4": [
      "self"
    ],
    "test_prepare": [
      "self"
    ],
    "test_mask_squash": [
      "self"
    ],
    "test_sparsity_levels": [
      "self"
    ]
  },
  "TestNearlyDiagonalSparsifier": {
    "test_constructor": [
      "self"
    ],
    "test_step": [
      "self"
    ],
    "test_prepare": [
      "self"
    ],
    "test_mask_squash": [
      "self"
    ],
    "test_sparsity_levels": [
      "self"
    ],
    "_verify_nearliness": [
      "self",
      "mask",
      "nearliness"
    ]
  },
  "test_parq_conversion": [
    "dtype",
    "granularity",
    "bit_width",
    "lead_dim"
  ],
  "test_export": [
    "dtype",
    "granularity",
    "bit_width",
    "lead_dim"
  ],
  "_CUDA_IS_AVAILABLE": [],
  "TestWeightOnlyQuantNaive": {
    "test_quantization_intNwo": [
      "self"
    ]
  },
  "TestGroupwiseLowbitWeightLut": {
    "TEST_CASES": [],
    "test_e2e_accuracy_vs_reference": [
      "self",
      "code_dtype",
      "lut_group_size",
      "weight_dtype",
      "has_bias"
    ],
    "tearDown": [
      "self"
    ],
    "test_export_compile_aoti": [
      "self",
      "code_dtype",
      "lut_group_size",
      "weight_dtype",
      "has_bias"
    ]
  },
  "_AVAILABLE_DEVICES": [],
  "TwoLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features"
    ],
    "forward": [
      "self",
      "x",
      "y"
    ]
  },
  "_is_two_linear": [
    "mod",
    "fqn"
  ],
  "ModelWithInplaceOp": {
    "__init__": [
      "self",
      "DIM"
    ],
    "forward": [
      "self",
      "x",
      "idx"
    ]
  },
  "M2": {
    "__init__": [
      "self",
      "DIM"
    ],
    "forward": [
      "self",
      "x",
      "idx"
    ]
  },
  "_check_params_and_buffers_type": [
    "module",
    "check_fun"
  ],
  "TestAutoRound": {
    "test_auto_round": [
      "self",
      "device"
    ],
    "test_wrap_model_with_multi_tensor": [
      "self",
      "device"
    ]
  },
  "ImplementedScheduler": {
    "get_sl": [
      "self"
    ]
  },
  "TestScheduler": {
    "test_constructor": [
      "self"
    ],
    "test_order_of_steps": [
      "self"
    ],
    "test_step": [
      "self"
    ],
    "test_lambda_scheduler": [
      "self"
    ]
  },
  "TestCubicScheduler": {
    "setUp": [
      "self"
    ],
    "_make_model": [
      "self"
    ],
    "_make_scheduler": [
      "self",
      "model"
    ],
    "_get_sparsity_levels": [
      "sparsifier",
      "precision"
    ],
    "test_constructor": [
      "self"
    ],
    "test_step": [
      "self"
    ]
  },
  "TestInt4OpaqueTensor": {
    "test_linear": [
      "self",
      "sizes",
      "dtype",
      "group_size",
      "use_hqq"
    ],
    "test_module_path": [
      "self",
      "dtype",
      "use_hqq"
    ],
    "test_activation_prescaling": [
      "self",
      "use_hqq"
    ]
  },
  "device_list": [],
  "TestSmoothQuant": {
    "setUpClass": [
      "cls"
    ],
    "test_smoothquant_accuracy": [
      "self",
      "alpha",
      "base_config",
      "device",
      "input_dtype"
    ],
    "test_observer_insertion": [
      "self",
      "base_config"
    ],
    "test_prepare_for_loading": [
      "self",
      "base_config"
    ]
  },
  "TestGGUFQuantization": {
    "setUp": [
      "self"
    ],
    "test_choose_qparams_gguf": [
      "self"
    ],
    "test_gguf_quantized_tensor_from_float": [
      "self"
    ],
    "test_quantize_api": [
      "self"
    ]
  },
  "ModelUnderTest": {
    "__init__": [
      "self",
      "bias"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "TestFakeSparsity": {
    "test_masking_logic": [
      "self"
    ],
    "test_weights_parametrized": [
      "self"
    ],
    "test_state_dict_preserved": [
      "self"
    ],
    "test_jit_trace": [
      "self"
    ]
  },
  "device_mesh_2d": [],
  "test_moe_training_fsdp_tp": [
    "target_fqns",
    "compile",
    "recipe_config",
    "device_mesh_2d"
  ],
  "apply_moe_ep_tp": [
    "model",
    "tp_mesh",
    "ep_mesh",
    "ep_tp_mesh"
  ],
  "test_valid_scaled_grouped_mm_2d_3d": [
    "m",
    "n",
    "k",
    "n_groups"
  ],
  "test_K_or_N_dim_not_multiple_of_16": [
    "m",
    "n",
    "k"
  ],
  "compute_reference_forward": [
    "result",
    "A",
    "B_t",
    "n_groups",
    "out_dtype",
    "offs"
  ],
  "test_emulate_mxfp8_grouped_gemm_2d_3d": [
    "M",
    "K",
    "N",
    "num_experts"
  ],
  "test_emulate_mxfp8_grouped_gemm_2d_2d": [
    "M",
    "N",
    "num_experts"
  ],
  "test_mxfp8_grouped_gemm_with_dq_fwd_bwd": [
    "M",
    "K",
    "N",
    "num_experts",
    "wgrad_with_hp",
    "use_compile",
    "kernel_preference",
    "scale_mode"
  ],
  "device_mesh_1d": [],
  "test_moe_training_fsdp": [
    "target_fqns",
    "compile",
    "recipe_config",
    "device_mesh_1d"
  ],
  "TOKEN_GROUP_ALIGN_SIZE_M": [],
  "ValidTokenGroupAlignmentSize": [],
  "set_token_group_alignment_size_m": [
    "alignment_size"
  ],
  "_permute": [
    "x",
    "num_tokens_per_expert",
    "ep_degree",
    "num_local_experts"
  ],
  "_unpermute": [
    "out",
    "input_shape",
    "permuted_indices"
  ],
  "indices_padding_wrapper": [
    "func"
  ],
  "MoEArgs": {},
  "_run_experts_for_loop": [
    "w1",
    "w2",
    "w3",
    "x",
    "num_tokens_per_expert"
  ],
  "_run_experts_grouped_mm": [
    "w1",
    "w2",
    "w3",
    "x",
    "num_tokens_per_expert"
  ],
  "GroupedExperts": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim",
      "num_experts",
      "use_grouped_mm"
    ],
    "forward": [
      "self",
      "x",
      "num_tokens_per_expert"
    ],
    "init_weights": [
      "self",
      "init_std"
    ]
  },
  "TokenChoiceTopKRouter": {
    "__init__": [
      "self",
      "dim",
      "num_experts",
      "num_expert_groups",
      "num_limited_groups",
      "top_k",
      "score_func",
      "route_norm",
      "route_scale",
      "_debug_force_load_balance"
    ],
    "_debug_force_load_balance_routing": [
      "self",
      "scores"
    ],
    "_get_node_limited_routing_scores": [
      "self",
      "scores_for_choice"
    ],
    "forward": [
      "self",
      "x",
      "expert_bias"
    ],
    "init_weights": [
      "self",
      "init_std"
    ]
  },
  "TokenReorderer": {
    "__init__": [
      "self",
      "num_experts",
      "top_k"
    ],
    "forward": [
      "self",
      "top_scores",
      "selected_experts_indices"
    ]
  },
  "MoE": {
    "__init__": [
      "self",
      "moe_args",
      "dim",
      "hidden_dim"
    ],
    "forward": [
      "self",
      "x"
    ],
    "init_weights": [
      "self",
      "init_std",
      "buffer_device"
    ]
  },
  "test_moe_training": [
    "target_fqns",
    "compile",
    "kernel_preference",
    "recipe_config"
  ],
  "test_row_major_with_jagged_rowwise_scales": [
    "round_scales_to_power_of_2"
  ],
  "test_row_major_with_jagged_rowwise_scales_transpose_method": [
    "round_scales_to_power_of_2"
  ],
  "test_column_major_with_jagged_colwise_scales": [
    "round_scales_to_power_of_2"
  ],
  "test_fp8_rowwise_3d_transpose_rhs_atomic": [
    "round_scales_to_power_of_2"
  ],
  "test_fp8_rowwise_3d_transpose_rhs_reduction": [
    "round_scales_to_power_of_2"
  ],
  "test_triton_mx_block_rearrange_2d_M_groups": [
    "m",
    "k",
    "n_groups"
  ],
  "test_cuda_mx_block_rearrange_2d_M_groups": [
    "m",
    "k",
    "n_groups",
    "chunks_per_tb"
  ],
  "test_mxfp8_per_group_blocked_scales_3d": [
    "e",
    "n",
    "k"
  ],
  "test_triton_mx_block_rearrange_2d_K_groups": [
    "m",
    "total_k",
    "n_groups"
  ],
  "test_cuda_mx_dim1_3d_numerics": [
    "E",
    "N",
    "K",
    "input_dtype",
    "scaling_mode"
  ],
  "_validate_model_conversion": [
    "root_module",
    "target_fqns"
  ],
  "generate_split_sizes": [
    "K",
    "N",
    "device"
  ],
  "NoParallel": {
    "_apply": [
      "self",
      "module",
      "device_mesh"
    ]
  },
  "TensorParallel": {
    "_apply": [
      "self",
      "module",
      "device_mesh"
    ]
  },
  "ExpertParallel": {
    "_apply": [
      "self",
      "module",
      "device_mesh"
    ]
  },
  "ExpertTensorParallel": {
    "__init__": [
      "self",
      "tp_mesh",
      "ep_mesh"
    ],
    "_apply": [
      "self",
      "module",
      "device_mesh"
    ]
  },
  "test_moe_training_tp": [
    "target_fqns",
    "kernel_preference",
    "compile",
    "recipe_config",
    "device_mesh_1d"
  ],
  "repo_root": [],
  "standard_pipeline": [
    "input_tensor",
    "expert_weights_t",
    "num_tokens_per_expert",
    "num_tokens_per_expert_group",
    "input_splits_list",
    "output_splits_list",
    "ep_degree",
    "num_experts",
    "group"
  ],
  "mxfp8_pipeline": [
    "input_tensor",
    "expert_weights_t",
    "num_tokens_per_expert",
    "num_tokens_per_expert_group",
    "input_splits_list",
    "output_splits_list",
    "ep_degree",
    "num_experts",
    "group"
  ],
  "TestIntegrationCompiled": {
    "setUp": [
      "self"
    ],
    "world_size": [
      "self"
    ],
    "device": [
      "self"
    ],
    "_init_process": [
      "self"
    ],
    "test_full_pipeline_compiled": [
      "self"
    ]
  },
  "rank_0_print": [
    "msg"
  ],
  "test_mxfp8_permute_forward": [],
  "test_triton_permute_bwd": [
    "num_tokens",
    "hidden_dim",
    "num_local_experts",
    "ep_degree",
    "alignment"
  ],
  "TestA2ADispatch": {
    "setUp": [
      "self"
    ],
    "world_size": [
      "self"
    ],
    "device": [
      "self"
    ],
    "_init_process": [
      "self"
    ],
    "test": [
      "self"
    ]
  },
  "TestIntegration": {
    "setUp": [
      "self"
    ],
    "world_size": [
      "self"
    ],
    "device": [
      "self"
    ],
    "_init_process": [
      "self"
    ],
    "test_full_pipeline": [
      "self"
    ]
  },
  "MXFP8OnDeviceAllToAllVTest": {
    "setUp": [
      "self"
    ],
    "world_size": [
      "self"
    ],
    "device": [
      "self"
    ],
    "_init_process": [
      "self"
    ],
    "_init_device": [
      "self"
    ],
    "test_a2a_fwd_bwd": [
      "self"
    ]
  },
  "ToMXFP8AllToAllVDequantTest": {
    "setUp": [
      "self"
    ],
    "world_size": [
      "self"
    ],
    "device": [
      "self"
    ],
    "_init_process": [
      "self"
    ],
    "_init_device": [
      "self"
    ],
    "test_a2a_fwd_bwd": [
      "self"
    ]
  }
}