{
  "__version__": [],
  "__all__": [],
  "_updated_scale": [
    "scale",
    "new_scale",
    "momentum"
  ],
  "absmax_scale": [
    "base",
    "qtype",
    "axis"
  ],
  "Calibration": {
    "__init__": [
      "self"
    ],
    "__torch_function__": [
      "self",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_val",
      "exc_tb"
    ],
    "calibrate_input": [
      "self",
      "module",
      "input",
      "momentum"
    ],
    "calibrate_output": [
      "self",
      "module",
      "input",
      "output"
    ],
    "tag_outputs": [
      "self",
      "module",
      "input",
      "output"
    ]
  },
  "set_module_by_name": [
    "parent_module",
    "name",
    "child_module"
  ],
  "_quantize_submodule": [
    "model",
    "name",
    "module",
    "weights",
    "activations",
    "optimizer"
  ],
  "quantize": [
    "model",
    "weights",
    "activations",
    "optimizer",
    "include",
    "exclude"
  ],
  "requantize": [
    "model",
    "state_dict",
    "quantization_map",
    "device"
  ],
  "freeze": [
    "model"
  ],
  "quantization_map": [
    "model"
  ],
  "pack_weights": [
    "intweights",
    "bits"
  ],
  "PackedTensor": {
    "__new__": [
      "cls",
      "data",
      "bits",
      "size",
      "stride",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "data",
      "bits",
      "size",
      "stride",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "pack": [
      "cls",
      "t",
      "bits"
    ],
    "unpack": [
      "self"
    ],
    "bits": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "load_from_state_dict": [
      "state_dict",
      "prefix",
      "bits",
      "size",
      "stride",
      "missing_keys"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ],
    "numpy": [
      "self"
    ]
  },
  "dtype_info": [
    "dtype"
  ],
  "axis_to_dim": [
    "t",
    "axis"
  ],
  "qtype": {
    "__str__": [
      "self"
    ],
    "__hash__": [
      "self"
    ]
  },
  "qint": [
    "bits"
  ],
  "qint2": [],
  "qint4": [],
  "qint8": [],
  "qfloat": [
    "dtype"
  ],
  "qfloat8_e4m3fn": [],
  "qfloat8_e4m3fnuz": [],
  "qfloat8_e5m2": [],
  "qfloat8": [],
  "qtypes": [],
  "grouped_shape": [
    "shape",
    "axis",
    "group_size"
  ],
  "group": [
    "base",
    "axis",
    "group_size"
  ],
  "ungroup": [
    "grouped",
    "axis",
    "orig_shape"
  ],
  "QBytesDequantizer": {
    "forward": [
      "ctx",
      "t"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "QBytesTensor": {
    "__init__": [
      "self",
      "qtype",
      "axis",
      "size",
      "stride",
      "data",
      "scale",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "dequantize": [
      "self"
    ]
  },
  "QuantizedLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "other",
      "bias"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "QBitsDequantizer": {
    "forward": [
      "ctx",
      "t"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "QBitsTensor": {
    "__init__": [
      "self",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "dequantize": [
      "self"
    ]
  },
  "qfallback": [
    "callable"
  ],
  "QTensor": {
    "__init__": [
      "self",
      "qtype",
      "axis"
    ],
    "dequantize": [
      "self"
    ],
    "save_to_state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ],
    "axis": [
      "self"
    ],
    "qtype": [
      "self"
    ],
    "numpy": [
      "self"
    ],
    "equal": [
      "self",
      "other"
    ]
  },
  "quantize_activation": [
    "t",
    "qtype",
    "scale"
  ],
  "ActivationQBytesQuantizer": {
    "forward": [
      "ctx",
      "base",
      "qtype",
      "scale"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "ActivationQBytesTensor": {
    "__new__": [
      "cls",
      "qtype",
      "size",
      "stride",
      "data",
      "scale",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "qtype",
      "size",
      "stride",
      "data",
      "scale",
      "requires_grad"
    ],
    "quantize": [
      "cls",
      "base",
      "qtype",
      "scale"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ]
  },
  "_QBYTESTENSOR_OP_TABLE": [],
  "register_qbytestensor_op": [
    "aten_ops"
  ],
  "get_qbytestensor_op_dispatch": [
    "aten_op"
  ],
  "is_scalar": [
    "t"
  ],
  "_to_copy": [
    "op",
    "t",
    "dtype"
  ],
  "detach": [
    "op",
    "t"
  ],
  "cat": [
    "op",
    "inputs",
    "dim"
  ],
  "lt": [
    "op",
    "input",
    "other"
  ],
  "clone": [
    "op",
    "t",
    "memory_format"
  ],
  "copy_": [
    "op",
    "dest",
    "src"
  ],
  "div": [
    "op",
    "input",
    "other"
  ],
  "neg": [
    "op",
    "input"
  ],
  "unary_type_agnostic_op": [
    "op",
    "input"
  ],
  "is_same_size": [
    "op",
    "input",
    "other"
  ],
  "cannot_mm": [
    "t"
  ],
  "bmm": [
    "op",
    "input",
    "other"
  ],
  "mul": [
    "op",
    "input",
    "other"
  ],
  "relu": [
    "op",
    "input"
  ],
  "_softmax": [
    "op",
    "input",
    "dim",
    "half_to_float"
  ],
  "stack": [
    "op",
    "inputs",
    "dim"
  ],
  "split": [
    "op",
    "input"
  ],
  "transpose": [
    "op",
    "input"
  ],
  "transpose2d": [
    "op",
    "input"
  ],
  "view": [
    "op",
    "input"
  ],
  "where": [
    "op",
    "condition",
    "input",
    "other"
  ],
  "quantize_weight": [
    "t",
    "qtype",
    "axis",
    "scale",
    "shift",
    "group_size",
    "activation_qtype",
    "optimized"
  ],
  "WeightQBytesQuantizer": {
    "forward": [
      "ctx",
      "base",
      "qtype",
      "axis",
      "scale",
      "activation_qtype",
      "optimized"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "WeightQBytesLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "other",
      "bias"
    ]
  },
  "WeightQBytesTensor": {
    "create": [
      "qtype",
      "axis",
      "size",
      "stride",
      "data",
      "scale",
      "activation_qtype",
      "requires_grad"
    ],
    "__new__": [
      "cls",
      "qtype",
      "axis",
      "size",
      "stride",
      "data",
      "scale",
      "activation_qtype",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "qtype",
      "axis",
      "size",
      "stride",
      "data",
      "scale",
      "activation_qtype",
      "requires_grad"
    ],
    "quantize": [
      "cls",
      "base",
      "qtype",
      "axis",
      "scale",
      "activation_qtype",
      "optimized"
    ],
    "load_from_state_dict": [
      "state_dict",
      "prefix",
      "qtype",
      "axis",
      "size",
      "stride",
      "activation_qtype",
      "missing_keys"
    ],
    "optimize": [
      "self"
    ],
    "save_to_state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ],
    "weight_qbytes_tensor": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ]
  },
  "unpack_int32_to_uint8": [
    "packed",
    "bits"
  ],
  "WeightsQBitsQuantizer": {
    "forward": [
      "ctx",
      "base",
      "qtype",
      "axis",
      "group_size",
      "scale",
      "shift",
      "optimized"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "WeightQBitsTensor": {
    "create": [
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "__new__": [
      "cls",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "quantize": [
      "cls",
      "base",
      "qtype",
      "axis",
      "group_size",
      "scale",
      "shift",
      "optimized"
    ],
    "load_from_state_dict": [
      "state_dict",
      "prefix",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "missing_keys"
    ],
    "optimize": [
      "self"
    ],
    "save_to_state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ],
    "weight_qbits_tensor": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ]
  },
  "reorder": [
    "t",
    "permutation"
  ],
  "reverse": [
    "permutation"
  ],
  "AWQ_ORDER": [],
  "AWQ_REVERSE_ORDER": [],
  "pack": [
    "unpacked",
    "reorder"
  ],
  "reverse_awq_order": [
    "t"
  ],
  "unpack": [
    "packed",
    "reorder"
  ],
  "pack_v2": [
    "unpacked"
  ],
  "unpack_v2": [
    "packed"
  ],
  "AWQPacking": {
    "V1": [],
    "V2": []
  },
  "AWQPackedTensor": {
    "__new__": [
      "cls",
      "data",
      "packing",
      "reorder",
      "size",
      "stride",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "data",
      "packing",
      "reorder",
      "size",
      "stride",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "pack": [
      "cls",
      "t",
      "packing",
      "reorder"
    ],
    "unpack": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ],
    "numpy": [
      "self"
    ]
  },
  "AWQWeightQBitsDequantizer": {
    "forward": [
      "ctx",
      "t"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "AWQWeightQBitsLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "other",
      "bias"
    ]
  },
  "AWQWeightQBitsTensor": {
    "__new__": [
      "cls",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "dequantize": [
      "self"
    ],
    "weight_qbits_tensor": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ]
  },
  "TinyGemmPackedTensor": {
    "__new__": [
      "cls",
      "data",
      "size",
      "stride",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "data",
      "size",
      "stride",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "pack": [
      "cls",
      "t"
    ],
    "unpack": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ],
    "numpy": [
      "self"
    ]
  },
  "TinyGemmQBitsDequantizer": {
    "forward": [
      "ctx",
      "t"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "TinyGemmQBitsLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "other",
      "bias"
    ]
  },
  "TinyGemmWeightQBitsTensor": {
    "__new__": [
      "cls",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale_shift",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale_shift",
      "requires_grad"
    ],
    "dequantize": [
      "self"
    ],
    "weight_qbits_tensor": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ]
  },
  "_get_perms": [],
  "_get_inverted_perms": [],
  "marlin_permute": [
    "t",
    "reverse"
  ],
  "_get_perm": [],
  "_perm": [],
  "_rev_perm": [],
  "MarlinInt4PackedTensor": {
    "__new__": [
      "cls",
      "data",
      "size",
      "stride",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "data",
      "size",
      "stride",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "pack": [
      "cls",
      "t"
    ],
    "unpack": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ],
    "numpy": [
      "self"
    ]
  },
  "MarlinQBitsDequantizer": {
    "forward": [
      "ctx",
      "t"
    ],
    "backward": [
      "ctx",
      "gO"
    ]
  },
  "MarlinQBitsLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "other",
      "bias"
    ]
  },
  "MarlinInt4WeightQBitsTensor": {
    "__new__": [
      "cls",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "qtype",
      "axis",
      "group_size",
      "size",
      "stride",
      "data",
      "scale",
      "shift",
      "requires_grad"
    ],
    "dequantize": [
      "self"
    ],
    "weight_qbits_tensor": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ]
  },
  "pack_fp8_as_int32": [
    "fp8_tensor"
  ],
  "unpack_int32_to_fp8": [
    "int32_tensor"
  ],
  "get_scale_perms": [],
  "get_row_permutation": [
    "n_rows"
  ],
  "get_column_permutation": [
    "n_col"
  ],
  "MarlinF8PackedTensor": {
    "__new__": [
      "cls",
      "data",
      "size",
      "stride",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "data",
      "size",
      "stride",
      "requires_grad"
    ],
    "__repr__": [
      "self"
    ],
    "pack": [
      "cls",
      "tensor"
    ],
    "unpack": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [],
    "__torch_dispatch__": [
      "cls",
      "op",
      "types",
      "args",
      "kwargs"
    ]
  },
  "MarlinF8QBytesLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "other",
      "bias"
    ]
  },
  "MarlinF8QBytesTensor": {
    "__new__": [
      "cls",
      "qtype",
      "axis",
      "size",
      "stride",
      "data",
      "scale",
      "requires_grad"
    ],
    "__init__": [
      "self",
      "qtype",
      "axis",
      "size",
      "stride",
      "data",
      "scale",
      "requires_grad"
    ],
    "dequantize": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "weight_qbytes_tensor": [
      "self"
    ],
    "__tensor_flatten__": [
      "self"
    ],
    "__tensor_unflatten__": [
      "inner_tensors",
      "meta",
      "outer_size",
      "outer_stride"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ]
  },
  "MaxOptimizer": {
    "optimize": [
      "self",
      "base",
      "qtype",
      "axis"
    ]
  },
  "AffineOptimizer": {
    "__call__": [
      "self",
      "base",
      "qtype",
      "axis",
      "group_size",
      "zeropoint"
    ],
    "optimize": [
      "self",
      "base",
      "qtype",
      "axis"
    ]
  },
  "SymmetricOptimizer": {
    "__call__": [
      "self",
      "base",
      "qtype",
      "axis"
    ],
    "optimize": [
      "self",
      "base",
      "qmax",
      "axis"
    ]
  },
  "AbsmaxOptimizer": {
    "optimize": [
      "self",
      "base",
      "qtype",
      "axis"
    ]
  },
  "shrink_lp_op": [
    "x",
    "beta",
    "lp_norm"
  ],
  "HqqOptimizer": {
    "__init__": [
      "self",
      "lp_norm",
      "beta",
      "kappa",
      "iters",
      "verbose"
    ],
    "optimize": [
      "self",
      "base",
      "qtype",
      "axis"
    ]
  },
  "Optimizer": {
    "__call__": [
      "self",
      "base",
      "bits",
      "axis",
      "group_size"
    ]
  },
  "_QMODULE_TABLE": [],
  "register_qmodule": [
    "module_cls"
  ],
  "quantize_module": [
    "module",
    "weights",
    "activations",
    "optimizer"
  ],
  "QModuleMixin": {
    "__init__": [
      "self"
    ],
    "disable_output_quantization": [
      "self"
    ],
    "_save_to_state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "from_module": [
      "cls",
      "module",
      "weights",
      "activations",
      "optimizer"
    ],
    "qcreate": [
      "cls",
      "module",
      "weights",
      "activations",
      "optimizer",
      "device"
    ],
    "qweight": [
      "self"
    ],
    "qforward": [
      "self",
      "input"
    ],
    "quantize_input": [
      "self",
      "module",
      "input"
    ],
    "quantize_output": [
      "self",
      "module",
      "input",
      "output"
    ],
    "freeze": [
      "self"
    ],
    "frozen": [
      "self"
    ]
  },
  "QLinear": {
    "qcreate": [
      "cls",
      "module",
      "weights",
      "activations",
      "optimizer",
      "device"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "QLayerNorm": {
    "qcreate": [
      "cls",
      "module",
      "weights",
      "activations",
      "optimizer",
      "device"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "QConv2d": {
    "qcreate": [
      "cls",
      "module",
      "weights",
      "activations",
      "optimizer",
      "device"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "quantize_symmetric": [
    "base",
    "dtype",
    "axis",
    "scale"
  ],
  "quantize_affine": [
    "base",
    "bits",
    "axis",
    "group_size",
    "scale",
    "shift"
  ],
  "qbytes_mm": [
    "activations",
    "weights",
    "output_scales"
  ],
  "qbytes_int_mm": [
    "activations",
    "weights",
    "output_scales"
  ],
  "qbytes_int8pack_mm": [
    "activations",
    "weights",
    "output_scales"
  ],
  "qbytes_mm_impl_default": [
    "activations",
    "weights",
    "output_scales"
  ],
  "qbytes_mm_impl_cuda": [
    "activations",
    "weights",
    "output_scales"
  ],
  "qbytes_mm_impl_cpu": [
    "activations",
    "weights",
    "output_scales"
  ],
  "qbytes_mm_impl_mps": [
    "activations",
    "weights",
    "output_scales"
  ],
  "Extension": {
    "__init__": [
      "self",
      "name",
      "root_dir",
      "sources",
      "extra_cflags",
      "extra_cuda_cflags"
    ],
    "lib": [
      "self"
    ]
  },
  "_extensions": [],
  "register_extension": [
    "extension"
  ],
  "get_extension": [
    "extension_type"
  ],
  "is_extension_available": [
    "extension_type"
  ],
  "ext": [],
  "unpack_hip": [
    "t",
    "bits"
  ],
  "unpack_cpp": [
    "t",
    "bits"
  ],
  "unpack_mps": [
    "t",
    "bits"
  ],
  "get_max_cuda_arch": [],
  "extra_cflags": [],
  "extra_cuda_cflags": [],
  "quanto_cuda_arch": [],
  "module_path": [],
  "sources": [],
  "unpack_cuda": [
    "t",
    "bits"
  ],
  "gemm_f16i4_awq": [
    "input",
    "other",
    "scales",
    "shift",
    "rows",
    "out_cols",
    "in_cols",
    "bits",
    "group_size"
  ],
  "fp8_marlin_gemm": [
    "a",
    "b_q_weight",
    "b_scales",
    "workspace",
    "num_bits",
    "size_m",
    "size_n",
    "size_k"
  ],
  "gptq_marlin_repack": [
    "b_q_weight",
    "perm",
    "size_k",
    "size_n",
    "num_bits"
  ],
  "gemm_f16i4_marlin": [
    "input",
    "other",
    "scales",
    "zeropoint",
    "workspace"
  ],
  "QuantoCommand": {
    "COMMAND": [],
    "SUBCOMMANDS": []
  },
  "SUPPORTED_LIBRARIES": [],
  "parse_quantize_args": [
    "parser"
  ],
  "QuantizeCommand": {
    "parse_args": [
      "parser"
    ],
    "run": [
      "self"
    ]
  },
  "ShardedStateDict": {
    "__init__": [
      "self",
      "base_dir",
      "tensor_index"
    ],
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__contains__": [
      "self",
      "key"
    ],
    "keys": [
      "self"
    ]
  },
  "QuantizedDiffusersModel": {
    "BASE_NAME": [],
    "base_class": [],
    "__init__": [
      "self",
      "model"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "forward": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "_qmap_name": [],
    "quantize": [
      "cls",
      "model",
      "weights",
      "activations",
      "optimizer",
      "include",
      "exclude"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "_save_pretrained": [
      "self",
      "save_directory"
    ]
  },
  "QuantizedPixArtTransformer2DModel": {
    "base_class": []
  },
  "is_transformers_available": [],
  "is_diffusers_available": [],
  "QuantizedTransformersModel": {
    "BASE_NAME": [],
    "auto_class": [],
    "__init__": [
      "self",
      "model"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "forward": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "_qmap_name": [],
    "quantize": [
      "cls",
      "model",
      "weights",
      "activations",
      "optimizer",
      "include",
      "exclude"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "_save_pretrained": [
      "self",
      "save_directory"
    ]
  },
  "QuantizedModelForCausalLM": {
    "auto_class": []
  }
}