{
  "op4_mode_mappings": [],
  "op3_mode_mappings": [],
  "all_mode_mappings": [],
  "mode_mappings_to_int": [],
  "int_mappings_to_mode": [],
  "int_mode": [],
  "mode_is_op4": [
    "mode"
  ],
  "sort_tensor_product_info_for_operand": [
    "path_offsets_and_dims",
    "path_cg_values",
    "op_idx"
  ],
  "path_offsets_to_csr": [
    "path_offsets_and_dims",
    "op_idx",
    "op_stride"
  ],
  "make_tensor_product_info": [
    "op_idx",
    "op_stride",
    "path_offsets",
    "path_cg_values"
  ],
  "make_tensor_product_infos": [
    "strides",
    "path_offsets_and_dims",
    "path_cg_values",
    "math_dtype"
  ],
  "TensorProductPathData": {
    "__init__": [
      "self",
      "path_csr_offsets",
      "path_offsets",
      "path_cg_values",
      "math_dtype"
    ],
    "forward": [
      "self"
    ]
  },
  "update_batch_size": [
    "batch_size",
    "t"
  ],
  "get_batch_size": [
    "in0",
    "in1",
    "in2"
  ],
  "tensor_product_info_as_ctype": [
    "path_csr_offsets",
    "path_offsets",
    "path_cg_values"
  ],
  "_": [
    "grad_grad_in0",
    "grad_grad_in1",
    "grad_grad_in2",
    "grad_out",
    "in0",
    "in1",
    "in2",
    "tp_path_csr_offsets_fwd",
    "tp_path_csr_offsets_dgrad_in0",
    "tp_path_csr_offsets_dgrad_in1",
    "tp_path_csr_offsets_dgrad_in2",
    "tp_path_offsets_fwd",
    "tp_path_offsets_dgrad_in0",
    "tp_path_offsets_dgrad_in1",
    "tp_path_offsets_dgrad_in2",
    "tp_path_cg_values_fwd",
    "tp_path_cg_values_dgrad_in0",
    "tp_path_cg_values_dgrad_in1",
    "tp_path_cg_values_dgrad_in2",
    "connection_mode",
    "needs_grad_in0",
    "needs_grad_in1",
    "needs_grad_in2"
  ],
  "fused_tensor_product_setup_fwd_context": [
    "ctx",
    "inputs",
    "output"
  ],
  "fused_tensor_product_setup_bwd_context": [
    "ctx",
    "inputs",
    "output"
  ],
  "fused_tensor_product_bwd_bwd": [
    "ctx",
    "grad_grads"
  ],
  "fused_tensor_product_bwd": [
    "ctx",
    "grad_output"
  ],
  "fused_tensor_product": [
    "in0",
    "in1",
    "in2",
    "tp_path_csr_offsets_fwd",
    "tp_path_csr_offsets_dgrad_in0",
    "tp_path_csr_offsets_dgrad_in1",
    "tp_path_csr_offsets_dgrad_in2",
    "tp_path_offsets_fwd",
    "tp_path_offsets_dgrad_in0",
    "tp_path_offsets_dgrad_in1",
    "tp_path_offsets_dgrad_in2",
    "tp_path_cg_values_fwd",
    "tp_path_cg_values_dgrad_in0",
    "tp_path_cg_values_dgrad_in1",
    "tp_path_cg_values_dgrad_in2",
    "connection_mode",
    "output_stride"
  ],
  "FusedTensorProductOp4": {
    "__init__": [
      "self",
      "operand_segment_modes",
      "operand_segment_offsets",
      "operand_segment_shapes",
      "path_indices",
      "path_coefficients",
      "math_dtype"
    ],
    "forward": [
      "self",
      "in0",
      "in1",
      "in2"
    ],
    "_opcheck": [
      "self",
      "in0",
      "in1",
      "in2"
    ]
  },
  "FusedTensorProductOp3": {
    "__init__": [
      "self",
      "operand_segment_modes",
      "operand_segment_offsets",
      "operand_segment_shapes",
      "path_indices",
      "path_coefficients",
      "math_dtype"
    ],
    "forward": [
      "self",
      "in0",
      "in1"
    ],
    "_opcheck": [
      "self",
      "in0",
      "in1"
    ]
  },
  "__all__": [],
  "_DeprecatedClass": {
    "__init__": [
      "self"
    ]
  },
  "TensorProductUniform1d": {
    "_migration_message": []
  },
  "TensorProductUniform4x1d": {
    "_migration_message": []
  },
  "TensorProductUniform3x1dIndexed": {
    "_migration_message": []
  },
  "TensorProductUniform4x1dIndexed": {
    "_migration_message": []
  },
  "SymmetricTensorContraction": {
    "_migration_message": []
  },
  "register_plugins": [],
  "trt_to_torch_dtype_dict": [],
  "torch_to_trt_dtype_dict": [],
  "_to_meta_one": [
    "desc"
  ],
  "_from_meta_one": [
    "m"
  ],
  "_to_meta": [],
  "_from_meta": [],
  "_to_torch": [],
  "_tri_attn": [
    "q_",
    "k_",
    "v_",
    "b_",
    "mask_",
    "scale",
    "ret",
    "lse",
    "lse_max",
    "stream"
  ],
  "_tri_mul_update_impl": [
    "stream",
    "direction",
    "eps",
    "precision",
    "valid_optional_inputs"
  ],
  "_kernel_heuristic": [
    "B",
    "N",
    "D",
    "dtype"
  ],
  "_allocate_outputs_bwd": [
    "x",
    "layout"
  ],
  "_allocate_outputs": [
    "x",
    "layout"
  ],
  "_layer_norm_transpose": [
    "x",
    "w",
    "b",
    "eps",
    "elementwise_affine",
    "layout",
    "out",
    "mean",
    "rstd",
    "B",
    "N",
    "D"
  ],
  "_layer_norm_transpose_bwd": [
    "grad_out",
    "x",
    "w",
    "mean",
    "rstd",
    "elementwise_affine",
    "layout"
  ],
  "_layer_norm_transpose_fake": [
    "x",
    "w",
    "b",
    "eps",
    "elementwise_affine",
    "layout"
  ],
  "_setup_context": [
    "ctx",
    "inputs",
    "output"
  ],
  "_layer_norm_transpose_backward": [
    "ctx",
    "grad_out",
    "mean",
    "rstd"
  ],
  "layer_norm_transpose": [
    "x",
    "w",
    "b",
    "eps",
    "elementwise_affine",
    "layout"
  ],
  "ALLOW_NVTX": [],
  "INT_TO_DTYPE": [],
  "DTYPE_TO_INT": [],
  "is_in_export_mode": [],
  "maybe_to": [
    "arg",
    "dtype"
  ],
  "identity_2": [
    "s",
    "z"
  ],
  "nvtx_range_push": [
    "name"
  ],
  "nvtx_range_pop": [],
  "get_operator_from_module": [
    "module",
    "operator_base_str",
    "dtypes"
  ],
  "get_tensor_meta_data": [
    "tensor"
  ],
  "maybe_detach": [
    "tensor"
  ],
  "maybe_size": [
    "tensor",
    "dim"
  ],
  "maybe_empty_like": [
    "input",
    "create_tensor"
  ],
  "maybe_empty": [
    "size",
    "device",
    "dtype",
    "create_tensor"
  ],
  "maybe_zeros_like": [
    "input",
    "create_tensor"
  ],
  "maybe_zeros": [
    "size",
    "device",
    "dtype",
    "create_tensor"
  ],
  "BenchmarkMode": {
    "FLUSH_CACHE": [],
    "FLUSH_CACHE_PEAK_PROXY": [],
    "ROT_BUFFER": [],
    "ROT_BUFFER_PEAK_PROXY": []
  },
  "run_decoy": [
    "f",
    "input_dict"
  ],
  "run_bench": [
    "f",
    "input_dict",
    "warmup_iter",
    "run_iter",
    "bench_mode"
  ],
  "_get_workspace": [
    "counts",
    "device"
  ],
  "indexed_linear_B": [
    "ptr_A",
    "ptr_B",
    "counts",
    "transpose_B",
    "Z",
    "C",
    "u",
    "v",
    "coefficient",
    "workspace"
  ],
  "indexed_linear_C": [
    "ptr_A",
    "ptr_B",
    "counts",
    "Z",
    "C",
    "u",
    "v",
    "coefficient",
    "workspace"
  ],
  "indexed_linear": [
    "A",
    "B",
    "counts",
    "u",
    "v",
    "C",
    "Z",
    "subscripts",
    "coefficient"
  ],
  "_setup_context_base": [
    "ctx",
    "inputs",
    "needs_grad_indices"
  ],
  "indexed_linear_B_backward": [
    "ctx",
    "grad_output"
  ],
  "indexed_linear_B_setup_context": [
    "ctx",
    "inputs",
    "output"
  ],
  "indexed_linear_C_backward": [
    "ctx",
    "grad_output"
  ],
  "indexed_linear_C_setup_context": [
    "ctx",
    "inputs",
    "output"
  ],
  "_attention_pair_bias_mask_torch": [
    "z",
    "mask",
    "w_proj_z",
    "b_proj_z",
    "w_ln",
    "b_ln",
    "num_heads",
    "multiplicity",
    "eps",
    "inf",
    "return_z_proj",
    "is_cached_z_proj"
  ],
  "_attention_pair_bias_torch": [
    "s",
    "q",
    "k",
    "v",
    "z",
    "mask",
    "num_heads",
    "w_proj_z",
    "w_proj_g",
    "w_proj_o",
    "w_ln_z",
    "b_ln_z",
    "b_proj_z",
    "b_proj_g",
    "b_proj_o",
    "inf",
    "eps",
    "attn_scale",
    "return_z_proj",
    "is_cached_z_proj",
    "multiplicity"
  ],
  "_attention_pair_bias_mask": [
    "z",
    "w_proj_z",
    "b_proj_z",
    "w_ln",
    "b_ln",
    "mask",
    "num_heads",
    "multiplicity",
    "eps",
    "inf",
    "grad_enabled",
    "return_z_proj",
    "is_cached_z_proj"
  ],
  "_attention_pair_bias_mask_fake": [
    "z",
    "w_proj_z",
    "b_proj_z",
    "w_ln",
    "b_ln",
    "mask",
    "num_heads",
    "multiplicity",
    "eps",
    "inf",
    "grad_enabled",
    "return_z_proj",
    "is_cached_z_proj",
    "valid_optional_inputs"
  ],
  "_backward": [
    "ctx",
    "grad_out_mask",
    "grad_out_z_proj"
  ],
  "attention_pair_bias_mask": [
    "z",
    "mask",
    "w_proj_z",
    "b_proj_z",
    "w_ln",
    "b_ln",
    "num_heads",
    "multiplicity",
    "eps",
    "inf",
    "return_z_proj",
    "is_cached_z_proj"
  ],
  "attention_pair_bias": [
    "s",
    "q",
    "k",
    "v",
    "z",
    "mask",
    "num_heads",
    "w_proj_z",
    "w_proj_g",
    "w_proj_o",
    "w_ln_z",
    "b_ln_z",
    "b_proj_z",
    "b_proj_g",
    "b_proj_o",
    "inf",
    "eps",
    "attn_scale",
    "return_z_proj",
    "is_cached_z_proj"
  ],
  "_tri_mul_torch": [
    "x",
    "direction",
    "mask"
  ],
  "_parse_precision": [
    "precision_input",
    "p_in_weight",
    "g_in_weight"
  ],
  "_prod": [
    "nums"
  ],
  "ensure_dims": [
    "ten",
    "n"
  ],
  "_calculate_fan": [
    "linear_weight_shape",
    "fan"
  ],
  "trunc_normal_init_": [
    "weights",
    "scale",
    "fan"
  ],
  "lecun_normal_init_": [
    "weights"
  ],
  "bias_init_zero_": [
    "bias"
  ],
  "bias_init_one_": [
    "bias"
  ],
  "_tri_mul_update_dispatch": [
    "x",
    "mask",
    "norm_in_weight",
    "norm_in_bias",
    "p_in_weight",
    "p_in_bias",
    "g_in_weight",
    "g_in_bias",
    "norm_out_weight",
    "norm_out_bias",
    "p_out_weight",
    "p_out_bias",
    "g_out_weight",
    "g_out_bias",
    "direction",
    "eps",
    "precision"
  ],
  "_tri_mul_update": [
    "x",
    "mask",
    "norm_in_weight",
    "norm_in_bias",
    "p_in_weight",
    "p_in_bias",
    "g_in_weight",
    "g_in_bias",
    "norm_out_weight",
    "norm_out_bias",
    "p_out_weight",
    "p_out_bias",
    "g_out_weight",
    "g_out_bias",
    "direction",
    "eps",
    "precision"
  ],
  "triangle_multiplicative_update": [
    "x",
    "direction",
    "mask",
    "norm_in_weight",
    "norm_in_bias",
    "p_in_weight",
    "p_in_bias",
    "g_in_weight",
    "g_in_bias",
    "norm_out_weight",
    "norm_out_bias",
    "p_out_weight",
    "p_out_bias",
    "g_out_weight",
    "g_out_bias",
    "eps",
    "precision"
  ],
  "NS": [],
  "torch_ops": [],
  "BATCH_DIM_SHARED": [],
  "BATCH_DIM_BATCHED": [],
  "BATCH_DIM_INDEXED": [],
  "BATCH_DIM_AUTO": [],
  "_handle_batch_dim_auto": [
    "batch_size",
    "batch_dim",
    "tensors",
    "index_tensors"
  ],
  "_do_bwd_jit": [
    "ctx",
    "grad"
  ],
  "transpose_segment_setup_context": [
    "ctx",
    "inputs",
    "output"
  ],
  "segmented_transpose_bwd": [
    "ctx",
    "grad_output"
  ],
  "segmented_transpose": [
    "tensor",
    "segment_info",
    "input_contiguous_as_info"
  ],
  "sleep": [
    "seconds",
    "input_tensor"
  ],
  "_get_device_cc": [
    "device"
  ],
  "_fallback_threshold": [],
  "_should_use_tf32": [
    "q"
  ],
  "_permute_final_dims": [
    "tensor",
    "inds"
  ],
  "_triangle_attention_torch": [
    "query",
    "key",
    "value",
    "bias",
    "mask",
    "scale",
    "out"
  ],
  "_can_run_sm100f": [
    "q",
    "k",
    "training"
  ],
  "_convert_bias": [
    "bias",
    "q",
    "run_sm100f"
  ],
  "_allocate_s_kv": [
    "mask",
    "run_sm100f"
  ],
  "triangle_attention": [
    "q",
    "k",
    "v",
    "bias",
    "mask",
    "scale",
    "return_aux"
  ],
  "forward_kernel_input_generator": [
    "M",
    "N",
    "K",
    "dtype_input",
    "two_inputs",
    "precision"
  ],
  "forward_kernel_config_to_key": [
    "M",
    "N",
    "K",
    "dtype_input",
    "two_inputs",
    "precision"
  ],
  "forward_kernel_input_to_key": [
    "x1",
    "x2",
    "w1",
    "w2",
    "mask",
    "precision"
  ],
  "fused_sigmoid_gated_dual_gemm_forward_kernel_wrapper": [
    "x1",
    "x2",
    "w1",
    "w2",
    "b1",
    "b2",
    "mask",
    "TRANSPOSE_OUT",
    "TILE_M",
    "TILE_N",
    "TILE_K",
    "num_stages",
    "num_warps",
    "precision"
  ],
  "backward_kernel_config_to_key": [
    "M",
    "N",
    "K",
    "dtype_input",
    "two_inputs"
  ],
  "backward_kernel_input_to_key": [
    "grad_out",
    "x1",
    "x2",
    "w1",
    "w2",
    "mask"
  ],
  "backard_kernel_input_generator": [
    "M",
    "N",
    "K",
    "dtype_input",
    "two_inputs"
  ],
  "fused_sigmoid_gated_dual_gemm_backward_pregemm_kernel_wrapper": [
    "grad_out",
    "x1",
    "x2",
    "w1",
    "w2",
    "b1",
    "b2",
    "mask",
    "TRANSPOSE_OUT",
    "TILE_M",
    "TILE_N",
    "TILE_K",
    "num_stages",
    "num_warps",
    "precision"
  ],
  "_fused_gated_dual_gemm": [
    "x1",
    "x2",
    "w1",
    "w2",
    "b1",
    "b2",
    "mask",
    "transpose_out",
    "precision"
  ],
  "_dual_gemm_setup_context": [
    "ctx",
    "inputs",
    "output"
  ],
  "_dual_gemm_backward": [
    "ctx",
    "grad_out"
  ],
  "fused_sigmoid_gated_dual_gemm": [
    "x",
    "w1",
    "w2",
    "mask",
    "transpose_out",
    "precision",
    "b1",
    "b2"
  ],
  "fused_sigmoid_gated_dual_gemm_dual_x": [
    "x1",
    "x2",
    "w1",
    "w2",
    "mask",
    "transpose_out",
    "precision",
    "b1",
    "b2"
  ],
  "__version__": [],
  "__git_commit__": [],
  "_typing_cleanup": []
}