{
  "basestring": [],
  "decode_string": [
    "v",
    "encoding"
  ],
  "encode_string": [
    "v",
    "encoding"
  ],
  "decode_utf8": [],
  "encode_utf8": [],
  "isnumeric": [
    "strg"
  ],
  "lazydict": {
    "load": [
      "self"
    ],
    "_lazy": [
      "self",
      "method"
    ],
    "__repr__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__contains__": [
      "self"
    ],
    "__getitem__": [
      "self"
    ],
    "__setitem__": [
      "self"
    ],
    "setdefault": [
      "self"
    ],
    "get": [
      "self"
    ],
    "items": [
      "self"
    ],
    "keys": [
      "self"
    ],
    "values": [
      "self"
    ],
    "update": [
      "self"
    ],
    "pop": [
      "self"
    ],
    "popitem": [
      "self"
    ]
  },
  "lazylist": {
    "load": [
      "self"
    ],
    "_lazy": [
      "self",
      "method"
    ],
    "__repr__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__contains__": [
      "self"
    ],
    "insert": [
      "self"
    ],
    "append": [
      "self"
    ],
    "extend": [
      "self"
    ],
    "remove": [
      "self"
    ],
    "pop": [
      "self"
    ]
  },
  "UNIVERSAL": [],
  "penntreebank2universal": [
    "token",
    "tag"
  ],
  "TOKEN": [],
  "PUNCTUATION": [],
  "punctuation": [],
  "ABBREVIATIONS": [],
  "abbreviations": [],
  "RE_ABBR1": [],
  "RE_ABBR2": [],
  "RE_ABBR3": [],
  "EMOTICONS": [],
  "RE_EMOTICONS": [],
  "RE_SARCASM": [],
  "replacements": [],
  "EOS": [],
  "find_tokens": [
    "string",
    "punctuation",
    "abbreviations",
    "replace",
    "linebreak"
  ],
  "_read": [
    "path",
    "encoding",
    "comment"
  ],
  "Lexicon": {
    "__init__": [
      "self",
      "path",
      "morphology",
      "context",
      "entities",
      "NNP",
      "language"
    ],
    "load": [
      "self"
    ],
    "path": [
      "self"
    ],
    "language": [
      "self"
    ]
  },
  "Rules": {
    "__init__": [
      "self",
      "lexicon",
      "cmd"
    ],
    "apply": [
      "self",
      "x"
    ]
  },
  "Morphology": {
    "__init__": [
      "self",
      "lexicon",
      "path"
    ],
    "path": [
      "self"
    ],
    "load": [
      "self"
    ],
    "apply": [
      "self",
      "token",
      "previous",
      "next"
    ],
    "insert": [
      "self",
      "i",
      "tag",
      "affix",
      "cmd",
      "tagged"
    ],
    "append": [
      "self"
    ],
    "extend": [
      "self",
      "rules"
    ]
  },
  "Context": {
    "__init__": [
      "self",
      "lexicon",
      "path"
    ],
    "path": [
      "self"
    ],
    "load": [
      "self"
    ],
    "apply": [
      "self",
      "tokens"
    ],
    "insert": [
      "self",
      "i",
      "tag1",
      "tag2",
      "cmd",
      "x",
      "y"
    ],
    "append": [
      "self"
    ],
    "extend": [
      "self",
      "rules"
    ]
  },
  "RE_ENTITY1": [],
  "RE_ENTITY2": [],
  "RE_ENTITY3": [],
  "Entities": {
    "__init__": [
      "self",
      "lexicon",
      "path",
      "tag"
    ],
    "path": [
      "self"
    ],
    "load": [
      "self"
    ],
    "apply": [
      "self",
      "tokens"
    ],
    "append": [
      "self",
      "entity",
      "name"
    ],
    "extend": [
      "self",
      "entities"
    ]
  },
  "MOOD": [],
  "IRONY": [],
  "RE_SYNSET": [],
  "avg": [
    "list"
  ],
  "Score": {
    "__new__": [
      "self",
      "polarity",
      "subjectivity",
      "assessments"
    ],
    "__init__": [
      "self",
      "polarity",
      "subjectivity",
      "assessments"
    ]
  },
  "Sentiment": {
    "__init__": [
      "self",
      "path",
      "language",
      "synset",
      "confidence"
    ],
    "path": [
      "self"
    ],
    "language": [
      "self"
    ],
    "confidence": [
      "self"
    ],
    "load": [
      "self",
      "path"
    ],
    "synset": [
      "self",
      "id",
      "pos"
    ],
    "__call__": [
      "self",
      "s",
      "negation"
    ],
    "assessments": [
      "self",
      "words",
      "negation"
    ],
    "annotate": [
      "self",
      "word",
      "pos",
      "polarity",
      "subjectivity",
      "intensity",
      "label"
    ]
  },
  "CD": [],
  "_suffix_rules": [
    "token",
    "tag"
  ],
  "find_tags": [
    "tokens",
    "lexicon",
    "model",
    "morphology",
    "context",
    "entities",
    "default",
    "language",
    "map"
  ],
  "SEPARATOR": [],
  "NN": [],
  "VB": [],
  "JJ": [],
  "RB": [],
  "CHUNKS": [],
  "find_chunks": [
    "tagged",
    "language"
  ],
  "find_prepositions": [
    "chunked"
  ],
  "PTB": [],
  "PENN": [],
  "Parser": {
    "__init__": [
      "self",
      "lexicon",
      "default",
      "language"
    ],
    "find_tokens": [
      "self",
      "string"
    ],
    "find_tags": [
      "self",
      "tokens"
    ],
    "find_chunks": [
      "self",
      "tokens"
    ],
    "find_prepositions": [
      "self",
      "tokens"
    ],
    "find_labels": [
      "self",
      "tokens"
    ],
    "find_lemmata": [
      "self",
      "tokens"
    ],
    "parse": [
      "self",
      "s",
      "tokenize",
      "tags",
      "chunks",
      "relations",
      "lemmata",
      "encoding"
    ]
  },
  "TOKENS": [],
  "TaggedString": {
    "__new__": [
      "self",
      "string",
      "tags",
      "language"
    ],
    "split": [
      "self",
      "sep"
    ]
  },
  "Spelling": {
    "ALPHA": [],
    "__init__": [
      "self",
      "path"
    ],
    "load": [
      "self"
    ],
    "path": [
      "self"
    ],
    "language": [
      "self"
    ],
    "train": [
      "self",
      "s",
      "path"
    ],
    "_edit1": [
      "self",
      "w"
    ],
    "_edit2": [
      "self",
      "w"
    ],
    "_known": [
      "self",
      "words"
    ],
    "suggest": [
      "self",
      "w"
    ]
  },
  "DEFAULT_ENCODING": [],
  "BaseFormat": {
    "__init__": [
      "self",
      "fp"
    ],
    "to_iterable": [
      "self"
    ],
    "detect": [
      "cls",
      "stream"
    ]
  },
  "DelimitedFormat": {
    "delimiter": [],
    "__init__": [
      "self",
      "fp"
    ],
    "to_iterable": [
      "self"
    ],
    "detect": [
      "cls",
      "stream"
    ]
  },
  "CSV": {
    "delimiter": []
  },
  "TSV": {
    "delimiter": []
  },
  "JSON": {
    "__init__": [
      "self",
      "fp"
    ],
    "to_iterable": [
      "self"
    ],
    "detect": [
      "cls",
      "stream"
    ]
  },
  "_registry": [],
  "detect": [
    "fp",
    "max_read"
  ],
  "get_registry": [],
  "register": [
    "name",
    "format_class"
  ],
  "BaseTagger": {
    "tag": [
      "self",
      "text",
      "tokenize"
    ]
  },
  "BaseNPExtractor": {
    "extract": [
      "self",
      "text"
    ]
  },
  "BaseTokenizer": {
    "tokenize": [
      "self",
      "text"
    ],
    "itokenize": [
      "self",
      "text"
    ]
  },
  "DISCRETE": [],
  "CONTINUOUS": [],
  "BaseSentimentAnalyzer": {
    "kind": [],
    "__init__": [
      "self"
    ],
    "train": [
      "self"
    ],
    "analyze": [
      "self",
      "text"
    ]
  },
  "BaseParser": {
    "parse": [
      "self",
      "text"
    ]
  },
  "_get_words_from_dataset": [
    "dataset"
  ],
  "_get_document_tokens": [
    "document"
  ],
  "basic_extractor": [
    "document",
    "train_set"
  ],
  "contains_extractor": [
    "document"
  ],
  "BaseClassifier": {
    "__init__": [
      "self",
      "train_set",
      "feature_extractor",
      "format"
    ],
    "_read_data": [
      "self",
      "dataset",
      "format"
    ],
    "classifier": [
      "self"
    ],
    "classify": [
      "self",
      "text"
    ],
    "train": [
      "self",
      "labeled_featureset"
    ],
    "labels": [
      "self"
    ],
    "extract_features": [
      "self",
      "text"
    ]
  },
  "NLTKClassifier": {
    "nltk_class": [],
    "__init__": [
      "self",
      "train_set",
      "feature_extractor",
      "format"
    ],
    "__repr__": [
      "self"
    ],
    "classifier": [
      "self"
    ],
    "train": [
      "self"
    ],
    "labels": [
      "self"
    ],
    "classify": [
      "self",
      "text"
    ],
    "accuracy": [
      "self",
      "test_set",
      "format"
    ],
    "update": [
      "self",
      "new_data"
    ]
  },
  "NaiveBayesClassifier": {
    "nltk_class": [],
    "prob_classify": [
      "self",
      "text"
    ],
    "informative_features": [
      "self"
    ],
    "show_informative_features": [
      "self"
    ]
  },
  "DecisionTreeClassifier": {
    "nltk_class": [],
    "pretty_format": [
      "self"
    ],
    "pprint": [],
    "pseudocode": [
      "self"
    ]
  },
  "PositiveNaiveBayesClassifier": {
    "nltk_class": [],
    "__init__": [
      "self",
      "positive_set",
      "unlabeled_set",
      "feature_extractor",
      "positive_prob_prior"
    ],
    "__repr__": [
      "self"
    ],
    "train": [
      "self"
    ],
    "update": [
      "self",
      "new_positive_data",
      "new_unlabeled_data",
      "positive_prob_prior"
    ]
  },
  "MaxEntClassifier": {
    "__doc__": [],
    "nltk_class": [],
    "prob_classify": [
      "self",
      "text"
    ]
  },
  "wordnet": [],
  "Synset": [],
  "Lemma": [],
  "PUNCTUATION_REGEX": [],
  "strip_punc": [
    "s",
    "all"
  ],
  "lowerstrip": [
    "s",
    "all"
  ],
  "tree2str": [
    "tree",
    "concat"
  ],
  "filter_insignificant": [
    "chunk",
    "tag_suffixes"
  ],
  "is_filelike": [
    "obj"
  ],
  "_wordnet": [],
  "_penn_to_wordnet": [
    "tag"
  ],
  "Word": {
    "__new__": [
      "cls",
      "string",
      "pos_tag"
    ],
    "__init__": [
      "self",
      "string",
      "pos_tag"
    ],
    "__repr__": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "singularize": [
      "self"
    ],
    "pluralize": [
      "self"
    ],
    "spellcheck": [
      "self"
    ],
    "correct": [
      "self"
    ],
    "lemma": [
      "self"
    ],
    "lemmatize": [
      "self",
      "pos"
    ],
    "PorterStemmer": [],
    "LancasterStemmer": [],
    "SnowballStemmer": [],
    "stem": [
      "self",
      "stemmer"
    ],
    "synsets": [
      "self"
    ],
    "definitions": [
      "self"
    ],
    "get_synsets": [
      "self",
      "pos"
    ],
    "define": [
      "self",
      "pos"
    ]
  },
  "WordList": {
    "__init__": [
      "self",
      "collection"
    ],
    "__str__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__getslice__": [
      "self",
      "i",
      "j"
    ],
    "__setitem__": [
      "self",
      "index",
      "obj"
    ],
    "count": [
      "self",
      "strg",
      "case_sensitive"
    ],
    "append": [
      "self",
      "obj"
    ],
    "extend": [
      "self",
      "iterable"
    ],
    "upper": [
      "self"
    ],
    "lower": [
      "self"
    ],
    "singularize": [
      "self"
    ],
    "pluralize": [
      "self"
    ],
    "lemmatize": [
      "self"
    ],
    "stem": [
      "self"
    ]
  },
  "_validated_param": [
    "obj",
    "name",
    "base_class",
    "default",
    "base_class_name"
  ],
  "_initialize_models": [
    "obj",
    "tokenizer",
    "pos_tagger",
    "np_extractor",
    "analyzer",
    "parser",
    "classifier"
  ],
  "BaseBlob": {
    "np_extractor": [],
    "pos_tagger": [],
    "tokenizer": [],
    "analyzer": [],
    "parser": [],
    "__init__": [
      "self",
      "text",
      "tokenizer",
      "pos_tagger",
      "np_extractor",
      "analyzer",
      "parser",
      "classifier",
      "clean_html"
    ],
    "words": [
      "self"
    ],
    "tokens": [
      "self"
    ],
    "tokenize": [
      "self",
      "tokenizer"
    ],
    "parse": [
      "self",
      "parser"
    ],
    "classify": [
      "self"
    ],
    "sentiment": [
      "self"
    ],
    "sentiment_assessments": [
      "self"
    ],
    "polarity": [
      "self"
    ],
    "subjectivity": [
      "self"
    ],
    "noun_phrases": [
      "self"
    ],
    "pos_tags": [
      "self"
    ],
    "tags": [],
    "word_counts": [
      "self"
    ],
    "np_counts": [
      "self"
    ],
    "ngrams": [
      "self",
      "n"
    ],
    "correct": [
      "self"
    ],
    "_cmpkey": [
      "self"
    ],
    "_strkey": [
      "self"
    ],
    "__hash__": [
      "self"
    ],
    "__add__": [
      "self",
      "other"
    ],
    "split": [
      "self",
      "sep",
      "maxsplit"
    ]
  },
  "TextBlob": {
    "sentences": [
      "self"
    ],
    "words": [
      "self"
    ],
    "raw_sentences": [
      "self"
    ],
    "serialized": [
      "self"
    ],
    "to_json": [
      "self"
    ],
    "json": [
      "self"
    ],
    "_create_sentence_objects": [
      "self"
    ]
  },
  "Sentence": {
    "__init__": [
      "self",
      "sentence",
      "start_index",
      "end_index"
    ],
    "dict": [
      "self"
    ]
  },
  "Blobber": {
    "np_extractor": [],
    "pos_tagger": [],
    "tokenizer": [],
    "analyzer": [],
    "parser": [],
    "__init__": [
      "self",
      "tokenizer",
      "pos_tagger",
      "np_extractor",
      "analyzer",
      "parser",
      "classifier"
    ],
    "__call__": [
      "self",
      "text"
    ],
    "__repr__": [
      "self"
    ],
    "__str__": []
  },
  "WordTokenizer": {
    "tokenize": [
      "self",
      "text",
      "include_punc"
    ]
  },
  "SentenceTokenizer": {
    "tokenize": [
      "self",
      "text"
    ]
  },
  "sent_tokenize": [],
  "_word_tokenizer": [],
  "word_tokenize": [
    "text",
    "include_punc"
  ],
  "__all__": [],
  "MISSING_CORPUS_MESSAGE": [],
  "TextBlobError": {},
  "TextBlobException": [],
  "MissingCorpusError": {
    "__init__": [
      "self",
      "message"
    ]
  },
  "MissingCorpusException": [],
  "DeprecationError": {},
  "TranslatorError": {},
  "NotTranslated": {},
  "FormatError": {},
  "cached_property": {
    "__init__": [
      "self",
      "func"
    ],
    "__get__": [
      "self",
      "obj",
      "cls"
    ]
  },
  "requires_nltk_corpus": [
    "func"
  ],
  "ComparableMixin": {
    "_compare": [
      "self",
      "other",
      "method"
    ],
    "__lt__": [
      "self",
      "other"
    ],
    "__le__": [
      "self",
      "other"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__ge__": [
      "self",
      "other"
    ],
    "__gt__": [
      "self",
      "other"
    ],
    "__ne__": [
      "self",
      "other"
    ]
  },
  "BlobComparableMixin": {
    "_compare": [
      "self",
      "other",
      "method"
    ]
  },
  "StringlikeMixin": {
    "__repr__": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__contains__": [
      "self",
      "sub"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "find": [
      "self",
      "sub",
      "start",
      "end"
    ],
    "rfind": [
      "self",
      "sub",
      "start",
      "end"
    ],
    "index": [
      "self",
      "sub",
      "start",
      "end"
    ],
    "rindex": [
      "self",
      "sub",
      "start",
      "end"
    ],
    "startswith": [
      "self",
      "prefix",
      "start",
      "end"
    ],
    "endswith": [
      "self",
      "suffix",
      "start",
      "end"
    ],
    "starts_with": [],
    "ends_with": [],
    "title": [
      "self"
    ],
    "format": [
      "self"
    ],
    "split": [
      "self",
      "sep",
      "maxsplit"
    ],
    "strip": [
      "self",
      "chars"
    ],
    "upper": [
      "self"
    ],
    "lower": [
      "self"
    ],
    "join": [
      "self",
      "iterable"
    ],
    "replace": [
      "self",
      "old",
      "new",
      "count"
    ]
  },
  "MIN_CORPORA": [],
  "ADDITIONAL_CORPORA": [],
  "ALL_CORPORA": [],
  "download_lite": [],
  "download_all": [],
  "main": [],
  "spelling": [],
  "find_lemmata": [
    "tokens"
  ],
  "lexicon": [],
  "parser": [],
  "sentiment": [],
  "tokenize": [
    "s"
  ],
  "parse": [
    "s"
  ],
  "parsetree": [
    "s"
  ],
  "split": [
    "s",
    "token"
  ],
  "tag": [
    "s",
    "tokenize",
    "encoding"
  ],
  "suggest": [
    "w"
  ],
  "polarity": [
    "s"
  ],
  "subjectivity": [
    "s"
  ],
  "positive": [
    "s",
    "threshold"
  ],
  "PatternAnalyzer": {
    "kind": [],
    "RETURN_TYPE": [],
    "analyze": [
      "self",
      "text",
      "keep_assessments"
    ]
  },
  "_default_feature_extractor": [
    "words"
  ],
  "NaiveBayesAnalyzer": {
    "kind": [],
    "RETURN_TYPE": [],
    "__init__": [
      "self",
      "feature_extractor"
    ],
    "train": [
      "self"
    ],
    "analyze": [
      "self",
      "text"
    ]
  },
  "plural_prepositions": [],
  "plural_rules": [],
  "plural_categories": [],
  "pluralize": [
    "word",
    "pos",
    "custom",
    "classical"
  ],
  "singular_rules": [],
  "singular_uninflected": [],
  "singular_uncountable": [],
  "singular_ie": [],
  "singular_s": [],
  "singular_irregular": [],
  "singularize": [
    "word",
    "pos",
    "custom"
  ],
  "PatternTagger": {
    "tag": [
      "self",
      "text",
      "tokenize"
    ]
  },
  "NLTKTagger": {
    "tag": [
      "self",
      "text"
    ]
  },
  "ChunkParser": {
    "__init__": [
      "self"
    ],
    "train": [
      "self"
    ],
    "parse": [
      "self",
      "sentence"
    ]
  },
  "ConllExtractor": {
    "POS_TAGGER": [],
    "CFG": [],
    "INSIGNIFICANT_SUFFIXES": [],
    "__init__": [
      "self",
      "parser"
    ],
    "extract": [
      "self",
      "text"
    ],
    "_parse_sentence": [
      "self",
      "sentence"
    ]
  },
  "FastNPExtractor": {
    "CFG": [],
    "__init__": [
      "self"
    ],
    "train": [
      "self"
    ],
    "_tokenize_sentence": [
      "self",
      "sentence"
    ],
    "extract": [
      "self",
      "sentence"
    ]
  },
  "_normalize_tags": [
    "chunk"
  ],
  "_is_match": [
    "tagged_phrase",
    "cfg"
  ],
  "PatternParser": {
    "parse": [
      "self",
      "text"
    ]
  }
}