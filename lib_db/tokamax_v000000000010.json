{
  "TokamaxTest": {
    "test_full_example_gpu": [
      "self"
    ]
  },
  "TestUtilsTest": {
    "test_assert_trees_all_close": [
      "self"
    ]
  },
  "QArray": [],
  "LANES": [],
  "_adaptive_sublane_size": [],
  "ScalesTilingInfo": {
    "scales_inflation": [
      "self"
    ]
  },
  "_get_scale_tile_info": [
    "eps",
    "tile_size",
    "axis_size",
    "min_addressable_size"
  ],
  "quant_block_spec": [
    "x",
    "x_spec",
    "reduction_axis"
  ],
  "custom_buffered_pallas_call": [
    "kernel",
    "out_shape",
    "grid_spec",
    "compiler_params",
    "input_buffer_count"
  ],
  "enum_option": [],
  "ConfigTest": {
    "test_config_option_scope": [
      "self"
    ],
    "test_config_option_validation": [
      "self"
    ]
  },
  "PyTree": [],
  "einshape": [],
  "pad_dim_to": [
    "x",
    "n",
    "axis",
    "pad_value"
  ],
  "pad_to_next_multiple_of": [
    "x",
    "m",
    "axis",
    "pad_value"
  ],
  "upcast_broadcast": [],
  "contains_symbolic_shape": [
    "args"
  ],
  "DotAlgorithm": [],
  "DotAlgorithmPreset": [],
  "Precision": [],
  "PrecisionLike": [],
  "DTypeLike": [],
  "CanonicalPrecision": [],
  "canonicalize_precision": [
    "precision"
  ],
  "to_dot_algorithm_preset": [
    "a_dtype",
    "b_dtype",
    "precision"
  ],
  "_DEFAULT_PRECISIONS": [],
  "is_default": [
    "a_dtype",
    "b_dtype",
    "precision"
  ],
  "precision_input_dtype": [
    "precision"
  ],
  "default_output_dtype_from_input_dtypes": [],
  "T": [],
  "_optimization_barrier": [
    "x"
  ],
  "BenchmarkData": {
    "median_evaluation_time_ms": [
      "self"
    ],
    "asdict": [
      "self"
    ]
  },
  "XprofProfileSession": {
    "__init__": [
      "self",
      "hermetic",
      "use_jax_profiler"
    ],
    "total_op_time": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "exc_tb"
    ]
  },
  "standardize_function": [
    "f"
  ],
  "wallclock_timer": [
    "f",
    "args"
  ],
  "cupti_timer": [
    "f",
    "args"
  ],
  "xprof_timer": [
    "f",
    "args"
  ],
  "hermetic_xprof_timer": [
    "f",
    "args"
  ],
  "_DEFAULT_TIMING_METHOD": [],
  "_FALLBACK_TIMING_METHOD": [],
  "_get_metadata": [
    "lowered"
  ],
  "compile_benchmark": [
    "f",
    "x"
  ],
  "register_benchmark": [
    "name",
    "impl_name",
    "impl",
    "kwargs"
  ],
  "get_benchmark_registrar": [
    "impls"
  ],
  "R": [],
  "get_vjp_taking_residuals": [
    "fn"
  ],
  "KernelInfoBase": {},
  "TritonKernelInfo": {},
  "MosaicTpuKernelInfo": {},
  "MosaicGpuKernelInfo": {},
  "TokamaxXlaKernelInfo": {},
  "_get_op_name": [
    "loc"
  ],
  "_get_shape_dtype": [
    "ty"
  ],
  "_get_source_file_line": [
    "loc"
  ],
  "_get_common_kernel_info": [
    "op",
    "call_stack"
  ],
  "_get_pallas_kernel_info": [
    "op",
    "call_stack"
  ],
  "_kernel_info_getter": [
    "cls"
  ],
  "_get_tokamax_xla_kernel_info": [],
  "get_kernel_info": [
    "x",
    "include_xla_kernels"
  ],
  "get_opspecs": [
    "x",
    "include_xla_kernels"
  ],
  "AsQArray": {
    "__post_init__": [
      "self"
    ],
    "shape": [],
    "dtype": [],
    "ndim": [],
    "size": [],
    "as_qarray": [
      "self"
    ]
  },
  "as_array": [
    "x"
  ],
  "as_array_or_qarray": [
    "x"
  ],
  "as_array_or_qarray_without_zero_point": [
    "x"
  ],
  "assert_trees_all_close": [],
  "tokamax_testing_setup": [],
  "add_vectors_kernel": [
    "x_ref",
    "y_ref",
    "o_ref"
  ],
  "add_vector_two": [
    "x_ref",
    "o_ref"
  ],
  "add_vectors_pallas_triton": [
    "x",
    "y"
  ],
  "DumpHloLibTest": {
    "test_pallas_gpu_tpu": [
      "self"
    ],
    "test_simple_pallas_triton": [
      "self"
    ],
    "test_pallas_norm": [
      "self"
    ],
    "test_get_opspecs_from_lowered_jax": [
      "self"
    ],
    "test_normalization_spec_round_trip": [
      "self"
    ],
    "test_ragged_dot_spec_round_trip": [
      "self"
    ],
    "test_opspec_attention_all_implementations": [
      "self",
      "implementation"
    ]
  },
  "exact_div": [
    "a",
    "b"
  ],
  "split_merge": [
    "predicate",
    "xs"
  ],
  "GpuUtilsTest": {
    "test_triton_support": [
      "self"
    ],
    "test_mosaic_gpu_support": [
      "self"
    ],
    "test_is_sm80": [
      "self"
    ],
    "test_is_sm90": [
      "self"
    ],
    "test_is_sm100": [
      "self"
    ]
  },
  "NumericSummary": {},
  "DiffSummary": {
    "max_absolute_diff": [
      "self"
    ]
  },
  "array_diff_summary": [
    "expected",
    "actual",
    "rtol",
    "atol",
    "equal_nan"
  ],
  "array_numeric_summary": [
    "x"
  ],
  "ArrayInitializer": {
    "__call__": [
      "self",
      "rng"
    ],
    "shape": [
      "self"
    ],
    "dtype": [
      "self"
    ]
  },
  "RangedArrayInitializer": {
    "__init__": [
      "self",
      "shape",
      "dtype",
      "minval",
      "maxval"
    ],
    "__call__": [
      "self",
      "rng"
    ]
  },
  "_int_initializer": [
    "rng",
    "shape",
    "dtype",
    "minval",
    "maxval"
  ],
  "_as_vmap_shape": [
    "x"
  ],
  "random_initialize": [
    "x",
    "seed"
  ],
  "BenchmarkingTest": {
    "test_standardize_function": [
      "self"
    ],
    "test_standardize_function_with_batching": [
      "self"
    ],
    "test_compile_benchmark": [
      "self",
      "method"
    ],
    "test_benchmark_function_known_time": [
      "self"
    ],
    "test_xprof_profile_session": [
      "self"
    ],
    "test_xprof_profile_session_exception": [
      "self"
    ]
  },
  "ScalarInt": [],
  "jaxtyped": [],
  "disable_jaxtyping": [
    "disable"
  ],
  "_ABSENT": [],
  "_STATE": [],
  "_T": [],
  "_option_override_scope": [
    "name",
    "value"
  ],
  "_ConfigOption": {
    "__call__": [
      "self",
      "value"
    ],
    "value": [
      "self"
    ]
  },
  "autotuning_cache_miss_fallback": [],
  "cross_compile": [],
  "PrecisionTest": {
    "test_precision_equivalence": [
      "self",
      "dtype",
      "precision_old"
    ],
    "test_precision_input_dtype": [
      "self"
    ],
    "test_precision_jax_default": [
      "self",
      "default_precision"
    ],
    "test_to_dot_algorithm_preset_invalid_name": [
      "self"
    ]
  },
  "_P": [],
  "_zip": [],
  "_broadcast_prefix": [
    "prefix_tree",
    "full_tree"
  ],
  "vmap_maybe_bcast": [
    "f",
    "in_axes"
  ],
  "_split_dim": [
    "x",
    "axis",
    "num_parts"
  ],
  "vmap_split": [
    "f",
    "in_axes"
  ],
  "BatchedShapeDtype": {
    "__slots__": [],
    "__init__": [
      "self",
      "shape",
      "dtype",
      "vmap_axes"
    ],
    "vmap_shape": [
      "self"
    ],
    "vmap_axis_sizes": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__str__": [],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "get_vmap_axis_sizes": [
    "values"
  ],
  "_unique_not_none_value": [],
  "capture_batched_args": [
    "fn"
  ],
  "ShapeTest": {
    "test_upcast_broadcast": [
      "self"
    ],
    "test_contains_symbolic_shape": [
      "self"
    ]
  },
  "AutodiffTest": {
    "test_vjp_taking_no_residuals": [
      "self"
    ],
    "test_vjp_taking_residuals_from_inputs": [
      "self",
      "fn"
    ],
    "test_vjp_taking_residuals_from_outputs": [
      "self",
      "fn"
    ],
    "test_vjp_taking_residuals_from_invalid_outputs": [
      "self",
      "fn"
    ],
    "test_vjp_taking_residuals_from_intermediates": [
      "self"
    ],
    "test_vjp_taking_residuals_with_multiple_args_and_results": [
      "self"
    ]
  },
  "_eval_shape": [
    "spec"
  ],
  "_MyDataclass": {
    "__pydantic_config__": []
  },
  "_Foo": {},
  "_PL_ATTN_CFG": [],
  "_PL_DOT_CFG": [],
  "_OPS": [],
  "PydanticTest": {
    "test_power_of_two": [
      "self"
    ],
    "test_annotated_roundtrip": [
      "self",
      "typ",
      "data"
    ],
    "test_shape_dtype_roundtrip": [
      "self",
      "shape"
    ],
    "test_abstract_dataclass_roundtrip": [
      "self"
    ],
    "test_abstract_tuple_roundtrip": [
      "self"
    ],
    "test_op_roundtrip": [
      "self",
      "op"
    ],
    "test_any_instance_of_op_roundtrip": [
      "self",
      "op"
    ],
    "test_arg_specs_roundtrip": [
      "self",
      "op_cls",
      "arg_specs"
    ]
  },
  "f": [
    "x"
  ],
  "JaxtypingTest": {
    "test_disable_jaxtyping": [
      "self"
    ]
  },
  "_compute_capability": [
    "device"
  ],
  "is_sm80": [
    "device"
  ],
  "is_sm90": [
    "device"
  ],
  "is_sm100": [
    "device"
  ],
  "has_mosaic_gpu_support": [
    "device"
  ],
  "has_triton_support": [
    "device"
  ],
  "_int_power_of_two": [
    "n"
  ],
  "_validate_np_dtype": [
    "x"
  ],
  "annotate": [
    "ty"
  ],
  "TypeAdapter": {
    "dump_python": [
      "self",
      "instance",
      "info"
    ]
  },
  "get_adapter": [],
  "AnyInstanceOf": {
    "__class_getitem__": [
      "cls",
      "item"
    ],
    "__get_pydantic_core_schema__": [
      "cls",
      "source",
      "handler"
    ]
  },
  "Dataclass": {
    "__get_pydantic_core_schema__": [
      "cls",
      "source",
      "handler"
    ]
  },
  "EnumByName": [],
  "ShapeDtype": {
    "PATTERN": [],
    "SHORT_DTYPE_NAMES_MAP": [],
    "__get_pydantic_core_schema__": [
      "cls",
      "source",
      "handler"
    ]
  },
  "get_arg_spec_model": [
    "name",
    "signature"
  ],
  "_bsd": [
    "shape",
    "dtype",
    "vmap_axes"
  ],
  "BatchingTest": {
    "test_capture_batched_args": [
      "self",
      "in_axes"
    ],
    "test_batched_shape_dtype_equality": [
      "self"
    ]
  },
  "NumericsTest": {
    "test_initializer_consistency": [
      "self"
    ],
    "test_numeric_summary": [
      "self",
      "equal_nan"
    ],
    "test_array_diff_summary_no_nan": [
      "self"
    ],
    "test_random_initialize_consistency": [
      "self"
    ],
    "test_random_initialize_layout": [
      "self",
      "dtype"
    ],
    "test_random_initialize_qarray": [
      "self",
      "qtype",
      "scale"
    ],
    "test_ranged_array_initializer": [
      "self"
    ],
    "test_seed": [
      "self"
    ]
  },
  "_FakeOpConfig": {},
  "_HEURISTICS_CONFIG": [],
  "_AUTOTUNE_CONFIG": [],
  "_FakeOp": {
    "_fwd": [
      "self",
      "x",
      "y"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ]
  },
  "OpTest": {
    "test_bind": [
      "self"
    ]
  },
  "BoundArgumentsTest": {
    "test_get_config": [
      "self"
    ],
    "test_default_config": [
      "self"
    ],
    "test_heuristics_config": [
      "self"
    ],
    "test_autotuning_configs": [
      "self"
    ],
    "test_autotune": [
      "self"
    ],
    "test_autotune_vmap": [
      "self",
      "x_vmap_axes",
      "y_vmap_axes"
    ],
    "test_equals": [
      "self"
    ],
    "test_hash": [
      "self"
    ],
    "test_roundtrip": [
      "self",
      "op",
      "arg_specs"
    ]
  },
  "_R": [],
  "_Config": [],
  "_Key": [],
  "AutotuningData": [],
  "DeviceKind": [],
  "NullConfig": {},
  "_FlatTree": {},
  "Residuals": {
    "tree_flatten": [
      "self"
    ],
    "tree_unflatten": [
      "cls",
      "aux_data",
      "children"
    ]
  },
  "Op": {
    "__init_subclass__": [
      "cls"
    ],
    "__call__": [
      "self"
    ],
    "bind": [
      "self"
    ],
    "replace": [
      "self"
    ],
    "get_autotuning_cache": [
      "self",
      "device_kind"
    ],
    "_fwd": [
      "self"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_cache_key": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "_capture_batched_args": [
      "self",
      "fn"
    ],
    "signature": [
      "self"
    ],
    "_fwd_signature": [
      "self"
    ],
    "__pydantic_config__": [],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "_AUTOTUNING_CACHE_OVERLAY": [],
  "get_autotuning_cache_overlay_state": [],
  "AUTO": {},
  "BoundArguments": {
    "__post_init__": [
      "self"
    ],
    "signature": [
      "self"
    ],
    "args": [
      "self"
    ],
    "kwargs": [
      "self"
    ],
    "_bound_args": [
      "self"
    ],
    "vmap_axis_sizes": [
      "self"
    ],
    "default_config": [
      "self"
    ],
    "get_config": [
      "self",
      "check_autotuning_cache",
      "autotune_configs",
      "cache_autotuning_results",
      "allow_heuristics"
    ],
    "heuristics_config": [
      "self"
    ],
    "autotuning_cache_key": [
      "self"
    ],
    "cached_autotuning_data": [
      "self"
    ],
    "autotuning_configs": [
      "self"
    ],
    "benchmark": [
      "self",
      "mode"
    ],
    "autotune": [
      "self",
      "configs",
      "autotuner",
      "cache_results"
    ],
    "vjp_arg_spec": [
      "self"
    ],
    "__get_pydantic_core_schema__": [
      "cls",
      "source",
      "handler"
    ],
    "replace": []
  },
  "_AlwaysEqual": {
    "__call__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ]
  },
  "_get_arg_spec_adapter": [
    "op"
  ],
  "BOUND_ARGS_ADAPTER": [],
  "infer_devices": [
    "ba"
  ],
  "infer_device_kind": [
    "ba"
  ],
  "_abstractify": [
    "pytree"
  ],
  "ShapedArray": [],
  "_make_argspec": [],
  "_IMPLS": [],
  "_BENCHMARK_IMPLS_FWD": [],
  "_BENCHMARK_IMPLS_FWD_BWD": [],
  "_register_benchmarks": [],
  "_NUM_REGISTERS_PER_SM": [],
  "_normalization_vjp_kernel": [
    "dout_ref",
    "x_ref",
    "scale_ref",
    "mean_ref",
    "rstddev_ref",
    "dx_ref",
    "dscale_ref",
    "doffset_ref"
  ],
  "PallasTritonNormalizationVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "x",
      "scale",
      "offset"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_cache_key": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "Normalization": {
    "bind": [
      "self",
      "x",
      "scale",
      "offset"
    ],
    "_fwd": [
      "self",
      "x",
      "scale",
      "offset"
    ]
  },
  "NormalizationVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "x",
      "scale",
      "offset"
    ]
  },
  "NormalizationTest": {
    "__init__": [
      "self"
    ]
  },
  "IMPLEMENTATIONS": [],
  "_DEFAULT_IMPLEMENTATION": [],
  "layer_norm": [
    "x",
    "scale",
    "offset"
  ],
  "PallasTritonNormalizationTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "test_layer_norm_with_pre_scale": [
      "self"
    ],
    "_test_layer_norm_vmap": [
      "self",
      "axis",
      "vmap_in_axes"
    ],
    "test_remat": [
      "self"
    ],
    "test_remat_with_vmap": [
      "self"
    ]
  },
  "Config": {},
  "canonicalize_shape_3d": [
    "shape",
    "axis"
  ],
  "canonicalize_shape": [
    "shape",
    "axis"
  ],
  "_maybe_shape": [
    "x"
  ],
  "get_key": [
    "x",
    "scale",
    "offset"
  ],
  "get_heuristics_config": [
    "x",
    "scale",
    "offset"
  ],
  "_canonicalize_shape": [],
  "NAMED_ARG_SPECS": [],
  "NormalizationTestBase": {
    "__init__": [
      "self"
    ],
    "test_layer_norm": [
      "self",
      "shape",
      "use_scale_and_offset",
      "scale_offset"
    ],
    "test_layer_norm_axis": [
      "self",
      "axis"
    ],
    "test_layer_norm_vmap": [
      "self",
      "axis",
      "vmap_in_axes"
    ],
    "_test_layer_norm_vmap": [
      "self",
      "axis",
      "vmap_in_axes"
    ],
    "test_rms_norm": [
      "self",
      "shape"
    ],
    "_run_test": [
      "self",
      "x",
      "scale",
      "offset"
    ],
    "test_bench": [
      "self",
      "kwargs"
    ]
  },
  "FusedInputArray": [],
  "_normalization_kernel": [
    "x_value_refs",
    "scale_ref",
    "offset_ref",
    "y_ref",
    "mean_ref",
    "rstddev_ref"
  ],
  "PallasTritonNormalization": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "x",
      "scale",
      "offset"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_cache_key": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "FLAGS": [],
  "_get_input_data": [
    "shape",
    "dtype"
  ],
  "LayerNormTest": {
    "test_basic_api": [
      "self",
      "implementation"
    ]
  },
  "LayerNormTritonTest": {
    "IMPL": [],
    "__init__": [
      "self"
    ]
  },
  "LayerNormXlATest": {
    "IMPL": [],
    "__init__": [
      "self"
    ]
  },
  "COMPUTE_WGS": [],
  "STORE_WG": [],
  "MEMORY_WG": [],
  "ragged_dot_quantized_ws_async_store_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "SPEC_SHAPES": [],
  "_generate_group_sizes": [
    "target_m",
    "g"
  ],
  "_make_spec": [
    "name",
    "num_groups",
    "m",
    "n",
    "k",
    "lhs_dtype",
    "rhs_dtype",
    "group_sizes",
    "project"
  ],
  "ARG_SPECS": [],
  "ragged_dot_quantized_kernel_body": [
    "group_info",
    "mi",
    "ni",
    "w_gmem",
    "x_gmem",
    "w_scales_gmem",
    "o_gmem"
  ],
  "ragged_dot_quantized_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "_xla_only_group0": [
    "lhs",
    "rhs"
  ],
  "_xla_even_groups": [
    "lhs",
    "rhs"
  ],
  "_flops": [
    "lhs",
    "rhs"
  ],
  "_BENCHMARK_IMPLS": [],
  "_register_benchmark": [],
  "_transpose_rhs": [
    "x"
  ],
  "_MAIN_WG": [],
  "_DEQ_WG": [],
  "_STORE_WG": [],
  "_MMA_WARP": [],
  "_W_TMA_WARP": [],
  "_X_TMA_WARP": [],
  "_TMEM": [],
  "_TCGEN05": [],
  "_TCGEN05_ROW": [],
  "dequant": [
    "s_ref",
    "w"
  ],
  "ragged_dot_gpu_quant_post_scale_blackwell_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "_DotAlgorithmLike": [],
  "ActivationFunction": [],
  "DEFAULT_RAGGED_DOT_DIM_NUMS": [],
  "TRANS_RHS_RAGGED_DOT_DIM_NUMS": [],
  "RAGGED_CONTRACTING_DOT_DIM_NUMS": [],
  "_STATIC": [],
  "GroupSizes": {
    "__post_init__": [
      "self"
    ],
    "__jax_array__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ],
    "__get_pydantic_core_schema__": [
      "cls",
      "source",
      "handler"
    ]
  },
  "RaggedDot": {
    "bind": [
      "self",
      "lhs",
      "rhs"
    ],
    "_fwd": [
      "self",
      "lhs",
      "rhs"
    ]
  },
  "vjp": [
    "residuals",
    "out",
    "dout",
    "lhs",
    "rhs"
  ],
  "TilingTuple": [],
  "InputBufferCount": [],
  "LUTKey": [],
  "LUTValue": [],
  "_group_sizes_to_indices": [
    "gs"
  ],
  "DLHS_RAGGED_DOT_DIM_NUMS": [],
  "DRHS_RAGGED_DOT_DIM_NUMS": [],
  "UNSUPPORTED_DIMENSIONS_MSG": [],
  "PallasMosaicTpuRaggedDot": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "lhs",
      "rhs"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "RaggedDotTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ]
  },
  "RaggedDotWithExplicitVjpTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ]
  },
  "ragged_dot": [
    "lhs",
    "rhs",
    "group_sizes",
    "precision",
    "preferred_element_type",
    "group_offset",
    "activation"
  ],
  "ragged_dot_general": [
    "lhs",
    "rhs",
    "group_sizes",
    "ragged_dot_dimension_numbers",
    "precision",
    "preferred_element_type",
    "group_offset",
    "activation"
  ],
  "MatmulDimension": {
    "M": [],
    "N": []
  },
  "GroupInfo": {
    "create": [
      "cls",
      "group_sizes",
      "tile",
      "tid_size"
    ],
    "create_aligned": [
      "cls",
      "group_sizes",
      "tile",
      "tid_size",
      "align_tile",
      "noops_at_end"
    ]
  },
  "calculate_group_info_tasks": [
    "group_sizes",
    "max_tasks",
    "block_m",
    "align_block_size",
    "noops_at_end"
  ],
  "store_acc_transposed": [
    "acc",
    "o_gmem",
    "ni",
    "m",
    "group_info",
    "o_smem"
  ],
  "ragged_kernel": [
    "body"
  ],
  "_validate_args": [
    "lhs",
    "rhs",
    "group_sizes"
  ],
  "GroupMetadata": [],
  "make_group_metadata": [],
  "_get_store_mask": [],
  "_quantize_as": [
    "x",
    "qdtype",
    "axis",
    "scale"
  ],
  "_scale_out_by_scale": [
    "out",
    "scales",
    "axis"
  ],
  "_TilingFn": [],
  "gmm": [
    "lhs",
    "rhs",
    "group_sizes",
    "precision",
    "out_dtype",
    "tiling",
    "input_buffer_count",
    "group_offset",
    "transpose_rhs",
    "interpret",
    "lhs_qdtype",
    "lhs_static_scale",
    "rhs_qdtype",
    "rhs_static_scale",
    "activation"
  ],
  "tgmm": [
    "lhs",
    "rhs",
    "group_sizes",
    "precision",
    "out_dtype",
    "tiling",
    "input_buffer_count",
    "group_offset",
    "num_actual_groups",
    "interpret",
    "lhs_qdtype",
    "lhs_static_scale",
    "rhs_qdtype",
    "rhs_static_scale",
    "activation"
  ],
  "PallasTritonRaggedDotTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "test_split_k": [
      "self",
      "split_k"
    ],
    "test_split_k_quantized": [
      "self"
    ],
    "test_simple": [
      "self",
      "dtype"
    ]
  },
  "PallasMosaicGpuRaggedDot": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "lhs",
      "rhs"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "_get_sm90_autotuning_configs": [
      "self",
      "ba"
    ],
    "_get_sm100_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "_STORE_WARP": [],
  "ragged_dot_gpu_quant_blackwell_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "_CONFIG": [],
  "PallasMosaicGpuRaggedDotTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ]
  },
  "_WGMMA": [],
  "ragged_dot_kernel_body": [
    "group_info",
    "mi",
    "ni",
    "lhs_gmem",
    "rhs_gmem",
    "o_gmem"
  ],
  "ragged_dot_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "ragged_contracting_dim_dot_kernel_body": [
    "group_sizes_gmem",
    "group_sizes_starts_gmem",
    "lhs_gmem",
    "rhs_gmem",
    "o_gmem"
  ],
  "ragged_contracting_dim_dot_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "_is_scale_tiling_supported": [
    "x",
    "axis"
  ],
  "_is_config_supported": [
    "lhs",
    "rhs",
    "config"
  ],
  "PallasMosaicTpuRaggedDotTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "test_vjp0": [
      "self"
    ],
    "_test_quantized": [
      "self",
      "dtype",
      "a_tile_shape",
      "b_tile_shape",
      "activation",
      "use_as_qarray"
    ],
    "_test_bench": [
      "self",
      "spec"
    ],
    "test_maxtext_config": [
      "self"
    ],
    "test_autotuning_configs": [
      "self"
    ]
  },
  "body": [
    "group_info",
    "mi",
    "ni",
    "w_gmem",
    "x_gmem",
    "w_scales_gmem",
    "o_gmem",
    "schedule_barrier"
  ],
  "ragged_dot_quantized_ws_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "_dot_fn_f32": [
    "dot_fn"
  ],
  "_jax_ragged_dot_f32": [],
  "ref": [
    "lhs",
    "rhs",
    "group_sizes",
    "activation"
  ],
  "override_chex_args": [],
  "relu": [
    "x"
  ],
  "RaggedDotTestBase": {
    "__init__": [
      "self"
    ],
    "_dot_fn_f32": [
      "self"
    ],
    "_create_inputs": [
      "self",
      "num_groups",
      "m",
      "k",
      "n",
      "dtype",
      "random_groups",
      "std"
    ],
    "_test_simple": [
      "self",
      "dtype"
    ],
    "test_simple": [
      "self",
      "dtype"
    ],
    "test_padded": [
      "self"
    ],
    "test_zero_group_sizes": [
      "self"
    ],
    "test_quantized": [
      "self",
      "dtype",
      "a_tile_shape",
      "b_tile_shape",
      "use_as_qarray",
      "activation"
    ],
    "_test_quantized": [
      "self",
      "dtype",
      "a_tile_shape",
      "b_tile_shape",
      "use_as_qarray",
      "activation"
    ],
    "test_preferred_element_type": [
      "self",
      "out_type"
    ],
    "test_vjp": [
      "self",
      "num_groups",
      "m",
      "k",
      "n",
      "activation"
    ],
    "test_group_sizes": [
      "self"
    ],
    "test_invalid_group_sizes": [
      "self",
      "group_sizes"
    ],
    "test_bench": [
      "self",
      "spec"
    ],
    "_test_bench": [
      "self",
      "spec"
    ]
  },
  "_ragged_dot_kernel": [
    "a_ref",
    "a_scales_ref",
    "b_ref",
    "b_scales_ref",
    "lo_ref",
    "hi_ref",
    "out_ref"
  ],
  "_ragged_dot": [
    "lhs",
    "rhs"
  ],
  "_ragged_contracting_dim_dot_kernel": [
    "a_ref",
    "b_ref",
    "lo_ref",
    "hi_ref",
    "out_ref"
  ],
  "_ragged_contracting_dim_dot": [
    "lhs",
    "rhs"
  ],
  "PallasTritonRaggedDot": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "lhs",
      "rhs"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "RaggedDotImplementationTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ]
  },
  "RaggedDotMosaicTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ]
  },
  "RaggedDotTritonTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "_test_bench": [
      "self",
      "spec"
    ]
  },
  "RaggedDotXlaTest": {
    "__init__": [
      "self"
    ]
  },
  "_COMPUTE_WG": [],
  "_TMA_WARP": [],
  "ragged_dot_gpu_non_quant_blackwell_kernel": [
    "lhs",
    "rhs",
    "group_sizes",
    "out_dtype",
    "config",
    "activation"
  ],
  "_create_arg_spec": [
    "batch_dim",
    "hidden_dim",
    "output_dim",
    "reduction",
    "x_dtype",
    "w_dtype",
    "project"
  ],
  "LinearSoftmaxCrossEntropyLoss": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "x",
      "labels",
      "w"
    ]
  },
  "LinearSoftmaxCrossEntropyLossVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "x",
      "labels",
      "w"
    ]
  },
  "validate_inputs": [
    "x",
    "labels",
    "w",
    "b_block_size",
    "h_block_size",
    "v_block_size"
  ],
  "linear_softmax_cross_entropy_loss_forward_pallas_kernel": [
    "x_ref",
    "labels_ref",
    "w_ref",
    "loss_ref",
    "lse_ref",
    "xw_tiled",
    "b_block_loss_ref",
    "reduction",
    "v_dim"
  ],
  "linear_softmax_cross_entropy_loss_fwd_pallas_mosaic_tpu": [
    "x",
    "labels",
    "w"
  ],
  "linear_softmax_cross_entropy_loss_backward_pallas_kernel": [
    "x_ref",
    "labels_ref",
    "w_ref",
    "lse_ref",
    "x_grad_hbm_ref",
    "w_grad_hbm_ref",
    "xw_scratch_ref",
    "x_grad_tile_ref",
    "w_grad_tile_ref",
    "x_read_sem",
    "w_read_sem",
    "x_write_sem",
    "w_write_sem",
    "reduction"
  ],
  "linear_softmax_cross_entropy_loss_bwd_pallas_mosaic_tpu": [
    "dout",
    "lse",
    "x",
    "labels",
    "w"
  ],
  "get_tpu_specific_default_config": [],
  "PallasMosaicTpuLinearSoftmaxCrossEntropyLoss": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "x",
      "labels",
      "w"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "PallasMosaicTpuLinearSoftmaxCrossEntropyLossVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "x",
      "labels",
      "w"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "LinearSoftmaxCrossEntropyLossBaseTest": {
    "test_reference_running_correctly": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction"
    ]
  },
  "generate_random_data": [
    "key",
    "b_dim",
    "h_dim",
    "v_dim"
  ],
  "linear_softmax_cross_entropy_loss": [
    "x",
    "labels",
    "weights"
  ],
  "FlashLceReferenceTest": {
    "test_reference_fwd_running_correctly": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction"
    ],
    "test_reference_bwd_matches_jax_grad": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction"
    ]
  },
  "FlashLcePallasMosaicTpuTest": {
    "setUp": [
      "self"
    ],
    "test_kernel_forward_matches_reference": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction"
    ],
    "test_kernel_bwd_matches_reference": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction"
    ],
    "test_op_wrapper_fwd_bwd": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction"
    ],
    "test_validation_errors": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim"
    ]
  },
  "linear_softmax_cross_entropy_loss_fwd_reference": [
    "x",
    "labels",
    "w"
  ],
  "linear_softmax_cross_entropy_loss_bwd_reference": [
    "dout",
    "lse",
    "x",
    "labels",
    "w"
  ],
  "ApiTest": {
    "test_api_fwd_bwd_matches_reference": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction",
      "test_impl",
      "reference_impl"
    ],
    "test_correct_implementation_used": [
      "self",
      "b_dim",
      "h_dim",
      "v_dim",
      "reduction",
      "test_impl"
    ]
  },
  "P": [],
  "MaskInfo": [],
  "partial": [],
  "NUM_LANES": [],
  "NUM_SUBLANES": [],
  "NN_DIM_NUMBERS": [],
  "NT_DIM_NUMBERS": [],
  "LOG2E": [],
  "LOG2E_INV": [],
  "_not": [
    "x"
  ],
  "SegmentIds": {},
  "MaskFunctionType": [],
  "get_kernel_name": [
    "is_mqa",
    "save_residuals",
    "is_segmented",
    "phase"
  ],
  "QKVLayout": {
    "HEAD_DIM_MINOR": [],
    "SEQ_MINOR": []
  },
  "from_head_minor": [
    "vals",
    "layout"
  ],
  "SplashConfig": {
    "__post_init__": [
      "self"
    ],
    "has_backward_blocks": [
      "self"
    ],
    "get_default": [
      "cls"
    ]
  },
  "to_i32": [],
  "_apply_mask_and_soft_cap": [
    "qk",
    "mask_value",
    "mask_ref",
    "q_sequence_ref",
    "q_segment_ids_ref",
    "kv_segment_ids_ref"
  ],
  "flash_attention_kernel": [
    "active_rows_ref",
    "active_cols_ref",
    "mask_next_ref",
    "bounds_start_ref",
    "bounds_end_ref",
    "block_mask_ref",
    "q_ref",
    "k_ref",
    "v_ref",
    "q_segment_ids_ref",
    "kv_segment_ids_ref",
    "sinks_ref",
    "mask_ref",
    "q_sequence_ref",
    "max_logit_value_ref",
    "o_ref",
    "logsumexp_ref",
    "l_linear_ref",
    "max_logits_ref",
    "m_scratch_ref",
    "l_scratch_ref",
    "o_scratch_ref"
  ],
  "_div": [
    "dividend",
    "divisor"
  ],
  "_bytes": [
    "x"
  ],
  "_splash_attention_forward": [
    "mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "sinks",
    "mask_value",
    "is_mqa",
    "config",
    "save_residuals",
    "mask_function",
    "fwd_mask_sparsity",
    "max_logit_value"
  ],
  "_splash_attention_custom": [
    "fwd_mask_info",
    "dkv_mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "sinks",
    "save_residuals",
    "mask_value",
    "is_mqa",
    "config",
    "mask_function",
    "fwd_mask_sparsity",
    "dkv_mask_sparsity",
    "max_logit_value"
  ],
  "_splash_attention_fwd": [
    "fwd_mask_info",
    "dkv_mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "sinks",
    "save_residuals",
    "mask_value",
    "is_mqa",
    "config",
    "mask_function",
    "fwd_mask_sparsity",
    "dkv_mask_sparsity",
    "max_logit_value"
  ],
  "_flash_attention_dq_kernel": [
    "active_rows_ref",
    "active_cols_ref",
    "mask_next_ref",
    "bounds_start_ref",
    "bounds_end_ref",
    "block_mask_ref",
    "q_ref",
    "k_ref",
    "v_ref",
    "q_segment_ids_ref",
    "kv_segment_ids_ref",
    "logsumexp_ref",
    "do_ref",
    "di_ref",
    "mask_ref",
    "q_sequence_ref",
    "dq_scratch_ref",
    "dq_ref"
  ],
  "_flash_attention_dkv_kernel": [
    "active_rows_ref",
    "active_cols_ref",
    "mask_next_ref",
    "bounds_start_ref",
    "bounds_end_ref",
    "block_mask_ref",
    "q_ref",
    "k_ref",
    "v_ref",
    "q_segment_ids_ref",
    "kv_segment_ids_ref",
    "logsumexp_ref",
    "do_ref",
    "di_ref",
    "mask_ref",
    "q_sequence_ref",
    "dq_alias",
    "dk_alias",
    "dv_alias",
    "dq_ref",
    "dk_ref",
    "dv_ref",
    "dq_scratch_ref",
    "dk_scratch_ref",
    "dv_scratch_ref"
  ],
  "_splash_attention_bwd_dkv": [
    "q",
    "k",
    "v",
    "segment_ids",
    "logsumexp",
    "do",
    "di"
  ],
  "_splash_attention_bwd": [
    "save_residuals",
    "mask_value",
    "is_mqa",
    "config",
    "mask_function",
    "fwd_mask_sparsity",
    "dkv_mask_sparsity",
    "res",
    "do"
  ],
  "_splash_attention": [
    "fwd_mask_info",
    "dkv_mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "sinks"
  ],
  "SplashAttentionKernel": {
    "__init__": [
      "self",
      "fwd_mask_info",
      "dkv_mask_info"
    ],
    "__call__": [
      "self"
    ],
    "manual_sharding_spec": [
      "self",
      "sharding"
    ],
    "tree_flatten": [
      "self"
    ],
    "tree_unflatten": [
      "cls",
      "kwargs",
      "values"
    ]
  },
  "_make_splash_attention": [
    "mask"
  ],
  "_make_dynamic_splash_attention": [
    "mask"
  ],
  "make_splash_mha": [],
  "make_splash_mqa": [],
  "make_splash_mha_single_device": [],
  "make_splash_mqa_single_device": [],
  "make_dynamic_splash_mqa": [],
  "make_dynamic_splash_mha": [],
  "lax": [],
  "MaskCallable": [],
  "find_bounds": [
    "arr"
  ],
  "_downcast_to_small_type": [
    "array"
  ],
  "_check_mask": [
    "mask"
  ],
  "_HashableNDArray": {
    "__slots__": [],
    "__init__": [
      "self",
      "array"
    ],
    "__hash__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ]
  },
  "_get_mask_info": [
    "mask",
    "block_shape",
    "coords_to_partial_mask_block_index",
    "q_seq_start",
    "q_seq_shard_size",
    "blocked_q_seq_start",
    "kv_seq_start",
    "kv_seq_shard_size",
    "blocked_kv_seq_start",
    "is_dkv",
    "return_dynamic_grid"
  ],
  "_process_dynamic_mask": [
    "mask",
    "block_shape",
    "is_dkv"
  ],
  "_process_mask": [
    "mask",
    "block_shape",
    "is_dkv"
  ],
  "process_mask": [],
  "process_mask_dkv": [],
  "process_dynamic_mask": [],
  "process_dynamic_mask_dkv": [],
  "SplashResidualsType": [],
  "_attention_reference_impl": [
    "q",
    "k",
    "v",
    "mask",
    "segment_ids",
    "sinks",
    "mask_value",
    "save_residuals",
    "attn_logits_soft_cap"
  ],
  "_attention_reference_custom_bwd": [
    "do",
    "q",
    "k",
    "v",
    "mask",
    "segment_ids",
    "sinks",
    "o",
    "logsumexp",
    "mask_value",
    "backward_impl",
    "attn_logits_soft_cap"
  ],
  "attention_reference": [
    "q",
    "k",
    "v",
    "mask",
    "segment_ids",
    "sinks"
  ],
  "attention_reference_vjp": [
    "do",
    "q",
    "k",
    "v",
    "mask",
    "segment_ids",
    "sinks",
    "o",
    "logsumexp"
  ],
  "Draw": [],
  "ModelConfig": {},
  "segment_ids_strategy": [
    "draw",
    "seq_len"
  ],
  "seed_strategy": [],
  "Mask": {
    "get_mask": [
      "self"
    ]
  },
  "full_mask_strategy": [
    "q_seq_len",
    "kv_seq_len"
  ],
  "SplitMask": {
    "get_mask": [
      "self"
    ]
  },
  "split_mask_strategy": [
    "q_seq_len",
    "kv_seq_len"
  ],
  "FullMask": {
    "get_mask": [
      "self"
    ]
  },
  "causal_mask_strategy": [
    "q_seq_len",
    "kv_seq_len"
  ],
  "CausalMask": {
    "get_mask": [
      "self"
    ]
  },
  "LocalAttentionMask": {
    "get_mask": [
      "self"
    ]
  },
  "local_attention_mask_strategy": [
    "draw",
    "seq_len"
  ],
  "RandomMask": {
    "get_mask": [
      "self"
    ]
  },
  "random_mask_strategy": [
    "draw",
    "q_seq_len",
    "kv_seq_len"
  ],
  "ComposeMask": {
    "get_mask": [
      "self"
    ]
  },
  "compose_mask_strategy": [
    "draw",
    "q_seq_len",
    "kv_seq_len"
  ],
  "mask_strategy": [
    "draw",
    "q_seq_len",
    "kv_seq_len"
  ],
  "model_config_strategy": [
    "draw"
  ],
  "check_mask_no_empty_rows": [
    "mask",
    "segment_ids"
  ],
  "block_sizes_strategy": [
    "draw",
    "q_seq_len",
    "kv_seq_len",
    "include_bwd_blocks"
  ],
  "_generate_inputs": [
    "data",
    "config",
    "is_mqa",
    "is_segmented",
    "use_sinks"
  ],
  "attn_logits_soft_cap_strategy": [],
  "SplashAttentionTest": {
    "setUp": [
      "self"
    ],
    "test_splash_attention": [
      "self",
      "is_mqa",
      "is_segmented",
      "is_dynamic_mask",
      "data"
    ],
    "test_splash_attention_fwd": [
      "self",
      "is_mqa",
      "is_segmented",
      "is_dynamic_mask",
      "use_base2_exp",
      "use_max_logit_estimate",
      "fuse_reciprocal",
      "use_sinks",
      "data"
    ],
    "test_splash_attention_bwd": [
      "self",
      "is_mqa",
      "is_segmented",
      "is_dynamic_mask",
      "use_max_logit_estimate",
      "dq_reduction_steps",
      "use_sinks",
      "data"
    ]
  },
  "PartitionSpec": [],
  "PallasBaseTest": {
    "INTERPRET": [],
    "setUp": [
      "self"
    ]
  },
  "SplashAttentionShardingTest": {
    "setUp": [
      "self"
    ],
    "test_manual_partitioning_mha_fwd": [
      "self",
      "topology",
      "num_heads",
      "dtype",
      "is_dynamic_mask"
    ],
    "test_manual_partitioning_mha_bwd": [
      "self",
      "topology",
      "num_heads",
      "dtype",
      "is_dynamic_mask"
    ]
  },
  "make_causal_mask": [
    "shape",
    "offset"
  ],
  "make_local_attention_mask": [
    "shape",
    "window_size"
  ],
  "make_chunk_attention_mask": [
    "shape",
    "chunk_size"
  ],
  "make_random_mask": [
    "shape",
    "sparsity",
    "seed"
  ],
  "LogicalOr": {
    "__init__": [
      "self",
      "left",
      "right"
    ],
    "shape": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__hash__": [
      "self"
    ]
  },
  "LogicalAnd": {
    "__init__": [
      "self",
      "left",
      "right"
    ],
    "shape": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__hash__": [
      "self"
    ]
  },
  "_ComputableMask": {
    "__init__": [
      "self",
      "shape",
      "mask_function",
      "shard_count"
    ],
    "shape": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "ChunkedCausalMask": {
    "__init__": [
      "self",
      "shape",
      "chunk_size",
      "shard_count"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "LocalMask": {
    "__init__": [
      "self",
      "shape",
      "window_size",
      "offset",
      "shard_count"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "NumpyMask": {
    "__post_init__": [
      "self"
    ],
    "shape": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "_fill_slice": [
    "inp_slice",
    "size"
  ],
  "test_device_matches": [
    "devices"
  ],
  "thread_unsafe_test_class": [],
  "SplashAttentionTestCase": {
    "INTERPRET": [],
    "setUp": [
      "self"
    ],
    "_assert_array_equal": [
      "self",
      "x",
      "y"
    ],
    "_assert_allclose": [
      "self",
      "x",
      "y"
    ]
  },
  "RingAttentionTest": {
    "setUp": [
      "self"
    ],
    "test_ring_attention_mha_fwd_bwd": [
      "self",
      "topology",
      "batch_size",
      "num_heads",
      "head_dim",
      "dtype",
      "is_mqa",
      "padding_factor",
      "mask",
      "fused_bwd"
    ]
  },
  "RING_AXIS": [],
  "SplashCustomReturnType": [],
  "DEFAULT_MASK_VALUE": [],
  "_ring_attention_forward": [
    "fwd_mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "mask_value",
    "is_mqa",
    "config",
    "mask_function",
    "fwd_mask_sparsity",
    "sinks",
    "ring_axis"
  ],
  "_ring_attention_backward": [
    "res",
    "do"
  ],
  "_ring_attention_bwd": [
    "mask_value",
    "is_mqa",
    "config",
    "mask_function",
    "fwd_mask_sparsity",
    "dkv_mask_sparsity",
    "save_residuals",
    "ring_axis",
    "res",
    "do"
  ],
  "_ring_attention_fwd": [
    "fwd_mask_info",
    "dkv_mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "sinks",
    "mask_value",
    "is_mqa",
    "config",
    "mask_function",
    "fwd_mask_sparsity",
    "dkv_mask_sparsity",
    "save_residuals",
    "ring_axis"
  ],
  "_ring_attention_custom": [
    "fwd_mask_info",
    "dkv_mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "sinks",
    "mask_value",
    "is_mqa",
    "config",
    "mask_function",
    "fwd_mask_sparsity",
    "dkv_mask_sparsity",
    "save_residuals",
    "ring_axis"
  ],
  "_has_axis": [
    "axis_name"
  ],
  "_ring_attention": [
    "fwd_mask_info",
    "dkv_mask_info",
    "q",
    "k",
    "v",
    "segment_ids",
    "sinks"
  ],
  "RingSplashAttentionKernel": {
    "__init__": [
      "self",
      "fwd_mask_info",
      "dkv_mask_info",
      "ring_axis"
    ],
    "__call__": [
      "self"
    ],
    "tree_flatten": [
      "self"
    ],
    "tree_unflatten": [
      "cls",
      "aux_data",
      "children"
    ]
  },
  "make_ring_attention": [
    "mask"
  ],
  "_make_lazy_causal_mask": [],
  "_make_causal_mask": [],
  "_make_lazy_local_attention_mask": [],
  "_make_local_attention_mask": [],
  "_make_lazy_chunked_causal_mask": [
    "shape",
    "chunk_size"
  ],
  "_make_chunked_causal_mask": [
    "shape",
    "chunk_size"
  ],
  "SplashAttentionMaskTest": {
    "setUp": [
      "self"
    ],
    "test_causal_mask": [
      "self",
      "make_causal_mask"
    ],
    "test_local_attention_mask": [
      "self",
      "make_local_attention_mask"
    ],
    "test_local_attention_mask_wide_rectangle": [
      "self",
      "make_local_attention_mask"
    ],
    "test_local_attention_mask_tall_rectangle": [
      "self",
      "make_local_attention_mask"
    ],
    "test_lazy_causal_mask_chunking": [
      "self",
      "block_size",
      "shape"
    ],
    "test_lazy_local_mask_chunking": [
      "self",
      "block_size",
      "shape",
      "window_size",
      "offset"
    ],
    "test_chunked_causal_mask": [
      "self",
      "make_chunked_mask"
    ],
    "test_lazy_chunked_causal_mask_chunking": [
      "self",
      "block_size",
      "shape",
      "chunk_size"
    ],
    "test_chunked_causal_mask_invalid_chunk_size": [
      "self"
    ],
    "test_chunked_causal_mask_minimal_equality_hash": [
      "self"
    ],
    "test_using_logical_operators_raises_exception": [
      "self"
    ],
    "test_lazy_mask_or": [
      "self",
      "shape"
    ],
    "test_lazy_mask_and": [
      "self",
      "shape"
    ],
    "test_lazy_full_mask": [
      "self",
      "shape"
    ],
    "_compare_masks": [
      "self",
      "dense_mask",
      "lazy_mask",
      "block_size"
    ]
  },
  "SplashAttentionMaskInfoTest": {
    "_assert_mask_info_match": [
      "self",
      "actual",
      "expected"
    ],
    "_process_mask": [
      "self"
    ],
    "test_full_mask": [
      "self",
      "is_lazy_mask"
    ],
    "test_no_partial_mask_blocks": [
      "self"
    ],
    "test_rectangular_wide_causal_mask": [
      "self",
      "is_lazy_mask",
      "return_dynamic_grid"
    ],
    "test_rectangular_tall_causal_mask": [
      "self",
      "is_lazy_mask"
    ],
    "test_local_mask": [
      "self",
      "is_lazy_mask"
    ],
    "test_local_mask_narrow": [
      "self",
      "is_lazy_mask"
    ],
    "test_two_qseq_shards_causal_local_stacked": [
      "self"
    ],
    "test_two_qseq_shards_local_wide_local_narrow_stacked": [
      "self"
    ],
    "test_huge_mask": [
      "self"
    ],
    "test_huge_mask2": [
      "self"
    ],
    "test_process_invalid_mask": [
      "self"
    ],
    "test_dynamic_mask": [
      "self"
    ],
    "test_find_bounds": [
      "self"
    ]
  },
  "TriangleMultiplication": {
    "bind": [
      "self",
      "x",
      "mask",
      "projection_in_weights",
      "gate_in_weights",
      "projection_out_weights",
      "gate_out_weights",
      "layernorm_in_scale",
      "layernorm_in_offset",
      "layernorm_out_scale",
      "layernorm_out_offset",
      "triangle_type"
    ],
    "_fwd": [
      "self",
      "x",
      "mask",
      "projection_in_weights",
      "gate_in_weights",
      "projection_out_weights",
      "gate_out_weights",
      "layernorm_in_scale",
      "layernorm_in_offset",
      "layernorm_out_scale",
      "layernorm_out_offset",
      "triangle_type"
    ]
  },
  "_get_params": [
    "n",
    "c",
    "h",
    "d",
    "dtype"
  ],
  "TriangleMultiplicationTest": {
    "test_triangle_multiplication": [
      "self",
      "triangle_type",
      "dtype",
      "precision"
    ],
    "test_triangle_multiplication_grad": [
      "self",
      "triangle_type",
      "dtype",
      "precision"
    ]
  },
  "triangle_multiplication": [
    "x",
    "mask",
    "projection_in_weights",
    "gate_in_weights",
    "projection_out_weights",
    "gate_out_weights",
    "layernorm_in_scale",
    "layernorm_in_offset",
    "layernorm_out_scale",
    "layernorm_out_offset",
    "triangle_type"
  ],
  "ScoreMod": [],
  "MaskMod": [],
  "FlexAttention": {
    "__call__": [
      "self",
      "q",
      "k",
      "v"
    ],
    "bind": [
      "self",
      "q",
      "k",
      "v"
    ],
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ]
  },
  "_softmax": [
    "x",
    "normalize"
  ],
  "_softmax_jvp": [
    "normalize",
    "primals",
    "tangents"
  ],
  "FlexAttentionTest": {
    "__init__": [
      "self"
    ]
  },
  "WrappedFlexAttentionTest": {
    "__init__": [
      "self"
    ],
    "test_normalize_output": [
      "self"
    ]
  },
  "PallasTritonFlexAttentionTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ]
  },
  "WrappedPallasTritonFlexAttentionTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "_test_vmap": [
      "self",
      "vmap_in_axes"
    ]
  },
  "PagingInfo": [],
  "WrappedFlexAttention": {
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ]
  },
  "WrappedFlexAttentionTestBase": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ]
  },
  "FlexAttentionTestBase": {
    "__init__": [
      "self"
    ],
    "test_bias_upscale": [
      "self"
    ],
    "test_bias_relative_positional": [
      "self"
    ]
  },
  "_rescale": [
    "values",
    "scales_ref",
    "slice_k",
    "slice_d",
    "keep_quantized"
  ],
  "_fwd_kernel": [
    "q_ref",
    "k_ref",
    "v_ref",
    "score_mod_value_refs",
    "mask_mod_value_refs",
    "dropout_mask_ref",
    "out_ref",
    "l_ref",
    "m_ref",
    "block_k",
    "block_d",
    "block_d_out",
    "score_mod_fn",
    "score_mod_value_specs",
    "mask_mod_fn",
    "mask_mod_value_specs",
    "dropout_rate",
    "use_base2",
    "use_stable_softmax",
    "q_k_dot_precision",
    "weights_v_dot_precision",
    "normalize_output"
  ],
  "_tile_score_mod": [
    "score_mod",
    "scores",
    "block_q",
    "block_k"
  ],
  "_tile_mask_mod": [
    "mask_mod",
    "scores",
    "block_q",
    "block_k"
  ],
  "_fwd": [
    "q",
    "k",
    "v"
  ],
  "_can_have_block_d": [],
  "PallasTritonFlexAttention": {
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "_bwd_dkdv": [
    "dk",
    "dv",
    "q_ref",
    "k",
    "v",
    "bias_ref",
    "mask_ref",
    "do_ref",
    "m_ref",
    "l_ref",
    "delta_ref",
    "ds_ref",
    "lo",
    "hi"
  ],
  "_bwd_dq": [
    "dq",
    "q",
    "k_ref",
    "v_ref",
    "bias_ref",
    "mask_ref",
    "do",
    "m",
    "l",
    "delta",
    "lo",
    "hi"
  ],
  "_zero_ds": [
    "ds_ref",
    "lo",
    "hi"
  ],
  "_bwd_kernel": [
    "q_ref",
    "k_ref",
    "v_ref",
    "bias_ref",
    "mask_ref",
    "m_ref",
    "l_ref",
    "delta_ref",
    "dout_ref",
    "dq_ref",
    "dk_ref",
    "dv_ref",
    "ds_ref"
  ],
  "_bwd": [
    "q",
    "k",
    "v",
    "bias",
    "mask",
    "dropout_mask",
    "residuals",
    "out",
    "dout"
  ],
  "PallasTritonFlashAttentionVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "q",
      "k",
      "v"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "JaxNnDotProductAttention": {
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "CAUSAL_MASK": [],
  "DotProductAttention": {
    "__call__": [
      "self",
      "q",
      "k",
      "v"
    ],
    "bind": [
      "self",
      "q",
      "k",
      "v"
    ],
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ]
  },
  "needs_stable_softmax": [
    "logits_dtype",
    "logits_soft_cap"
  ],
  "fold_q_sequence_heads": [
    "q",
    "bias",
    "mask",
    "dropout_mask",
    "q_indices",
    "seq_len_k",
    "num_heads_k"
  ],
  "unfold_q_sequence_heads": [
    "out",
    "residuals",
    "orig_seq_len_q"
  ],
  "vmap_batch_dims": [
    "f"
  ],
  "combine_partial_results": [
    "out",
    "residuals",
    "normalize_output"
  ],
  "DotProductAttentionGrads": {},
  "DotProductAttentionVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "q",
      "k",
      "v"
    ]
  },
  "ConfigVjp": {
    "__post_init__": [
      "self"
    ]
  },
  "PallasMosaicTpuFlashAttention": {
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "MaskTest": {
    "test_as_array": [
      "self"
    ],
    "test_as_array_symbolic_args_specs": [
      "self"
    ],
    "test_take": [
      "self"
    ],
    "test_k_range": [
      "self"
    ],
    "test_q_range": [
      "self"
    ],
    "test_is_causal": [
      "self"
    ],
    "test_intersection": [
      "self"
    ]
  },
  "DotProductAttentionTest": {
    "__init__": [
      "self"
    ],
    "_run_test": [
      "self",
      "q_shape"
    ]
  },
  "DotProductAttentionWithExplicitVjpTest": {
    "__init__": [
      "self"
    ]
  },
  "Mesh": [],
  "NamedSharding": [],
  "ApiShardingTest": {
    "test_dot_product_attention_sharding": [
      "self",
      "implementation"
    ]
  },
  "_attend_chunk": [
    "q",
    "k",
    "v",
    "accum",
    "x_max",
    "denom"
  ],
  "_attend_chunked": [
    "q",
    "k",
    "v"
  ],
  "_attend_paged": [
    "q",
    "k",
    "v"
  ],
  "XlaChunkedDotProductAttention": {
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ]
  },
  "_CUDNN_CUSTOM_CALL_TARGET": [],
  "JaxNnDotProductAttentionTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "_run_test_with_inputs": [
      "self"
    ]
  },
  "JaxNnDotProductAttentionXlaTest": {
    "__init__": [
      "self"
    ]
  },
  "JaxNnDotProductAttentionCudnnTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "_run_test_with_inputs": [
      "self"
    ],
    "test_padding_mask_with_nans": [
      "self"
    ],
    "test_bench_veo3_veo3": [
      "self"
    ],
    "_test_vmap": [
      "self",
      "vmap_in_axes"
    ],
    "test_impl_in_hlo": [
      "self"
    ]
  },
  "dot_product_attention": [
    "query",
    "key",
    "value",
    "bias",
    "mask"
  ],
  "load_bcast": [
    "ref",
    "idx"
  ],
  "num_bits": [
    "dtype"
  ],
  "tile_swizzle_transforms": [
    "shape",
    "dtype",
    "what"
  ],
  "_F32PrecisionXlaAttentionVjp": {
    "__call__": [
      "self"
    ]
  },
  "PallasTritonFlashAttentionTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "_run_test": [
      "self",
      "q_shape"
    ],
    "test_block_d": [
      "self"
    ],
    "test_small_block_q": [
      "self",
      "block_q"
    ],
    "test_split_k": [
      "self",
      "split_k"
    ]
  },
  "PallasTritonFlashAttentionWithPallasTritonVjpTest": {
    "__init__": [
      "self"
    ],
    "_run_test_with_inputs": [
      "self"
    ],
    "test_normalize_output": [
      "self"
    ]
  },
  "_is_precision_supported": [
    "precision"
  ],
  "_broadcast_to_rank": [
    "x",
    "rank"
  ],
  "_decompose_mask": [
    "mask",
    "q",
    "k",
    "q_indices",
    "k_indices"
  ],
  "PallasMosaicGpuFlashAttention": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "XlaChunkedAttentionTest": {
    "__init__": [
      "self"
    ],
    "_run_test": [
      "self",
      "q_shape"
    ],
    "_run_test_with_inputs": [
      "self"
    ],
    "test_chunk_sizes": [
      "self"
    ],
    "test_normalize_output": [
      "self"
    ]
  },
  "_split_into_pages": [
    "unpadded_k",
    "unpadded_v",
    "max_num_pages",
    "max_page_size"
  ],
  "_concatenate_pages": [
    "k_pages",
    "v_pages",
    "max_num_active_pages",
    "num_active_pages",
    "active_page_indices",
    "max_page_size"
  ],
  "XlaPagedAttentionTest": {
    "__init__": [
      "self"
    ],
    "_run_test_with_inputs": [
      "self"
    ],
    "test_normalize_output": [
      "self"
    ],
    "_test_invalid_shapes": [
      "self"
    ],
    "_test_quantized_int8": [
      "self",
      "tile_shape",
      "quantize_q"
    ],
    "_test_quantized_int4": [
      "self",
      "subchannel_size"
    ]
  },
  "PallasMosaicGpuFlashAttentionTest": {
    "__init__": [
      "self"
    ],
    "_run_test_with_inputs": [
      "self",
      "q",
      "k",
      "v"
    ],
    "test_causal_mask": [
      "self"
    ],
    "test_causal_mask_cross_attention0": [
      "self"
    ],
    "test_causal_mask_cross_attention1": [
      "self"
    ],
    "test_padding_mask_with_nans": [
      "self"
    ],
    "test_normalize_output": [
      "self"
    ],
    "test_op_parameters": [
      "self",
      "use_base2",
      "use_stable_softmax"
    ],
    "_test_op_parameters": [
      "self",
      "use_base2",
      "use_stable_softmax"
    ],
    "_test_bench": [
      "self",
      "spec"
    ],
    "test_autotune": [
      "self"
    ],
    "test_split_k": [
      "self"
    ],
    "_test_small_sequences": [
      "self",
      "seq_q",
      "seq_kv"
    ]
  },
  "PallasMosaicGpuFlashAttentionVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "q",
      "k",
      "v"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "_MIN_SWIZZLE": [],
  "_WGMMA_ROW": [],
  "_WGMMA_COL": [],
  "_load_bcast": [],
  "PallasMosaicTpuFlashAttentionTest": {
    "setUp": [
      "self"
    ],
    "test_simple": [
      "self",
      "dtype",
      "is_mqa",
      "masking",
      "logits_soft_cap"
    ]
  },
  "_create_inputs": [],
  "_run_test": [
    "q",
    "k",
    "v"
  ],
  "override_test_args": [],
  "_ref_impl_tanh": [
    "q",
    "k",
    "v"
  ],
  "AttentionTestBase": {
    "__init__": [
      "self"
    ],
    "_run_test": [
      "self",
      "q_shape"
    ],
    "_run_test_with_inputs": [
      "self"
    ],
    "test_self_attention": [
      "self",
      "dtype"
    ],
    "test_cross_attention": [
      "self",
      "has_bias"
    ],
    "test_different_output_head_dim": [
      "self",
      "input_dim",
      "output_dim"
    ],
    "test_small_sequences": [
      "self",
      "seq_q",
      "seq_kv"
    ],
    "_test_small_sequences": [
      "self",
      "seq_q",
      "seq_kv"
    ],
    "test_multi_query_attention": [
      "self"
    ],
    "test_grouped_query_attention": [
      "self"
    ],
    "test_no_batch_dim": [
      "self"
    ],
    "test_multiple_batch_dims": [
      "self"
    ],
    "test_non_power_of_two_q_seq_len": [
      "self"
    ],
    "test_non_power_of_two_head_dim": [
      "self"
    ],
    "test_bias": [
      "self",
      "bias_shape"
    ],
    "_test_bias": [
      "self",
      "bias_shape"
    ],
    "test_mask": [
      "self",
      "mask_shape"
    ],
    "_test_mask": [
      "self",
      "mask_shape"
    ],
    "test_causal_mask": [
      "self"
    ],
    "test_causal_mask_cross_attention": [
      "self",
      "seq_len_k"
    ],
    "test_causal_mask_q_indices": [
      "self"
    ],
    "test_causal_mask_k_indices": [
      "self"
    ],
    "test_q_start_end_indices": [
      "self"
    ],
    "test_padding_mask": [
      "self"
    ],
    "test_padding_mask_with_nans": [
      "self"
    ],
    "test_local_attention_mask": [
      "self"
    ],
    "test_mask_api": [
      "self"
    ],
    "_test_mask_api": [
      "self"
    ],
    "test_tanh_clipping": [
      "self",
      "use_bias"
    ],
    "test_dropout": [
      "self"
    ],
    "test_vmap": [
      "self",
      "vmap_in_axes"
    ],
    "_test_vmap": [
      "self",
      "vmap_in_axes"
    ],
    "test_invalid_shapes": [
      "self"
    ],
    "_test_invalid_shapes": [
      "self"
    ],
    "test_quantized_int8": [
      "self",
      "channelwise_axes",
      "quantize_q"
    ],
    "_test_quantized_int8": [
      "self",
      "channelwise_axes",
      "quantize_q"
    ],
    "test_quantized_int4": [
      "self",
      "subchannel_size"
    ],
    "_test_quantized_int4": [
      "self",
      "subchannel_size"
    ],
    "test_logits_dtype": [
      "self",
      "dtype"
    ],
    "test_normalize_output": [
      "self"
    ],
    "test_partial_attention": [
      "self"
    ],
    "test_bench": [
      "self",
      "spec"
    ],
    "_test_bench": [
      "self",
      "spec"
    ]
  },
  "AttentionManualPartitioningTestBase": {
    "_TEST_SHAPE": [],
    "_ATTENTION_AXES": [],
    "_BIAS_MASK_AXES": [],
    "_PARTITION_AXES": [],
    "_BIAS_MASK_SHAPES": [],
    "__init__": [
      "self"
    ],
    "_run_test": [
      "self"
    ],
    "test_self_attention_one_axis": [
      "self",
      "partition_axis"
    ],
    "test_self_attention_one_axis_shorter_spec": [
      "self",
      "partition_axis"
    ],
    "test_causal_attention_one_axis": [
      "self",
      "partition_axis"
    ],
    "test_self_attention_two_axes": [
      "self",
      "partition_axes"
    ],
    "test_self_attention_bias": [
      "self",
      "bias_shape"
    ],
    "test_self_attention_mask": [
      "self",
      "mask_shape"
    ],
    "_test_self_attention_mask": [
      "self",
      "mask_shape"
    ],
    "test_self_attention_mask_api_local": [
      "self",
      "partition_axes"
    ],
    "test_broadcasted_multi_query_attention": [
      "self",
      "partition_axis"
    ],
    "test_vmap": [
      "self",
      "vmap_in_axes",
      "partition_axis"
    ],
    "test_quantized_int8": [
      "self",
      "partition_axis",
      "channelwise_axes",
      "quantize_q"
    ]
  },
  "flash_attention_vjp_kernel": [
    "q",
    "k",
    "v",
    "residuals",
    "out",
    "dout",
    "bias",
    "mask",
    "k_start",
    "k_end"
  ],
  "_fwd_kernel_impl": [
    "q_start",
    "q_end",
    "q_ref",
    "k_ref",
    "v_ref",
    "bias_ref",
    "mask_ref",
    "dropout_mask_ref",
    "k_start_ref",
    "k_end_ref",
    "out_ref",
    "l_ref",
    "m_ref",
    "block_k",
    "block_d",
    "block_d_out",
    "sm_scale",
    "is_causal",
    "dropout_rate",
    "logits_dtype",
    "logits_soft_cap",
    "use_base2",
    "use_stable_softmax",
    "q_k_dot_precision",
    "weights_v_dot_precision",
    "normalize_output",
    "pack_mask"
  ],
  "PallasTritonFlashAttention": {
    "__post_init__": [
      "self"
    ],
    "_fwd": [
      "self",
      "q",
      "k",
      "v"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "DotProductAttentionMosaicTest": {
    "IMPL": [],
    "setUp": [
      "self"
    ]
  },
  "DotProductAttentionTritonTest": {
    "IMPL": [],
    "setUp": [
      "self"
    ]
  },
  "DotProductAttentionCudnnTest": {
    "IMPL": [],
    "setUp": [
      "self"
    ],
    "test_impl_in_hlo": [
      "self"
    ]
  },
  "DotProductAttentionXlaTest": {
    "IMPL": [],
    "test_precision": [
      "self"
    ]
  },
  "DotProductAttentionXlaChunkedTest": {
    "IMPL": []
  },
  "_FwdFn": [],
  "GatedLinearUnit": {
    "__post_init__": [
      "self"
    ],
    "bind": [
      "self",
      "x",
      "weights"
    ],
    "_fwd": [
      "self",
      "x",
      "weights"
    ],
    "_with_vmap": [
      "self",
      "fwd"
    ]
  },
  "GatedLinearUnitVjp": {
    "_fwd": [
      "self",
      "residuals",
      "out",
      "dout",
      "x",
      "weights"
    ]
  },
  "GatedLinearUnitTest": {
    "__init__": [
      "self"
    ],
    "test_explicit_vjp": [
      "self",
      "dtype",
      "batch_dim",
      "precision"
    ]
  },
  "gated_linear_unit": [
    "x",
    "weights"
  ],
  "PallasTritonGatedLinearUnitTest": {
    "__init__": [
      "self"
    ],
    "setUp": [
      "self"
    ],
    "test_autotuning_search_space": [
      "self"
    ]
  },
  "GatedLinearUnitTestBase": {
    "__init__": [
      "self"
    ],
    "test_gated_linear_unit": [
      "self"
    ]
  },
  "_gated_linear_unit_kernel": [
    "x_ref",
    "weights_ref",
    "out_ref",
    "residuals_ref"
  ],
  "_get_best_block_size": [
    "m",
    "n"
  ],
  "PallasTritonGatedLinearUnit": {
    "_fwd": [
      "self",
      "x",
      "weights"
    ],
    "_get_heuristics_config": [
      "self",
      "ba"
    ],
    "_get_autotuning_configs": [
      "self",
      "ba"
    ],
    "supported_on": [
      "self",
      "device"
    ]
  },
  "GatedLinearUnitTritonTest": {
    "__init__": [
      "self"
    ]
  },
  "GatedLinearUnitXlaTest": {
    "__init__": [
      "self"
    ]
  },
  "_get_group_size_cost": [
    "group_size_m",
    "grid_m",
    "grid_n",
    "block_m_cost",
    "block_n_cost"
  ],
  "get_grid_pids": [
    "pid",
    "grid_m",
    "grid_n",
    "group_size_m"
  ],
  "get_cheapest_grid_pids": [
    "pid"
  ],
  "_SKIP_TPU_TEST_REASON": [],
  "BlockTest": {
    "test_block_ref_bounds": [
      "self",
      "block_shape",
      "expected_bounds",
      "at"
    ],
    "test_block_ref_bounds_checked": [
      "self",
      "block_shape",
      "expect_checked",
      "at"
    ],
    "test_block_ref_inbounds_mask": [
      "self",
      "block_shape",
      "use_grid_spec"
    ],
    "test_block_ref_at_masks": [
      "self"
    ]
  },
  "dslice": [
    "idx",
    "size"
  ],
  "ds": [],
  "_is_scalar_indexer": [
    "idx"
  ],
  "_pids": [],
  "BlockRef": {
    "at": [
      "self"
    ],
    "bounds": [
      "self"
    ],
    "bounds_checked": [
      "self"
    ],
    "inbounds_masks": [
      "self"
    ],
    "inbounds_mask": [
      "self"
    ],
    "load": [
      "self"
    ],
    "store": [
      "self",
      "val"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "_visible_axes": [
      "self"
    ],
    "_ndindexer": [
      "self"
    ]
  },
  "BlockRefIndexer": {
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "_PL_LOAD_STORE_PATCH_LOCK": [],
  "_block_ref": [
    "ref",
    "value",
    "spec"
  ],
  "_as_tuple": [
    "x"
  ],
  "pallas_call": [
    "kernel",
    "out_shape"
  ],
  "ArgSpec": {
    "full_name": [
      "self"
    ]
  },
  "_get_cache_adapter": [
    "op"
  ],
  "AutotuningCache": {
    "__init__": [
      "self",
      "op"
    ],
    "__missing__": [
      "self",
      "device_kind"
    ],
    "_load_cache": [
      "self",
      "device_kind",
      "cache_path"
    ]
  },
  "_serialize_bound_args_autotuning_data": [
    "value",
    "info"
  ],
  "_validate_bound_args_autotuning_data": [
    "value"
  ],
  "AutotuningResult": {
    "dump": [
      "self",
      "fp"
    ],
    "dumps": [
      "self"
    ],
    "dump_cache_str": [
      "self"
    ],
    "load": [
      "cls",
      "fp"
    ],
    "loads": [
      "cls",
      "json_data"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ],
    "__or__": [
      "self",
      "other"
    ]
  },
  "_AUTOTUNING_RESULT_ADAPTER": [],
  "_BOUND_ARGS_ADAPTER": [],
  "get_bound_args": [
    "f"
  ],
  "get_op_implementations": [
    "op",
    "device"
  ],
  "autotune": [
    "f"
  ],
  "_compile": [
    "fn_factory",
    "config",
    "args",
    "kwargs"
  ],
  "_benchmark": [
    "fn_factory",
    "config",
    "args",
    "kwargs"
  ],
  "_SyncExecutor": {
    "submit": []
  },
  "Autotuner": {
    "autotune": [
      "self",
      "fn_factory",
      "configs"
    ]
  },
  "CacheTest": {
    "test_load_cache": [
      "self"
    ],
    "test_default_cache": [
      "self"
    ]
  },
  "get_fn_and_args_and_expected_bound_args": [
    "x_shape",
    "vmap"
  ],
  "AutotuningTest": {
    "test_get_op_implementations": [
      "self"
    ],
    "test_get_bound_args_from_callable": [
      "self"
    ],
    "test_get_bound_args_from_lowered": [
      "self",
      "vmap"
    ],
    "test_get_bound_args_unique": [
      "self"
    ],
    "test_get_bound_args_vjp": [
      "self"
    ],
    "test_autotune": [
      "self"
    ],
    "test_autotuning_result_context": [
      "self"
    ],
    "test_autotuning_result_context_retraced": [
      "self"
    ],
    "test_ragged_dot_autotuning": [
      "self"
    ]
  },
  "generate_group_sizes": [
    "key"
  ],
  "generate_gmm_inputs": [
    "key"
  ],
  "lax_gmm_fwd": [
    "lhs",
    "rhs",
    "group_sizes",
    "tile_m",
    "tile_k",
    "tile_n"
  ],
  "lax_dlhs_bwd": [
    "out",
    "rhs",
    "group_sizes",
    "tile_m",
    "tile_k",
    "tile_n"
  ],
  "lax_drhs_bwd": [
    "lhs",
    "out",
    "group_sizes",
    "tile_m",
    "tile_k",
    "tile_n"
  ],
  "gmm_fwd": [
    "lhs",
    "rhs",
    "group_sizes",
    "tile_m",
    "tile_k",
    "tile_n",
    "input_buffer_count"
  ],
  "dlhs_bwd": [
    "out",
    "rhs",
    "group_sizes",
    "tile_m",
    "tile_k",
    "tile_n",
    "input_buffer_count"
  ],
  "drhs_bwd": [
    "lhs",
    "out",
    "group_sizes",
    "tile_m",
    "tile_k",
    "tile_n",
    "input_buffer_count"
  ],
  "run_benchmark": [],
  "main": [
    "argv"
  ],
  "_mask_name_to_fn": [],
  "get_benchmark_func": [
    "mask",
    "config",
    "mode",
    "mqa"
  ],
  "randn_init": [
    "key",
    "shape",
    "dtype"
  ],
  "_pallas_call_hlo_pattern": [
    "mode",
    "mqa"
  ],
  "SummaryWriter": [],
  "_TENSORBOARD_OUTPUT_ENV_VAR": [],
  "_SKIP_IMPLEMENTATIONS": [],
  "dtype": [],
  "get_example": [
    "n",
    "c",
    "h",
    "d"
  ],
  "TriangleMultiplicationBenchmark": {
    "test_triangle_multiplication": [
      "self",
      "implementation",
      "benchmark_mode",
      "n"
    ]
  },
  "EXAMPLE": [],
  "AttentionBenchmark": {
    "test_attention": [
      "self",
      "implementation",
      "benchmark_mode"
    ]
  }
}