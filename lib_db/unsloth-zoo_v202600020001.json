{
  "__all__": [],
  "mean_of_trained_tokens": [
    "model",
    "eps"
  ],
  "add_new_tokens": [
    "model",
    "tokenizer",
    "new_tokens",
    "method",
    "interpolation"
  ],
  "fix_untrained_tokens": [
    "model",
    "tokenizer",
    "train_dataset",
    "IGNORED_TOKENIZER_NAMES",
    "eps"
  ],
  "POSSIBLE_RESERVED_TOKENS": [],
  "patch_tokenizer": [
    "model",
    "tokenizer"
  ],
  "_is_conversation_format": [
    "text"
  ],
  "patch_processor_call": [
    "processor"
  ],
  "UNSLOTH_ENABLE_LOGGING": [],
  "logger": [],
  "FIRST_PASS": [],
  "UNSLOTH_ENABLE_TILED_LOGGING": [],
  "torch_amp_custom_fwd": [],
  "torch_amp_custom_bwd": [],
  "get_max_flat_qlen": [
    "hd",
    "mlp_size",
    "nbytes",
    "target_gb",
    "padded_length"
  ],
  "TiledMLP": {
    "handle_output": [
      "output",
      "extra_lists"
    ],
    "structure_output": [
      "main_output",
      "extra_lists"
    ],
    "forward": [
      "ctx",
      "mlp_forward",
      "mlp_module",
      "x",
      "preserve_rng_state",
      "num_shards",
      "max_flat_qlen"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "patch_mlp": [
    "mlp_module",
    "target_arctic",
    "target_gb",
    "padded_length"
  ],
  "is_custom_module": [
    "name",
    "custom_modules"
  ],
  "patch_tiled_mlp": [
    "model",
    "patch_options_str",
    "padded_length"
  ],
  "MODEL_CARD": [],
  "find_skipped_quantized_modules": [
    "model"
  ],
  "create_huggingface_repo": [
    "model",
    "repo_id",
    "private",
    "token"
  ],
  "_merge_lora": [
    "W",
    "lora_stats",
    "name"
  ],
  "_get_modules_to_save_weight": [
    "module"
  ],
  "check_if_quantized": [
    "module"
  ],
  "expand_module_keys": [
    "name",
    "module",
    "original_keys"
  ],
  "LoraStats": {},
  "assert_same_keys": [
    "model",
    "new_state_dict"
  ],
  "create_lora_statistics": [
    "model",
    "merge_into_original",
    "return_state_dict"
  ],
  "_merge_and_overwrite_lora": [
    "save_directory",
    "filename",
    "lora_weights",
    "output_dtype",
    "model_class_name",
    "base_model_is_quantized",
    "quant_type",
    "save_method",
    "counted_lora_modules"
  ],
  "_merge_moe_gate_expert": [
    "gate_W",
    "lora_stats",
    "expert_idx",
    "num_experts",
    "output_dtype"
  ],
  "_merge_moe_up_expert": [
    "up_W",
    "lora_stats",
    "expert_idx",
    "num_experts",
    "output_dtype"
  ],
  "_merge_moe_down_proj_expert": [
    "down_W",
    "lora_stats",
    "expert_idx",
    "num_experts",
    "output_dtype"
  ],
  "_resolve_moe_num_experts": [
    "prefix",
    "lora_stats",
    "moe_num_experts"
  ],
  "_merge_moe_experts_file": [
    "mm",
    "header_metadata",
    "length_of_header",
    "file",
    "converted_lora_weights",
    "moe_num_experts",
    "output_dtype",
    "counted_lora_modules",
    "processed_mxfp4_keys"
  ],
  "_merge_moe_fused_gate_up_expert": [
    "gate_up_W",
    "lora_stats",
    "output_dtype"
  ],
  "_merge_moe_fused_down_proj_expert": [
    "down_W",
    "lora_stats",
    "output_dtype"
  ],
  "_merge_and_overwrite_lora_mxfp4": [
    "save_directory",
    "filename",
    "lora_weights",
    "output_dtype",
    "model_class_name",
    "base_model_is_quantized",
    "quant_type"
  ],
  "get_torch_storage_size_new": [
    "x",
    "element_size"
  ],
  "get_torch_storage_id_new": [
    "x"
  ],
  "prepare_saving": [
    "model",
    "save_directory",
    "push_to_hub",
    "max_shard_size",
    "private",
    "token",
    "output_dtype",
    "merge_into_original",
    "low_disk_space_usage",
    "min_size_in_bytes",
    "use_temp_file"
  ],
  "_remove_quantization_config": [
    "config_path"
  ],
  "_remove_transformers_version": [
    "config_path"
  ],
  "fix_tokenizer_config_json": [
    "tokenizer",
    "saved_folder"
  ],
  "is_hf_sharded_safetensors": [
    "filenames"
  ],
  "merge_and_overwrite_lora": [
    "get_model_name",
    "model",
    "tokenizer",
    "save_directory",
    "push_to_hub",
    "private",
    "token",
    "save_method",
    "output_dtype",
    "low_disk_space_usage",
    "use_temp_file",
    "cleanup_temp_file"
  ],
  "_try_copy_all_from_cache": [
    "repo_id",
    "filenames_to_check",
    "target_dir_str",
    "hf_cache_dir",
    "token"
  ],
  "_copy_file_from_source": [
    "src_path",
    "target_dir_str",
    "filename"
  ],
  "_get_hf_cache_dir": [],
  "_PUSHING_CODE": [],
  "incremental_save_pretrained": [
    "save_pretrained",
    "low_disk_space_usage",
    "use_temp_file",
    "repo_id",
    "revision"
  ],
  "merge_and_dequantize_lora": [
    "model",
    "tokenizer",
    "save_directory",
    "push_to_hub",
    "max_shard_size",
    "safe_serialization",
    "token",
    "private",
    "revision",
    "output_dtype",
    "low_disk_space_usage",
    "use_temp_file"
  ],
  "get_original_model_id": [
    "local_path"
  ],
  "_get_checkpoint_conversion_mapping": [
    "model_class_name"
  ],
  "detect_keys_format": [
    "keys_to_check",
    "forward_mapping"
  ],
  "_convert_lora_keys_to_safetensor_format": [
    "lora_weights",
    "safetensor_keys",
    "model_class_name"
  ],
  "find_lora_base_model": [
    "model_to_inspect"
  ],
  "check_hf_model_exists": [
    "model_name",
    "token"
  ],
  "check_local_model_exists": [
    "model_path"
  ],
  "check_model_quantization_status": [
    "model_name_or_path",
    "token"
  ],
  "determine_base_model_source": [
    "model_name",
    "token"
  ],
  "get_memory_stats": [],
  "format_bytes": [
    "bytes_value"
  ],
  "calculate_combined_score": [
    "speed_score",
    "chunk_size"
  ],
  "_choose_mxfp4_processing_strategy": [
    "blocks_tensor",
    "scales_tensor"
  ],
  "should_split_shards": [
    "is_t4",
    "model_config",
    "safetensors_list"
  ],
  "split_safetensor_file": [
    "filename",
    "save_directory",
    "max_shard_size_gb"
  ],
  "renumber_safetensor_files": [
    "file_list",
    "save_directory"
  ],
  "split_safetensors_to_shards": [
    "file_path",
    "max_shard_size_gb"
  ],
  "_write_tensor_direct_torch": [
    "mm",
    "header_metadata",
    "length_of_header",
    "output_key",
    "tensor",
    "output_dtype"
  ],
  "dummy_lora_has_scaling_factor": [
    "create_dummy_lora"
  ],
  "_call_create_lora_manager": [
    "model",
    "vllm_config"
  ],
  "WorkerLoRAManager": {
    "dummy_lora_cache": [
      "self"
    ],
    "is_enabled": [
      "self"
    ],
    "create_lora_manager": [
      "self",
      "model",
      "vllm_config"
    ],
    "_load_adapter": [
      "self",
      "lora_request"
    ],
    "add_dummy_lora": [
      "self",
      "lora_request",
      "rank"
    ],
    "pin_adapter": [
      "self",
      "adapter_id"
    ],
    "set_active_adapters": [
      "self",
      "requests",
      "mapping"
    ],
    "_apply_adapters": [
      "self",
      "adapter_requests"
    ],
    "add_adapter": [
      "self",
      "adapter_request"
    ],
    "remove_adapter": [
      "self",
      "adapter_id"
    ],
    "remove_all_adapters": [
      "self"
    ],
    "list_adapters": [
      "self"
    ]
  },
  "old_init": [
    "self",
    "max_num_seqs",
    "max_num_batched_tokens",
    "vocab_size",
    "lora_config",
    "device",
    "embedding_modules",
    "embedding_padding_modules",
    "lora_model_cls",
    "max_position_embeddings"
  ],
  "new_init": [
    "self",
    "vllm_config",
    "device",
    "embedding_modules",
    "embedding_padding_modules",
    "lora_model_cls"
  ],
  "LRUCacheWorkerLoRAManager": {
    "create_lora_manager": [
      "self",
      "model",
      "vllm_config"
    ],
    "_apply_adapters": [
      "self",
      "lora_requests"
    ],
    "add_adapter": [
      "self",
      "lora_request"
    ]
  },
  "RL_REPLACEMENTS": [],
  "selective_log_softmax": [
    "logits",
    "index"
  ],
  "chunked_selective_log_softmax": [
    "logits",
    "index"
  ],
  "chunked_hidden_states_selective_log_softmax": [
    "hidden_states",
    "lm_head",
    "index",
    "chunks",
    "logit_scale_multiply",
    "logit_scale_divide",
    "logit_softcapping",
    "temperature"
  ],
  "calculate_pad_tokens_in_prompt": [
    "input_ids",
    "logits_to_keep",
    "pad_token_id"
  ],
  "create_completion_attention_mask": [
    "completion_input_ids",
    "left_pad_tokens_per_prompt",
    "max_left_pad",
    "pad_token_id"
  ],
  "left_pack_padding": [
    "tensor",
    "pad_id"
  ],
  "align_logprobs_with_mask": [
    "logprob_tensor",
    "attention_mask",
    "pad_value"
  ],
  "autotune_batch_and_chunks": [
    "total_input_rows",
    "seq_len",
    "hidden_size",
    "vocab_size",
    "dtype_bytes",
    "multiplier"
  ],
  "grpo_update_SamplingParams": [
    "SamplingParams",
    "generation_kwargs",
    "vllm_sampling_params"
  ],
  "grpo_compute_loss": [
    "ref",
    "new",
    "old",
    "sampling_per_token_logps",
    "input_ids",
    "mask",
    "beta",
    "advantages"
  ],
  "UnslothEfficientGRPO": {
    "forward": [
      "ctx",
      "_new_logps",
      "_old_logps",
      "_ref_logps",
      "_sampling_per_token_logps",
      "lm_head",
      "_input_ids",
      "_mask",
      "_advantages",
      "beta",
      "scaler",
      "n_chunks",
      "extra_kwargs"
    ],
    "backward": [
      "ctx",
      "grad_output",
      "dcompletion_length",
      "dmean_kl",
      "ddelta",
      "ddflat_is_ratio",
      "dcoef_1"
    ]
  },
  "grpo_accumulated_loss": [
    "trainer",
    "input_ids",
    "attention_mask",
    "logits_to_keep",
    "completion_mask",
    "advantages",
    "old_logps",
    "ref_logps",
    "n_chunks"
  ],
  "LLAMA_CPP_CONVERT_FILE": [],
  "COMMANDS_NOT_FOUND": [],
  "LLAMA_CPP_TARGETS": [],
  "PIP_OPTIONS": [],
  "BAD_OUTCOMES": [],
  "keynames": [],
  "IS_COLAB_ENVIRONMENT": [],
  "IS_KAGGLE_ENVIRONMENT": [],
  "KAGGLE_TMP": [],
  "use_local_gguf": [],
  "install_package": [
    "package",
    "sudo",
    "print_output",
    "print_outputs",
    "system_type"
  ],
  "do_we_need_sudo": [
    "system_type"
  ],
  "check_pip": [],
  "try_execute": [
    "command",
    "sudo",
    "print_output",
    "print_outputs",
    "cwd",
    "system_type",
    "ignore_deprecation"
  ],
  "try_execute_with_auto_install": [
    "command",
    "sudo",
    "print_output",
    "print_outputs",
    "cwd",
    "system_type",
    "ignore_deprecation"
  ],
  "check_llama_cpp": [
    "llama_cpp_folder"
  ],
  "install_llama_cpp": [
    "llama_cpp_folder",
    "llama_cpp_targets",
    "print_output",
    "gpu_support",
    "just_clone_repo"
  ],
  "_load_module_from_path": [
    "filepath",
    "module_name"
  ],
  "_download_convert_hf_to_gguf": [
    "name"
  ],
  "_split_str_to_n_bytes": [
    "split_str"
  ],
  "_convert_to_gguf": [
    "command",
    "output_filename",
    "print_output",
    "print_outputs"
  ],
  "check_quantization_type": [
    "quantization_type"
  ],
  "check_max_shard_size": [
    "max_shard_size"
  ],
  "convert_to_gguf": [
    "model_name",
    "input_folder",
    "model_dtype",
    "quantization_type",
    "converter_location",
    "supported_text_archs",
    "supported_vision_archs",
    "is_vlm",
    "is_gpt_oss",
    "max_shard_size",
    "print_output",
    "print_outputs"
  ],
  "quantize_gguf": [
    "input_gguf",
    "output_gguf",
    "quant_type",
    "quantizer_location",
    "n_threads",
    "print_output"
  ],
  "_assert_correct_gguf": [
    "model_name",
    "model",
    "tokenizer"
  ],
  "assert_correct_gguf": [
    "model_name",
    "model",
    "tokenizer"
  ],
  "check_build_requirements": [],
  "check_libcurl_dev": [],
  "check_linux_type": [],
  "_check_llama_cpp_appended_system_message": [],
  "add_llama_cpp_system_message": [
    "messages",
    "tools",
    "inplace"
  ],
  "is_comparable": [
    "val"
  ],
  "compare_dicts": [
    "orig_dict",
    "new_dict",
    "prefix"
  ],
  "compare_attributes": [
    "original_model",
    "new_model"
  ],
  "_extract_all_config_keys": [
    "config"
  ],
  "copy_attributes": [
    "original_model",
    "new_model"
  ],
  "create_empty_causal_lm": [
    "config",
    "dtype"
  ],
  "_set_config_attrs": [
    "config_obj",
    "attrs_to_set"
  ],
  "create_empty_vision_model": [
    "config",
    "dtype"
  ],
  "create_empty_model": [
    "config",
    "dtype",
    "is_vision_model"
  ],
  "set_additional_modules": [
    "new_model",
    "quant_state_dict",
    "config"
  ],
  "get_model_layer_config": [
    "return_non_layered"
  ],
  "get_model_type": [
    "config"
  ],
  "get_model_layer_counts": [
    "config"
  ],
  "_get_nested_attr": [
    "obj",
    "attr_path"
  ],
  "extract_vision_layers": [
    "vllm_internals",
    "state_dict",
    "quant_state_dict",
    "get_state_dict"
  ],
  "is_hip": [],
  "get_device_type": [],
  "DEVICE_TYPE_TORCH": [],
  "get_device_count": [],
  "device_synchronize": [],
  "Version": [
    "version"
  ],
  "__DTYPE_MAP": [],
  "_get_dtype": [
    "dtype"
  ],
  "torch_distributed_is_initialized": [],
  "torch_distributed_is_torchelastic_launched": [],
  "torch_distributed_get_rank": [],
  "is_main_process": [],
  "is_distributed": [],
  "distributed_function": [
    "n",
    "function"
  ],
  "_lock_path_for": [
    "target"
  ],
  "get_lock": [
    "target",
    "timeout"
  ],
  "get_quant_type": [
    "config"
  ],
  "METRICS_MOVE_TO_END": [],
  "REMOVED_METRICS": [],
  "NotebookProgressCallback_on_train_begin": [
    "Trainer_metrics"
  ],
  "NotebookProgressCallback_on_log": [
    "Trainer_metrics"
  ],
  "NotebookTrainingTracker_write_line": [
    "Trainer_metrics"
  ],
  "_PatchRLStatistics": [
    "metrics",
    "algorithm"
  ],
  "get_trl_metrics": [],
  "PatchRLStatistics": [
    "algorithm",
    "other_metrics"
  ],
  "stateless_init_process_group": [
    "master_address",
    "master_port",
    "rank",
    "world_size",
    "device"
  ],
  "WorkerExtension": {
    "init_weight_update_group": [
      "self",
      "master_address",
      "master_port",
      "rank_offset",
      "world_size"
    ],
    "update_weight": [
      "self",
      "name",
      "dtype",
      "shape"
    ],
    "check_weights_changed": [
      "self"
    ]
  },
  "ColocateWorkerExtension": {
    "report_device_id": [
      "self"
    ],
    "update_weights_from_ipc_handles": [
      "self",
      "ipc_handles"
    ],
    "get_model_runner": [
      "self"
    ],
    "check_weights_changed": [
      "self"
    ],
    "get_weight_ipc_handles": [
      "self"
    ]
  },
  "__version__": [],
  "torch_version": [],
  "delete_key": [
    "key"
  ],
  "torchao_logger": [],
  "HideLoggingMessage": {
    "__slots__": [],
    "__init__": [
      "self",
      "text"
    ],
    "filter": [
      "self",
      "x"
    ]
  },
  "fix_zero_training_loss": [
    "model",
    "tokenizer",
    "train_dataset"
  ],
  "prepare_model_for_training": [
    "model",
    "use_gradient_checkpointing",
    "use_reentrant",
    "full_finetuning",
    "train_layernorms",
    "train_embedding",
    "train_lm_head",
    "float32_mixed_precision",
    "patch_modules_to_save"
  ],
  "get_max_steps": [
    "training_args",
    "n_training_samples",
    "train_dataset"
  ],
  "set_training": [
    "model"
  ],
  "unset_training": [
    "model"
  ],
  "Trainer_Stats": {},
  "unsloth_train": [
    "trainer"
  ],
  "patch_compiling_bitsandbytes": [],
  "patch_layernorm": [
    "fast_layernorm"
  ],
  "patch_torch_compile": [
    "debug",
    "O3",
    "ignore_errors"
  ],
  "get_model": [
    "model"
  ],
  "verify_and_set_device": [
    "module"
  ],
  "patch_to_dict": [],
  "patch_model_and_tokenizer": [
    "model",
    "tokenizer",
    "downcast_rope",
    "fix_embeddings",
    "do_forced_float32",
    "correct_dtype"
  ],
  "patch_compiled_autograd": [],
  "check_conversion_mappings": [
    "model",
    "current_key_name_str",
    "skip_modules"
  ],
  "_mark_parent": [
    "child",
    "parent_type"
  ],
  "_unmark_parent": [
    "child"
  ],
  "parsed_statement": [
    "code"
  ],
  "WrapRecursiveCall": {
    "function_name": [],
    "mark_statement": [],
    "unmark_statement": [],
    "visit_Assign": [
      "self",
      "node"
    ]
  },
  "T": [],
  "_stdlib_names": [],
  "check_python_modules": [
    "code"
  ],
  "check_signal_escape_patterns": [
    "code"
  ],
  "_is_docstring_stmt": [
    "node"
  ],
  "_is_safe_literal": [
    "node"
  ],
  "_ensure_no_calls_in_signature": [
    "func"
  ],
  "validate_single_function_source": [
    "source"
  ],
  "load_single_function": [
    "source"
  ],
  "create_locked_down_function": [
    "function"
  ],
  "_retry_eintr": [
    "func"
  ],
  "time_limit": [
    "seconds"
  ],
  "RemoteTracebackError": {},
  "_run_in_subprocess": [
    "func",
    "seconds",
    "args",
    "kwargs"
  ],
  "_VALID_START_METHODS": [],
  "execute_with_time_limit": [
    "seconds"
  ],
  "Benchmarker": {
    "__init__": [
      "self",
      "trials",
      "loops",
      "timeout"
    ],
    "thrash": [
      "self"
    ],
    "benchmark": [
      "self",
      "function",
      "arguments"
    ]
  },
  "is_port_open": [
    "host",
    "port"
  ],
  "_get_openenv_pythonpath": [
    "working_directory"
  ],
  "launch_openenv": [
    "port",
    "openenv_process",
    "working_directory",
    "server",
    "environment",
    "openenv_class"
  ],
  "SKIP_QUANTIZATION_MODULES": [],
  "get_peft_regex": [
    "model",
    "finetune_vision_layers",
    "finetune_language_layers",
    "finetune_attention_modules",
    "finetune_mlp_modules",
    "target_modules",
    "vision_tags",
    "language_tags",
    "attention_tags",
    "mlp_tags"
  ],
  "get_lora_layer_modules": [],
  "requires_grad_for_gradient_checkpointing": [
    "model"
  ],
  "_old_longest_common_substring": [
    "arr"
  ],
  "_longest_common_sublist": [
    "lists"
  ],
  "_find_common_token_ids": [
    "component",
    "tokenizer",
    "force_match"
  ],
  "train_on_responses_only": [
    "trainer",
    "instruction_part",
    "response_part",
    "force_match",
    "tokenizer",
    "return_function",
    "num_proc"
  ],
  "standardize_data_formats": [
    "dataset",
    "tokenizer",
    "aliases_for_system",
    "aliases_for_user",
    "aliases_for_assistant",
    "batch_size",
    "num_proc"
  ],
  "sft_prepare_dataset": [
    "self",
    "dataset",
    "processing_class",
    "args",
    "packing",
    "formatting_func",
    "dataset_name"
  ],
  "patch_torchcodec_audio_decoder": [],
  "COMBINED_UNSLOTH_NAME": [],
  "UNSLOTH_COMPILE_USE_TEMP": [],
  "OLD_TORCH_VERSION": [],
  "major": [],
  "minor": [],
  "OLD_TRITON_VERSION": [],
  "DISABLED_KEYWORDS": [],
  "_full_license_header": [],
  "_license_header": [],
  "_disabled_sdpa_code": [],
  "_patch_functions": [],
  "no_update_causal_mask": [],
  "replace_with_grouped_query_attention": [
    "module",
    "source"
  ],
  "_get_compile_folder": [
    "use_tempfile"
  ],
  "get_compile_folder": [
    "use_tempfile"
  ],
  "get_mask_functions": [],
  "higher_precision_softmax": [
    "source"
  ],
  "higher_precision_sqrt_mean": [
    "source"
  ],
  "fix_rotary_embedding_dtype": [
    "source"
  ],
  "higher_precision_layernorms": [
    "modeling_file"
  ],
  "disble_use_cache_logging": [],
  "_SKIP_TOKENS": [],
  "_chunk_param_name": [
    "chunk"
  ],
  "_process_param_tokens": [
    "param_tokens"
  ],
  "_rewrite_kwargs_param": [
    "source",
    "func_name"
  ],
  "_insert_kwargs_alias": [
    "source",
    "func_name",
    "replacement"
  ],
  "create_new_function": [
    "name",
    "new_source",
    "model_location",
    "functions",
    "prepend",
    "append",
    "overwrite",
    "add_torch_compile"
  ],
  "create_standalone_class": [
    "module",
    "model_location",
    "functions",
    "fullgraph",
    "forward_source",
    "disable",
    "add_loss_kwargs",
    "new_init",
    "new_methods"
  ],
  "_cross_entropy_code": [],
  "__DYNAMO__RECOMPILING__": [],
  "cross_entropy_find_1": [],
  "cross_entropy_replacement_1": [],
  "cross_entropy_find_2": [],
  "cross_entropy_replacement_2": [],
  "cross_entropy_find_3": [],
  "cross_entropy_replacement_3": [],
  "ce_finders": [],
  "apply_fused_lm_head": [
    "forward",
    "module"
  ],
  "test_apply_fused_lm_head": [],
  "apply_mask_attention_mask_out": [
    "source"
  ],
  "convert_attention_masks_to_bool": [
    "module",
    "old_source"
  ],
  "custom_gradient_checkpointing_replacements": [],
  "replace_gradient_checkpointing": [],
  "patch_gradient_checkpointing": [
    "module",
    "source"
  ],
  "strip_kw_from_module_calls": [
    "src",
    "modulelist_item"
  ],
  "patch_gradient_checkpointing_layer_caller": [
    "module",
    "source"
  ],
  "DTYPE_MISMATCH_FIND": [],
  "DTYPE_MISMATCH_REPLACE": [],
  "patch_finfo_attention_mask_dtype_mismatch": [
    "module",
    "source"
  ],
  "MOE_ROUTING_WEIGHTS_CAST_PATTERN": [],
  "MOE_ROUTING_WEIGHTS_CAST_REPLACE": [],
  "patch_moe_routing_weights_cast": [
    "module_cls",
    "source"
  ],
  "COMPILED_LORA_FORWARD": [],
  "COMPILED_LORA_FORWARD_forced_float32": [],
  "patch_lora_forwards": [
    "torch_compile_options"
  ],
  "patch_residual_stream": [
    "source"
  ],
  "patch_gradient_accumulation": [
    "modeling_file",
    "module"
  ],
  "fixup_fused_lm_head": [
    "source"
  ],
  "rms_norm2d": [
    "x",
    "normalized_shape",
    "weight",
    "eps"
  ],
  "compile_timm_models": [
    "UNSLOTH_ENABLE_LOGGING",
    "torch_compile_options"
  ],
  "compile_causal_conv1d": [
    "UNSLOTH_ENABLE_LOGGING"
  ],
  "compile_mamba_ssm": [
    "UNSLOTH_ENABLE_LOGGING"
  ],
  "DISABLE_COMPILE_MODULES": [],
  "FIX_GC_LAYER_CALLER_MODULES": [],
  "unsloth_compile_transformers": [
    "model_type",
    "sdpa_dynamic_mask",
    "sdpa_bool_masks",
    "sdpa_gqa_replace",
    "sdpa_dynamic_compile",
    "compile_attention",
    "disable_causal_masks",
    "compile_torch_modules",
    "compile_custom_modules",
    "compile_function_calls",
    "fuse_lm_head",
    "gradient_checkpointing",
    "manual_replacements",
    "fast_lora_forwards",
    "fast_residual_stream",
    "accurate_accumulation",
    "epilogue_fusion",
    "max_autotune",
    "shape_padding",
    "cudagraphs",
    "debug",
    "fullgraph",
    "import_from_cache",
    "disable",
    "return_logits",
    "supports_sdpa"
  ],
  "LoRARequest": {
    "adapter_id": [
      "self"
    ],
    "name": [
      "self"
    ],
    "path": [
      "self"
    ],
    "tensors": [
      "self"
    ],
    "config": [
      "self"
    ],
    "embeddings": [
      "self"
    ],
    "local_path": [
      "self",
      "value"
    ],
    "__eq__": [
      "self",
      "value"
    ],
    "__hash__": [
      "self"
    ]
  },
  "torch_nn_functional_cross_entropy": [],
  "patch_loss_functions": [
    "_fast_cross_entropy_loss",
    "torch_compile"
  ],
  "post_patch_loss_function": [
    "model"
  ],
  "current_device": [],
  "fused_linear_cross_entropy": [
    "hidden_states",
    "lm_weight",
    "labels",
    "num_items_in_batch",
    "ignore_index",
    "reduction",
    "logit_softcapping",
    "accuracy_threshold"
  ],
  "fast_linear_cross_entropy": [
    "hidden_states",
    "lm_head",
    "labels",
    "num_items_in_batch",
    "ignore_index",
    "reduction",
    "logit_softcapping",
    "logit_scale_multiply",
    "logit_scale_divide",
    "attention_mask"
  ],
  "ALLOWED_NUM_ITEMS_IN_BATCH": [],
  "TRAINING_ITERATIONS": [],
  "mark_static": [],
  "mark_dynamic": [],
  "_unsloth_get_batch_samples": [
    "self",
    "epoch_iterator",
    "num_batches",
    "device"
  ],
  "layer_norm": [
    "input",
    "normalized_shape",
    "weight",
    "bias",
    "eps"
  ],
  "cross_entropy": [
    "input",
    "target",
    "weight",
    "size_average",
    "ignore_index",
    "reduce",
    "reduction",
    "label_smoothing"
  ],
  "patch_torch_functions": [],
  "compiler_replacements": [],
  "IMAGE_TOKENS": [],
  "IMAGE_FACTOR": [],
  "MIN_PIXELS": [],
  "MAX_PIXELS": [],
  "MAX_RATIO": [],
  "VIDEO_MIN_PIXELS": [],
  "VIDEO_MAX_PIXELS": [],
  "VIDEO_TOTAL_PIXELS": [],
  "FRAME_FACTOR": [],
  "FPS": [],
  "FPS_MIN_FRAMES": [],
  "FPS_MAX_FRAMES": [],
  "round_by_factor": [
    "number",
    "factor"
  ],
  "ceil_by_factor": [
    "number",
    "factor"
  ],
  "floor_by_factor": [
    "number",
    "factor"
  ],
  "smart_resize": [
    "height",
    "width",
    "factor",
    "min_pixels",
    "max_pixels"
  ],
  "fetch_image": [
    "ele",
    "size_factor"
  ],
  "smart_nframes": [
    "ele",
    "total_frames",
    "video_fps"
  ],
  "_read_video_torchvision": [
    "ele"
  ],
  "is_decord_available": [],
  "calculate_video_frame_range": [
    "ele",
    "total_frames",
    "video_fps"
  ],
  "_read_video_decord": [
    "ele"
  ],
  "is_torchcodec_available": [],
  "_read_video_torchcodec": [
    "ele"
  ],
  "VIDEO_READER_BACKENDS": [],
  "FORCE_UNSLOTH_VIDEO_READER": [],
  "get_video_reader_backend": [],
  "fetch_video": [
    "ele",
    "image_factor",
    "return_video_sample_fps"
  ],
  "collapse_fps": [
    "fps",
    "tol"
  ],
  "extract_vision_info": [
    "conversations"
  ],
  "process_vision_info": [
    "conversations",
    "size_factor",
    "return_video_kwargs"
  ],
  "get_padding_tokens_ids": [
    "tokenizer"
  ],
  "LANCZOS": [],
  "UnslothVisionDataCollator": {
    "__slots__": [],
    "__init__": [
      "self",
      "model",
      "processor",
      "max_seq_length",
      "formatting_func",
      "resize",
      "ignore_index",
      "train_on_responses_only",
      "instruction_part",
      "response_part",
      "force_match",
      "num_proc",
      "completion_only_loss",
      "pad_to_multiple_of",
      "resize_dimension",
      "snap_to_patch_size"
    ],
    "_get_padding_token_ids_on_device": [
      "self",
      "device"
    ],
    "__call__": [
      "self",
      "examples"
    ],
    "_select_messages_or_raw": [
      "self",
      "example"
    ],
    "_validate_and_normalize_first_message": [
      "self",
      "messages"
    ],
    "_collapse_assistant_content": [
      "self",
      "messages"
    ],
    "_render_chat": [
      "self",
      "prompt_messages",
      "completion_messages",
      "add_generation_prompt",
      "continue_final_message"
    ],
    "_extract_images_videos_for_example": [
      "self",
      "example",
      "messages"
    ],
    "_extract_images_for_pc": [
      "self",
      "example",
      "p_msgs",
      "c_msgs"
    ],
    "_resize_images_inplace": [
      "self",
      "image"
    ],
    "_cast_pixel_values_dtype_inplace": [
      "self",
      "batch",
      "key"
    ],
    "_tokenizer_padding_side": [
      "self"
    ],
    "_pad_token_id_or_fail": [
      "self"
    ],
    "_flush_to_side": [
      "self",
      "attention_mask",
      "input_ids",
      "side",
      "pad_token_id",
      "extra_tensors",
      "extra_pad_values"
    ],
    "_truncate_by_side": [
      "self",
      "input_ids",
      "attention_mask",
      "completion_mask",
      "side",
      "max_len",
      "token_type_ids"
    ],
    "_pad_to_multiple": [
      "self",
      "input_ids",
      "attention_mask",
      "completion_mask",
      "side",
      "pad_id",
      "multiple",
      "token_type_ids",
      "token_type_pad_id"
    ],
    "_collate_prompt_completion": [
      "self",
      "examples"
    ],
    "_clean_none_keys": [
      "self",
      "messages"
    ]
  },
  "INITIAL_CPU_BUFFER_SIZE": [],
  "INITIAL_GPU_BUFFER_SIZE": [],
  "INITIAL_CPU_BUFFER_COUNT": [],
  "_calculate_n_gradient_checkpoints": [
    "n_layers",
    "method"
  ],
  "calculate_n_gradient_checkpoints": [
    "n_layers",
    "layers_per_checkpoint"
  ],
  "prepare_n_gradient_checkpoints": [
    "model",
    "layers_per_checkpoint",
    "use_reentrant"
  ],
  "Unsloth_Offloaded_Gradient_Checkpointer": {
    "forward": [
      "ctx",
      "forward_function",
      "hidden_states"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "Unsloth_Gradient_Checkpointer": {
    "forward": [
      "ctx",
      "forward_function",
      "hidden_states"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "unsloth_gradient_checkpoint": [
    "function"
  ],
  "patch_unsloth_gradient_checkpointing": [],
  "unpatch_unsloth_gradient_checkpointing": [],
  "unpatch_gradient_checkpointing": [],
  "set_device_states": [
    "devices",
    "states"
  ],
  "CPU_BUFFERS": [],
  "CPU_INDEX": [],
  "initialize_unsloth_gradient_checkpointing": [
    "dtype"
  ],
  "UnslothCheckpointFunction": {
    "forward": [
      "ctx",
      "run_function",
      "preserve_rng_state"
    ],
    "backward": [
      "ctx"
    ]
  },
  "unsloth_checkpoint": [
    "function"
  ],
  "patch_unsloth_smart_gradient_checkpointing": [
    "dtype"
  ],
  "unpatch_unsloth_smart_gradient_checkpointing": [],
  "reset_unsloth_gradient_checkpointing_buffers": [],
  "unsloth_offloaded_gradient_checkpoint": [
    "function"
  ],
  "_maybe_set_flashinfer_workspace_base": [],
  "_return_nothing": [],
  "_return_self": [
    "self"
  ],
  "_return_self_tokenizer": [
    "self"
  ],
  "get_target_device": [
    "index"
  ],
  "get_mem_info": [],
  "patch_vllm_enable_sleep_mode": [],
  "patch_vllm_graph_capture": [],
  "patch_vllm": [
    "debug"
  ],
  "vllm_dynamic_quant_supported": [
    "model_name",
    "config"
  ],
  "get_vllm_state_dict": [
    "llm",
    "return_state_dict",
    "config",
    "is_vision_model",
    "load_in_fp8"
  ],
  "_get_vllm_state_dict": [
    "llm",
    "return_state_dict",
    "config",
    "is_vision_model"
  ],
  "assert_same_state_dict": [
    "old_state_dict",
    "new_state_dict"
  ],
  "convert_vllm_to_huggingface": [
    "quant_state_dict",
    "config",
    "dtype",
    "bnb_config",
    "is_vision_model"
  ],
  "approximate_vllm_memory_usage": [
    "config",
    "load_in_4bit",
    "load_in_8bit",
    "max_seq_length",
    "gpu_memory_utilization",
    "enable_lora",
    "max_lora_rank",
    "max_loras",
    "float8_kv_cache",
    "account_for_gradients",
    "parallel_sequences",
    "cuda_graph_overhead"
  ],
  "get_lora_supported_ranks": [],
  "determine_max_lora_rank": [
    "lora_rank"
  ],
  "vllm_supports_flashinfer": [
    "config"
  ],
  "_get_torchao_fp8_config": [
    "fp8_mode"
  ],
  "load_vllm": [
    "model_name",
    "config",
    "gpu_memory_utilization",
    "max_seq_length",
    "dtype",
    "training",
    "float8_kv_cache",
    "random_state",
    "enable_lora",
    "max_lora_rank",
    "max_loras",
    "use_async",
    "use_engine",
    "disable_log_stats",
    "enforce_eager",
    "enable_prefix_caching",
    "compilation_config",
    "conservativeness",
    "max_logprobs",
    "use_bitsandbytes",
    "unsloth_vllm_standby",
    "is_vision_model",
    "return_args",
    "max_num_seqs",
    "fp8_mode"
  ],
  "create_batches": [
    "requests",
    "num_sequences"
  ],
  "save_lora": [
    "model",
    "save_directory"
  ],
  "get_peft_config": [
    "save_directory"
  ],
  "vllm_lora_already_loaded": [
    "model"
  ],
  "prepare_vllm_lora_loading": [
    "model"
  ],
  "load_lora_directly": [
    "model"
  ],
  "convert_lora_modules": [
    "model",
    "dtype"
  ],
  "return_lora_modules": [
    "model",
    "state_dict",
    "dtype"
  ],
  "load_lora": [
    "model",
    "save_directory",
    "load_tensors",
    "lora_request_id"
  ],
  "generate_batches": [
    "llm",
    "inputs",
    "n_batches",
    "lora_request"
  ],
  "delete_vllm": [
    "llm"
  ],
  "_test_same_model": [
    "model",
    "new_model",
    "input_ids"
  ],
  "test_model_conversion": [
    "original_model",
    "new_model"
  ],
  "_test_is_same_vlm": [
    "model",
    "new_model",
    "processor",
    "test_backward"
  ],
  "_read_unsloth_vision_source": [],
  "get_vllm_supported_vlm": [
    "_VAR_NAME"
  ],
  "_test_get_vllm_state_dict": [
    "model_name",
    "dtype",
    "gpu_memory_utilization",
    "counts",
    "conservativeness",
    "float8_kv_cache",
    "unsloth_vllm_standby",
    "load_in_4bit",
    "skip_generation",
    "is_vision_model"
  ],
  "test_get_vllm_state_dict": [],
  "HAS_TORCH_DTYPE": [],
  "dtype_from_config": [
    "config"
  ],
  "set_dtype_in_config": [
    "config",
    "dtype"
  ],
  "set_dtype_in_config_fallback": [
    "config",
    "dtype"
  ],
  "add_dtype_kwargs": [
    "dtype",
    "kwargs_dict"
  ],
  "_dtype_stringify": [
    "x"
  ],
  "_normalize_dict_dtypes": [
    "obj"
  ],
  "get_transformers_model_type": [
    "config",
    "trust_remote_code"
  ],
  "fix_lora_auto_mapping": [
    "model"
  ],
  "get_auto_processor": [
    "name"
  ],
  "test_fused_ce_loss": [
    "compute_fused_ce_loss",
    "unsloth_fused_ce_loss",
    "bszs",
    "qlens",
    "hds",
    "vocab_sizes",
    "dtypes",
    "devices",
    "scalings",
    "logit_scale_multiplys",
    "logit_scale_divides",
    "logit_softcappings",
    "seeds",
    "mask_ratios",
    "lm_head_requires_grads",
    "lm_bias_requires_grads"
  ],
  "TARGET_GB": [],
  "N_CHUNKS": [],
  "_get_mapping": [
    "autograd"
  ],
  "apply_autograd_function": [
    "autograd",
    "mapping"
  ],
  "compute_fused_ce_loss": [
    "hidden_states",
    "lm_head_weight",
    "lm_head_bias",
    "labels",
    "n_items",
    "scaling",
    "shift_labels"
  ],
  "_get_chunk_multiplier": [
    "vocab_size",
    "target_gb"
  ],
  "get_chunk_size": [
    "bsz",
    "qlen",
    "vocab_size",
    "target_gb"
  ],
  "UnslothFusedLoss": {
    "forward": [
      "ctx",
      "loss_function",
      "hidden_states",
      "lm_head_weight",
      "lm_head_bias",
      "labels",
      "mask",
      "n_items",
      "scaling",
      "shift_labels",
      "target_gb",
      "torch_compile",
      "overwrite",
      "extra_kwargs"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "unsloth_fused_ce_loss": [
    "trainer",
    "hidden_states",
    "lm_head_weight",
    "lm_head_bias",
    "labels",
    "mask",
    "n_items",
    "scaling",
    "target_gb",
    "torch_compile",
    "overwrite"
  ],
  "_UNSLOTH_FLEX_ATTENTION_DISABLED": [],
  "patch_Gemma3Processor": [],
  "patch_Gemma3ForConditionalGeneration_causal_mask": [],
  "patch_Gemma3TextScaledWordEmbedding": [],
  "patch_Gemma3RMSNorm": [],
  "patch_Gemma3MLP": [],
  "patch_Gemma3Attention": [],
  "patch_Gemma3RMSNorm_generic": [],
  "patch_Gemma3Attention_generic": [],
  "UNSLOTH_COMPILE_DISABLE": [],
  "inductor_config_source": [],
  "determine_compile_threads": [],
  "get_torch_compile_options": [
    "epilogue_fusion",
    "max_autotune",
    "shape_padding",
    "debug",
    "cudagraphs",
    "coordinate_descent_tuning",
    "logging",
    "combo_kernels",
    "group_fusion",
    "memory_planning",
    "multi_kernel",
    "use_block_ptr"
  ],
  "torch_compile_options": [],
  "F": [],
  "noop": [],
  "TEMPORARY_PATCHES": [],
  "UNSLOTH_COMPILE_LOCATION": [],
  "install_to_cache": [
    "source_path",
    "destination_filename"
  ],
  "_grouped_mm_with_backward_fix": [
    "inputs",
    "weight",
    "offsets"
  ],
  "_GROUPED_GEMM_AVAILABLE": [],
  "_TORCH_GROUPED_MM_AVAILABLE": [],
  "_TORCH_GROUPED_MM_SUPPORTED": [],
  "_check_torch_grouped_mm_supported": [],
  "_TRITON_ALLOCATOR_INITIALIZED": [],
  "_PERSISTENT_BUFFER": [],
  "_init_triton_allocator": [],
  "_check_grouped_gemm_available": [],
  "select_moe_backend": [],
  "forward_moe_backend": [
    "self",
    "hidden_states",
    "top_k_index",
    "top_k_weights"
  ],
  "_get_routing_indices": [
    "selected_experts",
    "num_experts"
  ],
  "_silu_and_mul": [
    "x"
  ],
  "_has_lora_adapters": [
    "param"
  ],
  "_extract_lora_from_wrapper": [
    "wrapper",
    "adapter_name",
    "experts_module"
  ],
  "_extract_lora_weights": [
    "param",
    "adapter_name",
    "num_experts",
    "experts_module"
  ],
  "_get_base_weight": [
    "param"
  ],
  "_get_lora_wrapper_for_param": [
    "experts_module",
    "param_name"
  ],
  "native_moe_grouped_mm": [
    "inputs",
    "weight",
    "offsets"
  ],
  "_apply_lora_grouped_mm": [
    "inputs",
    "lora_B",
    "lora_A",
    "offsets",
    "scaling",
    "grouped_mm_func"
  ],
  "_should_use_separated_lora": [],
  "_WEIGHT_PREPROCESSORS": [],
  "register_weight_preprocessor": [
    "model_type",
    "preprocessor_fn"
  ],
  "get_weight_preprocessor": [
    "model_type"
  ],
  "preprocess_weight": [
    "weight",
    "proj_type",
    "hidden_dim",
    "model_type"
  ],
  "_is_moe_experts_module": [
    "module"
  ],
  "_get_moe_lora_weights": [],
  "_original_param_wrapper_forward": [],
  "_patched_param_wrapper_forward": [
    "self",
    "x"
  ],
  "patch_param_wrapper_for_moe": [],
  "forward_native_grouped_mm": [
    "self",
    "hidden_states",
    "top_k_index",
    "top_k_weights"
  ],
  "forward_triton_grouped_gemm": [
    "self",
    "hidden_states",
    "top_k_index",
    "top_k_weights"
  ],
  "forward_native_moe_loop": [
    "self",
    "hidden_states",
    "top_k_index",
    "top_k_weights"
  ],
  "patch_glm4_moe": [],
  "UNSLOTH_MXFP4_NO_DEQUANTIZE": [],
  "_check_triton_kernels_available": [],
  "_TRITON_KERNELS_AVAILABLE": [],
  "is_triton_kernels_available": [],
  "should_dequantize_mxfp4": [],
  "get_mxfp4_config_for_training": [],
  "patch_convert_moe_packed_tensors": [],
  "patch_PixtralAttention": [],
  "EMPTY": [],
  "raise_error": [
    "f",
    "exception"
  ],
  "PROCESS_RETURN_ALLOWED_TYPES": [],
  "process_return": [
    "output_class",
    "return_dict"
  ],
  "KWARGS_TYPE": [],
  "process_output_options": [
    "self",
    "locals_items",
    "kwargs"
  ],
  "TransformersKwargs": [],
  "FlashAttentionKwargs": [],
  "LossKwargs": [],
  "KwargsForCausalLM": [],
  "Cache": [],
  "DynamicCache": [],
  "HybridCache": [],
  "StaticCache": [],
  "TextInput": [],
  "PreTokenizedInput": [],
  "ImageInput": [],
  "ImagesKwargs": [],
  "MultiModalData": [],
  "ProcessingKwargs": [],
  "ProcessorMixin": [],
  "VAR_KEYWORD_ID": [],
  "VAR_POSITIONAL_ID": [],
  "TYPE_MAPPINGS": [],
  "_canonicalize_annotation": [
    "annotation"
  ],
  "canonicalize_annotation": [
    "annotation"
  ],
  "get_function_fingerprint": [
    "func"
  ],
  "removed_flags": [
    "old_fp",
    "new_fp"
  ],
  "can_safely_patch": [
    "original_func",
    "new_func",
    "match_level"
  ],
  "_get_unique_storage_name": [
    "target_obj",
    "attr_name"
  ],
  "patch_function": [
    "target_obj",
    "attr_name",
    "new_func",
    "force",
    "store_original",
    "match_level",
    "fullgraph",
    "dynamic"
  ],
  "patch_function_past_key_values": [
    "target_obj",
    "attr_name",
    "new_functions",
    "force",
    "store_original",
    "match_level",
    "fullgraph",
    "dynamic"
  ],
  "patch_multiple": [
    "patches",
    "force",
    "fail_fast",
    "match_level",
    "fullgraph",
    "dynamic"
  ],
  "restore_original": [
    "target_obj",
    "attr_name"
  ],
  "list_stored_originals": [
    "target_obj"
  ],
  "restore_multiple": [
    "target_objs_and_attrs"
  ],
  "patch_qwen3_moe": [],
  "patch_bitsandbytes_linear4bit_forward": [],
  "patch_Gemma3nConvNormAct_forward": [],
  "Gemma3nRMSNorm_forward": [
    "self",
    "x"
  ],
  "patch_Gemma3nMultimodalEmbedder_forward": [],
  "patch_Gemma3nTextAltUp_predict": [],
  "patch_Gemma3nConv_Embed_forwards": [],
  "patch_Gemma3nTextAltUp_functions": [],
  "patch_Gemma3nModel_get_placeholder_mask": [],
  "patch_qwen3_vl_moe": [],
  "_patch_attempted": [],
  "_patch_successful": [],
  "_do_patch_deepseek_v3": [],
  "patch_deepseek_v3": [],
  "DeepSeekV3Finder": {
    "find_module": [
      "self",
      "fullname",
      "path"
    ],
    "find_spec": [
      "self",
      "fullname",
      "path",
      "target"
    ],
    "load_module": [
      "self",
      "fullname"
    ]
  },
  "_setup_meta_path_hook": [],
  "_setup_import_hook": [],
  "patch_ministral3_config_mapping": [],
  "patch_tokenizer_convert_added_tokens": [],
  "patch_tokenizer_extra_special_tokens": [],
  "patch_merge_quantization_configs": [],
  "patch_CsmDepthDecoderForCausalLM_forward": [],
  "patch_CsmForConditionalGeneration_forward": [],
  "patch_transformers_masks": [],
  "patch_modernbert_attention_mask": [],
  "patch_CsmForConditionalGeneration_merge": [],
  "patch_causal_conv1d_cuda_probe": [],
  "patch_GraniteMoeHybridMambaLayer_cuda_kernels_forward": [],
  "fix_mamba_ssm_float32": [],
  "patch_MllamaVisionEncoderLayer": [],
  "patch_SiglipEncoderLayer": [],
  "patch_Lfm2VlMultiModalProjector": [],
  "patch_peft_dispatch_bnb_4bit": [],
  "patch_trl_push_to_hub_token": [],
  "patch_trl_vision_model_mapping": [],
  "patch_vllm_safe_apply_chat_template": [],
  "transformers_version": [],
  "has_static_cache": [],
  "torch_cuda_device": [],
  "swiglu_torch_forward": [
    "a",
    "alpha",
    "limit",
    "dtype"
  ],
  "swiglu_torch_backward": [
    "pre_act",
    "alpha",
    "limit",
    "g1"
  ],
  "patch_gpt_oss": [],
  "ParameterModule": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "shape_3d",
      "permute_to_2d",
      "permute_to_3d"
    ],
    "extra_repr": [
      "self"
    ],
    "get_param": [
      "self"
    ],
    "set_weight_from_3d": [
      "self",
      "weight_3d"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ]
  },
  "GptOssExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "hidden_states",
      "router_indices",
      "routing_weights"
    ]
  },
  "_RouterLinearParams": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "dtype"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "GptOssTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "weight": [
      "self",
      "value"
    ],
    "bias": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GptOssExpertsBnb4bit": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "router_indices",
      "routing_weights"
    ]
  },
  "GptOssTopKRouterBnb4bit": {
    "__init__": [
      "self",
      "config"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "patch_gpt_oss_bnb4bit": [],
  "restore_gpt_oss_original": [],
  "patch_gpt_oss_bnb4bit_auto": [],
  "use_combo_kernels": [],
  "fused_torch_compile_options": [],
  "no_combo_fused_torch_compile_options": [],
  "moe_forward_inference": [
    "self",
    "hidden_states"
  ],
  "moe_router_forward": [
    "self",
    "hidden_states"
  ],
  "_moe_forward_inference_bf16_kernel": [
    "hidden_states",
    "routing_weights",
    "gate_up_proj",
    "gate_up_proj_bias",
    "down_proj",
    "down_proj_bias",
    "limit",
    "alpha",
    "hidden_size"
  ],
  "_unwrap_peft_experts": [
    "module"
  ],
  "moe_forward_inference_bf16": [
    "self",
    "hidden_states"
  ],
  "GptOssMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "_should_use_gpt_oss_bnb4bit": [],
  "_is_gpt_oss_4bit_load": [],
  "_is_transformers_v5": [],
  "patch_gpt_oss_moe_for_lora": [],
  "_MXFP4_LORA_PATH_LOGGED": [],
  "forward_mxfp4_gpt_oss_with_lora": [
    "self",
    "hidden_states",
    "routing_data",
    "gather_idx",
    "scatter_idx"
  ],
  "patch_mxfp4_gpt_oss_for_lora": [],
  "_MXFP4_DEQUANT_WARNED": [],
  "torch_native_forward": [
    "self",
    "hidden_states",
    "router_indices",
    "routing_weights"
  ],
  "patch_gpt_oss_linearized": [],
  "patch_GptOssAttention": [],
  "patch_GptOssModel": [],
  "encode_conversations_with_harmony": [
    "messages",
    "reasoning_effort",
    "add_generation_prompt",
    "tool_calls",
    "developer_instructions",
    "model_identity"
  ],
  "patch_gpt_oss_init_weights_modulelist_fix": [],
  "patch_gpt_oss_for_grpo": [],
  "_check_bnb_available": [],
  "MoeExperts4bit": {
    "_warned_loop_based": [],
    "__init__": [
      "self",
      "num_experts",
      "hidden_dim",
      "intermediate_dim",
      "compute_dtype",
      "compress_statistics",
      "quant_type",
      "quant_storage",
      "device"
    ],
    "set_compute_type": [
      "self",
      "x"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "_quantize_and_store": [
      "self",
      "gate_up_proj",
      "down_proj"
    ],
    "_apply": [
      "self",
      "fn",
      "recurse"
    ],
    "_matmul_4bit": [
      "self",
      "x",
      "weight"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ],
    "_save_to_state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ]
  },
  "Qwen3MoeExperts4bit": {},
  "Qwen3VLMoeExperts4bit": {},
  "replace_with_bnb_moe_experts": [
    "model",
    "quantization_config"
  ],
  "is_moe_quantized": [
    "module"
  ],
  "quantize_moe_experts_inplace": [
    "model",
    "verbose"
  ],
  "post_quantize_moe_experts": [
    "model",
    "load_in_4bit",
    "verbose"
  ],
  "causal_mask_with_sink": [
    "batch",
    "head",
    "q_idx",
    "kv_idx"
  ],
  "generate_sliding_window_with_sink": [
    "window_size"
  ],
  "generate_sink_score_mod": [
    "sink_weights"
  ],
  "old_flex_attention_with_sink": [
    "self_attn",
    "query",
    "key",
    "value",
    "attention_mask",
    "scale",
    "sliding_window",
    "compile"
  ],
  "is_flex_attention_decoding": [
    "self_attn",
    "query"
  ],
  "flex_attention_with_sink": [
    "self_attn",
    "query",
    "key",
    "value",
    "attention_mask",
    "scale",
    "sliding_window",
    "compile",
    "has_static_cache"
  ],
  "flex_attention_with_sink_decoding": [
    "self_attn",
    "query",
    "key",
    "value",
    "scale"
  ],
  "flex_attention_add_sinks": [
    "self_attn",
    "attn_output",
    "logsumexp"
  ],
  "flash_attention_left_padded": [
    "self_attn",
    "query_states",
    "key_states",
    "value_states",
    "attention_mask",
    "is_causal",
    "window_size_left",
    "dropout_p",
    "scale"
  ],
  "FLEX_ATTENTION_KV_INCREMENT": []
}