{
  "AttentionMaskConverter": {
    "__init__": [
      "self",
      "is_causal",
      "sliding_window"
    ],
    "to_causal_4d": [
      "self",
      "batch_size",
      "query_length",
      "key_value_length",
      "dtype",
      "device"
    ],
    "to_4d": [
      "self",
      "attention_mask_2d",
      "query_length",
      "dtype",
      "key_value_length"
    ],
    "_make_causal_mask": [
      "input_ids_shape",
      "dtype",
      "device",
      "past_key_values_length",
      "sliding_window"
    ],
    "_expand_mask": [
      "mask",
      "dtype",
      "tgt_len"
    ],
    "_unmask_unattended": [
      "expanded_mask",
      "min_dtype"
    ],
    "_ignore_causal_mask_sdpa": [
      "attention_mask",
      "inputs_embeds",
      "past_key_values_length",
      "sliding_window",
      "is_training"
    ]
  },
  "_prepare_4d_causal_attention_mask": [
    "attention_mask",
    "input_shape",
    "inputs_embeds",
    "past_key_values_length",
    "sliding_window"
  ],
  "_prepare_4d_causal_attention_mask_for_sdpa": [
    "attention_mask",
    "input_shape",
    "inputs_embeds",
    "past_key_values_length",
    "sliding_window"
  ],
  "_prepare_4d_attention_mask": [
    "mask",
    "dtype",
    "tgt_len"
  ],
  "_prepare_4d_attention_mask_for_sdpa": [
    "mask",
    "dtype",
    "tgt_len"
  ],
  "_create_4d_causal_attention_mask": [
    "input_shape",
    "dtype",
    "device",
    "past_key_values_length",
    "sliding_window"
  ],
  "logger": [],
  "SequenceFeatureExtractor": {
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value"
    ],
    "pad": [
      "self",
      "processed_features",
      "padding",
      "max_length",
      "truncation",
      "pad_to_multiple_of",
      "return_attention_mask",
      "return_tensors"
    ],
    "_pad": [
      "self",
      "processed_features",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "return_attention_mask"
    ],
    "_truncate": [
      "self",
      "processed_features",
      "max_length",
      "pad_to_multiple_of",
      "truncation"
    ],
    "_get_padding_strategies": [
      "self",
      "padding",
      "max_length"
    ],
    "fetch_audio": [
      "self",
      "audio_url_or_urls"
    ]
  },
  "_MODEL_TO_CONVERSION_PATTERN": [],
  "_build_checkpoint_conversion_mapping": [],
  "_checkpoint_conversion_mapping_cache": [],
  "get_checkpoint_conversion_mapping": [
    "model_type"
  ],
  "register_checkpoint_conversion_mapping": [
    "model_type",
    "mapping",
    "overwrite"
  ],
  "VLMS": [],
  "get_model_conversion_mapping": [
    "model",
    "key_mapping",
    "hf_quantizer",
    "add_legacy"
  ],
  "SpecificProcessorType": [],
  "transformers_module": [],
  "_LazyAutoProcessorMapping": {
    "_MAPPING_NAMES": [],
    "__getitem__": [
      "self",
      "key"
    ],
    "__contains__": [
      "self",
      "key"
    ],
    "keys": [
      "self"
    ]
  },
  "MODALITY_TO_AUTOPROCESSOR_MAPPING": [],
  "MODALITY_TO_BASE_CLASS_MAPPING": [],
  "_get_modality_for_attribute": [
    "attribute_name"
  ],
  "TextKwargs": {},
  "ImagesKwargs": {},
  "VideosKwargs": {},
  "AudioKwargs": {},
  "ProcessingKwargs": {
    "_defaults": []
  },
  "TokenizerChatTemplateKwargs": {},
  "ProcessorChatTemplateKwargs": {},
  "AllKwargsForChatTemplate": {},
  "MultiModalData": {
    "__contains__": [
      "self",
      "key"
    ],
    "__getitem__": [
      "self",
      "key"
    ]
  },
  "ProcessorMixin": {
    "_auto_class": [],
    "valid_processor_kwargs": [],
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos",
      "audio"
    ],
    "check_argument_for_proper_class": [
      "self",
      "argument_name",
      "argument"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ],
    "__repr__": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "get_processor_dict": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "from_args_and_dict": [
      "cls",
      "args",
      "processor_dict"
    ],
    "_merge_kwargs": [
      "self",
      "ModelProcessorKwargs",
      "tokenizer_init_kwargs"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir",
      "force_download",
      "local_files_only",
      "token",
      "revision"
    ],
    "get_attributes": [
      "cls"
    ],
    "register_for_auto_class": [
      "cls",
      "auto_class"
    ],
    "_load_tokenizer_from_pretrained": [
      "cls",
      "sub_processor_type",
      "pretrained_model_name_or_path",
      "subfolder"
    ],
    "_get_arguments_from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "processor_dict"
    ],
    "get_possibly_dynamic_module": [
      "module_name"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "model_input_names": [
      "self"
    ],
    "validate_init_kwargs": [
      "processor_config",
      "valid_kwargs"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template"
    ],
    "post_process_multimodal_output": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "generation_mode"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens"
    ],
    "_check_special_mm_tokens": [
      "self",
      "text",
      "text_inputs",
      "modalities"
    ]
  },
  "to_channel_dimension_format": [
    "image",
    "channel_dim",
    "input_channel_dim"
  ],
  "rescale": [
    "image",
    "scale",
    "data_format",
    "dtype",
    "input_data_format"
  ],
  "_rescale_for_pil_conversion": [
    "image"
  ],
  "to_pil_image": [
    "image",
    "do_rescale",
    "image_mode",
    "input_data_format"
  ],
  "get_size_with_aspect_ratio": [
    "image_size",
    "size",
    "max_size"
  ],
  "get_resize_output_image_size": [
    "input_image",
    "size",
    "default_to_square",
    "max_size",
    "input_data_format"
  ],
  "resize": [
    "image",
    "size",
    "resample",
    "reducing_gap",
    "data_format",
    "return_numpy",
    "input_data_format"
  ],
  "normalize": [
    "image",
    "mean",
    "std",
    "data_format",
    "input_data_format"
  ],
  "center_crop": [
    "image",
    "size",
    "data_format",
    "input_data_format"
  ],
  "_center_to_corners_format_torch": [
    "bboxes_center"
  ],
  "_center_to_corners_format_numpy": [
    "bboxes_center"
  ],
  "center_to_corners_format": [
    "bboxes_center"
  ],
  "_corners_to_center_format_torch": [
    "bboxes_corners"
  ],
  "_corners_to_center_format_numpy": [
    "bboxes_corners"
  ],
  "corners_to_center_format": [
    "bboxes_corners"
  ],
  "rgb_to_id": [
    "color"
  ],
  "id_to_rgb": [
    "id_map"
  ],
  "PaddingMode": {
    "CONSTANT": [],
    "REFLECT": [],
    "REPLICATE": [],
    "SYMMETRIC": []
  },
  "pad": [
    "image",
    "padding",
    "mode",
    "constant_values",
    "data_format",
    "input_data_format"
  ],
  "convert_to_rgb": [
    "image"
  ],
  "flip_channel_order": [
    "image",
    "data_format",
    "input_data_format"
  ],
  "split_to_tiles": [
    "images",
    "num_tiles_height",
    "num_tiles_width"
  ],
  "_group_images_by_shape": [
    "nested_images"
  ],
  "_reconstruct_nested_structure": [
    "indices",
    "processed_images"
  ],
  "_iterate_items": [
    "items",
    "is_nested"
  ],
  "_get_device_from_images": [
    "images",
    "is_nested"
  ],
  "group_images_by_shape": [
    "images"
  ],
  "reorder_images": [
    "processed_images",
    "grouped_images_index",
    "is_nested"
  ],
  "DEFAULT_CALLBACKS": [],
  "DEFAULT_PROGRESS_CALLBACK": [],
  "_is_peft_model": [
    "model"
  ],
  "_get_fsdp_ckpt_kwargs": [],
  "safe_globals": [],
  "TRAINING_ARGS_NAME": [],
  "TRAINER_STATE_NAME": [],
  "OPTIMIZER_NAME": [],
  "SCALER_NAME": [],
  "OPTIMIZER_NAME_BIN": [],
  "SCHEDULER_NAME": [],
  "FSDP_MODEL_NAME": [],
  "Trainer": {
    "__init__": [
      "self",
      "model",
      "args",
      "data_collator",
      "train_dataset",
      "eval_dataset",
      "processing_class",
      "model_init",
      "compute_loss_func",
      "compute_metrics",
      "callbacks",
      "optimizers",
      "optimizer_cls_and_kwargs",
      "preprocess_logits_for_metrics"
    ],
    "_activate_neftune": [
      "self",
      "model"
    ],
    "_deactivate_neftune": [
      "self",
      "model"
    ],
    "add_callback": [
      "self",
      "callback"
    ],
    "pop_callback": [
      "self",
      "callback"
    ],
    "remove_callback": [
      "self",
      "callback"
    ],
    "_move_model_to_device": [
      "self",
      "model",
      "device"
    ],
    "_align_special_tokens": [
      "self"
    ],
    "_set_signature_columns_if_needed": [
      "self"
    ],
    "_remove_unused_columns": [
      "self",
      "dataset",
      "description"
    ],
    "_get_collator_with_removed_columns": [
      "self",
      "data_collator",
      "description"
    ],
    "_get_train_sampler": [
      "self",
      "train_dataset"
    ],
    "_get_dataloader": [
      "self",
      "dataset",
      "description",
      "batch_size",
      "sampler_fn",
      "is_training",
      "dataloader_key"
    ],
    "get_train_dataloader": [
      "self"
    ],
    "_get_eval_sampler": [
      "self",
      "eval_dataset"
    ],
    "get_eval_dataloader": [
      "self",
      "eval_dataset"
    ],
    "get_test_dataloader": [
      "self",
      "test_dataset"
    ],
    "create_optimizer_and_scheduler": [
      "self",
      "num_training_steps"
    ],
    "get_decay_parameter_names": [
      "self",
      "model"
    ],
    "create_optimizer": [
      "self"
    ],
    "get_num_trainable_parameters": [
      "self"
    ],
    "get_learning_rates": [
      "self"
    ],
    "get_optimizer_group": [
      "self",
      "param"
    ],
    "get_optimizer_cls_and_kwargs": [
      "args",
      "model"
    ],
    "create_scheduler": [
      "self",
      "num_training_steps",
      "optimizer"
    ],
    "num_examples": [
      "self",
      "dataloader"
    ],
    "num_tokens": [
      "train_dl",
      "max_steps"
    ],
    "_hp_search_setup": [
      "self",
      "trial"
    ],
    "_report_to_hp_search": [
      "self",
      "trial",
      "step",
      "metrics"
    ],
    "_tune_save_checkpoint": [
      "self",
      "checkpoint_dir"
    ],
    "call_model_init": [
      "self",
      "trial"
    ],
    "compare_trainer_and_checkpoint_args": [
      "self",
      "training_args",
      "trainer_state"
    ],
    "_wrap_model": [
      "self",
      "model",
      "training",
      "dataloader"
    ],
    "train": [
      "self",
      "resume_from_checkpoint",
      "trial",
      "ignore_keys_for_eval"
    ],
    "get_sp_size": [
      "self"
    ],
    "get_cp_size": [
      "self"
    ],
    "get_tp_size": [
      "self"
    ],
    "get_total_train_batch_size": [
      "self",
      "args"
    ],
    "_inner_training_loop": [
      "self",
      "batch_size",
      "args",
      "resume_from_checkpoint",
      "trial",
      "ignore_keys_for_eval"
    ],
    "_get_output_dir": [
      "self",
      "trial"
    ],
    "_load_from_checkpoint": [
      "self",
      "resume_from_checkpoint",
      "model"
    ],
    "_load_best_model": [
      "self"
    ],
    "_issue_warnings_after_load": [
      "self",
      "load_result"
    ],
    "_evaluate": [
      "self",
      "trial",
      "ignore_keys_for_eval",
      "skip_scheduler"
    ],
    "_maybe_log_save_evaluate": [
      "self",
      "tr_loss",
      "grad_norm",
      "model",
      "trial",
      "epoch",
      "ignore_keys_for_eval",
      "start_time",
      "learning_rate"
    ],
    "_load_rng_state": [
      "self",
      "checkpoint"
    ],
    "_determine_best_metric": [
      "self",
      "metrics",
      "trial"
    ],
    "_save_checkpoint": [
      "self",
      "model",
      "trial"
    ],
    "_save_rng_state": [
      "self",
      "output_dir"
    ],
    "_save_optimizer_and_scheduler": [
      "self",
      "output_dir"
    ],
    "_load_optimizer_and_scheduler": [
      "self",
      "checkpoint"
    ],
    "_save_scaler": [
      "self",
      "output_dir"
    ],
    "_load_scaler": [
      "self",
      "checkpoint"
    ],
    "_load_callback_state": [
      "self"
    ],
    "hyperparameter_search": [
      "self",
      "hp_space",
      "compute_objective",
      "n_trials",
      "direction",
      "backend",
      "hp_name"
    ],
    "log": [
      "self",
      "logs",
      "start_time"
    ],
    "_prepare_input": [
      "self",
      "data"
    ],
    "_prepare_inputs": [
      "self",
      "inputs"
    ],
    "_is_attention_mask_causal": [
      "self",
      "attention_mask"
    ],
    "_prepare_context_parallel_inputs": [
      "self",
      "model",
      "inputs"
    ],
    "compute_loss_context_manager": [
      "self"
    ],
    "autocast_smart_context_manager": [
      "self",
      "cache_enabled"
    ],
    "training_step": [
      "self",
      "model",
      "inputs",
      "num_items_in_batch"
    ],
    "compute_loss": [
      "self",
      "model",
      "inputs",
      "return_outputs",
      "num_items_in_batch"
    ],
    "_deepspeed_sp_compute_loss": [
      "self",
      "model",
      "inputs",
      "return_outputs",
      "pc"
    ],
    "is_local_process_zero": [
      "self"
    ],
    "is_world_process_zero": [
      "self"
    ],
    "save_model": [
      "self",
      "output_dir",
      "_internal_call"
    ],
    "_save_tpu": [
      "self",
      "output_dir"
    ],
    "_save": [
      "self",
      "output_dir",
      "state_dict"
    ],
    "store_flos": [
      "self"
    ],
    "_sorted_checkpoints": [
      "self",
      "output_dir",
      "checkpoint_prefix",
      "use_mtime"
    ],
    "_rotate_checkpoints": [
      "self",
      "use_mtime",
      "output_dir"
    ],
    "evaluate": [
      "self",
      "eval_dataset",
      "ignore_keys",
      "metric_key_prefix"
    ],
    "predict": [
      "self",
      "test_dataset",
      "ignore_keys",
      "metric_key_prefix"
    ],
    "evaluation_loop": [
      "self",
      "dataloader",
      "description",
      "prediction_loss_only",
      "ignore_keys",
      "metric_key_prefix"
    ],
    "_nested_gather": [
      "self",
      "tensors",
      "name"
    ],
    "prediction_step": [
      "self",
      "model",
      "inputs",
      "prediction_loss_only",
      "ignore_keys"
    ],
    "floating_point_ops": [
      "self",
      "inputs"
    ],
    "init_hf_repo": [
      "self",
      "token"
    ],
    "create_model_card": [
      "self",
      "language",
      "license",
      "tags",
      "model_name",
      "finetuned_from",
      "tasks",
      "dataset_tags",
      "dataset",
      "dataset_args"
    ],
    "_push_from_checkpoint": [
      "self",
      "checkpoint_folder"
    ],
    "_finish_current_push": [
      "self"
    ],
    "push_to_hub": [
      "self",
      "commit_message",
      "blocking",
      "token",
      "revision"
    ],
    "_add_sm_patterns_to_gitignore": [
      "self"
    ],
    "create_accelerator_and_postprocess": [
      "self"
    ],
    "propagate_args_to_deepspeed": [
      "self",
      "auto_find_batch_size"
    ],
    "_fsdp_qlora_plugin_updates": [
      "self"
    ],
    "_get_num_items_in_batch": [
      "self",
      "batch_samples",
      "device"
    ],
    "get_batch_samples": [
      "self",
      "epoch_iterator",
      "num_batches",
      "device"
    ],
    "set_initial_training_values": [
      "self",
      "args",
      "dataloader",
      "total_train_batch_size"
    ]
  },
  "TASK_MAPPING": [],
  "AUTOGENERATED_TRAINER_COMMENT": [],
  "TASK_TAG_TO_NAME_MAPPING": [],
  "METRIC_TAGS": [],
  "_listify": [
    "obj"
  ],
  "_insert_values_as_list": [
    "metadata",
    "name",
    "values"
  ],
  "infer_metric_tags_from_eval_results": [
    "eval_results"
  ],
  "_insert_value": [
    "metadata",
    "name",
    "value"
  ],
  "is_hf_dataset": [
    "dataset"
  ],
  "_get_mapping_values": [
    "mapping"
  ],
  "TrainingSummary": {
    "__post_init__": [
      "self"
    ],
    "create_model_index": [
      "self",
      "metric_mapping"
    ],
    "create_metadata": [
      "self"
    ],
    "to_model_card": [
      "self"
    ],
    "from_trainer": [
      "cls",
      "trainer",
      "language",
      "license",
      "tags",
      "model_name",
      "finetuned_from",
      "tasks",
      "dataset_tags",
      "dataset_metadata",
      "dataset",
      "dataset_args"
    ]
  },
  "parse_log_history": [
    "log_history"
  ],
  "_maybe_round": [
    "v",
    "decimals"
  ],
  "_regular_table_line": [
    "values",
    "col_widths"
  ],
  "_second_table_line": [
    "col_widths"
  ],
  "make_markdown_table": [
    "lines"
  ],
  "_TRAINING_ARGS_KEYS": [],
  "extract_hyperparameters_from_trainer": [
    "trainer"
  ],
  "ImageProcessorType": [],
  "BatchFeature": {},
  "ImageProcessingMixin": {
    "_auto_class": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir",
      "force_download",
      "local_files_only",
      "token",
      "revision"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "get_image_processor_dict": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "from_dict": [
      "cls",
      "image_processor_dict"
    ],
    "to_dict": [
      "self"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ],
    "__repr__": [
      "self"
    ],
    "register_for_auto_class": [
      "cls",
      "auto_class"
    ],
    "fetch_images": [
      "self",
      "image_url_or_urls"
    ]
  },
  "INIT_SERVICE_KWARGS": [],
  "BaseImageProcessor": {
    "valid_kwargs": [],
    "is_fast": [
      "self"
    ],
    "__call__": [
      "self",
      "images"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "rescale": [
      "self",
      "image",
      "scale",
      "data_format",
      "input_data_format"
    ],
    "normalize": [
      "self",
      "image",
      "mean",
      "std",
      "data_format",
      "input_data_format"
    ],
    "center_crop": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "to_dict": [
      "self"
    ]
  },
  "VALID_SIZE_DICT_KEYS": [],
  "is_valid_size_dict": [
    "size_dict"
  ],
  "convert_to_size_dict": [
    "size",
    "max_size",
    "default_to_square",
    "height_width_order"
  ],
  "get_size_dict": [
    "size",
    "max_size",
    "height_width_order",
    "default_to_square",
    "param_name"
  ],
  "select_best_resolution": [
    "original_size",
    "possible_resolutions"
  ],
  "get_patch_output_size": [
    "image",
    "target_resolution",
    "input_data_format"
  ],
  "_torch_distributed_available": [],
  "build_glob_alternation": [
    "globs"
  ],
  "ConversionOps": {
    "__repr__": [
      "self"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "Chunk": {
    "__init__": [
      "self",
      "dim"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "get_target_patterns": [
      "self",
      "input_dict",
      "target_patterns"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "Concatenate": {
    "__init__": [
      "self",
      "dim"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "get_target_pattern": [
      "self",
      "target_patterns"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "MergeModulelist": {
    "__init__": [
      "self",
      "dim"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "get_target_pattern": [
      "self",
      "input_dict",
      "source_pattern",
      "target_patterns"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "SplitModulelist": {
    "__init__": [
      "self",
      "dim"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "get_target_patterns": [
      "self",
      "input_dict",
      "source_pattern",
      "target_patterns",
      "sizes"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "Transpose": {
    "__init__": [
      "self",
      "dim0",
      "dim1"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "get_target_pattern": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "PermuteForRope": {
    "__init__": [
      "self"
    ],
    "_apply": [
      "self",
      "tensor"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns",
      "config"
    ]
  },
  "ErnieFuseAndSplitTextVisionExperts": {
    "__init__": [
      "self",
      "stack_dim",
      "concat_dim"
    ],
    "split_list_into_chunks": [
      "self",
      "tensor_list",
      "chunks"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns",
      "config"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "ErnieSplitAndDecoupleTextVisionExperts": {
    "__init__": [
      "self",
      "stack_dim",
      "concat_dim"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns",
      "config"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "Force16BytesAlignment": {
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "get_target_pattern": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "process_target_pattern": [
    "pattern"
  ],
  "WeightTransform": {
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "__post_init__": [
      "self"
    ],
    "add_tensor": [
      "self",
      "target_key",
      "source_key",
      "source_pattern",
      "future"
    ],
    "rename_source_key": [
      "self",
      "source_key"
    ],
    "reverse_transform": [
      "self"
    ],
    "materialize_tensors": [
      "self"
    ]
  },
  "WeightRenaming": {
    "convert": [
      "self",
      "layer_name",
      "model",
      "config",
      "hf_quantizer",
      "loading_info"
    ]
  },
  "_INTERNAL_MANY_TO_MANY_CONVERSIONS": [],
  "WeightConverter": {
    "__post_init__": [
      "self"
    ],
    "convert": [
      "self",
      "layer_name",
      "model",
      "config",
      "hf_quantizer",
      "loading_info"
    ]
  },
  "GLOBAL_WORKERS": [],
  "_materialize_copy": [
    "tensor",
    "device",
    "dtype"
  ],
  "spawn_materialize": [
    "thread_pool",
    "tensor",
    "device",
    "dtype"
  ],
  "spawn_tp_materialize": [
    "thread_pool",
    "tensor",
    "sharding_method",
    "tensor_idx",
    "device",
    "dtype"
  ],
  "dot_natural_key": [
    "s"
  ],
  "log_conversion_errors": [
    "first_target_key",
    "loading_info",
    "extras",
    "op"
  ],
  "set_param_for_module": [
    "model",
    "target_name",
    "param_value",
    "loading_info",
    "distributed_operation",
    "hf_quantizer"
  ],
  "offload_and_maybe_resave_param": [
    "target_name",
    "param",
    "loading_info",
    "disk_offload_folder",
    "disk_offload_index",
    "applied_ops"
  ],
  "SkipParameters": {},
  "rename_source_key": [
    "source_key",
    "weight_renamings",
    "weight_converters",
    "prefix",
    "meta_state_dict"
  ],
  "convert_and_load_state_dict_in_model": [
    "model",
    "state_dict",
    "load_config",
    "tp_plan",
    "disk_offload_index"
  ],
  "revert_weight_conversion": [
    "model",
    "state_dict"
  ],
  "_is_torch_greater_or_equal_than_2_5": [],
  "_is_torch_greater_or_equal_than_2_6": [],
  "_is_torch_xpu_available": [],
  "and_masks": [],
  "or_masks": [],
  "causal_mask_function": [
    "batch_idx",
    "head_idx",
    "q_idx",
    "kv_idx"
  ],
  "bidirectional_mask_function": [
    "batch_idx",
    "head_idx",
    "q_idx",
    "kv_idx"
  ],
  "sliding_window_overlay": [
    "sliding_window"
  ],
  "chunked_overlay": [
    "chunk_size",
    "left_padding"
  ],
  "sliding_window_causal_mask_function": [
    "sliding_window"
  ],
  "sliding_window_bidirectional_overlay": [
    "sliding_window"
  ],
  "sliding_window_bidirectional_mask_function": [
    "sliding_window"
  ],
  "chunked_causal_mask_function": [
    "chunk_size",
    "left_padding"
  ],
  "padding_mask_function": [
    "padding_mask"
  ],
  "packed_sequence_mask_function": [
    "packed_sequence_mask"
  ],
  "add_offsets_to_mask_function": [
    "mask_function",
    "q_offset",
    "kv_offset"
  ],
  "prepare_padding_mask": [
    "attention_mask",
    "kv_length",
    "kv_offset"
  ],
  "_can_skip_causal_mask_xpu": [
    "padding_mask",
    "query_length",
    "kv_length",
    "local_attention_size"
  ],
  "_ignore_causal_mask_sdpa": [
    "padding_mask",
    "query_length",
    "kv_length",
    "kv_offset",
    "local_attention_size"
  ],
  "_can_skip_bidirectional_mask_xpu": [
    "padding_mask",
    "kv_length",
    "local_attention_size"
  ],
  "_ignore_bidirectional_mask_sdpa": [
    "padding_mask",
    "kv_length",
    "local_attention_size"
  ],
  "_vmap_expansion_sdpa": [
    "mask_function"
  ],
  "_non_vmap_expansion_sdpa": [
    "batch_indices",
    "head_indices",
    "q_indices",
    "kv_indices"
  ],
  "sdpa_mask": [
    "batch_size",
    "cache_position",
    "kv_length",
    "kv_offset",
    "mask_function",
    "attention_mask",
    "local_size",
    "allow_is_causal_skip",
    "allow_is_bidirectional_skip",
    "allow_torch_fix",
    "use_vmap"
  ],
  "eager_mask": [
    "batch_size",
    "cache_position",
    "kv_length",
    "kv_offset",
    "mask_function",
    "attention_mask",
    "dtype",
    "allow_is_bidirectional_skip",
    "use_vmap"
  ],
  "flash_attention_mask": [
    "batch_size",
    "cache_position",
    "kv_length",
    "kv_offset",
    "mask_function",
    "attention_mask"
  ],
  "flex_attention_mask": [
    "batch_size",
    "cache_position",
    "kv_length",
    "kv_offset",
    "mask_function",
    "attention_mask"
  ],
  "AttentionMaskInterface": {
    "_global_mapping": []
  },
  "find_packed_sequence_indices": [
    "position_ids"
  ],
  "_preprocess_mask_arguments": [
    "config",
    "input_embeds",
    "attention_mask",
    "cache_position",
    "past_key_values",
    "position_ids",
    "layer_idx"
  ],
  "create_causal_mask": [
    "config",
    "input_embeds",
    "attention_mask",
    "cache_position",
    "past_key_values",
    "position_ids",
    "or_mask_function",
    "and_mask_function"
  ],
  "create_bidirectional_mask": [
    "config",
    "input_embeds",
    "attention_mask",
    "encoder_hidden_states",
    "or_mask_function",
    "and_mask_function"
  ],
  "create_sliding_window_causal_mask": [
    "config",
    "input_embeds",
    "attention_mask",
    "cache_position",
    "past_key_values",
    "position_ids",
    "or_mask_function",
    "and_mask_function"
  ],
  "create_bidirectional_sliding_window_mask": [
    "config",
    "input_embeds",
    "attention_mask",
    "or_mask_function",
    "and_mask_function"
  ],
  "create_chunked_causal_mask": [
    "config",
    "input_embeds",
    "attention_mask",
    "cache_position",
    "past_key_values",
    "position_ids",
    "or_mask_function",
    "and_mask_function"
  ],
  "LAYER_PATTERN_TO_MASK_FUNCTION_MAPPING": [],
  "create_masks_for_generate": [
    "config",
    "input_embeds",
    "attention_mask",
    "cache_position",
    "past_key_values",
    "position_ids",
    "or_mask_function",
    "and_mask_function"
  ],
  "GREEN": [],
  "YELLOW": [],
  "RESET": [],
  "BLACK_SQUARE": [],
  "WHITE_SQUARE": [],
  "GREY_SQUARE": [],
  "LOW_TRIANGLE": [],
  "UPPER_TRIANGLE": [],
  "get_style": [
    "style"
  ],
  "YELLOW_SQUARE": [],
  "GREEN_SQUARE": [],
  "tensor_to_mask_visual": [
    "original_tensor",
    "grid_size",
    "style"
  ],
  "AttentionMask": {
    "__new__": [
      "cls",
      "data",
      "style"
    ],
    "__init__": [
      "self",
      "data"
    ],
    "to_string": [
      "self",
      "grid_size",
      "limit"
    ],
    "__repr__": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "from_tensor": [
      "cls",
      "tensor",
      "style"
    ]
  },
  "_is_rank_zero": [],
  "MEMORY_ADDRESS_REGEX": [],
  "_sanitize_repr_for_diff": [
    "x_str"
  ],
  "_dtensor_repr": [
    "x"
  ],
  "_serialize_tensor_like_io": [
    "value",
    "debug_path",
    "use_repr",
    "path_to_value"
  ],
  "_serialize_io": [
    "value",
    "debug_path",
    "use_repr",
    "path_to_value"
  ],
  "_repr_to_list": [
    "value"
  ],
  "prune_outputs_if_children": [
    "node"
  ],
  "LAYER_SUFFIX_RE": [],
  "is_layer_block": [
    "node"
  ],
  "prune_intermediate_layers": [
    "node"
  ],
  "log_model_debug_trace": [
    "debug_path",
    "model"
  ],
  "_attach_debugger_logic": [
    "model",
    "debug_path",
    "do_prune_layers",
    "use_repr"
  ],
  "model_addition_debugger_context": [
    "model",
    "debug_path",
    "do_prune_layers",
    "use_repr"
  ],
  "TOKENIZER_CLASSES": [],
  "convert_slow_checkpoint_to_fast": [
    "tokenizer_name",
    "checkpoint_name",
    "dump_path",
    "force_download"
  ],
  "GELUTanh": {
    "__init__": [
      "self",
      "use_gelu_tanh_python"
    ],
    "_gelu_tanh_python": [
      "self",
      "input"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "PytorchGELUTanh": [],
  "NewGELUActivation": {
    "forward": [
      "self",
      "input"
    ]
  },
  "GELUActivation": {
    "__init__": [
      "self",
      "use_gelu_python"
    ],
    "_gelu_python": [
      "self",
      "input"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "SiLUActivation": {
    "forward": [
      "self",
      "input"
    ]
  },
  "FastGELUActivation": {
    "forward": [
      "self",
      "input"
    ]
  },
  "QuickGELUActivation": {
    "forward": [
      "self",
      "input"
    ]
  },
  "ClippedGELUActivation": {
    "__init__": [
      "self",
      "min",
      "max"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "AccurateGELUActivation": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "MishActivation": {
    "__init__": [
      "self"
    ],
    "_mish_python": [
      "self",
      "input"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "LinearActivation": {
    "forward": [
      "self",
      "input"
    ]
  },
  "LaplaceActivation": {
    "forward": [
      "self",
      "input",
      "mu",
      "sigma"
    ]
  },
  "ReLUSquaredActivation": {
    "forward": [
      "self",
      "input"
    ]
  },
  "ClassInstantier": {
    "__getitem__": [
      "self",
      "key"
    ]
  },
  "XIELUActivation": {
    "__init__": [
      "self",
      "alpha_p_init",
      "alpha_n_init",
      "beta",
      "eps",
      "dtype",
      "with_vector_loads"
    ],
    "_xielu_python": [
      "self",
      "x"
    ],
    "_xielu_cuda": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "ACT2CLS": [],
  "ACT2FN": [],
  "get_activation": [
    "activation_string"
  ],
  "gelu_python": [],
  "gelu_new": [],
  "gelu": [],
  "gelu_fast": [],
  "quick_gelu": [],
  "silu": [],
  "mish": [],
  "linear_act": [],
  "TORCH_INIT_FUNCTIONS": [],
  "uniform_": [
    "tensor",
    "a",
    "b",
    "generator"
  ],
  "normal_": [
    "tensor",
    "mean",
    "std",
    "generator"
  ],
  "constant_": [
    "tensor",
    "val"
  ],
  "ones_": [
    "tensor"
  ],
  "zeros_": [
    "tensor"
  ],
  "eye_": [
    "tensor"
  ],
  "dirac_": [
    "tensor",
    "groups"
  ],
  "xavier_uniform_": [
    "tensor",
    "gain",
    "generator"
  ],
  "xavier_normal_": [
    "tensor",
    "gain",
    "generator"
  ],
  "kaiming_uniform_": [
    "tensor",
    "a",
    "mode",
    "nonlinearity",
    "generator"
  ],
  "kaiming_normal_": [
    "tensor",
    "a",
    "mode",
    "nonlinearity",
    "generator"
  ],
  "trunc_normal_": [
    "tensor",
    "mean",
    "std",
    "a",
    "b",
    "generator"
  ],
  "orthogonal_": [
    "tensor",
    "gain",
    "generator"
  ],
  "sparse_": [
    "tensor",
    "sparsity",
    "std",
    "generator"
  ],
  "copy_": [
    "tensor",
    "other"
  ],
  "_variance_scaling": [
    "tensor",
    "mode",
    "distribution"
  ],
  "lecun_normal_": [
    "tensor"
  ],
  "default_flax_embed_init_": [
    "tensor"
  ],
  "TORCH_MODULES_TO_PATCH": [],
  "guard_torch_init_functions": [],
  "no_init_weights": [],
  "no_tie_weights": [],
  "CheckpointManager": {
    "__init__": [
      "self",
      "trainer",
      "kill_wait"
    ],
    "setup_signal_handler": [
      "self"
    ],
    "_sigterm_handler": [
      "self",
      "signum",
      "frame"
    ],
    "_enable_checkpoint": [
      "self"
    ],
    "execute_jit_checkpoint": [
      "self"
    ]
  },
  "JITCheckpointCallback": {
    "__init__": [
      "self"
    ],
    "set_trainer": [
      "self",
      "trainer"
    ],
    "on_pre_optimizer_step": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_epoch_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "validate_fast_preprocess_arguments": [
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_center_crop",
    "crop_size",
    "do_resize",
    "size",
    "interpolation",
    "return_tensors",
    "data_format"
  ],
  "safe_squeeze": [
    "tensor",
    "axis"
  ],
  "max_across_indices": [
    "values"
  ],
  "get_max_height_width": [
    "images"
  ],
  "divide_to_patches": [
    "image",
    "patch_size"
  ],
  "BaseImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_pad": [],
    "pad_size": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "return_tensors": [],
    "data_format": [],
    "input_data_format": [],
    "device": [],
    "model_input_names": [],
    "image_seq_length": [],
    "valid_kwargs": [],
    "unused_kwargs": [],
    "__init__": [
      "self"
    ],
    "is_fast": [
      "self"
    ],
    "pad": [
      "self",
      "images",
      "pad_size",
      "fill_value",
      "padding_mode",
      "return_mask",
      "disable_grouping",
      "is_nested"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias"
    ],
    "compile_friendly_resize": [
      "image",
      "new_size",
      "interpolation",
      "antialias"
    ],
    "rescale": [
      "self",
      "image",
      "scale"
    ],
    "normalize": [
      "self",
      "image",
      "mean",
      "std"
    ],
    "_fuse_mean_std_and_rescale_factor": [
      "self",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_rescale",
      "rescale_factor",
      "device"
    ],
    "rescale_and_normalize": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "center_crop": [
      "self",
      "image",
      "size"
    ],
    "convert_to_rgb": [
      "self",
      "image"
    ],
    "filter_out_unused_kwargs": [
      "self",
      "kwargs"
    ],
    "_prepare_images_structure": [
      "self",
      "images",
      "expected_ndims"
    ],
    "_process_image": [
      "self",
      "image",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_prepare_image_like_inputs": [
      "self",
      "images",
      "do_convert_rgb",
      "input_data_format",
      "device",
      "expected_ndims"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "crop_size",
      "pad_size",
      "default_to_square",
      "image_mean",
      "image_std",
      "data_format"
    ],
    "_validate_preprocess_kwargs": [
      "self",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_resize",
      "size",
      "do_center_crop",
      "crop_size",
      "interpolation",
      "return_tensors",
      "data_format"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "disable_grouping",
      "return_tensors"
    ],
    "to_dict": [
      "self"
    ]
  },
  "TOKENIZER_FILE": [],
  "SPECIAL_TOKENS_MAP_FILE": [],
  "TOKENIZER_CONFIG_FILE": [],
  "TIKTOKEN_VOCAB_FILE": [],
  "ADDED_TOKENS_FILE": [],
  "MODEL_TO_TRAINER_MAPPING": [],
  "VOCAB_FILES_NAMES": [],
  "TokenizersBackend": {
    "vocab_files_names": [],
    "model": [],
    "_tokenizer": [],
    "convert_to_native_format": [
      "cls",
      "trust_remote_code"
    ],
    "__init__": [
      "self"
    ],
    "is_fast": [
      "self"
    ],
    "can_save_slow_tokenizer": [
      "self"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "update_post_processor": [
      "self"
    ],
    "add_eos_token": [
      "self",
      "value"
    ],
    "add_bos_token": [
      "self",
      "value"
    ],
    "_post_init": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "vocab": [
      "self"
    ],
    "added_tokens_encoder": [
      "self"
    ],
    "added_tokens_decoder": [
      "self"
    ],
    "get_added_vocab": [
      "self"
    ],
    "__bool__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "backend_tokenizer": [
      "self"
    ],
    "decoder": [
      "self"
    ],
    "_convert_encoding": [
      "self",
      "encoding",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_convert_token_to_id_with_added_voc": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "_add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "num_special_tokens_to_add": [
      "self",
      "pair"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "set_truncation_and_padding": [
      "self",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "split_special_tokens"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "_save_pretrained": [
      "self",
      "save_directory",
      "file_names",
      "legacy_format",
      "filename_prefix"
    ],
    "train_new_from_iterator": [
      "self",
      "text_iterator",
      "vocab_size",
      "length",
      "new_special_tokens",
      "special_tokens_map"
    ],
    "_patch_mistral_regex": [
      "cls",
      "tokenizer",
      "pretrained_model_name_or_path",
      "token",
      "cache_dir",
      "local_files_only",
      "_commit_hash",
      "is_local",
      "init_kwargs",
      "fix_mistral_regex"
    ]
  },
  "PreTrainedTokenizerFast": [],
  "MBART_LANGUAGES": [],
  "MBART50_LANGUAGES": [],
  "import_protobuf": [
    "error_message"
  ],
  "_get_prepend_scheme": [
    "add_prefix_space",
    "original_tokenizer"
  ],
  "generate_merges": [
    "vocab",
    "vocab_scores",
    "skip_tokens"
  ],
  "SentencePieceExtractor": {
    "__init__": [
      "self",
      "model"
    ],
    "extract": [
      "self",
      "model_type"
    ]
  },
  "GemmaSentencePieceExtractor": {
    "extract": [
      "self",
      "vocab_scores"
    ]
  },
  "check_number_comma": [
    "piece"
  ],
  "Converter": {
    "__init__": [
      "self",
      "original_tokenizer"
    ],
    "converted": [
      "self"
    ]
  },
  "BertConverter": {
    "converted": [
      "self"
    ]
  },
  "SplinterConverter": {
    "converted": [
      "self"
    ]
  },
  "FunnelConverter": {
    "converted": [
      "self"
    ]
  },
  "MPNetConverter": {
    "converted": [
      "self"
    ]
  },
  "OpenAIGPTConverter": {
    "converted": [
      "self"
    ]
  },
  "GPT2Converter": {
    "converted": [
      "self",
      "vocab",
      "merges"
    ]
  },
  "HerbertConverter": {
    "converted": [
      "self"
    ]
  },
  "Qwen2Converter": {
    "converted": [
      "self",
      "vocab",
      "merges"
    ]
  },
  "RobertaConverter": {
    "converted": [
      "self"
    ]
  },
  "RoFormerConverter": {
    "converted": [
      "self"
    ]
  },
  "DebertaConverter": {
    "converted": [
      "self"
    ]
  },
  "SpmConverter": {
    "handle_byte_fallback": [],
    "SpmExtractor": [],
    "special_tokens": [],
    "convert_from_spm": [
      "cls",
      "vocab"
    ],
    "__init__": [
      "self"
    ],
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "tokenizer": [
      "self",
      "proto"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "pre_tokenizer": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "post_processor": [
      "self"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "converted": [
      "self"
    ]
  },
  "AlbertConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ]
  },
  "BarthezConverter": {
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ]
  },
  "CamembertConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ],
    "convert_from_spm": [
      "cls",
      "vocab"
    ]
  },
  "DebertaV2Converter": {
    "pre_tokenizer": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ]
  },
  "MBartConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ],
    "convert_from_spm": [
      "cls",
      "vocab"
    ]
  },
  "MBart50Converter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ],
    "convert_from_spm": [
      "cls",
      "vocab"
    ]
  },
  "NllbConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ],
    "convert_from_spm": [
      "cls",
      "vocab"
    ]
  },
  "SeamlessM4TConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ]
  },
  "XLMRobertaConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ],
    "convert_from_spm": [
      "cls",
      "vocab"
    ]
  },
  "XLNetConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ]
  },
  "ReformerConverter": {},
  "RemBertConverter": {
    "normalizer": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ]
  },
  "BertGenerationConverter": {},
  "PegasusConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "pre_tokenizer": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "post_processor": [
      "self"
    ]
  },
  "T5Converter": {
    "vocab": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ],
    "convert_from_spm": [
      "cls",
      "vocab"
    ]
  },
  "UdopConverter": {
    "post_processor": [
      "self"
    ]
  },
  "WhisperConverter": {
    "converted": [
      "self"
    ]
  },
  "BigBirdConverter": {
    "post_processor": [
      "self"
    ]
  },
  "CLIPConverter": {
    "converted": [
      "self"
    ]
  },
  "LayoutLMv2Converter": {
    "converted": [
      "self"
    ]
  },
  "BlenderbotConverter": {
    "converted": [
      "self"
    ]
  },
  "XGLMConverter": {
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ]
  },
  "GemmaConverter": {
    "handle_byte_fallback": [],
    "SpmExtractor": [],
    "special_tokens": [],
    "normalizer": [
      "self",
      "proto"
    ],
    "vocab": [
      "self",
      "proto"
    ],
    "pre_tokenizer": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ]
  },
  "LlamaConverter": {
    "handle_byte_fallback": [],
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "pre_tokenizer": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "post_processor": [
      "self"
    ]
  },
  "MarkupLMConverter": {
    "converted": [
      "self"
    ]
  },
  "MoshiConverter": {
    "handle_byte_fallback": [],
    "__init__": [
      "self",
      "vocab_file"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "pre_tokenizer": [
      "self",
      "replacement",
      "add_prefix_space"
    ]
  },
  "HeliumConverter": {
    "handle_byte_fallback": [],
    "__init__": [
      "self",
      "vocab_file"
    ],
    "tokenizer": [
      "self",
      "proto"
    ],
    "vocab": [
      "self",
      "proto"
    ],
    "unk_id": [
      "self",
      "proto"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "pre_tokenizer": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "post_processor": [
      "self"
    ]
  },
  "ParakeetConverter": {
    "handle_byte_fallback": [],
    "__init__": [
      "self",
      "vocab_file"
    ],
    "tokenizer": [
      "self",
      "proto"
    ]
  },
  "bytes_to_unicode": [],
  "TikTokenConverter": {
    "__init__": [
      "self",
      "vocab_file",
      "pattern",
      "add_prefix_space",
      "extra_special_tokens"
    ],
    "extract_vocab_merges_from_model": [
      "self",
      "tiktoken_url"
    ],
    "tokenizer": [
      "self"
    ],
    "converted": [
      "self"
    ]
  },
  "MistralConverter": {
    "__init__": [
      "self",
      "vocab_file",
      "pattern",
      "add_prefix_space",
      "additional_special_tokens"
    ],
    "extract_vocab_merges_from_model": [
      "self",
      "tiktoken_url"
    ],
    "tokenizer": [
      "self"
    ],
    "converted": [
      "self"
    ]
  },
  "SLOW_TO_FAST_CONVERTERS": [],
  "convert_slow_tokenizer": [
    "transformer_tokenizer",
    "from_tiktoken"
  ],
  "previous_pr": [
    "api",
    "model_id",
    "pr_title",
    "token"
  ],
  "spawn_conversion": [
    "token",
    "private",
    "model_id"
  ],
  "get_conversion_pr_reference": [
    "api",
    "model_id"
  ],
  "auto_conversion": [
    "pretrained_model_name_or_path",
    "ignore_errors_during_conversion",
    "safe_weights_name",
    "safe_weights_index_name"
  ],
  "AudioInput": [],
  "load_audio": [
    "audio",
    "sampling_rate",
    "timeout"
  ],
  "load_audio_torchcodec": [
    "audio",
    "sampling_rate"
  ],
  "load_audio_librosa": [
    "audio",
    "sampling_rate",
    "timeout"
  ],
  "load_audio_as": [
    "audio",
    "return_format",
    "timeout",
    "force_mono",
    "sampling_rate"
  ],
  "conv1d_output_length": [
    "module",
    "input_length"
  ],
  "is_valid_audio": [
    "audio"
  ],
  "is_valid_list_of_audio": [
    "audio"
  ],
  "make_list_of_audio": [
    "audio"
  ],
  "hertz_to_mel": [
    "freq",
    "mel_scale"
  ],
  "mel_to_hertz": [
    "mels",
    "mel_scale"
  ],
  "hertz_to_octave": [
    "freq",
    "tuning",
    "bins_per_octave"
  ],
  "_create_triangular_filter_bank": [
    "fft_freqs",
    "filter_freqs"
  ],
  "chroma_filter_bank": [
    "num_frequency_bins",
    "num_chroma",
    "sampling_rate",
    "tuning",
    "power",
    "weighting_parameters",
    "start_at_c_chroma"
  ],
  "mel_filter_bank": [
    "num_frequency_bins",
    "num_mel_filters",
    "min_frequency",
    "max_frequency",
    "sampling_rate",
    "norm",
    "mel_scale",
    "triangularize_in_mel_space"
  ],
  "optimal_fft_length": [
    "window_length"
  ],
  "window_function": [
    "window_length",
    "name",
    "periodic",
    "frame_length",
    "center"
  ],
  "spectrogram": [
    "waveform",
    "window",
    "frame_length",
    "hop_length",
    "fft_length",
    "power",
    "center",
    "pad_mode",
    "onesided",
    "dither",
    "preemphasis",
    "mel_filters",
    "mel_floor",
    "log_mel",
    "reference",
    "min_value",
    "db_range",
    "remove_dc_offset",
    "dtype"
  ],
  "spectrogram_batch": [
    "waveform_list",
    "window",
    "frame_length",
    "hop_length",
    "fft_length",
    "power",
    "center",
    "pad_mode",
    "onesided",
    "dither",
    "preemphasis",
    "mel_filters",
    "mel_floor",
    "log_mel",
    "reference",
    "min_value",
    "db_range",
    "remove_dc_offset",
    "dtype"
  ],
  "power_to_db": [
    "spectrogram",
    "reference",
    "min_value",
    "db_range"
  ],
  "power_to_db_batch": [
    "spectrogram",
    "reference",
    "min_value",
    "db_range"
  ],
  "amplitude_to_db": [
    "spectrogram",
    "reference",
    "min_value",
    "db_range"
  ],
  "amplitude_to_db_batch": [
    "spectrogram",
    "reference",
    "min_value",
    "db_range"
  ],
  "_get_constant_lambda": [
    "_"
  ],
  "get_constant_schedule": [
    "optimizer",
    "last_epoch"
  ],
  "get_reduce_on_plateau_schedule": [
    "optimizer"
  ],
  "_get_constant_schedule_with_warmup_lr_lambda": [
    "current_step"
  ],
  "get_constant_schedule_with_warmup": [
    "optimizer",
    "num_warmup_steps",
    "last_epoch"
  ],
  "_get_linear_schedule_with_warmup_lr_lambda": [
    "current_step"
  ],
  "get_linear_schedule_with_warmup": [
    "optimizer",
    "num_warmup_steps",
    "num_training_steps",
    "last_epoch"
  ],
  "_get_cosine_schedule_with_warmup_lr_lambda": [
    "current_step"
  ],
  "get_cosine_schedule_with_warmup": [
    "optimizer",
    "num_warmup_steps",
    "num_training_steps",
    "num_cycles",
    "last_epoch"
  ],
  "_get_cosine_with_hard_restarts_schedule_with_warmup_lr_lambda": [
    "current_step"
  ],
  "get_cosine_with_hard_restarts_schedule_with_warmup": [
    "optimizer",
    "num_warmup_steps",
    "num_training_steps",
    "num_cycles",
    "last_epoch"
  ],
  "_get_polynomial_decay_schedule_with_warmup_lr_lambda": [
    "current_step"
  ],
  "get_polynomial_decay_schedule_with_warmup": [
    "optimizer",
    "num_warmup_steps",
    "num_training_steps",
    "lr_end",
    "power",
    "last_epoch"
  ],
  "_get_inverse_sqrt_schedule_lr_lambda": [
    "current_step"
  ],
  "get_inverse_sqrt_schedule": [
    "optimizer",
    "num_warmup_steps",
    "timescale",
    "last_epoch"
  ],
  "get_cosine_with_min_lr_schedule_with_warmup": [
    "optimizer",
    "num_warmup_steps",
    "num_training_steps",
    "num_cycles",
    "last_epoch",
    "min_lr",
    "min_lr_rate"
  ],
  "_get_cosine_with_min_lr_schedule_with_warmup_lr_rate_lambda": [
    "current_step"
  ],
  "get_cosine_with_min_lr_schedule_with_warmup_lr_rate": [
    "optimizer",
    "num_warmup_steps",
    "num_training_steps",
    "num_cycles",
    "last_epoch",
    "min_lr",
    "min_lr_rate",
    "warmup_lr_rate"
  ],
  "_get_wsd_scheduler_lambda": [
    "current_step"
  ],
  "get_wsd_schedule": [
    "optimizer",
    "num_warmup_steps",
    "num_decay_steps",
    "num_training_steps",
    "num_stable_steps",
    "warmup_type",
    "decay_type",
    "min_lr_ratio",
    "num_cycles",
    "last_epoch"
  ],
  "TYPE_TO_SCHEDULER_FUNCTION": [],
  "get_scheduler": [
    "name",
    "optimizer",
    "num_warmup_steps",
    "num_training_steps",
    "scheduler_specific_kwargs"
  ],
  "Adafactor": {
    "__init__": [
      "self",
      "params",
      "lr",
      "eps",
      "clip_threshold",
      "decay_rate",
      "beta1",
      "weight_decay",
      "scale_parameter",
      "relative_step",
      "warmup_init"
    ],
    "_get_lr": [
      "param_group",
      "param_state"
    ],
    "_get_options": [
      "param_group",
      "param_shape"
    ],
    "_rms": [
      "tensor"
    ],
    "_approx_sq_grad": [
      "exp_avg_sq_row",
      "exp_avg_sq_col"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdafactorSchedule": {
    "__init__": [
      "self",
      "optimizer",
      "initial_lr"
    ],
    "get_lr": [
      "self"
    ]
  },
  "get_adafactor_schedule": [
    "optimizer",
    "initial_lr"
  ],
  "__version__": [],
  "_import_structure": [],
  "ALL_LAYERNORM_LAYERS": [],
  "is_torch_greater_or_equal_than_2_8": [],
  "is_torch_greater_or_equal_than_2_6": [],
  "is_torch_greater_or_equal_than_2_4": [],
  "is_torch_greater_or_equal_than_2_3": [],
  "is_torch_greater_or_equal_than_2_2": [],
  "is_torch_greater_or_equal_than_2_1": [],
  "is_torch_greater_or_equal_than_2_0": [],
  "is_torch_greater_or_equal_than_1_13": [],
  "is_torch_greater_or_equal_than_1_12": [],
  "softmax_backward_data": [
    "parent",
    "grad_output",
    "output"
  ],
  "prune_linear_layer": [
    "layer",
    "index",
    "dim"
  ],
  "Conv1D": {
    "__init__": [
      "self",
      "nf",
      "nx"
    ],
    "__repr__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "apply_chunking_to_forward": [
    "forward_fn",
    "chunk_size",
    "chunk_dim"
  ],
  "meshgrid": [],
  "id_tensor_storage": [
    "tensor"
  ],
  "compile_compatible_method_lru_cache": [],
  "_is_torch_greater_or_equal_than_2_7": [],
  "CacheLayerMixin": {
    "is_compileable": [],
    "__init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "lazy_initialization": [
      "self",
      "key_states",
      "value_states"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "get_seq_length": [
      "self"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "offload": [
      "self"
    ],
    "prefetch": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ]
  },
  "DynamicLayer": {
    "is_sliding": [],
    "lazy_initialization": [
      "self",
      "key_states",
      "value_states"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "get_seq_length": [
      "self"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "crop": [
      "self",
      "max_length"
    ],
    "batch_repeat_interleave": [
      "self",
      "repeats"
    ],
    "batch_select_indices": [
      "self",
      "indices"
    ]
  },
  "DynamicSlidingWindowLayer": {
    "is_sliding": [],
    "__init__": [
      "self",
      "sliding_window"
    ],
    "lazy_initialization": [
      "self",
      "key_states",
      "value_states"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "get_seq_length": [
      "self"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "crop": [
      "self",
      "max_length"
    ]
  },
  "StaticLayer": {
    "is_compileable": [],
    "is_sliding": [],
    "__init__": [
      "self",
      "max_cache_len"
    ],
    "lazy_initialization": [
      "self",
      "key_states",
      "value_states"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "get_seq_length": [
      "self"
    ],
    "get_max_cache_shape": [
      "self"
    ]
  },
  "StaticSlidingWindowLayer": {
    "is_sliding": [],
    "__init__": [
      "self",
      "max_cache_len",
      "sliding_window"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "get_seq_length": [
      "self"
    ]
  },
  "QuantizedLayer": {
    "__init__": [
      "self",
      "nbits",
      "axis_key",
      "axis_value",
      "q_group_size",
      "residual_length"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "_quantize": [
      "self",
      "tensor",
      "axis"
    ],
    "_dequantize": [
      "self",
      "q_tensor"
    ],
    "get_seq_length": [
      "self"
    ]
  },
  "QuantoQuantizedLayer": {
    "__init__": [
      "self",
      "nbits",
      "axis_key",
      "axis_value",
      "q_group_size",
      "residual_length"
    ],
    "_quantize": [
      "self",
      "tensor",
      "axis"
    ],
    "_dequantize": [
      "self",
      "qtensor"
    ]
  },
  "HQQQuantizedLayer": {
    "__init__": [
      "self",
      "nbits",
      "axis_key",
      "axis_value",
      "q_group_size",
      "residual_length"
    ],
    "_quantize": [
      "self",
      "tensor",
      "axis"
    ],
    "_dequantize": [
      "self",
      "qtensor"
    ]
  },
  "Cache": {
    "__init__": [
      "self",
      "layers",
      "layer_class_to_replicate",
      "offloading",
      "offload_only_non_sliding"
    ],
    "__repr__": [
      "self"
    ],
    "prefetch": [
      "self",
      "layer_idx",
      "only_non_sliding"
    ],
    "offload": [
      "self",
      "layer_idx",
      "only_non_sliding"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "early_initialization": [
      "self",
      "batch_size",
      "num_heads",
      "head_dim",
      "dtype",
      "device"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position",
      "layer_idx"
    ],
    "get_max_cache_shape": [
      "self",
      "layer_idx"
    ],
    "reset": [
      "self"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ],
    "crop": [
      "self",
      "max_length"
    ],
    "batch_repeat_interleave": [
      "self",
      "repeats"
    ],
    "batch_select_indices": [
      "self",
      "indices"
    ],
    "max_batch_size": [
      "self"
    ],
    "max_cache_len": [
      "self"
    ],
    "is_compileable": [
      "self"
    ],
    "is_initialized": [
      "self"
    ],
    "is_sliding": [
      "self"
    ],
    "__len__": [
      "self"
    ]
  },
  "DynamicCache": {
    "__init__": [
      "self",
      "ddp_cache_data",
      "config",
      "offloading",
      "offload_only_non_sliding"
    ],
    "__iter__": [
      "self"
    ]
  },
  "StaticCache": {
    "__init__": [
      "self",
      "config",
      "max_cache_len",
      "offloading",
      "offload_only_non_sliding"
    ]
  },
  "QuantizedCache": {
    "__init__": [
      "self",
      "backend",
      "config",
      "nbits",
      "axis_key",
      "axis_value",
      "q_group_size",
      "residual_length"
    ]
  },
  "EncoderDecoderCache": {
    "__init__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "reset": [
      "self"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ],
    "check_dynamic_cache": [
      "self",
      "method"
    ],
    "crop": [
      "self",
      "maximum_length"
    ],
    "batch_split": [
      "self",
      "full_batch_size",
      "split_size"
    ],
    "batch_repeat_interleave": [
      "self",
      "repeats"
    ],
    "batch_select_indices": [
      "self",
      "indices"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position",
      "layer_idx"
    ],
    "is_sliding": [
      "self"
    ],
    "is_compileable": [
      "self"
    ]
  },
  "TrainerState": {
    "__post_init__": [
      "self"
    ],
    "save_to_json": [
      "self",
      "json_path"
    ],
    "load_from_json": [
      "cls",
      "json_path"
    ],
    "compute_steps": [
      "self",
      "args",
      "max_steps"
    ],
    "init_training_references": [
      "self",
      "trainer",
      "max_steps",
      "num_train_epochs",
      "trial"
    ]
  },
  "ExportableState": {
    "state": [
      "self"
    ],
    "from_state": [
      "cls",
      "state"
    ]
  },
  "TrainerControl": {
    "_new_training": [
      "self"
    ],
    "_new_epoch": [
      "self"
    ],
    "_new_step": [
      "self"
    ],
    "state": [
      "self"
    ]
  },
  "TrainerCallback": {
    "on_init_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_epoch_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_epoch_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_pre_optimizer_step": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_optimizer_step": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_substep_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_evaluate": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_prediction_step": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_push_begin": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "CallbackHandler": {
    "__init__": [
      "self",
      "callbacks",
      "model",
      "processing_class",
      "optimizer",
      "lr_scheduler"
    ],
    "add_callback": [
      "self",
      "callback"
    ],
    "pop_callback": [
      "self",
      "callback"
    ],
    "remove_callback": [
      "self",
      "callback"
    ],
    "callback_list": [
      "self"
    ],
    "on_init_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_epoch_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_epoch_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_pre_optimizer_step": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_optimizer_step": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_substep_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_evaluate": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs"
    ],
    "on_prediction_step": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_push_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "call_event": [
      "self",
      "event",
      "args",
      "state",
      "control"
    ]
  },
  "DefaultFlowCallback": {
    "on_step_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_epoch_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "ProgressCallback": {
    "__init__": [
      "self",
      "max_str_len"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_prediction_step": [
      "self",
      "args",
      "state",
      "control",
      "eval_dataloader"
    ],
    "on_evaluate": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "PrinterCallback": {
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs"
    ]
  },
  "EarlyStoppingCallback": {
    "__init__": [
      "self",
      "early_stopping_patience",
      "early_stopping_threshold"
    ],
    "check_metric_value": [
      "self",
      "args",
      "state",
      "control",
      "metric_value"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_evaluate": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ],
    "state": [
      "self"
    ]
  },
  "URL": [],
  "Path": [],
  "VideoInput": [],
  "VideoMetadata": {
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "item"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ],
    "timestamps": [
      "self"
    ],
    "sampled_fps": [
      "self"
    ],
    "update": [
      "self",
      "dictionary"
    ]
  },
  "VideoMetadataType": [],
  "is_valid_video_frame": [
    "frame"
  ],
  "is_valid_video": [
    "video"
  ],
  "valid_videos": [
    "videos"
  ],
  "is_batched_video": [
    "videos"
  ],
  "is_scaled_video": [
    "video"
  ],
  "convert_pil_frames_to_video": [
    "videos"
  ],
  "make_batched_videos": [
    "videos"
  ],
  "make_batched_metadata": [
    "videos",
    "video_metadata"
  ],
  "get_video_size": [
    "video",
    "channel_dim"
  ],
  "get_uniform_frame_indices": [
    "total_num_frames",
    "num_frames"
  ],
  "default_sample_indices_fn": [
    "metadata",
    "num_frames",
    "fps"
  ],
  "read_video_opencv": [
    "video_path",
    "sample_indices_fn"
  ],
  "read_video_decord": [
    "video_path",
    "sample_indices_fn"
  ],
  "read_video_pyav": [
    "video_path",
    "sample_indices_fn"
  ],
  "read_video_torchvision": [
    "video_path",
    "sample_indices_fn"
  ],
  "read_video_torchcodec": [
    "video_path",
    "sample_indices_fn"
  ],
  "VIDEO_DECODERS": [],
  "load_video": [
    "video",
    "num_frames",
    "fps",
    "backend",
    "sample_indices_fn"
  ],
  "group_videos_by_shape": [
    "videos"
  ],
  "reorder_videos": [
    "processed_videos",
    "grouped_videos_index"
  ],
  "Trie": {
    "__init__": [
      "self"
    ],
    "update": [
      "self"
    ],
    "add": [
      "self",
      "word"
    ],
    "split": [
      "self",
      "text"
    ],
    "cut_text": [
      "self",
      "text",
      "offsets"
    ]
  },
  "ExtensionsTrie": {
    "__init__": [
      "self"
    ],
    "extensions": [
      "self",
      "prefix"
    ],
    "_get_node": [
      "self",
      "token"
    ],
    "_collect_tokens": [
      "self",
      "node"
    ]
  },
  "_is_whitespace": [
    "char"
  ],
  "_is_control": [
    "char"
  ],
  "_is_punctuation": [
    "char"
  ],
  "_is_end_of_word": [
    "text"
  ],
  "_is_start_of_word": [
    "text"
  ],
  "_insert_one_token_to_ordered_list": [
    "token_list",
    "new_token"
  ],
  "PythonBackend": {
    "__init__": [
      "self"
    ],
    "is_fast": [
      "self"
    ],
    "added_tokens_encoder": [
      "self"
    ],
    "added_tokens_decoder": [
      "self",
      "value"
    ],
    "get_added_vocab": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "_update_total_vocab_size": [
      "self"
    ],
    "_add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "_update_trie": [
      "self",
      "unique_no_split_tokens"
    ],
    "num_special_tokens_to_add": [
      "self",
      "pair"
    ],
    "tokenize": [
      "self",
      "text"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id_with_added_voc": [
      "self",
      "token"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose"
    ],
    "prepare_for_tokenization": [
      "self",
      "text",
      "is_split_into_words"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "prepare_for_model": [
      "self",
      "ids",
      "pair_ids",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose",
      "prepend_batch_axis"
    ],
    "truncate_sequences": [
      "self",
      "ids",
      "pair_ids",
      "num_tokens_to_remove",
      "truncation_strategy",
      "stride"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "PreTrainedTokenizer": [],
  "SMALL_MODEL_IDENTIFIER": [],
  "DUMMY_UNKNOWN_IDENTIFIER": [],
  "DUMMY_DIFF_TOKENIZER_IDENTIFIER": [],
  "USER": [],
  "ENDPOINT_STAGING": [],
  "TOKEN": [],
  "_COMMON_MODEL_NAMES_MAP": [],
  "parse_flag_from_env": [
    "key",
    "default"
  ],
  "parse_int_from_env": [
    "key",
    "default"
  ],
  "_run_slow_tests": [],
  "_run_flaky_tests": [],
  "_run_custom_tokenizers": [],
  "_run_staging": [],
  "_run_pipeline_tests": [],
  "_run_agent_tests": [],
  "_run_training_tests": [],
  "is_staging_test": [
    "test_case"
  ],
  "is_pipeline_test": [
    "test_case"
  ],
  "is_agent_test": [
    "test_case"
  ],
  "is_training_test": [
    "test_case"
  ],
  "slow": [
    "test_case"
  ],
  "tooslow": [
    "test_case"
  ],
  "skip_if_not_implemented": [
    "test_func"
  ],
  "apply_skip_if_not_implemented": [
    "cls"
  ],
  "custom_tokenizers": [
    "test_case"
  ],
  "require_bs4": [
    "test_case"
  ],
  "require_galore_torch": [
    "test_case"
  ],
  "require_apollo_torch": [
    "test_case"
  ],
  "require_torch_optimi": [
    "test_case"
  ],
  "require_lomo": [
    "test_case"
  ],
  "require_grokadamw": [
    "test_case"
  ],
  "require_schedulefree": [
    "test_case"
  ],
  "require_cv2": [
    "test_case"
  ],
  "require_levenshtein": [
    "test_case"
  ],
  "require_nltk": [
    "test_case"
  ],
  "require_accelerate": [
    "test_case",
    "min_version"
  ],
  "require_triton": [
    "min_version"
  ],
  "require_gguf": [
    "test_case",
    "min_version"
  ],
  "require_fsdp": [
    "test_case",
    "min_version"
  ],
  "require_g2p_en": [
    "test_case"
  ],
  "require_rjieba": [
    "test_case"
  ],
  "require_jinja": [
    "test_case"
  ],
  "require_jmespath": [
    "test_case"
  ],
  "require_onnx": [
    "test_case"
  ],
  "require_timm": [
    "test_case"
  ],
  "require_natten": [
    "test_case"
  ],
  "require_torch": [
    "test_case"
  ],
  "require_torch_greater_or_equal": [
    "version"
  ],
  "require_huggingface_hub_greater_or_equal": [
    "version"
  ],
  "require_flash_attn": [
    "test_case"
  ],
  "require_kernels": [
    "test_case"
  ],
  "require_flash_attn_3": [
    "test_case"
  ],
  "require_peft": [
    "test_case"
  ],
  "require_torchvision": [
    "test_case"
  ],
  "require_torchcodec": [
    "test_case"
  ],
  "require_torchaudio": [
    "test_case"
  ],
  "require_sentencepiece": [
    "test_case"
  ],
  "require_sacremoses": [
    "test_case"
  ],
  "require_seqio": [
    "test_case"
  ],
  "require_scipy": [
    "test_case"
  ],
  "require_tokenizers": [
    "test_case"
  ],
  "require_pandas": [
    "test_case"
  ],
  "require_pytesseract": [
    "test_case"
  ],
  "require_pytorch_quantization": [
    "test_case"
  ],
  "require_vision": [
    "test_case"
  ],
  "require_spacy": [
    "test_case"
  ],
  "require_torch_multi_gpu": [
    "test_case"
  ],
  "require_torch_multi_accelerator": [
    "test_case"
  ],
  "require_torch_non_multi_gpu": [
    "test_case"
  ],
  "require_torch_non_multi_accelerator": [
    "test_case"
  ],
  "require_torch_up_to_2_gpus": [
    "test_case"
  ],
  "require_torch_up_to_2_accelerators": [
    "test_case"
  ],
  "require_torch_xla": [
    "test_case"
  ],
  "require_torch_neuroncore": [
    "test_case"
  ],
  "require_torch_npu": [
    "test_case"
  ],
  "require_torch_multi_npu": [
    "test_case"
  ],
  "require_non_hpu": [
    "test_case"
  ],
  "require_torch_xpu": [
    "test_case"
  ],
  "require_non_xpu": [
    "test_case"
  ],
  "require_torch_multi_xpu": [
    "test_case"
  ],
  "require_torch_multi_hpu": [
    "test_case"
  ],
  "require_torchao": [
    "test_case"
  ],
  "require_torchao_version_greater_or_equal": [
    "torchao_version"
  ],
  "require_torch_tensorrt_fx": [
    "test_case"
  ],
  "require_torch_gpu": [
    "test_case"
  ],
  "require_torch_mps": [
    "test_case"
  ],
  "require_large_cpu_ram": [
    "test_case",
    "memory"
  ],
  "require_torch_large_gpu": [
    "test_case",
    "memory"
  ],
  "require_torch_large_accelerator": [
    "test_case"
  ],
  "require_torch_accelerator": [
    "test_case"
  ],
  "require_torch_fp16": [
    "test_case"
  ],
  "require_fp8": [
    "test_case"
  ],
  "require_torch_bf16": [
    "test_case"
  ],
  "require_deterministic_for_xpu": [
    "test_case"
  ],
  "require_torch_tf32": [
    "test_case"
  ],
  "require_detectron2": [
    "test_case"
  ],
  "require_faiss": [
    "test_case"
  ],
  "require_optuna": [
    "test_case"
  ],
  "require_ray": [
    "test_case"
  ],
  "require_swanlab": [
    "test_case"
  ],
  "require_trackio": [
    "test_case"
  ],
  "require_wandb": [
    "test_case"
  ],
  "require_clearml": [
    "test_case"
  ],
  "require_deepspeed": [
    "test_case"
  ],
  "require_apex": [
    "test_case"
  ],
  "require_aqlm": [
    "test_case"
  ],
  "require_vptq": [
    "test_case"
  ],
  "require_spqr": [
    "test_case"
  ],
  "require_av": [
    "test_case"
  ],
  "require_decord": [
    "test_case"
  ],
  "require_bitsandbytes": [
    "test_case"
  ],
  "require_optimum": [
    "test_case"
  ],
  "require_tensorboard": [
    "test_case"
  ],
  "require_gptqmodel": [
    "test_case"
  ],
  "require_hqq": [
    "test_case"
  ],
  "require_auto_round": [
    "test_case"
  ],
  "require_optimum_quanto": [
    "test_case"
  ],
  "require_compressed_tensors": [
    "test_case"
  ],
  "require_fbgemm_gpu": [
    "test_case"
  ],
  "require_quark": [
    "test_case"
  ],
  "require_flute_hadamard": [
    "test_case"
  ],
  "require_fp_quant": [
    "test_case"
  ],
  "require_qutlass": [
    "test_case"
  ],
  "require_phonemizer": [
    "test_case"
  ],
  "require_pyctcdecode": [
    "test_case"
  ],
  "require_numba": [
    "test_case"
  ],
  "require_librosa": [
    "test_case"
  ],
  "require_liger_kernel": [
    "test_case"
  ],
  "require_essentia": [
    "test_case"
  ],
  "require_pretty_midi": [
    "test_case"
  ],
  "cmd_exists": [
    "cmd"
  ],
  "require_usr_bin_time": [
    "test_case"
  ],
  "require_sudachi": [
    "test_case"
  ],
  "require_sudachi_projection": [
    "test_case"
  ],
  "require_jumanpp": [
    "test_case"
  ],
  "require_cython": [
    "test_case"
  ],
  "require_tiktoken": [
    "test_case"
  ],
  "require_speech": [
    "test_case"
  ],
  "require_openai": [
    "test_case"
  ],
  "require_mistral_common": [
    "test_case"
  ],
  "get_gpu_count": [],
  "get_tests_dir": [
    "append_path"
  ],
  "get_steps_per_epoch": [
    "trainer"
  ],
  "evaluate_side_effect_factory": [
    "side_effect_values"
  ],
  "apply_print_resets": [
    "buf"
  ],
  "assert_screenout": [
    "out",
    "what"
  ],
  "set_config_for_less_flaky_test": [
    "config"
  ],
  "set_model_for_less_flaky_test": [
    "model"
  ],
  "CaptureStd": {
    "__init__": [
      "self",
      "out",
      "err",
      "replay"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "CaptureStdout": {
    "__init__": [
      "self",
      "replay"
    ]
  },
  "CaptureStderr": {
    "__init__": [
      "self",
      "replay"
    ]
  },
  "CaptureLogger": {
    "__init__": [
      "self",
      "logger"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "LoggingLevel": [
    "level"
  ],
  "TemporaryHubRepo": {
    "__init__": [
      "self",
      "namespace",
      "token"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc",
      "value",
      "tb"
    ]
  },
  "ExtendSysPath": [
    "path"
  ],
  "TestCasePlus": {
    "setUp": [
      "self"
    ],
    "test_file_path": [
      "self"
    ],
    "test_file_path_str": [
      "self"
    ],
    "test_file_dir": [
      "self"
    ],
    "test_file_dir_str": [
      "self"
    ],
    "tests_dir": [
      "self"
    ],
    "tests_dir_str": [
      "self"
    ],
    "examples_dir": [
      "self"
    ],
    "examples_dir_str": [
      "self"
    ],
    "repo_root_dir": [
      "self"
    ],
    "repo_root_dir_str": [
      "self"
    ],
    "src_dir": [
      "self"
    ],
    "src_dir_str": [
      "self"
    ],
    "get_env": [
      "self"
    ],
    "get_auto_remove_tmp_dir": [
      "self",
      "tmp_dir",
      "before",
      "after",
      "return_pathlib_obj"
    ],
    "python_one_liner_max_rss": [
      "self",
      "one_liner_str"
    ],
    "tearDown": [
      "self"
    ]
  },
  "mockenv": [],
  "mockenv_context": [],
  "pytest_opt_registered": [],
  "pytest_addoption_shared": [
    "parser"
  ],
  "pytest_terminal_summary_main": [
    "tr",
    "id"
  ],
  "_RunOutput": {
    "__init__": [
      "self",
      "returncode",
      "stdout",
      "stderr"
    ]
  },
  "_read_stream": [
    "stream",
    "callback"
  ],
  "_stream_subprocess": [
    "cmd",
    "env",
    "stdin",
    "timeout",
    "quiet",
    "echo"
  ],
  "execute_subprocess_async": [
    "cmd",
    "env",
    "stdin",
    "timeout",
    "quiet",
    "echo"
  ],
  "pytest_xdist_worker_id": [],
  "get_torch_dist_unique_port": [],
  "nested_simplify": [
    "obj",
    "decimals"
  ],
  "check_json_file_has_correct_format": [
    "file_path"
  ],
  "to_2tuple": [
    "x"
  ],
  "SubprocessCallException": {},
  "run_command": [
    "command",
    "return_stdout"
  ],
  "RequestCounter": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "total_calls": [
      "self"
    ]
  },
  "is_flaky": [
    "max_attempts",
    "wait_before_retry",
    "description"
  ],
  "hub_retry": [
    "max_attempts",
    "wait_before_retry"
  ],
  "run_first": [
    "test_case"
  ],
  "run_test_in_subprocess": [
    "test_case",
    "target_func",
    "inputs",
    "timeout"
  ],
  "run_test_using_subprocess": [
    "func"
  ],
  "preprocess_string": [
    "string",
    "skip_cuda_tests"
  ],
  "HfDocTestParser": {
    "_EXAMPLE_RE": [],
    "parse": [
      "self",
      "string",
      "name"
    ]
  },
  "HfDoctestModule": {
    "collect": [
      "self"
    ]
  },
  "_device_agnostic_dispatch": [
    "device",
    "dispatch_table"
  ],
  "backend_manual_seed": [
    "device",
    "seed"
  ],
  "backend_empty_cache": [
    "device"
  ],
  "backend_device_count": [
    "device"
  ],
  "backend_reset_max_memory_allocated": [
    "device"
  ],
  "backend_reset_peak_memory_stats": [
    "device"
  ],
  "backend_max_memory_allocated": [
    "device"
  ],
  "backend_memory_allocated": [
    "device"
  ],
  "backend_synchronize": [
    "device"
  ],
  "backend_torch_accelerator_module": [
    "device"
  ],
  "compare_pipeline_output_to_hub_spec": [
    "output",
    "hub_spec"
  ],
  "cleanup": [
    "device",
    "gc_collect"
  ],
  "DeviceProperties": [],
  "PackedDeviceProperties": [],
  "get_device_properties": [],
  "unpack_device_properties": [
    "properties"
  ],
  "Expectations": {
    "get_expectation": [
      "self"
    ],
    "unpacked": [
      "self"
    ],
    "is_default": [
      "expectation_key"
    ],
    "score": [
      "properties",
      "other"
    ],
    "find_expectation": [
      "self",
      "properties"
    ],
    "__repr__": [
      "self"
    ]
  },
  "patch_torch_compile_force_graph": [],
  "_get_test_info": [],
  "_get_call_arguments": [
    "code_context"
  ],
  "_prepare_debugging_info": [
    "test_info",
    "info"
  ],
  "_patched_tearDown": [
    "self"
  ],
  "_patch_with_call_info": [
    "module_or_class",
    "attr_name",
    "_parse_call_info_func",
    "target_args"
  ],
  "_parse_call_info": [
    "func",
    "args",
    "kwargs",
    "call_argument_expressions",
    "target_args"
  ],
  "patch_testing_methods_to_collect_info": [],
  "torchrun": [
    "script",
    "nproc_per_node",
    "is_torchrun",
    "env"
  ],
  "_format_tensor": [
    "t",
    "indent_level",
    "sci_mode"
  ],
  "_quote_string": [
    "s"
  ],
  "_format_py_obj": [
    "obj",
    "indent",
    "mode",
    "cache",
    "prefix"
  ],
  "write_file": [
    "file",
    "content"
  ],
  "read_json_file": [
    "file"
  ],
  "Colors": {
    "RESET": [],
    "BOLD": [],
    "DIM": [],
    "RED": [],
    "GREEN": [],
    "YELLOW": [],
    "BLUE": [],
    "MAGENTA": [],
    "CYAN": [],
    "WHITE": [],
    "BRIGHT_RED": [],
    "BRIGHT_GREEN": [],
    "BRIGHT_YELLOW": [],
    "BRIGHT_BLUE": [],
    "BRIGHT_CYAN": []
  },
  "ColoredFormatter": {
    "LEVEL_COLORS": [],
    "DIMMED_LOGGERS": [],
    "__init__": [
      "self",
      "fmt",
      "datefmt"
    ],
    "format": [
      "self",
      "record"
    ]
  },
  "init_test_logger": [],
  "warn_once": [
    "logger_instance",
    "msg"
  ],
  "MemoryStats": [],
  "CPUMemoryMonitor": {
    "__init__": [
      "self"
    ],
    "_to_gib": [
      "self",
      "memory_in_bytes"
    ],
    "_to_pct": [
      "self",
      "memory_in_bytes"
    ],
    "_update_peak": [
      "self"
    ],
    "get_stats": [
      "self"
    ],
    "reset_peak_stats": [
      "self"
    ]
  },
  "build_cpu_memory_monitor": [
    "logger_instance"
  ],
  "convert_all_safetensors_to_bins": [
    "folder"
  ],
  "force_serialization_as_bin_files": [],
  "AffineTransformed": {
    "__init__": [
      "self",
      "base_distribution",
      "loc",
      "scale",
      "event_dim"
    ],
    "mean": [
      "self"
    ],
    "variance": [
      "self"
    ],
    "stddev": [
      "self"
    ]
  },
  "ParameterProjection": {
    "__init__": [
      "self",
      "in_features",
      "args_dim",
      "domain_map"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LambdaLayer": {
    "__init__": [
      "self",
      "function"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DistributionOutput": {
    "__init__": [
      "self",
      "dim"
    ],
    "_base_distribution": [
      "self",
      "distr_args"
    ],
    "distribution": [
      "self",
      "distr_args",
      "loc",
      "scale"
    ],
    "event_shape": [
      "self"
    ],
    "event_dim": [
      "self"
    ],
    "value_in_support": [
      "self"
    ],
    "get_parameter_projection": [
      "self",
      "in_features"
    ],
    "domain_map": [
      "self"
    ],
    "squareplus": [
      "x"
    ]
  },
  "StudentTOutput": {
    "domain_map": [
      "cls",
      "df",
      "loc",
      "scale"
    ]
  },
  "NormalOutput": {
    "domain_map": [
      "cls",
      "loc",
      "scale"
    ]
  },
  "NegativeBinomialOutput": {
    "domain_map": [
      "cls",
      "total_count",
      "logits"
    ],
    "_base_distribution": [
      "self",
      "distr_args"
    ],
    "distribution": [
      "self",
      "distr_args",
      "loc",
      "scale"
    ]
  },
  "flash_attn_supports_top_left_mask": [],
  "is_flash_attn_available": [],
  "_loaded_implementation": [],
  "_flash_fn": [],
  "_flash_varlen_fn": [],
  "_pad_fn": [],
  "_unpad_fn": [],
  "_process_flash_kwargs_fn": [],
  "_hf_api_to_flash_mapping": [],
  "_lazy_imports": [
    "implementation",
    "attention_wrapper"
  ],
  "_lazy_define_process_function": [
    "flash_function"
  ],
  "lazy_import_flash_attention": [
    "implementation",
    "attention_wrapper"
  ],
  "lazy_import_paged_flash_attention": [
    "implementation"
  ],
  "_index_first_axis": [
    "tensor",
    "indices"
  ],
  "_unpad_input": [
    "hidden_states",
    "attention_mask",
    "unused_mask"
  ],
  "_pad_input": [
    "hidden_states",
    "indices",
    "batch",
    "seqlen"
  ],
  "_get_unpad_data": [
    "attention_mask"
  ],
  "_upad_input": [
    "query_layer",
    "key_layer",
    "value_layer",
    "attention_mask",
    "query_length",
    "unpad_input_func"
  ],
  "prepare_fa_kwargs_from_position_ids": [
    "position_ids"
  ],
  "_prepare_from_posids": [
    "query",
    "key",
    "value",
    "position_ids"
  ],
  "_is_packed_sequence": [
    "position_ids",
    "batch_size"
  ],
  "fa_peft_integration_check": [
    "q",
    "k",
    "v",
    "target_dtype"
  ],
  "FlashAttentionKwargs": {},
  "_process_flash_attention_kwargs": [
    "query_length",
    "key_length",
    "is_causal",
    "dropout",
    "softmax_scale",
    "sliding_window",
    "use_top_left_mask",
    "softcap",
    "deterministic",
    "s_aux",
    "supports_mapping"
  ],
  "_flash_attention_forward": [
    "query_states",
    "key_states",
    "value_states",
    "attention_mask",
    "query_length",
    "is_causal",
    "dropout",
    "position_ids",
    "softmax_scale",
    "sliding_window",
    "use_top_left_mask",
    "softcap",
    "deterministic",
    "cu_seq_lens_q",
    "cu_seq_lens_k",
    "max_length_q",
    "max_length_k",
    "target_dtype",
    "attn_implementation"
  ],
  "BASE_VIDEO_PROCESSOR_DOCSTRING": [],
  "BaseVideoProcessor": {
    "_auto_class": [],
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "size_divisor": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": [],
    "fps": [],
    "num_frames": [],
    "video_metadata": [],
    "return_metadata": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "videos"
    ],
    "convert_to_rgb": [
      "self",
      "video"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "num_frames",
      "fps"
    ],
    "_decode_and_sample_videos": [
      "self",
      "videos",
      "video_metadata",
      "do_sample_frames",
      "sample_indices_fn"
    ],
    "_prepare_input_videos": [
      "self",
      "videos",
      "input_data_format",
      "device"
    ],
    "preprocess": [
      "self",
      "videos"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir",
      "force_download",
      "local_files_only",
      "token",
      "revision"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "get_video_processor_dict": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "from_dict": [
      "cls",
      "video_processor_dict"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ],
    "__repr__": [
      "self"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "register_for_auto_class": [
      "cls",
      "auto_class"
    ],
    "fetch_videos": [
      "self",
      "video_url_or_urls",
      "sample_indices_fn"
    ]
  },
  "PreTrainedFeatureExtractor": [],
  "SpecificFeatureExtractorType": [],
  "FeatureExtractionMixin": {
    "_auto_class": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir",
      "force_download",
      "local_files_only",
      "token",
      "revision"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "get_feature_extractor_dict": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "from_dict": [
      "cls",
      "feature_extractor_dict"
    ],
    "to_dict": [
      "self"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ],
    "__repr__": [
      "self"
    ],
    "register_for_auto_class": [
      "cls",
      "auto_class"
    ]
  },
  "log_levels": [],
  "trainer_log_levels": [],
  "get_int_from_env": [
    "env_keys",
    "default"
  ],
  "get_xla_device_type": [
    "device"
  ],
  "OptimizerNames": {
    "ADAMW_TORCH": [],
    "ADAMW_TORCH_FUSED": [],
    "ADAMW_TORCH_XLA": [],
    "ADAMW_TORCH_NPU_FUSED": [],
    "ADAMW_APEX_FUSED": [],
    "ADAFACTOR": [],
    "ADAMW_ANYPRECISION": [],
    "ADAMW_TORCH_4BIT": [],
    "ADAMW_TORCH_8BIT": [],
    "ADEMAMIX": [],
    "SGD": [],
    "ADAGRAD": [],
    "ADAMW_BNB": [],
    "ADAMW_8BIT": [],
    "ADEMAMIX_8BIT": [],
    "LION_8BIT": [],
    "LION": [],
    "PAGED_ADAMW": [],
    "PAGED_ADAMW_8BIT": [],
    "PAGED_ADEMAMIX": [],
    "PAGED_ADEMAMIX_8BIT": [],
    "PAGED_LION": [],
    "PAGED_LION_8BIT": [],
    "RMSPROP": [],
    "RMSPROP_BNB": [],
    "RMSPROP_8BIT": [],
    "RMSPROP_32BIT": [],
    "GALORE_ADAMW": [],
    "GALORE_ADAMW_8BIT": [],
    "GALORE_ADAFACTOR": [],
    "GALORE_ADAMW_LAYERWISE": [],
    "GALORE_ADAMW_8BIT_LAYERWISE": [],
    "GALORE_ADAFACTOR_LAYERWISE": [],
    "LOMO": [],
    "ADALOMO": [],
    "GROKADAMW": [],
    "SCHEDULE_FREE_RADAM": [],
    "SCHEDULE_FREE_ADAMW": [],
    "SCHEDULE_FREE_SGD": [],
    "APOLLO_ADAMW": [],
    "APOLLO_ADAMW_LAYERWISE": [],
    "STABLE_ADAMW": []
  },
  "_convert_str_dict": [
    "passed_value"
  ],
  "TrainingArguments": {
    "_VALID_DICT_FIELDS": [],
    "default_optim": [],
    "__post_init__": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "__repr__": [],
    "train_batch_size": [
      "self"
    ],
    "eval_batch_size": [
      "self"
    ],
    "ddp_timeout_delta": [
      "self"
    ],
    "_setup_devices": [
      "self"
    ],
    "device": [
      "self"
    ],
    "n_gpu": [
      "self"
    ],
    "parallel_mode": [
      "self"
    ],
    "world_size": [
      "self"
    ],
    "process_index": [
      "self"
    ],
    "local_process_index": [
      "self"
    ],
    "should_log": [
      "self"
    ],
    "should_save": [
      "self"
    ],
    "get_process_log_level": [
      "self"
    ],
    "place_model_on_device": [
      "self"
    ],
    "_no_sync_in_gradient_accumulation": [
      "self"
    ],
    "main_process_first": [
      "self",
      "local",
      "desc"
    ],
    "get_warmup_steps": [
      "self",
      "num_training_steps"
    ],
    "_dict_dtype_to_str": [
      "self",
      "d"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "to_sanitized_dict": [
      "self"
    ],
    "set_training": [
      "self",
      "learning_rate",
      "batch_size",
      "weight_decay",
      "num_epochs",
      "max_steps",
      "gradient_accumulation_steps",
      "seed",
      "gradient_checkpointing"
    ],
    "set_evaluate": [
      "self",
      "strategy",
      "steps",
      "batch_size",
      "accumulation_steps",
      "delay",
      "loss_only"
    ],
    "set_testing": [
      "self",
      "batch_size",
      "loss_only"
    ],
    "set_save": [
      "self",
      "strategy",
      "steps",
      "total_limit",
      "on_each_node"
    ],
    "set_logging": [
      "self",
      "strategy",
      "steps",
      "report_to",
      "level",
      "first_step",
      "nan_inf_filter",
      "on_each_node",
      "replica_level"
    ],
    "set_push_to_hub": [
      "self",
      "model_id",
      "strategy",
      "token",
      "private_repo",
      "always_push",
      "revision"
    ],
    "set_optimizer": [
      "self",
      "name",
      "learning_rate",
      "weight_decay",
      "beta1",
      "beta2",
      "epsilon",
      "args"
    ],
    "set_lr_scheduler": [
      "self",
      "name",
      "num_epochs",
      "max_steps",
      "warmup_steps",
      "warmup_ratio"
    ],
    "set_dataloader": [
      "self",
      "train_batch_size",
      "eval_batch_size",
      "drop_last",
      "num_workers",
      "pin_memory",
      "persistent_workers",
      "prefetch_factor",
      "auto_find_batch_size",
      "ignore_data_skip",
      "sampler_seed"
    ],
    "_process_fsdp_args": [
      "self"
    ]
  },
  "ParallelMode": {
    "NOT_PARALLEL": [],
    "NOT_DISTRIBUTED": [],
    "DISTRIBUTED": [],
    "SAGEMAKER_MODEL_PARALLEL": [],
    "SAGEMAKER_DATA_PARALLEL": [],
    "TPU": []
  },
  "str_to_bool": [
    "value",
    "to_bool"
  ],
  "_sanitize_module_name": [
    "name"
  ],
  "_HF_REMOTE_CODE_LOCK": [],
  "init_hf_modules": [],
  "create_dynamic_module": [
    "name"
  ],
  "get_relative_imports": [
    "module_file"
  ],
  "get_relative_import_files": [
    "module_file"
  ],
  "get_imports": [
    "filename"
  ],
  "check_imports": [
    "filename"
  ],
  "get_class_in_module": [
    "class_name",
    "module_path"
  ],
  "get_cached_module_file": [
    "pretrained_model_name_or_path",
    "module_file",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only",
    "repo_type",
    "_commit_hash"
  ],
  "get_class_from_dynamic_module": [
    "class_reference",
    "pretrained_model_name_or_path",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only",
    "repo_type",
    "code_revision"
  ],
  "custom_object_save": [
    "obj",
    "folder",
    "config"
  ],
  "_raise_timeout_error": [
    "signum",
    "frame"
  ],
  "TIME_OUT_REMOTE_CODE": [],
  "resolve_trust_remote_code": [
    "trust_remote_code",
    "model_name",
    "has_local_code",
    "has_remote_code",
    "error_message",
    "upstream_repo"
  ],
  "check_python_requirements": [
    "path_or_repo_id",
    "requirements_file"
  ],
  "Seq2SeqTrainingArguments": {
    "to_dict": [
      "self"
    ]
  },
  "GradientCheckpointingLayer": {
    "gradient_checkpointing": [],
    "__call__": [
      "self"
    ]
  },
  "GenericForSequenceClassification": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache"
    ]
  },
  "GenericForQuestionAnswering": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "GenericForTokenClassification": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache"
    ]
  },
  "dynamic_rope_update": [
    "rope_forward"
  ],
  "_compute_linear_scaling_rope_parameters": [
    "config",
    "device",
    "seq_len",
    "layer_type"
  ],
  "_compute_dynamic_ntk_parameters": [
    "config",
    "device",
    "seq_len",
    "layer_type"
  ],
  "_compute_yarn_parameters": [
    "config",
    "device",
    "seq_len",
    "layer_type"
  ],
  "_compute_longrope_parameters": [
    "config",
    "device",
    "seq_len",
    "layer_type"
  ],
  "_compute_llama3_parameters": [
    "config",
    "device",
    "seq_len",
    "layer_type"
  ],
  "ROPE_INIT_FUNCTIONS": [],
  "RopeParameters": {},
  "RotaryEmbeddingConfigMixin": {
    "default_theta": [],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ],
    "standardize_rope_params": [
      "self"
    ],
    "validate_rope": [
      "self",
      "ignore_keys"
    ],
    "_validate_default_rope_parameters": [
      "self",
      "rope_parameters",
      "ignore_keys"
    ],
    "_validate_linear_rope_parameters": [
      "self",
      "rope_parameters",
      "ignore_keys"
    ],
    "_validate_dynamic_rope_parameters": [
      "self",
      "rope_parameters",
      "ignore_keys"
    ],
    "_validate_yarn_rope_parameters": [
      "self",
      "rope_parameters",
      "ignore_keys"
    ],
    "_validate_longrope_rope_parameters": [
      "self",
      "rope_parameters",
      "ignore_keys"
    ],
    "_validate_llama3_rope_parameters": [
      "self",
      "rope_parameters",
      "ignore_keys"
    ],
    "_check_received_keys": [
      "rope_type",
      "received_keys",
      "required_keys",
      "optional_keys",
      "ignore_keys"
    ]
  },
  "rope_config_validation": [
    "config",
    "ignore_keys"
  ],
  "import_protobuf_decode_error": [
    "error_message"
  ],
  "flatten": [
    "arr"
  ],
  "VERY_LARGE_INTEGER": [],
  "LARGE_INTEGER": [],
  "TextInput": [],
  "PreTokenizedInput": [],
  "EncodedInput": [],
  "TextInputPair": [],
  "PreTokenizedInputPair": [],
  "EncodedInputPair": [],
  "FULL_TOKENIZER_FILE": [],
  "_re_tokenizer_file": [],
  "TruncationStrategy": {
    "ONLY_FIRST": [],
    "ONLY_SECOND": [],
    "LONGEST_FIRST": [],
    "DO_NOT_TRUNCATE": []
  },
  "CharSpan": {},
  "TokenSpan": {},
  "BatchEncoding": {
    "__init__": [
      "self",
      "data",
      "encoding",
      "tensor_type",
      "prepend_batch_axis",
      "n_sequences"
    ],
    "n_sequences": [
      "self"
    ],
    "__getitem__": [
      "self",
      "item"
    ],
    "__getattr__": [
      "self",
      "item"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "is_fast": [
      "self"
    ],
    "encodings": [
      "self"
    ],
    "tokens": [
      "self",
      "batch_index"
    ],
    "sequence_ids": [
      "self",
      "batch_index"
    ],
    "word_ids": [
      "self",
      "batch_index"
    ],
    "token_to_sequence": [
      "self",
      "batch_or_token_index",
      "token_index"
    ],
    "token_to_word": [
      "self",
      "batch_or_token_index",
      "token_index"
    ],
    "word_to_tokens": [
      "self",
      "batch_or_word_index",
      "word_index",
      "sequence_index"
    ],
    "token_to_chars": [
      "self",
      "batch_or_token_index",
      "token_index"
    ],
    "char_to_token": [
      "self",
      "batch_or_char_index",
      "char_index",
      "sequence_index"
    ],
    "word_to_chars": [
      "self",
      "batch_or_word_index",
      "word_index",
      "sequence_index"
    ],
    "char_to_word": [
      "self",
      "batch_or_char_index",
      "char_index",
      "sequence_index"
    ],
    "convert_to_tensors": [
      "self",
      "tensor_type",
      "prepend_batch_axis"
    ],
    "to": [
      "self",
      "device"
    ]
  },
  "ENCODE_KWARGS_DOCSTRING": [],
  "ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING": [],
  "INIT_TOKENIZER_DOCSTRING": [],
  "PreTrainedTokenizerBase": {
    "slow_tokenizer_class": [],
    "SPECIAL_TOKENS_ATTRIBUTES": [],
    "__init__": [
      "self"
    ],
    "_set_processor_class": [
      "self",
      "processor_class"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens_dict",
      "replace_extra_special_tokens"
    ],
    "add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "_add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "pad_token_type_id": [
      "self"
    ],
    "__setattr__": [
      "self",
      "key",
      "value"
    ],
    "__getattr__": [
      "self",
      "key"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "special_tokens_map": [
      "self"
    ],
    "all_special_tokens": [
      "self"
    ],
    "all_special_ids": [
      "self"
    ],
    "_set_model_specific_special_tokens": [
      "self",
      "special_tokens"
    ],
    "added_tokens_decoder": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "convert_tokens_to_ids": [
      "self",
      "tokens"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "_from_pretrained": [
      "cls",
      "resolved_vocab_files",
      "pretrained_model_name_or_path",
      "init_configuration"
    ],
    "convert_to_native_format": [
      "cls"
    ],
    "convert_added_tokens": [
      "cls",
      "obj",
      "save",
      "add_type_field"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "legacy_format",
      "filename_prefix",
      "push_to_hub"
    ],
    "_save_pretrained": [
      "self",
      "save_directory",
      "file_names",
      "legacy_format",
      "filename_prefix"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "encode": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "padding_side",
      "return_tensors"
    ],
    "num_special_tokens_to_add": [
      "self",
      "pair"
    ],
    "max_len_single_sentence": [
      "self",
      "value"
    ],
    "max_len_sentences_pair": [
      "self",
      "value"
    ],
    "_get_padding_truncation_strategies": [
      "self",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "verbose"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "text_target",
      "text_pair_target",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "tokenizer_kwargs"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "split_special_tokens"
    ],
    "pad": [
      "self",
      "encoded_inputs",
      "padding",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask",
      "return_tensors",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "decode": [
      "self",
      "token_ids",
      "skip_special_tokens"
    ],
    "batch_decode": [
      "self",
      "sequences",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "_eventual_warn_about_too_long_sequence": [
      "self",
      "ids",
      "max_length",
      "verbose"
    ],
    "register_for_auto_class": [
      "cls",
      "auto_class"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "tools",
      "documents",
      "chat_template",
      "add_generation_prompt",
      "continue_final_message",
      "tokenize",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "return_dict",
      "return_assistant_tokens_mask",
      "tokenizer_kwargs"
    ],
    "encode_message_with_chat_template": [
      "self",
      "message",
      "conversation_history"
    ],
    "get_chat_template": [
      "self",
      "chat_template",
      "tools"
    ],
    "save_chat_templates": [
      "self",
      "save_directory",
      "tokenizer_config",
      "filename_prefix",
      "save_jinja_files"
    ],
    "parse_response": [
      "self",
      "response",
      "schema"
    ]
  },
  "get_fast_tokenizer_file": [
    "tokenization_files"
  ],
  "find_sentencepiece_model_file": [
    "pretrained_model_name_or_path"
  ],
  "load_vocab_and_merges": [
    "pretrained_model_name_or_path"
  ],
  "BackboneType": {
    "TIMM": [],
    "TRANSFORMERS": []
  },
  "BackboneConfigMixin": {
    "set_output_features_output_indices": [
      "self",
      "out_features",
      "out_indices"
    ],
    "verify_out_features_out_indices": [
      "self"
    ],
    "out_features": [
      "self",
      "out_features"
    ],
    "out_indices": [
      "self",
      "out_indices"
    ],
    "to_dict": [
      "self"
    ]
  },
  "BackboneMixin": {
    "__init__": [
      "self"
    ],
    "_init_timm_backbone": [
      "self",
      "backbone"
    ],
    "_init_transformers_backbone": [
      "self"
    ],
    "out_features": [
      "self",
      "out_features"
    ],
    "out_indices": [
      "self",
      "out_indices"
    ],
    "out_feature_channels": [
      "self"
    ],
    "channels": [
      "self"
    ],
    "forward_with_filtered_kwargs": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "consolidate_backbone_kwargs_to_config": [
    "backbone_config",
    "default_backbone",
    "default_config_type",
    "default_config_kwargs",
    "timm_default_kwargs"
  ],
  "load_backbone": [
    "config"
  ],
  "SpecificPreTrainedConfigType": [],
  "_FLOAT_TAG_KEY": [],
  "_FLOAT_TAG_VALUES": [],
  "PreTrainedConfig": {
    "__setattr__": [
      "self",
      "key",
      "value"
    ],
    "__getattribute__": [
      "self",
      "key"
    ],
    "__init__": [
      "self"
    ],
    "_create_id_label_maps": [
      "self",
      "num_labels"
    ],
    "name_or_path": [
      "self",
      "value"
    ],
    "output_attentions": [
      "self",
      "value"
    ],
    "use_return_dict": [
      "self"
    ],
    "num_labels": [
      "self",
      "num_labels"
    ],
    "_attn_implementation": [
      "self",
      "value"
    ],
    "_experts_implementation": [
      "self",
      "value"
    ],
    "torch_dtype": [
      "self",
      "value"
    ],
    "rope_scaling": [
      "self",
      "value"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir",
      "force_download",
      "local_files_only",
      "token",
      "revision"
    ],
    "get_config_dict": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "_get_config_dict": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "from_dict": [
      "cls",
      "config_dict"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "_dict_from_json_file": [
      "cls",
      "json_file"
    ],
    "_encode_special_floats": [
      "cls",
      "obj"
    ],
    "_decode_special_floats": [
      "cls",
      "obj"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__repr__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "to_diff_dict": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self",
      "use_diff"
    ],
    "to_json_file": [
      "self",
      "json_file_path",
      "use_diff"
    ],
    "update": [
      "self",
      "config_dict"
    ],
    "update_from_string": [
      "self",
      "update_str"
    ],
    "dict_dtype_to_str": [
      "self",
      "d"
    ],
    "_remove_keys_not_serialized": [
      "self",
      "d"
    ],
    "register_for_auto_class": [
      "cls",
      "auto_class"
    ],
    "_get_generation_parameters": [
      "self"
    ],
    "get_text_config": [
      "self",
      "decoder",
      "encoder"
    ]
  },
  "get_configuration_file": [
    "configuration_files"
  ],
  "recursive_diff_dict": [
    "dict_a",
    "dict_b",
    "config_obj"
  ],
  "PretrainedConfig": [],
  "ALLOWED_ATTENTION_LAYER_TYPES": [],
  "ALLOWED_MLP_LAYER_TYPES": [],
  "layer_type_validation": [
    "layer_types",
    "num_hidden_layers",
    "attention"
  ],
  "XLA_USE_BF16": [],
  "XLA_DOWNCAST_BF16": [],
  "SpecificPreTrainedModelType": [],
  "_is_quantized": [],
  "_is_ds_init_called": [],
  "FLASH_ATTN_KERNEL_FALLBACK": [],
  "LoadStateDictConfig": {
    "is_quantized": [
      "self"
    ]
  },
  "is_local_dist_rank_0": [],
  "set_quantized_state": [],
  "set_zero3_state": [],
  "local_torch_dtype": [
    "dtype",
    "model_class_name"
  ],
  "get_torch_context_manager_or_global_device": [],
  "get_state_dict_dtype": [
    "state_dict"
  ],
  "str_to_torch_dtype": [],
  "load_state_dict": [
    "checkpoint_file",
    "map_location",
    "weights_only"
  ],
  "_end_ptr": [
    "tensor"
  ],
  "_get_tied_weight_keys": [
    "module"
  ],
  "_find_disjoint": [
    "tensors",
    "state_dict"
  ],
  "_find_identical": [
    "tensors",
    "state_dict"
  ],
  "remove_tied_weights_from_state_dict": [
    "state_dict",
    "model"
  ],
  "_load_parameter_into_model": [
    "model",
    "param_name",
    "tensor"
  ],
  "_add_variant": [
    "weights_name",
    "variant"
  ],
  "_get_resolved_checkpoint_files": [
    "pretrained_model_name_or_path",
    "variant",
    "gguf_file",
    "use_safetensors",
    "user_agent",
    "is_remote_code",
    "transformers_explicit_filename",
    "download_kwargs"
  ],
  "_get_dtype": [
    "dtype",
    "checkpoint_files",
    "config",
    "sharded_metadata",
    "state_dict",
    "weights_only",
    "hf_quantizer"
  ],
  "PipelineParallel": {
    "inputs": [],
    "outputs": []
  },
  "ModuleUtilsMixin": {
    "device": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "invert_attention_mask": [
      "self",
      "encoder_attention_mask"
    ],
    "create_extended_attention_mask_for_decoder": [
      "input_shape",
      "attention_mask"
    ],
    "get_extended_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "dtype"
    ],
    "num_parameters": [
      "self",
      "only_trainable",
      "exclude_embeddings"
    ]
  },
  "EmbeddingAccessMixin": {
    "_input_embed_layer": [],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ]
  },
  "PreTrainedModel": {
    "_auto_class": [],
    "_tp_size": [],
    "can_record_outputs": [
      "self"
    ],
    "dummy_inputs": [
      "self"
    ],
    "__init_subclass__": [
      "cls"
    ],
    "__init__": [
      "self",
      "config"
    ],
    "post_init": [
      "self"
    ],
    "tp_plan": [
      "self",
      "plan"
    ],
    "pp_plan": [
      "self",
      "plan"
    ],
    "dequantize": [
      "self",
      "dtype"
    ],
    "_backward_compatibility_gradient_checkpointing": [
      "self"
    ],
    "add_model_tags": [
      "self",
      "tags"
    ],
    "_from_config": [
      "cls",
      "config"
    ],
    "base_model": [
      "self"
    ],
    "can_generate": [
      "cls"
    ],
    "_flash_attn_2_can_dispatch": [
      "self",
      "is_init_check"
    ],
    "_flash_attn_3_can_dispatch": [
      "self",
      "is_init_check"
    ],
    "_sdpa_can_dispatch": [
      "self",
      "is_init_check"
    ],
    "_grouped_mm_can_dispatch": [
      "self"
    ],
    "_flex_attn_can_dispatch": [
      "self",
      "is_init_check"
    ],
    "_check_and_adjust_attn_implementation": [
      "self",
      "attn_implementation",
      "is_init_check"
    ],
    "_check_and_adjust_experts_implementation": [
      "self",
      "experts_implementation"
    ],
    "get_correct_attn_implementation": [
      "self",
      "requested_attention",
      "is_init_check"
    ],
    "get_correct_experts_implementation": [
      "self",
      "requested_experts"
    ],
    "_can_set_attn_implementation": [
      "cls"
    ],
    "_can_set_experts_implementation": [
      "cls"
    ],
    "set_attn_implementation": [
      "self",
      "attn_implementation"
    ],
    "set_experts_implementation": [
      "self",
      "experts_implementation"
    ],
    "enable_input_require_grads": [
      "self"
    ],
    "disable_input_require_grads": [
      "self"
    ],
    "get_encoder": [
      "self",
      "modality"
    ],
    "set_encoder": [
      "self",
      "encoder",
      "modality"
    ],
    "get_decoder": [
      "self"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_initialize_weights": [
      "self",
      "module"
    ],
    "initialize_weights": [
      "self"
    ],
    "get_expanded_tied_weights_keys": [
      "self",
      "all_submodels"
    ],
    "tie_weights": [
      "self",
      "missing_keys",
      "recompute_mapping"
    ],
    "_adjust_bias": [
      "self",
      "output_embeddings",
      "input_embeddings"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_get_resized_embeddings": [
      "self",
      "old_embeddings",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_get_resized_lm_head": [
      "self",
      "old_lm_head",
      "new_num_tokens",
      "transposed",
      "mean_resizing"
    ],
    "_init_added_embeddings_weights_with_mean": [
      "self",
      "old_embeddings",
      "new_embeddings",
      "old_num_tokens",
      "added_num_tokens"
    ],
    "_init_added_lm_head_weights_with_mean": [
      "self",
      "old_lm_head",
      "new_lm_head",
      "old_lm_head_dim",
      "old_num_tokens",
      "added_num_tokens",
      "transposed"
    ],
    "_init_added_lm_head_bias_with_mean": [
      "self",
      "old_lm_head",
      "new_lm_head",
      "added_num_tokens"
    ],
    "_copy_lm_head_original_to_resized": [
      "self",
      "new_lm_head",
      "old_lm_head",
      "num_tokens_to_copy",
      "transposed",
      "has_new_lm_head_bias"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "init_weights": [
      "self"
    ],
    "gradient_checkpointing_enable": [
      "self",
      "gradient_checkpointing_kwargs"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "enable",
      "gradient_checkpointing_func"
    ],
    "gradient_checkpointing_disable": [
      "self"
    ],
    "is_gradient_checkpointing": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "is_main_process",
      "state_dict",
      "push_to_hub",
      "max_shard_size",
      "variant",
      "token",
      "save_peft_format",
      "save_original_format"
    ],
    "push_to_hub": [
      "self"
    ],
    "get_memory_footprint": [
      "self",
      "return_buffers"
    ],
    "cuda": [
      "self"
    ],
    "to": [
      "self"
    ],
    "half": [
      "self"
    ],
    "float": [
      "self"
    ],
    "get_init_context": [
      "cls",
      "dtype",
      "is_quantized",
      "_is_ds_init_called"
    ],
    "_get_dtype_plan": [
      "self",
      "dtype"
    ],
    "set_use_kernels": [
      "self",
      "use_kernels",
      "kernel_config"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "_load_pretrained_model": [
      "model",
      "state_dict",
      "checkpoint_files",
      "load_config"
    ],
    "_finalize_model_loading": [
      "model",
      "load_config",
      "loading_info"
    ],
    "retrieve_modules_from_names": [
      "self",
      "names",
      "add_prefix",
      "remove_prefix"
    ],
    "register_for_auto_class": [
      "cls",
      "auto_class"
    ],
    "warn_if_padding_and_no_attention_mask": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "supports_tp_plan": [
      "self"
    ],
    "tp_size": [
      "self"
    ],
    "supports_pp_plan": [
      "self"
    ],
    "loss_function": [
      "self",
      "value"
    ],
    "kernelize": [
      "self",
      "mode"
    ],
    "use_kernels": [
      "self",
      "value"
    ],
    "get_compiled_call": [
      "self",
      "compile_config"
    ],
    "is_backend_compatible": [
      "cls"
    ],
    "_move_missing_keys_from_meta_to_device": [
      "self",
      "missing_keys",
      "device_map",
      "device_mesh",
      "hf_quantizer"
    ],
    "_initialize_missing_keys": [
      "self",
      "is_quantized"
    ],
    "_adjust_missing_and_unexpected_keys": [
      "self",
      "loading_info"
    ],
    "mark_tied_weights_as_initialized": [
      "self"
    ],
    "get_parameter_or_buffer": [
      "self",
      "target"
    ],
    "named_non_persistent_buffers": [
      "self",
      "recurse",
      "remove_duplicate"
    ],
    "train": [
      "self",
      "mode"
    ],
    "eval": [
      "self"
    ]
  },
  "unwrap_model": [
    "model",
    "recursive"
  ],
  "is_accelerator_device": [
    "device"
  ],
  "get_total_byte_count": [
    "model",
    "accelerator_device_map",
    "hf_quantizer"
  ],
  "caching_allocator_warmup": [
    "model",
    "expanded_device_map",
    "hf_quantizer"
  ],
  "AttentionInterface": {
    "_global_mapping": [],
    "get_interface": [
      "self",
      "attn_implementation",
      "default"
    ]
  },
  "PreTrainedAudioTokenizerBase": {
    "encode": [
      "self",
      "input_values"
    ],
    "decode": [
      "self",
      "audio_codes"
    ]
  },
  "pkgs_to_check_at_runtime": [],
  "dep_version_check": [
    "pkg",
    "hint"
  ],
  "MistralTokenizerType": {
    "spm": [],
    "tekken": []
  },
  "_maybe_remove_lang": [
    "text",
    "skip_special_tokens"
  ],
  "_MAP_SPECIAL_TOKENS": [],
  "_VALID_INIT_KWARGS": [],
  "MistralCommonBackend": {
    "SPECIAL_TOKENS_ATTRIBUTES": [],
    "__init__": [
      "self",
      "tokenizer_path",
      "mode",
      "model_max_length",
      "padding_side",
      "truncation_side",
      "model_input_names",
      "clean_up_tokenization_spaces"
    ],
    "mode": [
      "self"
    ],
    "all_special_ids": [
      "self"
    ],
    "all_special_tokens": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "encode": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "verbose",
      "return_offsets_mapping",
      "split_special_tokens"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "batch_decode": [
      "self",
      "sequences",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "_tekken_piece_to_id": [
      "self",
      "piece",
      "warn"
    ],
    "_piece_to_id": [
      "self",
      "piece",
      "warn"
    ],
    "convert_tokens_to_ids": [
      "self",
      "tokens"
    ],
    "_text_to_ids": [
      "self",
      "text",
      "add_special_tokens"
    ],
    "tokenize": [
      "self",
      "text",
      "return_offsets_mapping",
      "split_special_tokens"
    ],
    "_get_all_special_ids": [
      "self"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose",
      "return_offsets_mapping",
      "split_special_tokens"
    ],
    "prepare_for_model": [
      "self",
      "ids",
      "pair_ids",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose",
      "prepend_batch_axis",
      "return_offsets_mapping",
      "split_special_tokens"
    ],
    "truncate_sequences": [
      "self",
      "ids",
      "pair_ids",
      "num_tokens_to_remove",
      "truncation_strategy",
      "stride"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "tools",
      "add_generation_prompt",
      "continue_final_message",
      "tokenize",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "return_dict"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "num_special_tokens_to_add": [
      "self",
      "pair"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "text_target",
      "text_pair_target",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose",
      "return_offsets_mapping",
      "split_special_tokens"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub",
      "token",
      "commit_message",
      "repo_id",
      "private"
    ],
    "_get_validation_mode": [
      "mode"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens_dict",
      "replace_extra_special_tokens"
    ],
    "add_tokens": [
      "self",
      "special_tokens_dict",
      "replace_extra_special_tokens"
    ],
    "convert_added_tokens": [
      "cls",
      "obj",
      "save",
      "add_type_field"
    ],
    "get_chat_template": [
      "self",
      "chat_template",
      "tools"
    ],
    "save_chat_templates": [
      "self",
      "save_directory",
      "tokenizer_config",
      "filename_prefix",
      "save_jinja_files"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "MistralCommonTokenizer": [],
  "get_dataloader_sampler": [
    "dataloader"
  ],
  "atleast_1d": [
    "tensor_or_array"
  ],
  "torch_pad_and_concatenate": [
    "tensor1",
    "tensor2",
    "padding_index"
  ],
  "numpy_pad_and_concatenate": [
    "array1",
    "array2",
    "padding_index"
  ],
  "nested_concat": [
    "tensors",
    "new_tensors",
    "padding_index"
  ],
  "find_batch_size": [
    "tensors"
  ],
  "nested_numpify": [
    "tensors"
  ],
  "nested_detach": [
    "tensors"
  ],
  "nested_xla_mesh_reduce": [
    "tensors",
    "name"
  ],
  "distributed_concat": [
    "tensor",
    "num_total_examples"
  ],
  "distributed_broadcast_scalars": [
    "scalars",
    "num_total_examples",
    "device"
  ],
  "reissue_pt_warnings": [
    "caught_warnings"
  ],
  "torch_distributed_zero_first": [
    "local_rank"
  ],
  "DistributedSamplerWithLoop": {
    "__init__": [
      "self",
      "dataset",
      "batch_size"
    ],
    "__iter__": [
      "self"
    ]
  },
  "EvalLoopContainer": {
    "__init__": [
      "self",
      "do_nested_concat",
      "padding_index"
    ],
    "add": [
      "self",
      "tensors"
    ],
    "to_cpu_and_numpy": [
      "self"
    ],
    "get_arrays": [
      "self"
    ]
  },
  "get_tpu_sampler": [
    "dataset",
    "batch_size"
  ],
  "nested_new_like": [
    "arrays",
    "num_samples",
    "padding_index"
  ],
  "expand_like": [
    "arrays",
    "new_seq_length",
    "padding_index"
  ],
  "nested_truncate": [
    "tensors",
    "limit"
  ],
  "LabelSmoother": {
    "__call__": [
      "self",
      "model_output",
      "labels",
      "shift_labels"
    ]
  },
  "get_length_grouped_indices": [
    "lengths",
    "batch_size",
    "mega_batch_mult",
    "generator"
  ],
  "LengthGroupedSampler": {
    "__init__": [
      "self",
      "batch_size",
      "dataset",
      "lengths",
      "model_input_name",
      "generator"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ]
  },
  "DistributedLengthGroupedSampler": {
    "__init__": [
      "self",
      "batch_size",
      "dataset",
      "num_replicas",
      "rank",
      "seed",
      "drop_last",
      "lengths",
      "model_input_name"
    ],
    "__iter__": [
      "self"
    ]
  },
  "ShardSampler": {
    "__init__": [
      "self",
      "dataset",
      "batch_size",
      "drop_last",
      "num_processes",
      "process_index"
    ],
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ]
  },
  "IterableDatasetShard": {
    "__init__": [
      "self",
      "dataset",
      "batch_size",
      "drop_last",
      "num_processes",
      "process_index",
      "seed"
    ],
    "set_epoch": [
      "self",
      "epoch"
    ],
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ]
  },
  "_get_learning_rate": [
    "self"
  ],
  "_secs2timedelta": [
    "secs"
  ],
  "metrics_format": [
    "metrics"
  ],
  "log_metrics": [
    "self",
    "split",
    "metrics"
  ],
  "save_metrics": [
    "self",
    "split",
    "metrics",
    "combined"
  ],
  "save_state": [
    "self"
  ],
  "get_model_param_count": [
    "model",
    "trainable_only"
  ],
  "get_parameter_names": [
    "model",
    "forbidden_layer_types",
    "forbidden_layer_names"
  ],
  "get_module_class_from_name": [
    "module",
    "name"
  ],
  "remove_dummy_checkpoint": [
    "is_main_process",
    "output_dir",
    "filenames"
  ],
  "AcceleratorConfig": {
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "to_dict": [
      "self"
    ],
    "pop": [
      "self",
      "key",
      "default"
    ]
  },
  "LayerWiseDummyOptimizer": {
    "__init__": [
      "self",
      "optimizer_dict"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "LayerWiseDummyScheduler": {
    "__init__": [
      "self"
    ],
    "get_lr": [
      "self"
    ],
    "_get_closed_form_lr": [
      "self"
    ]
  },
  "set_rng_state_for_device": [
    "device_name",
    "device_module",
    "checkpoint_rng_state",
    "is_distributed"
  ],
  "GGUF_TO_TRANSFORMERS_MAPPING": [],
  "GGUF_SUPPORTED_ARCHITECTURES": [],
  "GGUFTensor": {},
  "TensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "preprocess_name": [
      "self",
      "hf_name"
    ],
    "perform_fallback_tensor_mapping": [
      "self",
      "gguf_to_hf_name_map",
      "suffix",
      "qual_name",
      "hf_name"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ]
  },
  "LlamaTensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ],
    "_reverse_permute_weights": [
      "self",
      "weights",
      "n_head",
      "num_kv_heads"
    ]
  },
  "Qwen2MoeTensorProcessor": {
    "HF_EXPERT_RENAME_PATTERN": [],
    "HF_MOE_W13_PATTERN": [],
    "GGUF_MOE_WEIGHTS_PATTERN": [],
    "__init__": [
      "self",
      "config"
    ],
    "preprocess_name": [
      "self",
      "hf_name"
    ],
    "perform_fallback_tensor_mapping": [
      "self",
      "gguf_to_hf_name_map",
      "suffix",
      "qual_name",
      "hf_name"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ],
    "_set_moe_expert_tensor": [
      "self",
      "weights",
      "parsed_parameters",
      "hf_name",
      "w"
    ]
  },
  "BloomTensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ],
    "_reverse_reshape_weights": [
      "self",
      "weights",
      "n_head",
      "n_embed"
    ],
    "_reverse_reshape_bias": [
      "self",
      "weights",
      "n_head",
      "n_embed"
    ]
  },
  "T5TensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ]
  },
  "GPT2TensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ]
  },
  "MambaTensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ]
  },
  "NemotronTensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ]
  },
  "Gemma2TensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ]
  },
  "Lfm2TensorProcessor": {
    "__init__": [
      "self",
      "config"
    ],
    "process": [
      "self",
      "weights",
      "name"
    ]
  },
  "TENSOR_PROCESSORS": [],
  "read_field": [
    "reader",
    "field"
  ],
  "get_gguf_hf_weights_map": [
    "hf_model",
    "processor",
    "model_type",
    "num_layers",
    "qual_name"
  ],
  "load_gguf_checkpoint": [
    "gguf_checkpoint_path",
    "return_tensors",
    "model_to_load"
  ],
  "SPIECE_UNDERLINE": [],
  "SentencePieceBackend": {
    "vocab_files_names": [],
    "__init__": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "_update_trie": [
      "self",
      "unique_no_split_tokens"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "spaces_between_special_tokens"
    ]
  },
  "seed_worker": [
    "worker_id",
    "num_workers",
    "rank"
  ],
  "enable_full_determinism": [
    "seed",
    "warn_only"
  ],
  "set_seed": [
    "seed",
    "deterministic"
  ],
  "neftune_post_forward_hook": [
    "module",
    "input",
    "output"
  ],
  "EvalPrediction": {
    "__init__": [
      "self",
      "predictions",
      "label_ids",
      "inputs",
      "losses"
    ],
    "__iter__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "EvalLoopOutput": {},
  "PredictionOutput": {},
  "TrainOutput": {},
  "PREFIX_CHECKPOINT_DIR": [],
  "_re_checkpoint": [],
  "get_last_checkpoint": [
    "folder"
  ],
  "IntervalStrategy": {
    "NO": [],
    "STEPS": [],
    "EPOCH": []
  },
  "SaveStrategy": {
    "NO": [],
    "STEPS": [],
    "EPOCH": [],
    "BEST": []
  },
  "HubStrategy": {
    "END": [],
    "EVERY_SAVE": [],
    "CHECKPOINT": [],
    "ALL_CHECKPOINTS": []
  },
  "BestRun": {},
  "default_compute_objective": [
    "metrics"
  ],
  "default_hp_space_optuna": [
    "trial"
  ],
  "default_hp_space_ray": [
    "trial"
  ],
  "default_hp_space_wandb": [
    "trial"
  ],
  "HPSearchBackend": {
    "OPTUNA": [],
    "RAY": [],
    "WANDB": []
  },
  "is_main_process": [
    "local_rank"
  ],
  "total_processes_number": [
    "local_rank"
  ],
  "speed_metrics": [
    "split",
    "start_time",
    "num_samples",
    "num_steps",
    "num_tokens"
  ],
  "SchedulerType": {
    "LINEAR": [],
    "COSINE": [],
    "COSINE_WITH_RESTARTS": [],
    "POLYNOMIAL": [],
    "CONSTANT": [],
    "CONSTANT_WITH_WARMUP": [],
    "INVERSE_SQRT": [],
    "REDUCE_ON_PLATEAU": [],
    "COSINE_WITH_MIN_LR": [],
    "COSINE_WARMUP_WITH_MIN_LR": [],
    "WARMUP_STABLE_DECAY": []
  },
  "TrainerMemoryTracker": {
    "stages": [],
    "__init__": [
      "self",
      "skip_memory_metrics"
    ],
    "derive_stage": [
      "self"
    ],
    "cpu_mem_used": [
      "self"
    ],
    "peak_monitor_func": [
      "self"
    ],
    "start": [
      "self"
    ],
    "stop": [
      "self",
      "stage"
    ],
    "update_metrics": [
      "self",
      "stage",
      "metrics"
    ],
    "stop_and_update_metrics": [
      "self",
      "metrics"
    ]
  },
  "has_length": [
    "dataset"
  ],
  "denumpify_detensorize": [
    "metrics"
  ],
  "number_of_arguments": [
    "func"
  ],
  "find_executable_batch_size": [
    "function",
    "starting_batch_size",
    "auto_find_batch_size"
  ],
  "FSDPOption": {
    "FULL_SHARD": [],
    "SHARD_GRAD_OP": [],
    "NO_SHARD": [],
    "HYBRID_SHARD": [],
    "HYBRID_SHARD_ZERO2": [],
    "OFFLOAD": [],
    "AUTO_WRAP": []
  },
  "RemoveColumnsCollator": {
    "__init__": [
      "self",
      "data_collator",
      "signature_columns",
      "logger",
      "model_name",
      "description"
    ],
    "_remove_columns": [
      "self",
      "feature"
    ],
    "__call__": [
      "self",
      "features"
    ]
  },
  "check_target_module_exists": [
    "optim_target_modules",
    "key",
    "return_is_regex"
  ],
  "load_sharded_checkpoint": [
    "model",
    "folder",
    "strict",
    "prefer_safe"
  ],
  "DebugUnderflowOverflow": {
    "__init__": [
      "self",
      "model",
      "max_frames_to_save",
      "trace_batch_nums",
      "abort_after_batch_num"
    ],
    "save_frame": [
      "self",
      "frame"
    ],
    "expand_frame": [
      "self",
      "line"
    ],
    "trace_frames": [
      "self"
    ],
    "reset_saved_frames": [
      "self"
    ],
    "dump_saved_frames": [
      "self"
    ],
    "analyse_model": [
      "self"
    ],
    "analyse_variable": [
      "self",
      "var",
      "ctx"
    ],
    "batch_start_frame": [
      "self"
    ],
    "batch_end_frame": [
      "self"
    ],
    "create_frame": [
      "self",
      "module",
      "input",
      "output"
    ],
    "register_forward_hook": [
      "self"
    ],
    "_register_forward_hook": [
      "self",
      "module"
    ],
    "forward_hook": [
      "self",
      "module",
      "input",
      "output"
    ]
  },
  "get_abs_min_max": [
    "var",
    "ctx"
  ],
  "detect_overflow": [
    "var",
    "ctx"
  ],
  "DebugOption": {
    "UNDERFLOW_OVERFLOW": [],
    "TPU_METRICS_DEBUG": []
  },
  "Seq2SeqTrainer": {
    "__init__": [
      "self",
      "model",
      "args",
      "data_collator",
      "train_dataset",
      "eval_dataset",
      "processing_class",
      "model_init",
      "compute_loss_func",
      "compute_metrics",
      "callbacks",
      "optimizers",
      "preprocess_logits_for_metrics"
    ],
    "load_generation_config": [
      "gen_config_arg"
    ],
    "evaluate": [
      "self",
      "eval_dataset",
      "ignore_keys",
      "metric_key_prefix"
    ],
    "predict": [
      "self",
      "test_dataset",
      "ignore_keys",
      "metric_key_prefix"
    ],
    "prediction_step": [
      "self",
      "model",
      "inputs",
      "prediction_loss_only",
      "ignore_keys"
    ],
    "_pad_tensors_to_max_len": [
      "self",
      "tensor",
      "max_length"
    ]
  },
  "DataClass": [],
  "DataClassType": [],
  "string_to_bool": [
    "v"
  ],
  "make_choice_type_function": [
    "choices"
  ],
  "HfArg": [],
  "HfArgumentParser": {
    "__init__": [
      "self",
      "dataclass_types"
    ],
    "_parse_dataclass_field": [
      "parser",
      "field"
    ],
    "_add_dataclass_arguments": [
      "self",
      "dtype"
    ],
    "parse_args_into_dataclasses": [
      "self",
      "args",
      "return_remaining_strings",
      "look_for_args_file",
      "args_filename",
      "args_file_flag"
    ],
    "parse_dict": [
      "self",
      "args",
      "allow_extra_keys"
    ],
    "parse_json_file": [
      "self",
      "json_file",
      "allow_extra_keys"
    ],
    "parse_yaml_file": [
      "self",
      "yaml_file",
      "allow_extra_keys"
    ]
  },
  "BaseModelOutput": {},
  "BaseModelOutputWithNoAttention": {},
  "BaseModelOutputWithPooling": {},
  "BaseModelOutputWithPoolingAndNoAttention": {},
  "BaseModelOutputWithPast": {},
  "BaseModelOutputWithCrossAttentions": {},
  "BaseModelOutputWithPoolingAndCrossAttentions": {},
  "BaseModelOutputWithPastAndCrossAttentions": {},
  "MoECausalLMOutputWithPast": {},
  "MoEModelOutput": {},
  "MoeModelOutputWithPast": {},
  "MoeCausalLMOutputWithPast": {},
  "MoEModelOutputWithPastAndCrossAttentions": {},
  "Seq2SeqModelOutput": {},
  "Seq2SeqMoEModelOutput": {},
  "CausalLMOutput": {},
  "CausalLMOutputWithPast": {},
  "CausalLMOutputWithCrossAttentions": {},
  "SequenceClassifierOutputWithPast": {},
  "MaskedLMOutput": {},
  "Seq2SeqLMOutput": {},
  "Seq2SeqMoEOutput": {},
  "NextSentencePredictorOutput": {},
  "SequenceClassifierOutput": {},
  "Seq2SeqSequenceClassifierOutput": {},
  "MultipleChoiceModelOutput": {},
  "TokenClassifierOutput": {},
  "QuestionAnsweringModelOutput": {},
  "Seq2SeqQuestionAnsweringModelOutput": {},
  "SemanticSegmenterOutput": {},
  "ImageClassifierOutput": {},
  "ImageClassifierOutputWithNoAttention": {},
  "DepthEstimatorOutput": {},
  "ImageSuperResolutionOutput": {},
  "Wav2Vec2BaseModelOutput": {},
  "XVectorOutput": {},
  "BackboneOutput": {},
  "BaseModelOutputWithPoolingAndProjection": {},
  "Seq2SeqSpectrogramOutput": {},
  "Seq2SeqTSModelOutput": {},
  "Seq2SeqTSPredictionOutput": {},
  "SampleTSPredictionOutput": {},
  "MaskedImageModelingOutput": {},
  "HyperParamSearchBackendBase": {
    "is_available": [],
    "run": [
      "self",
      "trainer",
      "n_trials",
      "direction"
    ],
    "default_hp_space": [
      "self",
      "trial"
    ],
    "ensure_available": [
      "self"
    ],
    "pip_install": [
      "cls"
    ]
  },
  "OptunaBackend": {
    "name": [],
    "is_available": [],
    "run": [
      "self",
      "trainer",
      "n_trials",
      "direction"
    ],
    "default_hp_space": [
      "self",
      "trial"
    ]
  },
  "RayTuneBackend": {
    "name": [],
    "pip_package": [],
    "is_available": [],
    "run": [
      "self",
      "trainer",
      "n_trials",
      "direction"
    ],
    "default_hp_space": [
      "self",
      "trial"
    ]
  },
  "WandbBackend": {
    "name": [],
    "is_available": [],
    "run": [
      "self",
      "trainer",
      "n_trials",
      "direction"
    ],
    "default_hp_space": [
      "self",
      "trial"
    ]
  },
  "ALL_HYPERPARAMETER_SEARCH_BACKENDS": [],
  "default_hp_search_backend": [],
  "ImageInput": [],
  "ChannelDimension": {
    "FIRST": [],
    "LAST": []
  },
  "AnnotationFormat": {
    "COCO_DETECTION": [],
    "COCO_PANOPTIC": []
  },
  "AnnotationType": [],
  "is_pil_image": [
    "img"
  ],
  "ImageType": {
    "PIL": [],
    "TORCH": [],
    "NUMPY": []
  },
  "get_image_type": [
    "image"
  ],
  "is_valid_image": [
    "img"
  ],
  "is_valid_list_of_images": [
    "images"
  ],
  "concatenate_list": [
    "input_list"
  ],
  "valid_images": [
    "imgs"
  ],
  "is_batched": [
    "img"
  ],
  "is_scaled_image": [
    "image"
  ],
  "make_list_of_images": [
    "images",
    "expected_ndims"
  ],
  "make_flat_list_of_images": [
    "images",
    "expected_ndims"
  ],
  "make_nested_list_of_images": [
    "images",
    "expected_ndims"
  ],
  "to_numpy_array": [
    "img"
  ],
  "infer_channel_dimension_format": [
    "image",
    "num_channels"
  ],
  "get_channel_dimension_axis": [
    "image",
    "input_data_format"
  ],
  "get_image_size": [
    "image",
    "channel_dim"
  ],
  "get_image_size_for_max_height_width": [
    "image_size",
    "max_height",
    "max_width"
  ],
  "is_valid_annotation_coco_detection": [
    "annotation"
  ],
  "is_valid_annotation_coco_panoptic": [
    "annotation"
  ],
  "valid_coco_detection_annotations": [
    "annotations"
  ],
  "valid_coco_panoptic_annotations": [
    "annotations"
  ],
  "load_image": [
    "image",
    "timeout"
  ],
  "load_images": [
    "images",
    "timeout"
  ],
  "validate_preprocess_arguments": [
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "pad_size",
    "do_center_crop",
    "crop_size",
    "do_resize",
    "size",
    "resample",
    "interpolation"
  ],
  "ImageFeatureExtractionMixin": {
    "_ensure_format_supported": [
      "self",
      "image"
    ],
    "to_pil_image": [
      "self",
      "image",
      "rescale"
    ],
    "convert_rgb": [
      "self",
      "image"
    ],
    "rescale": [
      "self",
      "image",
      "scale"
    ],
    "to_numpy_array": [
      "self",
      "image",
      "rescale",
      "channel_first"
    ],
    "expand_dims": [
      "self",
      "image"
    ],
    "normalize": [
      "self",
      "image",
      "mean",
      "std",
      "rescale"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "default_to_square",
      "max_size"
    ],
    "center_crop": [
      "self",
      "image",
      "size"
    ],
    "flip_channel_order": [
      "self",
      "image"
    ],
    "rotate": [
      "self",
      "image",
      "angle",
      "resample",
      "expand",
      "center",
      "translate",
      "fillcolor"
    ]
  },
  "validate_annotations": [
    "annotation_format",
    "supported_annotation_formats",
    "annotations"
  ],
  "validate_kwargs": [
    "valid_processor_keys",
    "captured_kwargs"
  ],
  "SizeDict": {
    "__getitem__": [
      "self",
      "key"
    ]
  },
  "deps": [],
  "_set_aux_loss": [
    "outputs_class",
    "outputs_coord"
  ],
  "_set_aux_loss2": [
    "outputs_class",
    "outputs_coord",
    "outputs_corners",
    "outputs_ref",
    "teacher_corners",
    "teacher_logits"
  ],
  "weighting_function": [
    "max_num_bins",
    "up",
    "reg_scale"
  ],
  "translate_gt": [
    "gt",
    "max_num_bins",
    "reg_scale",
    "up"
  ],
  "bbox2distance": [
    "points",
    "bbox",
    "max_num_bins",
    "reg_scale",
    "up",
    "eps"
  ],
  "DFineLoss": {
    "__init__": [
      "self",
      "config"
    ],
    "unimodal_distribution_focal_loss": [
      "self",
      "pred",
      "label",
      "weight_right",
      "weight_left",
      "weight",
      "reduction",
      "avg_factor"
    ],
    "loss_local": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes",
      "T"
    ],
    "get_loss": [
      "self",
      "loss",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ]
  },
  "DFineForObjectDetectionLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "config",
    "outputs_class",
    "outputs_coord",
    "enc_topk_logits",
    "enc_topk_bboxes",
    "denoising_meta_values",
    "predicted_corners",
    "initial_reference_points"
  ],
  "RTDetrHungarianMatcher": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "outputs",
      "targets"
    ]
  },
  "RTDetrLoss": {
    "__init__": [
      "self",
      "config"
    ],
    "loss_labels_vfl": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes",
      "log"
    ],
    "loss_labels": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes",
      "log"
    ],
    "loss_cardinality": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_boxes": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_masks": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_labels_bce": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes",
      "log"
    ],
    "_get_source_permutation_idx": [
      "self",
      "indices"
    ],
    "_get_target_permutation_idx": [
      "self",
      "indices"
    ],
    "loss_labels_focal": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes",
      "log"
    ],
    "get_loss": [
      "self",
      "loss",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "get_cdn_matched_indices": [
      "dn_meta",
      "targets"
    ],
    "forward": [
      "self",
      "outputs",
      "targets"
    ]
  },
  "RTDetrForObjectDetectionLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "config",
    "outputs_class",
    "outputs_coord",
    "enc_topk_logits",
    "enc_topk_bboxes",
    "denoising_meta_values"
  ],
  "DeformableDetrHungarianMatcher": {
    "forward": [
      "self",
      "outputs",
      "targets"
    ]
  },
  "DeformableDetrImageLoss": {
    "__init__": [
      "self",
      "matcher",
      "num_classes",
      "focal_alpha",
      "losses"
    ],
    "loss_labels": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ]
  },
  "DeformableDetrForSegmentationLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "pred_masks",
    "config",
    "outputs_class",
    "outputs_coord"
  ],
  "DeformableDetrForObjectDetectionLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "config",
    "outputs_class",
    "outputs_coord"
  ],
  "LwDetrHungarianMatcher": {
    "forward": [
      "self",
      "outputs",
      "targets",
      "group_detr"
    ]
  },
  "LwDetrImageLoss": {
    "__init__": [
      "self",
      "matcher",
      "num_classes",
      "focal_alpha",
      "losses",
      "group_detr"
    ],
    "loss_labels": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_cardinality": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_boxes": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_masks": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "_get_source_permutation_idx": [
      "self",
      "indices"
    ],
    "_get_target_permutation_idx": [
      "self",
      "indices"
    ],
    "get_loss": [
      "self",
      "loss",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "forward": [
      "self",
      "outputs",
      "targets"
    ]
  },
  "LwDetrForObjectDetectionLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "config",
    "outputs_class",
    "outputs_coord",
    "enc_outputs_class",
    "enc_outputs_coord"
  ],
  "dice_loss": [
    "inputs",
    "targets",
    "num_boxes"
  ],
  "sigmoid_focal_loss": [
    "inputs",
    "targets",
    "num_boxes",
    "alpha",
    "gamma"
  ],
  "ImageLoss": {
    "__init__": [
      "self",
      "matcher",
      "num_classes",
      "eos_coef",
      "losses"
    ],
    "loss_labels": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_cardinality": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_boxes": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "loss_masks": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "_get_source_permutation_idx": [
      "self",
      "indices"
    ],
    "_get_target_permutation_idx": [
      "self",
      "indices"
    ],
    "get_loss": [
      "self",
      "loss",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ],
    "forward": [
      "self",
      "outputs",
      "targets"
    ]
  },
  "HungarianMatcher": {
    "__init__": [
      "self",
      "class_cost",
      "bbox_cost",
      "giou_cost"
    ],
    "forward": [
      "self",
      "outputs",
      "targets"
    ]
  },
  "_upcast": [
    "t"
  ],
  "box_area": [
    "boxes"
  ],
  "box_iou": [
    "boxes1",
    "boxes2"
  ],
  "generalized_box_iou": [
    "boxes1",
    "boxes2"
  ],
  "_max_by_axis": [
    "the_list"
  ],
  "NestedTensor": {
    "__init__": [
      "self",
      "tensors",
      "mask"
    ],
    "to": [
      "self",
      "device"
    ],
    "decompose": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "nested_tensor_from_tensor_list": [
    "tensor_list"
  ],
  "ForSegmentationLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "pred_masks",
    "config",
    "outputs_class",
    "outputs_coord"
  ],
  "ForObjectDetectionLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "config",
    "outputs_class",
    "outputs_coord"
  ],
  "fixed_cross_entropy": [
    "source",
    "target",
    "num_items_in_batch",
    "ignore_index"
  ],
  "ForCausalLMLoss": [
    "logits",
    "labels",
    "vocab_size",
    "num_items_in_batch",
    "ignore_index",
    "shift_labels"
  ],
  "ForMaskedLMLoss": [
    "logits",
    "labels",
    "vocab_size",
    "num_items_in_batch",
    "ignore_index"
  ],
  "ForSequenceClassificationLoss": [
    "labels",
    "pooled_logits",
    "config"
  ],
  "ForQuestionAnsweringLoss": [
    "start_logits",
    "end_logits",
    "start_positions",
    "end_positions"
  ],
  "ForTokenClassification": [
    "logits",
    "labels",
    "config"
  ],
  "LOSS_MAPPING": [],
  "GroundingDinoHungarianMatcher": {
    "forward": [
      "self",
      "outputs",
      "targets"
    ]
  },
  "GroundingDinoImageLoss": {
    "__init__": [
      "self",
      "matcher",
      "focal_alpha",
      "losses"
    ],
    "_get_target_classes_one_hot": [
      "self",
      "outputs",
      "targets",
      "indices"
    ],
    "loss_labels": [
      "self",
      "outputs",
      "targets",
      "indices",
      "num_boxes"
    ]
  },
  "GroundingDinoForObjectDetectionLoss": [
    "logits",
    "labels",
    "device",
    "pred_boxes",
    "config",
    "label_maps",
    "text_mask",
    "outputs_class",
    "outputs_coord",
    "encoder_logits",
    "encoder_pred_boxes"
  ],
  "CURRENT_YEAR": [],
  "REPO_PATH": [],
  "COPYRIGHT": [],
  "add_new_model_like": [
    "repo_path"
  ],
  "ModelInfos": {
    "__init__": [
      "self",
      "lowercase_name"
    ]
  },
  "add_content_to_file": [
    "file_name",
    "new_content",
    "add_after"
  ],
  "add_model_to_auto_mappings": [
    "repo_path",
    "old_model_infos",
    "new_lowercase_name",
    "new_model_paper_name",
    "filenames_to_add"
  ],
  "create_doc_file": [
    "new_paper_name",
    "public_classes"
  ],
  "insert_model_in_doc_toc": [
    "repo_path",
    "old_lowercase_name",
    "new_lowercase_name",
    "new_model_paper_name"
  ],
  "create_init_file": [
    "old_lowercase_name",
    "new_lowercase_name",
    "filenames_to_add"
  ],
  "find_all_classes_from_file": [
    "module_name"
  ],
  "find_modular_structure": [
    "module_name",
    "old_model_infos",
    "new_cased_name"
  ],
  "create_modular_file": [
    "repo_path",
    "old_model_infos",
    "new_lowercase_name",
    "filenames_to_add"
  ],
  "create_test_files": [
    "repo_path",
    "old_model_infos",
    "new_lowercase_name",
    "filenames_to_add"
  ],
  "_add_new_model_like_internal": [
    "repo_path",
    "old_model_infos",
    "new_lowercase_name",
    "new_model_paper_name",
    "filenames_to_add",
    "create_fast_image_processor"
  ],
  "get_user_field": [
    "question",
    "default_value",
    "convert_to",
    "fallback_message"
  ],
  "convert_to_bool": [
    "x"
  ],
  "get_user_input": [],
  "app": [],
  "main": [],
  "env": [
    "accelerate_config_file"
  ],
  "version": [],
  "_format_dict": [
    "d"
  ],
  "serve_dependencies_available": [],
  "_TOOL_CALL_TOKENS": [],
  "_MODELS_WITH_TOOL_SUPPORT": [],
  "X_REQUEST_ID": [],
  "set_torch_seed": [
    "_seed"
  ],
  "reset_torch_cache": [],
  "torch_ones_like": [
    "_input_tensor"
  ],
  "Modality": {
    "LLM": [],
    "VLM": [],
    "STT": [],
    "TTS": []
  },
  "create_generation_config_from_req": [
    "req",
    "model_generation_config"
  ],
  "ToolState": {
    "__init__": [
      "self"
    ],
    "reset": [
      "self"
    ]
  },
  "TimedModel": {
    "__init__": [
      "self",
      "model",
      "timeout_seconds",
      "processor"
    ],
    "reset_timer": [
      "self"
    ],
    "delete_model": [
      "self"
    ],
    "timeout_reached": [
      "self"
    ],
    "is_deleted": [
      "self"
    ]
  },
  "Serve": {
    "__init__": [
      "self",
      "continuous_batching",
      "device",
      "dtype",
      "trust_remote_code",
      "attn_implementation",
      "quantization",
      "host",
      "port",
      "model_timeout",
      "log_level",
      "default_seed",
      "enable_cors",
      "input_validation",
      "force_model",
      "non_blocking"
    ],
    "start_server": [
      "self"
    ],
    "kill_server": [
      "self"
    ],
    "_validate_request": [
      "self",
      "request",
      "schema",
      "validator",
      "unused_fields"
    ],
    "validate_response_request": [
      "self",
      "request"
    ],
    "validate_chat_completion_request": [
      "self",
      "request"
    ],
    "validate_transcription_request": [
      "self",
      "request"
    ],
    "build_chat_completion_chunk": [
      "self",
      "request_id",
      "content",
      "model",
      "role",
      "finish_reason",
      "tool_calls",
      "decode_stream",
      "tokenizer"
    ],
    "chunk_to_sse_element": [
      "chunk"
    ],
    "get_gen_models": [
      "cache_dir"
    ],
    "continuous_batching_chat_completion": [
      "self",
      "req",
      "request_id"
    ],
    "get_model_modality": [
      "model",
      "processor"
    ],
    "get_processor_inputs_from_inbound_messages": [
      "messages",
      "modality"
    ],
    "generate_chat_completion": [
      "self",
      "req"
    ],
    "generate_response": [
      "self",
      "req"
    ],
    "generate_response_non_streaming": [
      "self",
      "req"
    ],
    "generate_transcription": [
      "self",
      "req"
    ],
    "is_continuation": [
      "self",
      "req"
    ],
    "get_quantization_config": [
      "self"
    ],
    "process_model_name": [
      "self",
      "model_id"
    ],
    "_load_model_and_data_processor": [
      "self",
      "model_id_and_revision"
    ],
    "load_model_and_processor": [
      "self",
      "model_id_and_revision"
    ],
    "load_audio_model_and_processor": [
      "self",
      "model_id_and_revision"
    ]
  },
  "TRANSFORMERS_PATH": [],
  "add_fast_image_processor": [
    "model_name"
  ],
  "add_fast_image_processor_to_model_init": [
    "fast_image_processing_module_file",
    "fast_image_processor_name",
    "model_name"
  ],
  "add_fast_image_processor_to_auto": [
    "image_processor_name",
    "fast_image_processor_name"
  ],
  "add_fast_image_processor_to_doc": [
    "fast_image_processor_name",
    "model_name"
  ],
  "add_fast_image_processor_to_tests": [
    "fast_image_processor_name",
    "model_name"
  ],
  "get_fast_image_processing_content_header": [
    "content"
  ],
  "write_default_fast_image_processor_file": [
    "fast_image_processing_module_file",
    "fast_image_processor_name",
    "content_base_file"
  ],
  "add_fast_image_processor_file": [
    "fast_image_processing_module_file",
    "fast_image_processor_name",
    "content_base_file"
  ],
  "download": [
    "model_id",
    "cache_dir",
    "force_download",
    "trust_remote_code"
  ],
  "DEFAULT_HTTP_ENDPOINT": [],
  "ALLOWED_KEY_CHARS": [],
  "ALLOWED_VALUE_CHARS": [],
  "DEFAULT_EXAMPLES": [],
  "HELP_STRING_MINIMAL": [],
  "HELP_STRING": [],
  "RichInterface": {
    "__init__": [
      "self",
      "model_id",
      "user_id"
    ],
    "stream_output": [
      "self",
      "stream"
    ],
    "input": [
      "self"
    ],
    "clear": [
      "self"
    ],
    "print_user_message": [
      "self",
      "text"
    ],
    "print_color": [
      "self",
      "text",
      "color"
    ],
    "confirm": [
      "self",
      "message",
      "default"
    ],
    "print_help": [
      "self",
      "minimal"
    ],
    "print_status": [
      "self",
      "config"
    ]
  },
  "Chat": {
    "__init__": [
      "self",
      "model_id",
      "base_url",
      "generate_flags",
      "user",
      "system_prompt",
      "save_folder",
      "examples_path",
      "generation_config"
    ],
    "check_health": [
      "url"
    ],
    "handle_non_exit_user_commands": [
      "self",
      "user_input",
      "interface",
      "examples",
      "config",
      "chat"
    ],
    "_inner_run": [
      "self"
    ]
  },
  "load_generation_config": [
    "generation_config"
  ],
  "parse_generate_flags": [
    "generate_flags"
  ],
  "new_chat_history": [
    "system_prompt"
  ],
  "save_chat": [
    "filename",
    "chat",
    "settings"
  ],
  "get_username": [],
  "WatermarkDetectorOutput": {},
  "WatermarkDetector": {
    "__init__": [
      "self",
      "model_config",
      "device",
      "watermarking_config",
      "ignore_repeated_ngrams",
      "max_cache_size"
    ],
    "_get_ngram_score": [
      "self",
      "prefix",
      "target"
    ],
    "_score_ngrams_in_passage": [
      "self",
      "input_ids"
    ],
    "_compute_z_score": [
      "self",
      "green_token_count",
      "total_num_tokens"
    ],
    "_compute_pval": [
      "self",
      "x",
      "loc",
      "scale"
    ],
    "__call__": [
      "self",
      "input_ids",
      "z_threshold",
      "return_dict"
    ]
  },
  "BayesianDetectorConfig": {
    "__init__": [
      "self",
      "watermarking_depth",
      "base_rate"
    ],
    "set_detector_information": [
      "self",
      "model_name",
      "watermarking_config"
    ]
  },
  "BayesianWatermarkDetectorModelOutput": {},
  "BayesianDetectorWatermarkedLikelihood": {
    "__init__": [
      "self",
      "watermarking_depth"
    ],
    "_compute_latents": [
      "self",
      "g_values"
    ],
    "forward": [
      "self",
      "g_values"
    ]
  },
  "BayesianDetectorModel": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_compute_posterior": [
      "self",
      "likelihoods_watermarked",
      "likelihoods_unwatermarked",
      "mask",
      "prior"
    ],
    "forward": [
      "self",
      "g_values",
      "mask",
      "labels",
      "loss_batch_weight",
      "return_dict"
    ]
  },
  "SynthIDTextWatermarkDetector": {
    "__init__": [
      "self",
      "detector_module",
      "logits_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "tokenized_outputs"
    ]
  },
  "ALL_CACHE_NAMES": [],
  "GENERATION_MODES_MAPPING": [],
  "GenerateDecoderOnlyOutput": {},
  "GenerateEncoderDecoderOutput": {},
  "GenerateBeamDecoderOnlyOutput": {},
  "GenerateBeamEncoderDecoderOutput": {},
  "GenerateNonBeamOutput": [],
  "GenerateBeamOutput": [],
  "GenerateOutput": [],
  "GenerationMixin": {
    "output_modalities": [],
    "adjust_generation_fn": [
      "self",
      "generation_config",
      "from_auto_class",
      "from_pipeline",
      "pretrained_model_name_or_path",
      "cache_dir",
      "force_download",
      "proxies",
      "local_files_only",
      "token",
      "revision",
      "subfolder",
      "trust_remote_code"
    ],
    "load_custom_generate": [
      "self",
      "pretrained_model_name_or_path",
      "trust_remote_code"
    ],
    "_cache_dependant_input_preparation": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_position"
    ],
    "_cache_dependant_input_preparation_exporting": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_position"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "is_first_iteration"
    ],
    "_prepare_model_inputs": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "_maybe_initialize_input_ids_for_generation": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "_prepare_attention_mask_for_generation": [
      "self",
      "inputs_tensor",
      "generation_config",
      "model_kwargs"
    ],
    "_prepare_encoder_decoder_kwargs_for_generation": [
      "self",
      "inputs_tensor",
      "model_kwargs",
      "model_input_name",
      "generation_config"
    ],
    "_prepare_decoder_input_ids_for_generation": [
      "self",
      "batch_size",
      "model_input_name",
      "model_kwargs",
      "decoder_start_token_id",
      "device"
    ],
    "_expand_inputs_for_generation": [
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ],
    "_get_candidate_generator": [
      "self",
      "generation_config",
      "input_ids",
      "inputs_tensor",
      "logits_processor",
      "model_kwargs",
      "assistant_model",
      "target_tokenizer",
      "assistant_tokenizer"
    ],
    "_get_logits_processor": [
      "self",
      "generation_config",
      "input_ids_seq_length",
      "encoder_input_ids",
      "prefix_allowed_tokens_fn",
      "logits_processor",
      "device",
      "model_kwargs",
      "negative_prompt_ids",
      "negative_prompt_attention_mask"
    ],
    "_get_stopping_criteria": [
      "self",
      "generation_config",
      "stopping_criteria",
      "tokenizer"
    ],
    "_merge_criteria_processor_list": [
      "self",
      "default_list",
      "custom_list"
    ],
    "compute_transition_scores": [
      "self",
      "sequences",
      "scores",
      "beam_indices",
      "normalize_logits"
    ],
    "_validate_generation_mode": [
      "self",
      "generation_mode",
      "generation_config",
      "generation_mode_kwargs"
    ],
    "_validate_model_kwargs": [
      "self",
      "model_kwargs"
    ],
    "_validate_generated_length": [
      "self",
      "generation_config",
      "input_ids_length",
      "has_default_max_length"
    ],
    "_prepare_generated_length": [
      "self",
      "generation_config",
      "has_default_max_length",
      "has_default_min_length",
      "model_input_name",
      "input_ids_length",
      "inputs_tensor"
    ],
    "_prepare_generation_config": [
      "self",
      "generation_config"
    ],
    "_get_initial_cache_position": [
      "self",
      "seq_length",
      "device",
      "model_kwargs"
    ],
    "_prepare_static_cache": [
      "self",
      "cache_implementation",
      "batch_size",
      "max_cache_len",
      "model_kwargs"
    ],
    "_supports_default_dynamic_cache": [
      "cls"
    ],
    "_prepare_cache_for_generation": [
      "self",
      "generation_config",
      "model_kwargs",
      "generation_mode",
      "batch_size",
      "max_cache_length"
    ],
    "_supports_logits_to_keep": [
      "self"
    ],
    "_prepare_special_tokens": [
      "self",
      "generation_config",
      "kwargs_has_attention_mask",
      "device"
    ],
    "_valid_auto_compile_criteria": [
      "self",
      "model_kwargs",
      "generation_config"
    ],
    "_optimize_model_for_decode": [
      "self"
    ],
    "_get_deprecated_gen_repo": [
      "self",
      "generation_mode",
      "trust_remote_code",
      "custom_generate"
    ],
    "_extract_generation_mode_kwargs": [
      "self",
      "custom_generate",
      "kwargs",
      "synced_gpus",
      "assistant_model",
      "streamer"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus",
      "assistant_model",
      "streamer",
      "negative_prompt_ids",
      "negative_prompt_attention_mask",
      "custom_generate"
    ],
    "_has_unfinished_sequences": [
      "self",
      "this_peer_finished",
      "synced_gpus",
      "device"
    ],
    "heal_tokens": [
      "self",
      "input_ids",
      "tokenizer"
    ],
    "_sample": [
      "self",
      "input_ids",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus",
      "streamer"
    ],
    "_flatten_beam_dim": [
      "tensor"
    ],
    "_unflatten_beam_dim": [
      "tensor",
      "batch_size",
      "num_beams"
    ],
    "_gather_beams": [
      "tensor",
      "beam_indices"
    ],
    "_check_early_stop_heuristic": [
      "is_early_stop_heuristic_unsatisfied",
      "running_beam_scores",
      "beam_scores",
      "is_sent_finished",
      "cur_len",
      "max_length",
      "decoder_prompt_len",
      "early_stopping",
      "length_penalty"
    ],
    "_beam_search_has_unfinished_sequences": [
      "is_early_stop_heuristic_unsatisfied",
      "is_sent_finished",
      "next_token_hits_stopping_criteria",
      "early_stopping"
    ],
    "_get_top_k_continuations": [
      "self",
      "accumulated_log_probs",
      "running_sequences",
      "running_beam_indices",
      "cur_len",
      "decoder_prompt_len",
      "do_sample",
      "beams_to_keep",
      "num_beams",
      "vocab_size",
      "batch_size"
    ],
    "_get_running_beams_for_next_iteration": [
      "self",
      "topk_log_probs",
      "topk_running_sequences",
      "topk_running_beam_indices",
      "next_token_hits_stopping_criteria",
      "num_beams"
    ],
    "_update_finished_beams": [
      "self",
      "sequences",
      "topk_running_sequences",
      "beam_scores",
      "topk_log_probs",
      "beam_indices",
      "topk_running_beam_indices",
      "is_early_stop_heuristic_unsatisfied",
      "is_sent_finished",
      "next_token_hits_stopping_criteria",
      "top_num_beam_mask",
      "num_beams",
      "cur_len",
      "decoder_prompt_len",
      "length_penalty",
      "early_stopping"
    ],
    "_beam_search": [
      "self",
      "input_ids",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus"
    ],
    "_assisted_decoding": [
      "self",
      "input_ids",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus",
      "streamer",
      "inputs_tensor",
      "assistant_model",
      "assistant_tokenizer",
      "tokenizer"
    ],
    "_prefill": [
      "self",
      "input_ids",
      "generation_config",
      "model_kwargs"
    ]
  },
  "_speculative_sampling": [
    "candidate_input_ids",
    "candidate_logits",
    "candidate_length",
    "new_logits",
    "is_done_candidate"
  ],
  "_split_model_outputs": [
    "outputs",
    "new_outputs",
    "cur_len",
    "added_len",
    "is_decoder_attention"
  ],
  "BaseStreamer": {
    "put": [
      "self",
      "value"
    ],
    "end": [
      "self"
    ]
  },
  "TextStreamer": {
    "__init__": [
      "self",
      "tokenizer",
      "skip_prompt"
    ],
    "put": [
      "self",
      "value"
    ],
    "end": [
      "self"
    ],
    "on_finalized_text": [
      "self",
      "text",
      "stream_end"
    ],
    "_is_chinese_char": [
      "self",
      "cp"
    ]
  },
  "TextIteratorStreamer": {
    "__init__": [
      "self",
      "tokenizer",
      "skip_prompt",
      "timeout"
    ],
    "on_finalized_text": [
      "self",
      "text",
      "stream_end"
    ],
    "__iter__": [
      "self"
    ],
    "__next__": [
      "self"
    ]
  },
  "AsyncTextIteratorStreamer": {
    "__init__": [
      "self",
      "tokenizer",
      "skip_prompt",
      "timeout"
    ],
    "on_finalized_text": [
      "self",
      "text",
      "stream_end"
    ],
    "__aiter__": [
      "self"
    ],
    "__anext__": [
      "self"
    ]
  },
  "CandidateGenerator": {
    "get_candidates": [
      "self",
      "input_ids",
      "is_first_iteration"
    ],
    "update_candidate_strategy": [
      "self",
      "input_ids",
      "scores",
      "num_matches"
    ]
  },
  "AssistedCandidateGenerator": {
    "__init__": [
      "self",
      "input_ids",
      "assistant_model",
      "generation_config",
      "model_kwargs",
      "inputs_tensor",
      "logits_processor"
    ],
    "get_candidates": [
      "self",
      "input_ids",
      "is_first_iteration"
    ],
    "update_candidate_strategy": [
      "self",
      "input_ids",
      "scores",
      "num_matches"
    ],
    "_calculate_new_tokens": [
      "self",
      "input_ids"
    ],
    "_update_past_and_masks": [
      "self",
      "input_ids",
      "remove_from_pkv",
      "num_added_tokens"
    ],
    "_prepare_generation_args": [
      "self",
      "input_ids",
      "min_new_tokens",
      "max_new_tokens",
      "is_first_iteration"
    ],
    "_generate_candidates": [
      "self",
      "generation_args"
    ]
  },
  "AssistedCandidateGeneratorDifferentTokenizers": {
    "__init__": [
      "self",
      "input_ids",
      "assistant_model",
      "target_tokenizer",
      "assistant_tokenizer",
      "generation_config",
      "model_kwargs",
      "inputs_tensor",
      "logits_processor"
    ],
    "_get_longest_diag_dict": [
      "input_matrix",
      "nonzero_idx"
    ],
    "_get_longest_diag_index": [
      "input_matrix"
    ],
    "_get_tokens_diag": [
      "prompt",
      "prompt_plus_new_tokens"
    ],
    "convert_source_tokens_to_target_tokens": [
      "self",
      "input_ids",
      "source_tokenizer",
      "destination_tokenizer"
    ],
    "get_candidates": [
      "self",
      "input_ids",
      "is_first_iteration"
    ],
    "_prepare_assistant_input_ids": [
      "self",
      "input_ids"
    ],
    "_process_assistant_outputs": [
      "self",
      "input_ids",
      "assistant_sequences"
    ]
  },
  "_PruneReindexingLMHead": {
    "__init__": [
      "self",
      "original_lm_head",
      "assistant_overlap_token_ids"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "_MapInputEmbedding": {
    "__init__": [
      "self",
      "original_embedding",
      "assistant_overlap_token_ids"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "AssistantToTargetTranslator": {
    "__init__": [
      "self",
      "target_tokenizer",
      "assistant_tokenizer",
      "target_vocab_size",
      "assistant_model",
      "assistant_prune_lm_head"
    ],
    "unmap_input_ids": [
      "self"
    ],
    "_get_assistant_to_target_input_ids": [
      "self"
    ],
    "_get_suppress_input_ids": [
      "self"
    ],
    "get_target_ids": [
      "self",
      "assistant_input_ids",
      "target_input_ids",
      "assistant_candidate_ids"
    ],
    "get_target_logits": [
      "self",
      "assistant_logits"
    ]
  },
  "AssistantVocabTranslatorCache": {
    "_cache": [],
    "get_translator": [
      "cls",
      "target_tokenizer",
      "assistant_tokenizer",
      "target_vocab_size",
      "assistant_model",
      "assistant_prune_lm_head"
    ],
    "cleanup": [
      "cls"
    ]
  },
  "UniversalSpeculativeDecodingGenerator": {
    "__init__": [
      "self",
      "input_ids",
      "assistant_model",
      "target_tokenizer",
      "assistant_tokenizer",
      "generation_config",
      "model_kwargs",
      "atm_translator",
      "inputs_tensor",
      "logits_processor"
    ],
    "get_candidates": [
      "self",
      "input_ids",
      "is_first_iteration"
    ],
    "_update_past_and_masks": [
      "self",
      "assistant_input_ids",
      "num_added_tokens"
    ],
    "_prepare_assistant_input_ids": [
      "self",
      "target_input_ids"
    ]
  },
  "PromptLookupCandidateGenerator": {
    "__init__": [
      "self",
      "eos_token_id",
      "num_output_tokens",
      "max_matching_ngram_size",
      "max_length",
      "logits_processor",
      "vocab_size"
    ],
    "get_candidates": [
      "self",
      "input_ids",
      "is_first_iteration"
    ],
    "update_candidate_strategy": [
      "self",
      "input_ids",
      "scores",
      "num_matches"
    ]
  },
  "EarlyExitCandidateGenerator": {
    "__init__": [
      "self",
      "input_ids",
      "assistant_model",
      "generation_config",
      "model_kwargs",
      "inputs_tensor",
      "logits_processor"
    ],
    "get_candidates": [
      "self",
      "input_ids",
      "is_first_iteration"
    ]
  },
  "_prepare_attention_mask": [
    "model_kwargs",
    "new_length",
    "is_encoder_decoder"
  ],
  "_prepare_token_type_ids": [
    "model_kwargs",
    "new_length"
  ],
  "METADATA_FIELDS": [],
  "STATIC_CACHE_IMPLEMENTATIONS": [],
  "DYNAMIC_CACHE_IMPLEMENTATIONS": [],
  "DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS": [],
  "ALL_STATIC_CACHE_IMPLEMENTATIONS": [],
  "ALL_CACHE_IMPLEMENTATIONS": [],
  "GenerationMode": {
    "CONTRASTIVE_SEARCH": [],
    "GREEDY_SEARCH": [],
    "SAMPLE": [],
    "ASSISTED_GENERATION": [],
    "DOLA_GENERATION": [],
    "BEAM_SEARCH": [],
    "BEAM_SAMPLE": [],
    "CONSTRAINED_BEAM_SEARCH": [],
    "GROUP_BEAM_SEARCH": []
  },
  "GenerationConfig": {
    "extra_output_flags": [],
    "__init__": [
      "self"
    ],
    "__hash__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__repr__": [
      "self"
    ],
    "get_generation_mode": [
      "self",
      "assistant_model"
    ],
    "_get_default_generation_params": [],
    "validate": [
      "self",
      "strict"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "config_file_name",
      "push_to_hub"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name",
      "config_file_name",
      "cache_dir",
      "force_download",
      "local_files_only",
      "token",
      "revision"
    ],
    "_dict_from_json_file": [
      "cls",
      "json_file"
    ],
    "from_dict": [
      "cls",
      "config_dict"
    ],
    "dict_dtype_to_str": [
      "self",
      "d"
    ],
    "to_diff_dict": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self",
      "use_diff",
      "ignore_metadata",
      "keys_to_pop"
    ],
    "to_json_file": [
      "self",
      "json_file_path",
      "use_diff",
      "keys_to_pop"
    ],
    "from_model_config": [
      "cls",
      "model_config"
    ],
    "update": [
      "self",
      "defaults_only",
      "allow_custom_entries"
    ]
  },
  "BaseWatermarkingConfig": {
    "from_dict": [
      "cls",
      "config_dict"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ],
    "to_dict": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "update": [
      "self"
    ],
    "validate": [
      "self"
    ],
    "construct_processor": [
      "self",
      "vocab_size"
    ]
  },
  "WatermarkingConfig": {
    "__init__": [
      "self",
      "greenlist_ratio",
      "bias",
      "hashing_key",
      "seeding_scheme",
      "context_width"
    ],
    "validate": [
      "self"
    ],
    "construct_processor": [
      "self",
      "vocab_size",
      "device"
    ]
  },
  "SynthIDTextWatermarkingConfig": {
    "__init__": [
      "self",
      "ngram_len",
      "keys",
      "context_history_size",
      "sampling_table_seed",
      "sampling_table_size",
      "skip_first_ngram_calls",
      "debug_mode"
    ],
    "validate": [
      "self"
    ],
    "construct_processor": [
      "self",
      "vocab_size",
      "device"
    ]
  },
  "CompileConfig": {
    "_compile_all_devices": [],
    "to_dict": [
      "self"
    ]
  },
  "LOGITS_PROCESSOR_INPUTS_DOCSTRING": [],
  "LogitsProcessor": {
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "LogitsProcessorList": {
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ],
    "set_continuous_batching_context": [
      "self",
      "logits_indices",
      "cu_seq_lens_q"
    ]
  },
  "MinLengthLogitsProcessor": {
    "__init__": [
      "self",
      "min_length",
      "eos_token_id",
      "device"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "MinNewTokensLengthLogitsProcessor": {
    "__init__": [
      "self",
      "prompt_length_to_skip",
      "min_new_tokens",
      "eos_token_id",
      "device"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "TemperatureLogitsWarper": {
    "__init__": [
      "self",
      "temperature"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "RepetitionPenaltyLogitsProcessor": {
    "__init__": [
      "self",
      "penalty",
      "prompt_ignore_length"
    ],
    "set_continuous_batching_context": [
      "self",
      "logits_indices",
      "cu_seq_lens_q"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "EncoderRepetitionPenaltyLogitsProcessor": {
    "__init__": [
      "self",
      "penalty",
      "encoder_input_ids"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "TopPLogitsWarper": {
    "__init__": [
      "self",
      "top_p",
      "filter_value",
      "min_tokens_to_keep"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "TopKLogitsWarper": {
    "__init__": [
      "self",
      "top_k",
      "filter_value",
      "min_tokens_to_keep"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "TopHLogitsWarper": {
    "__init__": [
      "self",
      "top_h",
      "filter_value"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "MinPLogitsWarper": {
    "__init__": [
      "self",
      "min_p",
      "filter_value",
      "min_tokens_to_keep"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "TypicalLogitsWarper": {
    "__init__": [
      "self",
      "mass",
      "filter_value",
      "min_tokens_to_keep"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "EpsilonLogitsWarper": {
    "__init__": [
      "self",
      "epsilon",
      "filter_value",
      "min_tokens_to_keep"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "EtaLogitsWarper": {
    "__init__": [
      "self",
      "epsilon",
      "filter_value",
      "min_tokens_to_keep",
      "device"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "_get_ngrams": [
    "ngram_size",
    "prev_input_ids",
    "num_hypos"
  ],
  "_get_generated_ngrams": [
    "banned_ngrams",
    "prev_input_ids",
    "ngram_size",
    "cur_len"
  ],
  "_calc_banned_ngram_tokens": [
    "ngram_size",
    "prev_input_ids",
    "num_hypos",
    "cur_len"
  ],
  "NoRepeatNGramLogitsProcessor": {
    "__init__": [
      "self",
      "ngram_size"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "EncoderNoRepeatNGramLogitsProcessor": {
    "__init__": [
      "self",
      "encoder_ngram_size",
      "encoder_input_ids"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "SequenceBiasLogitsProcessor": {
    "__init__": [
      "self",
      "sequence_bias"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ],
    "_prepare_bias_variables": [
      "self",
      "scores"
    ],
    "_validate_arguments": [
      "self"
    ],
    "_convert_list_arguments_into_dict": [
      "self"
    ]
  },
  "NoBadWordsLogitsProcessor": {
    "__init__": [
      "self",
      "bad_words_ids",
      "eos_token_id"
    ],
    "_validate_arguments": [
      "self"
    ]
  },
  "PrefixConstrainedLogitsProcessor": {
    "__init__": [
      "self",
      "prefix_allowed_tokens_fn",
      "num_beams"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "ForcedBOSTokenLogitsProcessor": {
    "__init__": [
      "self",
      "bos_token_id"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "ForcedEOSTokenLogitsProcessor": {
    "__init__": [
      "self",
      "max_length",
      "eos_token_id",
      "device"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "InfNanRemoveLogitsProcessor": {
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "ExponentialDecayLengthPenalty": {
    "__init__": [
      "self",
      "exponential_decay_length_penalty",
      "eos_token_id",
      "input_ids_seq_length"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "LogitNormalization": {
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "SuppressTokensAtBeginLogitsProcessor": {
    "__init__": [
      "self",
      "begin_suppress_tokens",
      "begin_index",
      "device"
    ],
    "set_begin_index": [
      "self",
      "begin_index"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "SuppressTokensLogitsProcessor": {
    "__init__": [
      "self",
      "suppress_tokens",
      "device"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "WhisperTimeStampLogitsProcessor": {
    "__init__": [
      "self",
      "generate_config",
      "begin_index",
      "_detect_timestamp_from_logprob"
    ],
    "set_begin_index": [
      "self",
      "begin_index"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "WhisperNoSpeechDetection": {
    "__init__": [
      "self",
      "no_speech_token",
      "begin_index",
      "scores_is_logprobs"
    ],
    "set_model": [
      "self",
      "model"
    ],
    "set_inputs": [
      "self",
      "inputs"
    ],
    "no_speech_prob": [
      "self"
    ],
    "set_begin_index": [
      "self",
      "begin_index"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "ClassifierFreeGuidanceLogitsProcessor": {
    "__init__": [
      "self",
      "guidance_scale"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "AlternatingCodebooksLogitsProcessor": {
    "__init__": [
      "self",
      "input_start_len",
      "semantic_vocab_size",
      "codebook_size"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "UnbatchedClassifierFreeGuidanceLogitsProcessor": {
    "__init__": [
      "self",
      "guidance_scale",
      "model",
      "unconditional_ids",
      "unconditional_attention_mask",
      "use_cache"
    ],
    "get_unconditional_logits": [
      "self",
      "input_ids"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "BarkEosPrioritizerLogitsProcessor": {
    "__init__": [
      "self",
      "eos_token_id",
      "min_eos_p",
      "device"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "WatermarkLogitsProcessor": {
    "__init__": [
      "self",
      "vocab_size",
      "device",
      "greenlist_ratio",
      "bias",
      "hashing_key",
      "seeding_scheme",
      "context_width"
    ],
    "set_seed": [
      "self",
      "input_seq"
    ],
    "_get_greenlist_ids": [
      "self",
      "input_seq"
    ],
    "_score_rejection_sampling": [
      "self",
      "input_seq",
      "scores"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "SynthIDTextWatermarkState": {
    "__init__": [
      "self",
      "batch_size",
      "ngram_len",
      "context_history_size",
      "device"
    ]
  },
  "SynthIDTextWatermarkLogitsProcessor": {
    "__init__": [
      "self",
      "ngram_len",
      "keys",
      "sampling_table_size",
      "sampling_table_seed",
      "context_history_size",
      "device",
      "skip_first_ngram_calls",
      "debug_mode"
    ],
    "_init_state": [
      "self",
      "batch_size"
    ],
    "update_scores": [
      "self",
      "scores",
      "g_values"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ],
    "accumulate_hash": [
      "self",
      "current_hash",
      "data",
      "multiplier",
      "increment"
    ],
    "compute_ngram_keys": [
      "self",
      "ngrams"
    ],
    "_compute_keys": [
      "self",
      "n_minus_1_grams",
      "indices"
    ],
    "sample_g_values": [
      "self",
      "ngram_keys"
    ],
    "_check_input_ids_shape": [
      "self",
      "input_ids"
    ],
    "compute_g_values": [
      "self",
      "input_ids"
    ],
    "compute_context_repetition_mask": [
      "self",
      "input_ids"
    ],
    "compute_eos_token_mask": [
      "self",
      "input_ids",
      "eos_token_id"
    ],
    "expected_mean_g_value": [
      "self",
      "vocab_size",
      "coinflip_prob"
    ]
  },
  "DiaClassifierFreeGuidanceLogitsProcessor": {
    "__init__": [
      "self",
      "guidance_scale",
      "guidance_top_k"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "DiaEOSChannelFilterLogitsProcessor": {
    "__init__": [
      "self",
      "num_channels",
      "eos_token_id"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "DiaEOSDelayPatternLogitsProcessor": {
    "__init__": [
      "self",
      "delay_pattern",
      "eos_token_id",
      "max_generation_len",
      "device"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "STOP_STRING_EMBEDDING_CACHE": [],
  "STOPPING_CRITERIA_INPUTS_DOCSTRING": [],
  "StoppingCriteria": {
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "MaxLengthCriteria": {
    "__init__": [
      "self",
      "max_length",
      "max_position_embeddings"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "MaxTimeCriteria": {
    "__init__": [
      "self",
      "max_time",
      "initial_timestamp"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "StopStringCriteria": {
    "__init__": [
      "self",
      "tokenizer",
      "stop_strings"
    ],
    "clean_and_embed_tokens_with_cache": [
      "self",
      "token_list",
      "token_indices",
      "tokenizer"
    ],
    "clean_tokenizer_vocab": [
      "tokenizer",
      "static_prefix"
    ],
    "_stop_string_get_matching_positions": [
      "token_list",
      "token_indices",
      "stop_strings"
    ],
    "_stop_string_create_embedding_vec": [
      "token_list",
      "token_indices",
      "stop_strings"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "EosTokenCriteria": {
    "__init__": [
      "self",
      "eos_token_id"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "ConfidenceCriteria": {
    "__init__": [
      "self",
      "assistant_confidence_threshold"
    ],
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ]
  },
  "StoppingCriteriaList": {
    "__call__": [
      "self",
      "input_ids",
      "scores"
    ],
    "max_length": [
      "self"
    ]
  },
  "validate_stopping_criteria": [
    "stopping_criteria",
    "max_length"
  ],
  "NUM_Q_PADDING_INTERVALS": [],
  "NUM_KV_PADDING_INTERVALS": [],
  "pad_by_intervals": [
    "size",
    "max_value",
    "nb_intervals"
  ],
  "ProtoPretrainedModel": {
    "set_attn_implementation": [
      "self",
      "attn_implementation"
    ],
    "_get_logits_processor": [
      "self",
      "generation_config"
    ]
  },
  "ContinuousBatchProcessor": {
    "__init__": [
      "self",
      "cache",
      "config",
      "generation_config",
      "input_queue",
      "output_queue",
      "stop_event",
      "model_device",
      "model_dtype",
      "scheduler",
      "manual_eviction",
      "use_cuda_graph",
      "q_padding_intervals",
      "kv_padding_intervals"
    ],
    "__repr__": [
      "self"
    ],
    "_get_new_requests": [
      "self"
    ],
    "_handle_request_error": [
      "self",
      "error",
      "state"
    ],
    "soft_reset_one_request": [
      "self"
    ],
    "prepare_next_batch": [
      "self"
    ],
    "_maybe_send_output": [
      "self",
      "state"
    ],
    "update_batch": [
      "self"
    ],
    "has_pending_requests": [
      "self"
    ],
    "handle_batch_error": [
      "self",
      "error"
    ],
    "fail_all_requests": [
      "self",
      "error"
    ],
    "_generation_step": [
      "self",
      "model",
      "logit_processor",
      "do_sample"
    ],
    "_forward_process_and_sample": [
      "self",
      "model",
      "batch_data",
      "logit_processor",
      "do_sample"
    ],
    "_model_forward": [
      "self",
      "model",
      "batch_data"
    ],
    "_process_logit": [
      "self",
      "batch_data",
      "logits",
      "logit_processor"
    ],
    "_sample": [
      "self",
      "probs",
      "batch_data",
      "do_sample"
    ]
  },
  "ContinuousBatchingManager": {
    "__init__": [
      "self",
      "model",
      "generation_config",
      "manual_eviction",
      "max_queue_size",
      "num_q_padding_intervals",
      "num_kv_padding_intervals",
      "allow_block_sharing"
    ],
    "_decide_use_cuda_graphs": [
      "self",
      "use_cuda_graph",
      "num_q_padding_intervals",
      "num_kv_padding_intervals",
      "compile_config"
    ],
    "start": [
      "self"
    ],
    "is_running": [
      "self"
    ],
    "stop": [
      "self",
      "block",
      "timeout"
    ],
    "join": [
      "self",
      "stop_trigger_time",
      "timeout"
    ],
    "add_request": [
      "self",
      "input_ids",
      "request_id",
      "max_new_tokens",
      "streaming",
      "record_timestamps"
    ],
    "add_requests": [
      "self",
      "inputs",
      "max_new_tokens",
      "streaming",
      "record_timestamps"
    ],
    "cancel_request": [
      "self",
      "request_id"
    ],
    "get_result": [
      "self",
      "request_id",
      "timeout"
    ],
    "__iter__": [
      "self"
    ],
    "request_id_iter": [
      "self",
      "request_id"
    ],
    "_generation_step": [
      "self"
    ],
    "_run_generation_loop": [
      "self"
    ],
    "_inner_generation_loop": [
      "self",
      "batch_processor"
    ],
    "_handle_critical_error": [
      "self",
      "error",
      "batch_processor"
    ],
    "evict_request_from_cache": [
      "self",
      "request_id"
    ]
  },
  "ContinuousMixin": {
    "continuous_batching_context_manager": [
      "self",
      "generation_config",
      "manual_eviction",
      "max_queue_size",
      "num_q_cuda_graphs",
      "num_kv_cuda_graphs",
      "allow_block_sharing",
      "block",
      "timeout"
    ],
    "init_continuous_batching": [
      "self",
      "generation_config",
      "manual_eviction",
      "max_queue_size",
      "num_q_padding_intervals",
      "num_kv_padding_intervals",
      "allow_block_sharing"
    ],
    "generate_batch": [
      "self",
      "inputs",
      "generation_config",
      "num_q_padding_intervals",
      "num_kv_padding_intervals",
      "allow_block_sharing",
      "record_timestamps",
      "progress_bar"
    ]
  },
  "T": [],
  "reverse_enumerate": [
    "xs"
  ],
  "Block": {
    "__init__": [
      "self",
      "id_",
      "parent_id",
      "group_id"
    ],
    "__repr__": [
      "self"
    ],
    "is_complete": [
      "self"
    ]
  },
  "BlockManager": {
    "__init__": [
      "self",
      "num_blocks",
      "block_size"
    ],
    "num_free_blocks": [
      "self"
    ],
    "has_enough_free_blocks": [
      "self",
      "n_blocks"
    ],
    "get_free_blocks": [
      "self",
      "n_blocks",
      "last_block_id",
      "shareable",
      "group_id"
    ],
    "fork_blocks": [
      "self",
      "parent_blocks",
      "num_forks",
      "shareable",
      "group_id"
    ],
    "increase_ref_count": [
      "self",
      "block_id"
    ],
    "decrease_ref_count": [
      "self",
      "block_id"
    ],
    "free_blocks": [
      "self",
      "blocks",
      "shareable"
    ],
    "uninitialize_unshared_block": [
      "self",
      "block_id"
    ],
    "mark_shareable_blocks_as_complete": [
      "self",
      "num_complete_blocks",
      "allocated_blocks",
      "prompt_ids"
    ],
    "compute_hash": [
      "self",
      "parent_hash",
      "tokens",
      "group_id"
    ]
  },
  "CacheAllocator": {
    "allocate_blocks": [
      "self",
      "n_blocks",
      "request_id",
      "block_manager"
    ],
    "free_blocks": [
      "self",
      "request_id",
      "block_manager"
    ],
    "get_read_indices": [
      "self",
      "request_id",
      "past_length",
      "query_length"
    ],
    "get_write_indices": [
      "self",
      "request_id",
      "past_length",
      "query_length"
    ],
    "fork_blocks": [
      "self",
      "parent_request_id",
      "children_request_ids",
      "block_manager"
    ]
  },
  "FullAttentionCacheAllocator": {
    "__init__": [
      "self",
      "index",
      "block_size",
      "allow_block_sharing"
    ],
    "allocate_blocks": [
      "self",
      "n_blocks",
      "request_id",
      "block_manager"
    ],
    "get_read_indices": [
      "self",
      "request_id",
      "past_length",
      "query_length"
    ],
    "get_write_indices": [
      "self",
      "request_id",
      "past_length",
      "query_length"
    ]
  },
  "SlidingAttentionCacheAllocator": {
    "__init__": [
      "self",
      "index",
      "block_size",
      "sliding_window"
    ],
    "allocate_blocks": [
      "self",
      "n_blocks",
      "request_id",
      "block_manager"
    ],
    "get_read_indices": [
      "self",
      "request_id",
      "past_length",
      "query_length"
    ],
    "get_write_indices": [
      "self",
      "request_id",
      "past_length",
      "query_length"
    ]
  },
  "group_layers_by_attn_type": [
    "config"
  ],
  "PagedAttentionCache": {
    "__init__": [
      "self",
      "config",
      "generation_config",
      "device",
      "dtype",
      "tp_size",
      "allow_block_sharing"
    ],
    "will_allocation_be_successful": [
      "self",
      "num_requested_blocks",
      "allocated_blocks"
    ],
    "allocate_blocks": [
      "self",
      "n_blocks",
      "request_id",
      "allocated_blocks"
    ],
    "free_blocks": [
      "self",
      "request_id"
    ],
    "get_num_free_blocks": [
      "self"
    ],
    "extend_read_and_write_indices": [
      "self",
      "request_id",
      "past_length",
      "query_length",
      "read_index",
      "write_index"
    ],
    "get_seqlens_k": [
      "self",
      "past_length",
      "query_length"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "read_index",
      "write_index"
    ],
    "search_prefix_match": [
      "self",
      "request_id",
      "prompt_ids"
    ],
    "mark_shareable_blocks_as_complete": [
      "self",
      "state"
    ],
    "copy_cache": [
      "self",
      "list_source_blocks",
      "list_forked_blocks"
    ],
    "fork_request": [
      "self",
      "source_request_id",
      "destination_request_ids"
    ]
  },
  "PagedAttentionMemoryHandler": {
    "_activation_dtype": [],
    "_input_dtype": [],
    "_upper_bound_max_batch_tokens": [],
    "_upper_bound_num_blocks": [],
    "__init__": [
      "self",
      "block_size",
      "page_size",
      "num_groups",
      "group_size",
      "peak_activation_per_token",
      "num_attention_masks"
    ],
    "get_available_memory": [
      "max_memory_percent"
    ],
    "infer_num_blocks_and_max_batch_tokens": [
      "self",
      "num_blocks",
      "max_batch_tokens",
      "max_memory_percent",
      "cache_dtype"
    ],
    "compute_num_blocks_and_max_batch_tokens": [
      "self",
      "max_memory_percent",
      "cache_dtype",
      "m"
    ],
    "compute_max_batch_tokens": [
      "self",
      "num_blocks",
      "max_memory_percent",
      "cache_dtype"
    ],
    "compute_num_blocks": [
      "self",
      "max_batch_tokens",
      "max_memory_percent",
      "cache_dtype"
    ],
    "compute_memory_footprint": [
      "self",
      "num_blocks",
      "max_batch_tokens",
      "cache_dtype"
    ]
  },
  "attn_mask_is_needed": [
    "config"
  ],
  "build_attention_mask": [
    "attention_mask",
    "cumulative_seqlens_q",
    "cumulative_seqlens_k",
    "sliding_window"
  ],
  "PagedAttentionArgs": {
    "asdict": [
      "self"
    ]
  },
  "ContinuousBatchingIOs": {
    "__init__": [
      "self",
      "cache",
      "config",
      "device",
      "model_dtype"
    ],
    "setup_static_tensors": [
      "self"
    ],
    "reset_static_tensors": [
      "self",
      "full_reset"
    ],
    "prepare_batch_tensors": [
      "self",
      "requests_in_batch"
    ],
    "get_model_kwargs": [
      "self",
      "padded_q_size",
      "padded_kv_cache_size"
    ]
  },
  "Scheduler": {
    "__init__": [
      "self",
      "cache",
      "retain_cache_on_finish"
    ],
    "add_waiting_request": [
      "self",
      "state"
    ],
    "schedule_batch": [
      "self",
      "token_budget",
      "cache_budget"
    ],
    "has_pending_requests": [
      "self"
    ],
    "finish_request": [
      "self",
      "request_id",
      "evict_from_cache"
    ],
    "get_active_request_static_outputs": [
      "self",
      "request_id"
    ],
    "set_request_cancellation": [
      "self",
      "request_id"
    ],
    "clear_cancelled_requests": [
      "self"
    ],
    "request_is_cancelled": [
      "self",
      "request_id"
    ],
    "_allocate_blocks_if_needed": [
      "self",
      "state",
      "len_next_tokens"
    ],
    "_infer_request_tokens": [
      "self",
      "state",
      "request_ids_to_remove_from_waiting"
    ],
    "_schedule_request": [
      "self",
      "state",
      "request_tokens",
      "token_budget",
      "request_ids_to_remove_from_waiting"
    ],
    "_process_candidates": [
      "self",
      "candidates",
      "token_budget",
      "cache_budget",
      "request_ids_to_remove_from_waiting",
      "safety_margin"
    ],
    "_cleanup_waiting_queue": [
      "self",
      "request_ids_to_remove_from_waiting"
    ]
  },
  "FIFOScheduler": {
    "__init__": [
      "self",
      "cache",
      "retain_cache_on_finish",
      "safety_margin"
    ],
    "schedule_batch": [
      "self",
      "token_budget",
      "cache_budget"
    ]
  },
  "PrefillFirstScheduler": {
    "schedule_batch": [
      "self",
      "token_budget",
      "cache_budget"
    ]
  },
  "SCHEDULER_MAPPING": [],
  "TMP_TOKEN_ID": [],
  "get_device_and_memory_breakdown": [],
  "RequestStatus": {
    "PENDING": [],
    "PREFILLING": [],
    "PREFILLING_SPLIT": [],
    "SPLIT_PENDING_REMAINDER": [],
    "DECODING": [],
    "FINISHED": [],
    "FAILED": []
  },
  "GenerationOutput": {
    "is_finished": [
      "self"
    ]
  },
  "RequestState": {
    "__post_init__": [
      "self"
    ],
    "status": [
      "self",
      "value"
    ],
    "timestamps": [
      "self"
    ],
    "log_end_of_request": [
      "self"
    ],
    "current_len": [
      "self"
    ],
    "generated_len": [
      "self"
    ],
    "update_and_check_completion": [
      "self",
      "token_id"
    ],
    "__repr__": [
      "self"
    ],
    "to_generation_output": [
      "self"
    ],
    "fork": [
      "self",
      "new_request_id"
    ],
    "create_equivalent_initial_request": [
      "self"
    ]
  },
  "__all__": [],
  "AwqQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ]
  },
  "EetqHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "get_quantize_ops": [
      "self"
    ]
  },
  "QuantoHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "adjust_max_memory": [
      "self",
      "max_memory"
    ],
    "param_element_size": [
      "self",
      "model",
      "param_name",
      "param"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ],
    "get_quantize_ops": [
      "self"
    ]
  },
  "HiggsHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self",
      "device_map"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_dequantize": [
      "self",
      "model"
    ]
  },
  "SpQRHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ]
  },
  "CompressedTensorsHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "update_tp_plan": [
      "self",
      "config"
    ],
    "is_trainable": [
      "self"
    ],
    "is_qat_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ]
  },
  "get_keys_to_not_convert": [
    "model"
  ],
  "_assign_is_quantized": [
    "model"
  ],
  "HfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "update_device_map": [
      "self",
      "device_map"
    ],
    "param_element_size": [
      "self",
      "model",
      "param_name",
      "param"
    ],
    "adjust_max_memory": [
      "self",
      "max_memory"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "validate_environment": [
      "self"
    ],
    "update_tp_plan": [
      "self",
      "config"
    ],
    "update_ep_plan": [
      "self",
      "config"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "preprocess_model": [
      "self",
      "model",
      "dtype"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "postprocess_model": [
      "self",
      "model"
    ],
    "remove_quantization_config": [
      "self",
      "model"
    ],
    "dequantize": [
      "self",
      "model",
      "dtype"
    ],
    "_dequantize": [
      "self",
      "model",
      "dtype"
    ],
    "get_param_name": [
      "self",
      "param_name"
    ],
    "get_modules_to_not_convert": [
      "model",
      "skip_modules",
      "keep_in_fp32_modules",
      "add_default_skips"
    ],
    "is_qat_trainable": [
      "self"
    ],
    "is_compileable": [
      "self"
    ],
    "get_state_dict_and_metadata": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "_convert_model_for_quantization": [
      "self",
      "model"
    ],
    "get_quantize_ops": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "SequentialLlama4TextExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MODULES_TO_PATCH_FOR_QUANTIZATION": [],
  "CHECKPOINT_KEYS": [],
  "QuarkHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "fuzzy_match_size": [
    "config_name"
  ],
  "_quantization_type": [
    "weight"
  ],
  "_linear_extra_repr": [
    "self"
  ],
  "TorchAoHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "get_state_dict_and_metadata": [
      "self",
      "model"
    ],
    "param_element_size": [
      "self",
      "model",
      "param_name",
      "param"
    ],
    "adjust_max_memory": [
      "self",
      "max_memory"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model",
      "checkpoint_files"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "is_compileable": [
      "self"
    ],
    "set_metadata": [
      "self",
      "checkpoint_files"
    ],
    "get_quantize_ops": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "FineGrainedFP8HfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "param_element_size": [
      "self",
      "model",
      "param_name",
      "param"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "update_tp_plan": [
      "self",
      "config"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "get_quantize_ops": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "FPQuantHfQuantizer": {
    "requires_calibration": [],
    "is_qat_trainable": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self",
      "device_map"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "get_quantize_ops": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "Bnb4BitHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "param_element_size": [
      "self",
      "model",
      "param_name",
      "param"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "adjust_max_memory": [
      "self",
      "max_memory"
    ],
    "update_device_map": [
      "self",
      "device_map"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model",
      "device_map"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "_dequantize": [
      "self",
      "model",
      "dtype"
    ],
    "get_quantize_ops": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "triton_kernels_hub": [],
  "Mxfp4HfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "_lazy_import_kernels": [
      "self"
    ],
    "validate_environment": [
      "self"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model",
      "use_kernels"
    ],
    "update_tp_plan": [
      "self",
      "config"
    ],
    "update_ep_plan": [
      "self",
      "config"
    ],
    "get_state_dict_and_metadata": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "get_quantize_ops": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "AutoRoundQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ]
  },
  "FbgemmFp8HfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "update_tp_plan": [
      "self",
      "config"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "get_quantize_ops": [
      "self"
    ]
  },
  "AUTO_QUANTIZER_MAPPING": [],
  "AUTO_QUANTIZATION_CONFIG_MAPPING": [],
  "AutoQuantizationConfig": {
    "from_dict": [
      "cls",
      "quantization_config_dict"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "AutoHfQuantizer": {
    "from_config": [
      "cls",
      "quantization_config"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "merge_quantization_configs": [
      "cls",
      "quantization_config",
      "quantization_config_from_args"
    ],
    "supports_quant_method": [
      "quantization_config_dict"
    ]
  },
  "register_quantization_config": [
    "method"
  ],
  "register_quantizer": [
    "name"
  ],
  "get_hf_quantizer": [
    "config",
    "quantization_config",
    "device_map",
    "weights_only",
    "user_agent"
  ],
  "VptqHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ]
  },
  "HqqHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_patch_layer_for_multigpu": [
      "self",
      "hqq_layer"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ]
  },
  "AqlmHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ]
  },
  "BitNetHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "adjust_max_memory": [
      "self",
      "max_memory"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "is_qat_trainable": [
      "self"
    ]
  },
  "Bnb8BitHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "adjust_max_memory": [
      "self",
      "max_memory"
    ],
    "update_device_map": [
      "self",
      "device_map"
    ],
    "param_element_size": [
      "self",
      "model",
      "param_name",
      "param"
    ],
    "param_needs_quantization": [
      "self",
      "model",
      "param_name"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model",
      "device_map"
    ],
    "is_serializable": [
      "self"
    ],
    "is_trainable": [
      "self"
    ],
    "_dequantize": [
      "self",
      "model",
      "dtype"
    ],
    "get_quantize_ops": [
      "self"
    ],
    "get_weight_conversions": [
      "self"
    ]
  },
  "get_module_from_name": [
    "module",
    "tensor_name"
  ],
  "should_convert_module": [
    "full_name",
    "patterns"
  ],
  "GptqHfQuantizer": {
    "requires_calibration": [],
    "__init__": [
      "self",
      "quantization_config"
    ],
    "validate_environment": [
      "self"
    ],
    "update_dtype": [
      "self",
      "dtype"
    ],
    "update_device_map": [
      "self",
      "device_map"
    ],
    "_process_model_before_weight_loading": [
      "self",
      "model"
    ],
    "_process_model_after_weight_loading": [
      "self",
      "model"
    ],
    "is_trainable": [
      "self"
    ],
    "is_serializable": [
      "self"
    ]
  },
  "DistributedConfig": {
    "from_dict": [
      "cls",
      "config_dict"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ],
    "to_dict": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "update": [
      "self"
    ]
  },
  "PATH_TO_TRANSFORMERS": [],
  "AUTODOC_FILES": [],
  "PLACEHOLDER_TO_AUTO_MODULE": [],
  "UNROLL_KWARGS_METHODS": [],
  "UNROLL_KWARGS_CLASSES": [],
  "HARDCODED_CONFIG_FOR_MODELS": [],
  "ImageProcessorArgs": {
    "images": [],
    "videos": [],
    "do_resize": [],
    "size": [],
    "size_divisor": [],
    "default_to_square": [],
    "resample": [],
    "do_center_crop": [],
    "crop_size": [],
    "do_pad": [],
    "pad_size": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_convert_rgb": [],
    "return_tensors": [],
    "data_format": [],
    "input_data_format": [],
    "device": [],
    "disable_grouping": [],
    "image_seq_length": []
  },
  "ProcessorArgs": {
    "image_processor": [],
    "tokenizer": [],
    "video_processor": [],
    "audio_processor": [],
    "feature_extractor": [],
    "chat_template": [],
    "text": [],
    "audio": [],
    "audios": [],
    "return_tensors": [],
    "add_special_tokens": [],
    "padding": [],
    "truncation": [],
    "max_length": [],
    "stride": [],
    "pad_to_multiple_of": [],
    "return_token_type_ids": [],
    "return_attention_mask": [],
    "return_overflowing_tokens": [],
    "return_special_tokens_mask": [],
    "return_offsets_mapping": [],
    "return_length": [],
    "verbose": [],
    "text_pair": [],
    "text_target": [],
    "text_pair_target": [],
    "is_split_into_words": [],
    "boxes": [],
    "word_labels": []
  },
  "ModelArgs": {
    "labels": [],
    "num_logits_to_keep": [],
    "input_ids": [],
    "input_values": [],
    "attention_mask": [],
    "decoder_attention_mask": [],
    "encoder_hidden_states": [],
    "encoder_attention_mask": [],
    "token_type_ids": [],
    "position_ids": [],
    "past_key_values": [],
    "inputs_embeds": [],
    "decoder_input_ids": [],
    "decoder_inputs_embeds": [],
    "use_cache": [],
    "output_attentions": [],
    "output_hidden_states": [],
    "return_dict": [],
    "cache_position": [],
    "hidden_states": [],
    "interpolate_pos_encoding": [],
    "position_embeddings": [],
    "config": [],
    "start_positions": [],
    "end_positions": [],
    "encoder_outputs": [],
    "output_router_logits": [],
    "logits_to_keep": [],
    "pixel_values": [],
    "pixel_values_videos": [],
    "vision_feature_layer": [],
    "vision_feature_select_strategy": [],
    "image_sizes": [],
    "pixel_mask": [],
    "input_features": []
  },
  "ModelOutputArgs": {
    "last_hidden_state": [],
    "past_key_values": [],
    "hidden_states": [],
    "attentions": [],
    "pooler_output": [],
    "cross_attentions": [],
    "decoder_hidden_states": [],
    "decoder_attentions": [],
    "encoder_last_hidden_state": [],
    "encoder_hidden_states": [],
    "encoder_attentions": [],
    "router_logits": [],
    "router_probs": [],
    "z_loss": [],
    "aux_loss": [],
    "start_logits": [],
    "end_logits": [],
    "feature_maps": [],
    "reconstruction": [],
    "spectrogram": [],
    "predicted_depth": [],
    "sequences": [],
    "params": [],
    "loc": [],
    "scale": [],
    "static_features": [],
    "embeddings": [],
    "extract_features": [],
    "projection_state": [],
    "image_hidden_states": [],
    "video_hidden_states": []
  },
  "ClassDocstring": {
    "PreTrainedModel": [],
    "Model": [],
    "ForPreTraining": [],
    "Decoder": [],
    "TextModel": [],
    "ForSequenceClassification": [],
    "ForQuestionAnswering": [],
    "ForMultipleChoice": [],
    "ForMaskedLM": [],
    "ForTokenClassification": [],
    "ForConditionalGeneration": [],
    "ForCausalLM": [],
    "ImageProcessorFast": [],
    "Backbone": [],
    "ForImageClassification": [],
    "ForSemanticSegmentation": [],
    "ForAudioClassification": [],
    "ForAudioFrameClassification": [],
    "ForPrediction": [],
    "WithProjection": []
  },
  "ClassAttrs": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_tied_weights_keys": []
  },
  "ARGS_TO_IGNORE": [],
  "get_indent_level": [
    "func"
  ],
  "equalize_indent": [
    "docstring",
    "indent_level"
  ],
  "set_min_indent": [
    "docstring",
    "indent_level"
  ],
  "parse_shape": [
    "docstring"
  ],
  "parse_default": [
    "docstring"
  ],
  "parse_docstring": [
    "docstring",
    "max_indent_level",
    "return_intro"
  ],
  "contains_type": [
    "type_hint",
    "target_type"
  ],
  "get_model_name": [
    "obj"
  ],
  "generate_processor_intro": [
    "cls"
  ],
  "get_placeholders_dict": [
    "placeholders",
    "model_name"
  ],
  "format_args_docstring": [
    "docstring",
    "model_name"
  ],
  "get_args_doc_from_source": [
    "args_classes"
  ],
  "get_checkpoint_from_config_class": [
    "config_class"
  ],
  "add_intro_docstring": [
    "func",
    "class_name",
    "indent_level"
  ],
  "_get_model_info": [
    "func",
    "parent_class"
  ],
  "_process_parameter_type": [
    "param"
  ],
  "_get_parameter_info": [
    "param_name",
    "documented_params",
    "source_args_dict",
    "param_type",
    "optional"
  ],
  "_process_regular_parameters": [
    "sig",
    "func",
    "class_name",
    "documented_params",
    "indent_level",
    "undocumented_parameters",
    "source_args_dict",
    "parent_class"
  ],
  "find_sig_line": [
    "lines",
    "line_end"
  ],
  "_is_processor_class": [
    "func",
    "parent_class"
  ],
  "_process_kwargs_parameters": [
    "sig",
    "func",
    "parent_class",
    "documented_kwargs",
    "indent_level",
    "undocumented_parameters"
  ],
  "_add_return_tensors_for_processor_call": [
    "func",
    "parent_class",
    "docstring",
    "indent_level"
  ],
  "_process_parameters_section": [
    "func_documentation",
    "sig",
    "func",
    "class_name",
    "model_name_lowercase",
    "parent_class",
    "indent_level",
    "source_args_dict"
  ],
  "_process_returns_section": [
    "func_documentation",
    "sig",
    "config_class",
    "indent_level"
  ],
  "_process_example_section": [
    "func_documentation",
    "func",
    "parent_class",
    "class_name",
    "model_name_lowercase",
    "config_class",
    "checkpoint",
    "indent_level"
  ],
  "auto_method_docstring": [
    "func",
    "parent_class",
    "custom_intro",
    "custom_args",
    "checkpoint",
    "source_args_dict"
  ],
  "auto_class_docstring": [
    "cls",
    "custom_intro",
    "custom_args",
    "checkpoint"
  ],
  "auto_docstring": [
    "obj"
  ],
  "infer_device": [
    "model"
  ],
  "add_to_mapping": [
    "layer_name",
    "device",
    "repo_name",
    "mode",
    "compatible_mapping"
  ],
  "add_to_mapping_local": [
    "layer_name",
    "device",
    "repo_name",
    "mode",
    "compatible_mapping"
  ],
  "KernelConfig": {
    "__init__": [
      "self",
      "kernel_mapping",
      "use_local_kernel"
    ],
    "update_kernel": [
      "self",
      "repo_id",
      "registered_name",
      "layer_name",
      "device",
      "mode",
      "revision"
    ],
    "store_registered_layer_names": [
      "self",
      "model"
    ],
    "sanitize_kernel_mapping": [
      "self",
      "model"
    ],
    "create_compatible_mapping": [
      "self",
      "model",
      "compile"
    ]
  },
  "GlueDataset": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "GlueDataTrainingArguments": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "SquadDataset": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "SquadDataTrainingArguments": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "BeamScorer": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "ConstrainedBeamSearchScorer": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "Constraint": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "ConstraintListState": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "DisjunctiveConstraint": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "PhrasalConstraint": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "TorchExportableModuleWithStaticCache": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "convert_and_export_with_cache": [],
  "QuantizationMethod": {
    "BITS_AND_BYTES": [],
    "GPTQ": [],
    "AWQ": [],
    "AQLM": [],
    "VPTQ": [],
    "QUANTO": [],
    "EETQ": [],
    "HIGGS": [],
    "HQQ": [],
    "COMPRESSED_TENSORS": [],
    "FBGEMM_FP8": [],
    "TORCHAO": [],
    "BITNET": [],
    "SPQR": [],
    "FP8": [],
    "QUARK": [],
    "FPQUANT": [],
    "AUTOROUND": [],
    "MXFP4": []
  },
  "AwqFormat": {
    "GEMM": [],
    "GEMV": [],
    "GEMV_FAST": [],
    "LLM_AWQ": []
  },
  "AwqBackend": {
    "LEGACY_AWQ": [],
    "AUTO": [],
    "AUTO_TRAINABLE": [],
    "MACHETE": [],
    "MARLIN": [],
    "EXLLAMA_V2": [],
    "EXLLAMA_V1": [],
    "GEMM": [],
    "GEMM_TRITON": [],
    "GEMV": [],
    "GEMV_FAST": [],
    "TORCH_AWQ": [],
    "TORCH_FUSED_AWQ": []
  },
  "QuantizationConfigMixin": {
    "from_dict": [
      "cls",
      "config_dict",
      "return_unused_kwargs"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ],
    "to_dict": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "to_json_string": [
      "self",
      "use_diff"
    ],
    "update": [
      "self"
    ]
  },
  "AutoRoundConfig": {
    "__init__": [
      "self",
      "bits",
      "group_size",
      "sym",
      "backend"
    ],
    "post_init": [
      "self"
    ],
    "get_loading_attributes": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "from_dict": [
      "cls",
      "config_dict",
      "return_unused_kwargs"
    ]
  },
  "HqqConfig": {
    "__init__": [
      "self",
      "nbits",
      "group_size",
      "view_as_float",
      "axis",
      "dynamic_config",
      "skip_modules"
    ],
    "post_init": [
      "self"
    ],
    "from_dict": [
      "cls",
      "config"
    ],
    "to_dict": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "to_diff_dict": [
      "self"
    ]
  },
  "BitsAndBytesConfig": {
    "__init__": [
      "self",
      "load_in_8bit",
      "load_in_4bit",
      "llm_int8_threshold",
      "llm_int8_skip_modules",
      "llm_int8_enable_fp32_cpu_offload",
      "llm_int8_has_fp16_weight",
      "bnb_4bit_compute_dtype",
      "bnb_4bit_quant_type",
      "bnb_4bit_use_double_quant",
      "bnb_4bit_quant_storage"
    ],
    "load_in_4bit": [
      "self",
      "value"
    ],
    "load_in_8bit": [
      "self",
      "value"
    ],
    "post_init": [
      "self"
    ],
    "is_quantizable": [
      "self"
    ],
    "quantization_method": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "to_diff_dict": [
      "self"
    ]
  },
  "ExllamaVersion": {
    "ONE": [],
    "TWO": []
  },
  "GPTQConfig": {
    "__init__": [
      "self",
      "bits",
      "tokenizer",
      "dataset",
      "group_size",
      "damp_percent",
      "desc_act",
      "act_group_aware",
      "sym",
      "true_sequential",
      "format",
      "meta",
      "backend",
      "model_seqlen",
      "block_name_to_quantize",
      "module_name_preceding_first_block",
      "batch_size",
      "pad_token_id",
      "max_input_length",
      "cache_block_outputs",
      "modules_in_block_to_quantize"
    ],
    "get_loading_attributes": [
      "self"
    ],
    "post_init": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "to_dict_optimum": [
      "self"
    ],
    "from_dict_optimum": [
      "cls",
      "config_dict"
    ]
  },
  "AwqConfig": {
    "__init__": [
      "self",
      "bits",
      "group_size",
      "zero_point",
      "backend",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ],
    "to_dict": [
      "self"
    ]
  },
  "AqlmConfig": {
    "__init__": [
      "self",
      "in_group_size",
      "out_group_size",
      "num_codebooks",
      "nbits_per_codebook",
      "linear_weights_not_to_quantize"
    ],
    "post_init": [
      "self"
    ]
  },
  "VptqLayerConfig": {
    "__init__": [
      "self",
      "enable_norm",
      "enable_perm",
      "group_num",
      "group_size",
      "in_features",
      "indices_as_float",
      "is_indice_packed",
      "num_centroids",
      "num_res_centroids",
      "out_features",
      "outlier_size",
      "vector_lens"
    ],
    "post_init": [
      "self"
    ]
  },
  "VptqConfig": {
    "__init__": [
      "self",
      "enable_proxy_error",
      "config_for_layers",
      "shared_layer_config",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ]
  },
  "QuantoConfig": {
    "__init__": [
      "self",
      "weights",
      "activations",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ]
  },
  "EetqConfig": {
    "__init__": [
      "self",
      "weights",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ]
  },
  "CompressedTensorsConfig": {
    "__init__": [
      "self",
      "config_groups",
      "format",
      "quantization_status",
      "kv_cache_scheme",
      "global_compression_ratio",
      "ignore",
      "sparsity_config",
      "quant_method",
      "run_compressed"
    ],
    "post_init": [
      "self"
    ],
    "from_dict": [
      "cls",
      "config_dict",
      "return_unused_kwargs"
    ],
    "to_dict": [
      "self"
    ],
    "to_diff_dict": [
      "self"
    ],
    "get_loading_attributes": [
      "self"
    ],
    "is_quantized": [
      "self"
    ],
    "is_quantization_compressed": [
      "self"
    ],
    "is_sparsification_compressed": [
      "self"
    ]
  },
  "FbgemmFp8Config": {
    "__init__": [
      "self",
      "activation_scale_ub",
      "modules_to_not_convert"
    ],
    "get_loading_attributes": [
      "self"
    ]
  },
  "HiggsConfig": {
    "__init__": [
      "self",
      "bits",
      "p",
      "modules_to_not_convert",
      "hadamard_size",
      "group_size",
      "tune_metadata"
    ],
    "post_init": [
      "self"
    ]
  },
  "FPQuantConfig": {
    "__init__": [
      "self",
      "forward_dtype",
      "forward_method",
      "backward_dtype",
      "store_master_weights",
      "hadamard_group_size",
      "pseudoquantization",
      "transform_init",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ]
  },
  "TorchAoConfig": {
    "__init__": [
      "self",
      "quant_type",
      "modules_to_not_convert",
      "include_input_output_embeddings",
      "untie_embedding_weights"
    ],
    "_get_ao_version": [],
    "post_init": [
      "self"
    ],
    "_validate_string_quant_type": [
      "self"
    ],
    "_get_torchao_quant_type_to_method": [
      "self"
    ],
    "get_apply_tensor_subclass": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "from_dict": [
      "cls",
      "config_dict",
      "return_unused_kwargs"
    ]
  },
  "BitNetQuantConfig": {
    "__init__": [
      "self",
      "modules_to_not_convert",
      "linear_class",
      "quantization_mode",
      "use_rms_norm",
      "rms_norm_eps"
    ],
    "post_init": [
      "self"
    ]
  },
  "SpQRConfig": {
    "__init__": [
      "self",
      "bits",
      "beta1",
      "beta2",
      "shapes",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ]
  },
  "FineGrainedFP8Config": {
    "__init__": [
      "self",
      "activation_scheme",
      "weight_block_size",
      "dequantize",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ],
    "get_loading_attributes": [
      "self"
    ]
  },
  "QuarkConfig": {
    "__init__": [
      "self"
    ]
  },
  "Mxfp4Config": {
    "__init__": [
      "self",
      "modules_to_not_convert",
      "dequantize"
    ],
    "get_loading_attributes": [
      "self"
    ],
    "to_dict": [
      "self"
    ]
  },
  "_DIGIT_RX": [],
  "_pattern_of": [
    "key"
  ],
  "_fmt_indices": [
    "values",
    "cutoff"
  ],
  "update_key_name": [
    "mapping"
  ],
  "_ansi_re": [],
  "_strip_ansi": [
    "s"
  ],
  "_pad": [
    "text",
    "width"
  ],
  "_make_table": [
    "rows",
    "headers"
  ],
  "PALETTE": [],
  "_color": [
    "s",
    "color"
  ],
  "_get_terminal_width": [
    "default"
  ],
  "LoadStateDictInfo": {
    "missing_and_mismatched": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "create_loading_report": [
      "self"
    ]
  },
  "log_state_dict_report": [
    "model",
    "pretrained_model_name_or_path",
    "ignore_mismatched_sizes",
    "loading_info",
    "logger"
  ],
  "_sym_db": [],
  "DESCRIPTOR": [],
  "_TRAINERSPEC_MODELTYPE": [],
  "_MODELPROTO_SENTENCEPIECE_TYPE": [],
  "_TRAINERSPEC": [],
  "_NORMALIZERSPEC": [],
  "_SELFTESTDATA_SAMPLE": [],
  "_SELFTESTDATA": [],
  "_MODELPROTO_SENTENCEPIECE": [],
  "_MODELPROTO": [],
  "TrainerSpec": [],
  "NormalizerSpec": [],
  "SelfTestData": [],
  "ModelProto": [],
  "attach_tracer": [
    "tracer_name_template"
  ],
  "traced": [
    "func"
  ],
  "ContinuousBatchProcessorMetrics": {
    "__init__": [
      "self",
      "max_batch_tokens"
    ],
    "_setup_metrics": [
      "self"
    ],
    "record_ttft_metric": [
      "self",
      "created_time",
      "request_id"
    ],
    "record_batch_metrics": [
      "self",
      "requests_in_batch"
    ],
    "record_kv_cache_memory_metrics": [
      "self",
      "cache"
    ],
    "record_queue_metrics": [
      "self",
      "active_requests",
      "waiting_requests"
    ],
    "record_request_completion": [
      "self",
      "created_time",
      "request_id"
    ]
  },
  "ASTFeatureExtractor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "Speech2TextFeatureExtractor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "Action": {
    "NONE": [],
    "NOTIFY": [],
    "NOTIFY_ALWAYS": [],
    "RAISE": []
  },
  "deprecate_kwarg": [
    "old_name",
    "version",
    "new_name",
    "warn_if_greater_or_equal_version",
    "raise_if_greater_or_equal_version",
    "raise_if_both_names",
    "additional_message"
  ],
  "LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE": [],
  "CHAT_TEMPLATE_FILE": [],
  "CHAT_TEMPLATE_DIR": [],
  "DownloadKwargs": {},
  "HF_MODULES_CACHE": [],
  "TRANSFORMERS_DYNAMIC_MODULE_NAME": [],
  "SESSION_ID": [],
  "S3_BUCKET_PREFIX": [],
  "CLOUDFRONT_DISTRIB_PREFIX": [],
  "_get_cache_file_to_return": [
    "path_or_repo_id",
    "full_filename",
    "cache_dir",
    "revision",
    "repo_type"
  ],
  "list_repo_templates": [
    "repo_id"
  ],
  "define_sagemaker_information": [],
  "http_user_agent": [
    "user_agent"
  ],
  "extract_commit_hash": [
    "resolved_file",
    "commit_hash"
  ],
  "cached_file": [
    "path_or_repo_id",
    "filename"
  ],
  "cached_files": [
    "path_or_repo_id",
    "filenames",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only",
    "subfolder",
    "repo_type",
    "user_agent",
    "_raise_exceptions_for_gated_repo",
    "_raise_exceptions_for_missing_entries",
    "_raise_exceptions_for_connection_errors",
    "_commit_hash"
  ],
  "has_file": [
    "path_or_repo",
    "filename",
    "revision",
    "proxies",
    "token"
  ],
  "PushToHubMixin": {
    "_get_files_timestamps": [
      "self",
      "working_dir"
    ],
    "_upload_modified_files": [
      "self",
      "working_dir",
      "repo_id",
      "files_timestamps",
      "commit_message",
      "token",
      "create_pr",
      "revision",
      "commit_description"
    ],
    "push_to_hub": [
      "self",
      "repo_id"
    ]
  },
  "convert_file_size_to_int": [
    "size"
  ],
  "get_checkpoint_shard_files": [
    "pretrained_model_name_or_path",
    "index_filename",
    "cache_dir",
    "force_download",
    "proxies",
    "local_files_only",
    "token",
    "user_agent",
    "revision",
    "subfolder",
    "_commit_hash"
  ],
  "create_and_tag_model_card": [
    "repo_id",
    "tags",
    "token"
  ],
  "PushInProgress": {
    "__init__": [
      "self",
      "jobs"
    ],
    "is_done": [
      "self"
    ],
    "wait_until_done": [
      "self"
    ],
    "cancel": [
      "self"
    ]
  },
  "TrialShortNamer": {
    "PREFIX": [],
    "DEFAULTS": [],
    "NAMING_INFO": [],
    "set_defaults": [
      "cls",
      "prefix",
      "defaults"
    ],
    "shortname_for_word": [
      "info",
      "word"
    ],
    "shortname_for_key": [
      "info",
      "param_name"
    ],
    "add_new_param_name": [
      "info",
      "param_name"
    ],
    "build_naming_info": [
      "cls"
    ],
    "shortname": [
      "cls",
      "params"
    ],
    "parse_repr": [
      "cls",
      "repr"
    ]
  },
  "GraniteSpeechFeatureExtractor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "GraniteSpeechProcessor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "MusicgenMelodyFeatureExtractor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "MusicgenMelodyProcessor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "TimmWrapperImageProcessor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "positive_any_number": [
    "value"
  ],
  "positive_int": [
    "value"
  ],
  "padding_validator": [
    "value"
  ],
  "truncation_validator": [
    "value"
  ],
  "image_size_validator": [
    "value"
  ],
  "device_validator": [
    "value"
  ],
  "resampling_validator": [
    "value"
  ],
  "video_metadata_validator": [
    "value"
  ],
  "tensor_type_validator": [
    "value"
  ],
  "ChatType": [],
  "BASIC_TYPES": [],
  "description_re": [],
  "args_re": [],
  "args_split_re": [],
  "returns_re": [],
  "TypeHintParsingException": {},
  "DocstringParsingException": {},
  "_get_json_schema_type": [
    "param_type"
  ],
  "_parse_type_hint": [
    "hint"
  ],
  "_convert_type_hints_to_json_schema": [
    "func"
  ],
  "parse_google_format_docstring": [
    "docstring"
  ],
  "get_json_schema": [
    "func"
  ],
  "_render_with_assistant_indices": [
    "compiled_template",
    "messages",
    "tools",
    "documents",
    "add_generation_prompt"
  ],
  "_compile_jinja_template": [
    "chat_template"
  ],
  "render_jinja_template": [
    "conversations",
    "tools",
    "documents",
    "chat_template",
    "return_assistant_tokens_mask",
    "continue_final_message",
    "add_generation_prompt"
  ],
  "is_valid_message": [
    "message"
  ],
  "WEIGHTS_NAME": [],
  "WEIGHTS_INDEX_NAME": [],
  "SAFE_WEIGHTS_NAME": [],
  "SAFE_WEIGHTS_INDEX_NAME": [],
  "CONFIG_NAME": [],
  "FEATURE_EXTRACTOR_NAME": [],
  "IMAGE_PROCESSOR_NAME": [],
  "VIDEO_PROCESSOR_NAME": [],
  "AUDIO_TOKENIZER_NAME": [],
  "PROCESSOR_NAME": [],
  "GENERATION_CONFIG_NAME": [],
  "MODEL_CARD_NAME": [],
  "SENTENCEPIECE_UNDERLINE": [],
  "MULTIPLE_CHOICE_DUMMY_INPUTS": [],
  "DUMMY_INPUTS": [],
  "DUMMY_MASK": [],
  "check_min_version": [
    "min_version"
  ],
  "get_available_devices": [],
  "_globals": [],
  "LayoutLMv2Model": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "_parse_re_match": [
    "node_match"
  ],
  "recursive_parse": [
    "node_content",
    "node_schema"
  ],
  "format_time": [
    "t"
  ],
  "html_progress_bar": [
    "value",
    "total",
    "prefix",
    "label",
    "width"
  ],
  "text_to_html_table": [
    "items"
  ],
  "NotebookProgressBar": {
    "warmup": [],
    "update_every": [],
    "__init__": [
      "self",
      "total",
      "prefix",
      "leave",
      "parent",
      "width"
    ],
    "update": [
      "self",
      "value",
      "force_update",
      "comment"
    ],
    "update_bar": [
      "self",
      "value",
      "comment"
    ],
    "display": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "NotebookTrainingTracker": {
    "__init__": [
      "self",
      "num_steps",
      "column_names"
    ],
    "display": [
      "self"
    ],
    "write_line": [
      "self",
      "values"
    ],
    "add_child": [
      "self",
      "total",
      "prefix",
      "width"
    ],
    "remove_child": [
      "self"
    ]
  },
  "NotebookProgressCallback": {
    "__init__": [
      "self"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_step_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_prediction_step": [
      "self",
      "args",
      "state",
      "control",
      "eval_dataloader"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs"
    ],
    "on_evaluate": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "PACKAGE_DISTRIBUTION_MAPPING": [],
  "_is_package_available": [
    "pkg_name",
    "return_version"
  ],
  "is_env_variable_true": [
    "env_variable"
  ],
  "is_env_variable_false": [
    "env_variable"
  ],
  "ENV_VARS_TRUE_VALUES": [],
  "ENV_VARS_TRUE_AND_AUTO_VALUES": [],
  "USE_TORCH_XLA": [],
  "ACCELERATE_MIN_VERSION": [],
  "BITSANDBYTES_MIN_VERSION": [],
  "SCHEDULEFREE_MIN_VERSION": [],
  "FSDP_MIN_VERSION": [],
  "GGUF_MIN_VERSION": [],
  "XLA_FSDPV2_MIN_VERSION": [],
  "HQQ_MIN_VERSION": [],
  "VPTQ_MIN_VERSION": [],
  "TORCHAO_MIN_VERSION": [],
  "AUTOROUND_MIN_VERSION": [],
  "TRITON_MIN_VERSION": [],
  "KERNELS_MIN_VERSION": [],
  "is_torch_available": [],
  "get_torch_version": [],
  "is_torch_greater_or_equal": [
    "library_version",
    "accept_dev"
  ],
  "is_torch_less_or_equal": [
    "library_version",
    "accept_dev"
  ],
  "is_torch_accelerator_available": [],
  "is_torch_cuda_available": [],
  "is_cuda_platform": [],
  "is_rocm_platform": [],
  "is_habana_gaudi1": [],
  "is_torch_mps_available": [
    "min_version"
  ],
  "is_torch_npu_available": [
    "check_device"
  ],
  "is_torch_xpu_available": [
    "check_device"
  ],
  "is_torch_mlu_available": [],
  "is_torch_musa_available": [
    "check_device"
  ],
  "is_torch_xla_available": [
    "check_is_tpu",
    "check_is_gpu"
  ],
  "is_torch_hpu_available": [],
  "is_torch_bf16_gpu_available": [],
  "is_torch_fp16_available_on_device": [
    "device"
  ],
  "is_torch_bf16_available_on_device": [
    "device"
  ],
  "is_torch_tf32_available": [],
  "enable_tf32": [
    "enable"
  ],
  "is_torch_flex_attn_available": [],
  "is_grouped_mm_available": [],
  "is_kenlm_available": [],
  "is_kernels_available": [
    "MIN_VERSION"
  ],
  "is_cv2_available": [],
  "is_yt_dlp_available": [],
  "is_libcst_available": [],
  "is_accelerate_available": [
    "min_version"
  ],
  "is_triton_available": [
    "min_version"
  ],
  "is_hadamard_available": [],
  "is_hqq_available": [
    "min_version"
  ],
  "is_pygments_available": [],
  "is_torchvision_available": [],
  "is_torchvision_v2_available": [],
  "is_galore_torch_available": [],
  "is_apollo_torch_available": [],
  "is_torch_optimi_available": [],
  "is_lomo_available": [],
  "is_grokadamw_available": [],
  "is_schedulefree_available": [
    "min_version"
  ],
  "is_pyctcdecode_available": [],
  "is_librosa_available": [],
  "is_essentia_available": [],
  "is_pydantic_available": [],
  "is_fastapi_available": [],
  "is_uvicorn_available": [],
  "is_openai_available": [],
  "is_pretty_midi_available": [],
  "is_mamba_ssm_available": [],
  "is_mamba_2_ssm_available": [],
  "is_flash_linear_attention_available": [],
  "is_causal_conv1d_available": [],
  "is_xlstm_available": [],
  "is_mambapy_available": [],
  "is_peft_available": [],
  "is_bs4_available": [],
  "is_coloredlogs_available": [],
  "is_onnx_available": [],
  "is_flute_available": [],
  "is_g2p_en_available": [],
  "is_torch_neuroncore_available": [
    "check_device"
  ],
  "is_torch_tensorrt_fx_available": [],
  "is_datasets_available": [],
  "is_detectron2_available": [],
  "is_rjieba_available": [],
  "is_psutil_available": [],
  "is_py3nvml_available": [],
  "is_sacremoses_available": [],
  "is_apex_available": [],
  "is_aqlm_available": [],
  "is_vptq_available": [
    "min_version"
  ],
  "is_av_available": [],
  "is_decord_available": [],
  "is_torchcodec_available": [],
  "is_ninja_available": [],
  "is_bitsandbytes_available": [
    "min_version"
  ],
  "is_flash_attn_2_available": [],
  "is_flash_attn_3_available": [],
  "is_flash_attn_greater_or_equal_2_10": [],
  "is_flash_attn_greater_or_equal": [
    "library_version"
  ],
  "is_huggingface_hub_greater_or_equal": [
    "library_version",
    "accept_dev"
  ],
  "is_quanto_greater": [
    "library_version",
    "accept_dev"
  ],
  "is_torchdistx_available": [],
  "is_faiss_available": [],
  "is_scipy_available": [],
  "is_sklearn_available": [],
  "is_sentencepiece_available": [],
  "is_seqio_available": [],
  "is_gguf_available": [
    "min_version"
  ],
  "is_protobuf_available": [],
  "is_fsdp_available": [
    "min_version"
  ],
  "is_optimum_available": [],
  "is_llm_awq_available": [],
  "is_auto_round_available": [
    "min_version"
  ],
  "is_optimum_quanto_available": [],
  "is_quark_available": [],
  "is_fp_quant_available": [],
  "is_qutlass_available": [],
  "is_compressed_tensors_available": [],
  "is_gptqmodel_available": [],
  "is_fbgemm_gpu_available": [],
  "is_levenshtein_available": [],
  "is_optimum_neuron_available": [],
  "is_tokenizers_available": [],
  "is_vision_available": [],
  "is_pytesseract_available": [],
  "is_pytest_available": [],
  "is_pytest_order_available": [],
  "is_spacy_available": [],
  "is_pytorch_quantization_available": [],
  "is_pandas_available": [],
  "is_soundfile_available": [],
  "is_timm_available": [],
  "is_natten_available": [],
  "is_nltk_available": [],
  "is_numba_available": [],
  "is_torchaudio_available": [],
  "is_torchao_available": [
    "min_version"
  ],
  "is_speech_available": [],
  "is_spqr_available": [],
  "is_phonemizer_available": [],
  "is_uroman_available": [],
  "is_sudachi_available": [],
  "is_sudachi_projection_available": [],
  "is_jumanpp_available": [],
  "is_cython_available": [],
  "is_jinja_available": [],
  "is_jmespath_available": [],
  "is_mlx_available": [],
  "is_num2words_available": [],
  "is_tiktoken_available": [],
  "is_liger_kernel_available": [],
  "is_rich_available": [],
  "is_matplotlib_available": [],
  "is_mistral_common_available": [],
  "is_opentelemetry_available": [],
  "check_torch_load_is_safe": [],
  "torch_only_method": [
    "fn"
  ],
  "is_torch_deterministic": [],
  "get_torch_major_and_minor_version": [],
  "is_torchdynamo_compiling": [],
  "is_torchdynamo_exporting": [],
  "is_torch_fx_proxy": [
    "x"
  ],
  "is_fake_tensor": [
    "x"
  ],
  "is_jax_jitting": [
    "x"
  ],
  "is_jit_tracing": [],
  "is_cuda_stream_capturing": [],
  "is_tracing": [
    "tensor"
  ],
  "torch_compilable_check": [
    "cond",
    "msg",
    "error_type"
  ],
  "is_in_notebook": [],
  "is_sagemaker_dp_enabled": [],
  "is_sagemaker_mp_enabled": [],
  "is_training_run_on_sagemaker": [],
  "AV_IMPORT_ERROR": [],
  "YT_DLP_IMPORT_ERROR": [],
  "DECORD_IMPORT_ERROR": [],
  "TORCHCODEC_IMPORT_ERROR": [],
  "CV2_IMPORT_ERROR": [],
  "DATASETS_IMPORT_ERROR": [],
  "TOKENIZERS_IMPORT_ERROR": [],
  "SENTENCEPIECE_IMPORT_ERROR": [],
  "PROTOBUF_IMPORT_ERROR": [],
  "FAISS_IMPORT_ERROR": [],
  "PYTORCH_IMPORT_ERROR": [],
  "TORCHVISION_IMPORT_ERROR": [],
  "BS4_IMPORT_ERROR": [],
  "SKLEARN_IMPORT_ERROR": [],
  "DETECTRON2_IMPORT_ERROR": [],
  "LEVENSHTEIN_IMPORT_ERROR": [],
  "G2P_EN_IMPORT_ERROR": [],
  "PYTORCH_QUANTIZATION_IMPORT_ERROR": [],
  "TORCHAUDIO_IMPORT_ERROR": [],
  "PANDAS_IMPORT_ERROR": [],
  "PHONEMIZER_IMPORT_ERROR": [],
  "UROMAN_IMPORT_ERROR": [],
  "SACREMOSES_IMPORT_ERROR": [],
  "SCIPY_IMPORT_ERROR": [],
  "SPEECH_IMPORT_ERROR": [],
  "TIMM_IMPORT_ERROR": [],
  "NATTEN_IMPORT_ERROR": [],
  "NUMEXPR_IMPORT_ERROR": [],
  "NLTK_IMPORT_ERROR": [],
  "VISION_IMPORT_ERROR": [],
  "PYDANTIC_IMPORT_ERROR": [],
  "FASTAPI_IMPORT_ERROR": [],
  "UVICORN_IMPORT_ERROR": [],
  "OPENAI_IMPORT_ERROR": [],
  "PYTESSERACT_IMPORT_ERROR": [],
  "PYCTCDECODE_IMPORT_ERROR": [],
  "ACCELERATE_IMPORT_ERROR": [],
  "ESSENTIA_IMPORT_ERROR": [],
  "LIBROSA_IMPORT_ERROR": [],
  "PRETTY_MIDI_IMPORT_ERROR": [],
  "CYTHON_IMPORT_ERROR": [],
  "RJIEBA_IMPORT_ERROR": [],
  "PEFT_IMPORT_ERROR": [],
  "JINJA_IMPORT_ERROR": [],
  "RICH_IMPORT_ERROR": [],
  "MISTRAL_COMMON_IMPORT_ERROR": [],
  "BACKENDS_MAPPING": [],
  "requires_backends": [
    "obj",
    "backends"
  ],
  "DummyObject": {
    "is_dummy": [],
    "__getattribute__": [
      "cls",
      "key"
    ]
  },
  "BACKENDS_T": [],
  "IMPORT_STRUCTURE_T": [],
  "_LazyModule": {
    "__init__": [
      "self",
      "name",
      "module_file",
      "import_structure",
      "module_spec",
      "extra_objects",
      "explicit_import_shortcut"
    ],
    "__dir__": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "_get_module": [
      "self",
      "module_name"
    ],
    "__reduce__": [
      "self"
    ]
  },
  "OptionalDependencyNotAvailable": {},
  "direct_transformers_import": [
    "path",
    "file"
  ],
  "VersionComparison": {
    "EQUAL": [],
    "NOT_EQUAL": [],
    "GREATER_THAN": [],
    "LESS_THAN": [],
    "GREATER_THAN_OR_EQUAL": [],
    "LESS_THAN_OR_EQUAL": [],
    "from_string": [
      "version_string"
    ]
  },
  "split_package_version": [
    "package_version_str"
  ],
  "Backend": {
    "__init__": [
      "self",
      "backend_requirement"
    ],
    "get_installed_version": [
      "self"
    ],
    "is_satisfied": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "error_message": [
      "self"
    ]
  },
  "requires": [],
  "BASE_FILE_REQUIREMENTS": [],
  "fetch__all__": [
    "file_content"
  ],
  "create_import_structure_from_path": [
    "module_path"
  ],
  "spread_import_structure": [
    "nested_import_structure"
  ],
  "define_import_structure": [
    "module_path",
    "prefix"
  ],
  "clear_import_cache": [],
  "_lock": [],
  "_default_log_level": [],
  "_tqdm_active": [],
  "_get_default_logging_level": [],
  "_get_library_name": [],
  "_get_library_root_logger": [],
  "_configure_library_root_logger": [],
  "_reset_library_root_logger": [],
  "get_log_levels_dict": [],
  "captureWarnings": [
    "capture"
  ],
  "get_logger": [
    "name"
  ],
  "get_verbosity": [],
  "set_verbosity": [
    "verbosity"
  ],
  "set_verbosity_info": [],
  "set_verbosity_warning": [],
  "set_verbosity_debug": [],
  "set_verbosity_error": [],
  "disable_default_handler": [],
  "enable_default_handler": [],
  "add_handler": [
    "handler"
  ],
  "remove_handler": [
    "handler"
  ],
  "disable_propagation": [],
  "enable_propagation": [],
  "enable_explicit_format": [],
  "reset_format": [],
  "warning_advice": [
    "self"
  ],
  "warning_once": [
    "self"
  ],
  "info_once": [
    "self"
  ],
  "EmptyTqdm": {
    "__init__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__getattr__": [
      "self",
      "_"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "type_",
      "value",
      "traceback"
    ]
  },
  "_tqdm_cls": {
    "__call__": [
      "self"
    ],
    "set_lock": [
      "self"
    ],
    "get_lock": [
      "self"
    ]
  },
  "tqdm": [],
  "is_progress_bar_enabled": [],
  "enable_progress_bar": [],
  "disable_progress_bar": [],
  "Pop2PianoFeatureExtractor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "Pop2PianoTokenizer": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "Pop2PianoProcessor": {
    "_backends": [],
    "__init__": [
      "self"
    ]
  },
  "_CAN_RECORD_REGISTRY": [],
  "_is_torch_available": [],
  "_is_mlx_available": [],
  "strtobool": [
    "val"
  ],
  "infer_framework_from_repr": [
    "x"
  ],
  "_get_frameworks_and_test_func": [
    "x"
  ],
  "is_tensor": [
    "x"
  ],
  "is_numpy_array": [
    "x"
  ],
  "is_torch_tensor": [
    "x"
  ],
  "is_torch_device": [
    "x"
  ],
  "is_torch_dtype": [
    "x"
  ],
  "_is_tensor_or_array_like": [
    "value"
  ],
  "maybe_autocast": [
    "device_type",
    "dtype",
    "enabled",
    "cache_enabled"
  ],
  "_is_mlx": [
    "x"
  ],
  "is_mlx_array": [
    "x"
  ],
  "is_flash_attention_requested": [
    "config",
    "requested_attention_implementation"
  ],
  "to_py_obj": [
    "obj"
  ],
  "to_numpy": [
    "obj"
  ],
  "safe_load_json_file": [
    "json_file"
  ],
  "ModelOutput": {
    "__init_subclass__": [
      "cls"
    ],
    "__init__": [
      "self"
    ],
    "__post_init__": [
      "self"
    ],
    "__delitem__": [
      "self"
    ],
    "setdefault": [
      "self"
    ],
    "pop": [
      "self"
    ],
    "update": [
      "self"
    ],
    "__getitem__": [
      "self",
      "k"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ],
    "__reduce__": [
      "self"
    ],
    "to_tuple": [
      "self"
    ]
  },
  "ExplicitEnum": {
    "_missing_": [
      "cls",
      "value"
    ]
  },
  "PaddingStrategy": {
    "LONGEST": [],
    "MAX_LENGTH": [],
    "DO_NOT_PAD": []
  },
  "TensorType": {
    "PYTORCH": [],
    "NUMPY": [],
    "MLX": []
  },
  "ContextManagers": {
    "__init__": [
      "self",
      "context_managers"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "can_return_loss": [
    "model_class"
  ],
  "find_labels": [
    "model_class"
  ],
  "flatten_dict": [
    "d",
    "parent_key",
    "delimiter"
  ],
  "transpose": [
    "array",
    "axes"
  ],
  "reshape": [
    "array",
    "newshape"
  ],
  "squeeze": [
    "array",
    "axis"
  ],
  "expand_dims": [
    "array",
    "axis"
  ],
  "tensor_size": [
    "array"
  ],
  "torch_int": [
    "x"
  ],
  "torch_float": [
    "x"
  ],
  "filter_out_non_signature_kwargs": [
    "extra"
  ],
  "TransformersKwargs": {},
  "is_timm_config_dict": [
    "config_dict"
  ],
  "is_timm_local_checkpoint": [
    "pretrained_model_path"
  ],
  "set_attribute_for_modules": [
    "module",
    "key",
    "value"
  ],
  "del_attribute_from_modules": [
    "module",
    "key"
  ],
  "can_return_tuple": [
    "func"
  ],
  "OutputRecorder": {},
  "check_model_inputs": [
    "func"
  ],
  "GeneralInterface": {
    "_global_mapping": [],
    "__init__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ],
    "__delitem__": [
      "self",
      "key"
    ],
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "register": [
      "cls",
      "key",
      "value"
    ],
    "valid_keys": [
      "self"
    ]
  },
  "ADAPTER_CONFIG_NAME": [],
  "ADAPTER_WEIGHTS_NAME": [],
  "ADAPTER_SAFE_WEIGHTS_NAME": [],
  "find_adapter_config_file": [
    "model_id",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only",
    "subfolder",
    "_commit_hash"
  ],
  "check_peft_version": [
    "min_version"
  ],
  "IMAGENET_DEFAULT_MEAN": [],
  "IMAGENET_DEFAULT_STD": [],
  "IMAGENET_STANDARD_MEAN": [],
  "IMAGENET_STANDARD_STD": [],
  "OPENAI_CLIP_MEAN": [],
  "OPENAI_CLIP_STD": [],
  "ops": [],
  "_compare_versions": [
    "op",
    "got_ver",
    "want_ver",
    "requirement",
    "pkg",
    "hint"
  ],
  "require_version": [
    "requirement",
    "hint"
  ],
  "require_version_core": [
    "requirement"
  ],
  "generate_attention_matrix_from_mask": [
    "words",
    "mask",
    "img_token",
    "sliding_window",
    "token_type_ids",
    "image_seq_length"
  ],
  "AttentionMaskVisualizer": {
    "__init__": [
      "self",
      "model_name"
    ],
    "__call__": [
      "self",
      "input_sentence",
      "suffix"
    ],
    "visualize_attention_mask": [
      "self",
      "input_sentence",
      "suffix"
    ]
  },
  "get_docstring_indentation_level": [
    "func"
  ],
  "add_start_docstrings": [],
  "add_start_docstrings_to_model_forward": [],
  "add_end_docstrings": [],
  "PT_RETURN_INTRODUCTION": [],
  "_get_indent": [
    "t"
  ],
  "_convert_output_args_doc": [
    "output_args_doc"
  ],
  "_prepare_output_docstrings": [
    "output_type",
    "config_class",
    "min_indent",
    "add_intro"
  ],
  "FAKE_MODEL_DISCLAIMER": [],
  "PT_TOKEN_CLASSIFICATION_SAMPLE": [],
  "PT_QUESTION_ANSWERING_SAMPLE": [],
  "PT_SEQUENCE_CLASSIFICATION_SAMPLE": [],
  "PT_MASKED_LM_SAMPLE": [],
  "PT_BASE_MODEL_SAMPLE": [],
  "PT_MULTIPLE_CHOICE_SAMPLE": [],
  "PT_CAUSAL_LM_SAMPLE": [],
  "PT_SPEECH_BASE_MODEL_SAMPLE": [],
  "PT_SPEECH_CTC_SAMPLE": [],
  "PT_SPEECH_SEQ_CLASS_SAMPLE": [],
  "PT_SPEECH_FRAME_CLASS_SAMPLE": [],
  "PT_SPEECH_XVECTOR_SAMPLE": [],
  "PT_VISION_BASE_MODEL_SAMPLE": [],
  "PT_VISION_SEQ_CLASS_SAMPLE": [],
  "PT_SAMPLE_DOCSTRINGS": [],
  "TEXT_TO_AUDIO_SPECTROGRAM_SAMPLE": [],
  "TEXT_TO_AUDIO_WAVEFORM_SAMPLE": [],
  "AUDIO_FRAME_CLASSIFICATION_SAMPLE": [],
  "AUDIO_XVECTOR_SAMPLE": [],
  "DEPTH_ESTIMATION_SAMPLE": [],
  "VIDEO_CLASSIFICATION_SAMPLE": [],
  "ZERO_SHOT_OBJECT_DETECTION_SAMPLE": [],
  "IMAGE_TO_IMAGE_SAMPLE": [],
  "IMAGE_FEATURE_EXTRACTION_SAMPLE": [],
  "DOCUMENT_QUESTION_ANSWERING_SAMPLE": [],
  "NEXT_SENTENCE_PREDICTION_SAMPLE": [],
  "MULTIPLE_CHOICE_SAMPLE": [],
  "PRETRAINING_SAMPLE": [],
  "MASK_GENERATION_SAMPLE": [],
  "VISUAL_QUESTION_ANSWERING_SAMPLE": [],
  "TEXT_GENERATION_SAMPLE": [],
  "IMAGE_CLASSIFICATION_SAMPLE": [],
  "IMAGE_SEGMENTATION_SAMPLE": [],
  "FILL_MASK_SAMPLE": [],
  "OBJECT_DETECTION_SAMPLE": [],
  "QUESTION_ANSWERING_SAMPLE": [],
  "TEXT_CLASSIFICATION_SAMPLE": [],
  "TABLE_QUESTION_ANSWERING_SAMPLE": [],
  "TOKEN_CLASSIFICATION_SAMPLE": [],
  "AUDIO_CLASSIFICATION_SAMPLE": [],
  "AUTOMATIC_SPEECH_RECOGNITION_SAMPLE": [],
  "ZERO_SHOT_IMAGE_CLASSIFICATION_SAMPLE": [],
  "IMAGE_TEXT_TO_TEXT_GENERATION_SAMPLE": [],
  "PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS": [],
  "MODELS_TO_PIPELINE": [],
  "filter_outputs_from_example": [
    "docstring"
  ],
  "add_code_sample_docstrings": [],
  "replace_return_docstrings": [
    "output_type",
    "config_class"
  ],
  "copy_func": [
    "f"
  ],
  "_base_test_name": [
    "nodeid"
  ],
  "_class_name": [
    "nodeid"
  ],
  "_file_path": [
    "nodeid"
  ],
  "_modeling_key": [
    "file_path"
  ],
  "summarize": [
    "report_path"
  ],
  "FbgemmFp8Quantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model"
    ]
  },
  "FbgemmFp8Linear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FbgemmFp8Llama4TextExperts": {
    "__init__": [
      "self",
      "config",
      "dtype"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "get_quantize_fp8_per_row": [],
  "replace_with_fbgemm_fp8_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config",
    "pre_quantized",
    "tp_plan"
  ],
  "convert_tekken_tokenizer": [
    "tokenizer_file"
  ],
  "_MIN_COMET_VERSION": [],
  "_has_neptune": [],
  "is_wandb_available": [],
  "is_trackio_available": [],
  "is_clearml_available": [],
  "is_comet_available": [],
  "is_tensorboard_available": [],
  "is_optuna_available": [],
  "is_ray_available": [],
  "is_ray_tune_available": [],
  "is_azureml_available": [],
  "is_mlflow_available": [],
  "is_dagshub_available": [],
  "is_neptune_available": [],
  "is_codecarbon_available": [],
  "is_flytekit_available": [],
  "is_flyte_deck_standard_available": [],
  "is_dvclive_available": [],
  "is_swanlab_available": [],
  "hp_params": [
    "trial"
  ],
  "run_hp_search_optuna": [
    "trainer",
    "n_trials",
    "direction"
  ],
  "run_hp_search_ray": [
    "trainer",
    "n_trials",
    "direction"
  ],
  "run_hp_search_wandb": [
    "trainer",
    "n_trials",
    "direction"
  ],
  "get_available_reporting_integrations": [],
  "rewrite_logs": [
    "d"
  ],
  "default_logdir": [],
  "TensorBoardCallback": {
    "__init__": [
      "self",
      "tb_writer"
    ],
    "_init_summary_writer": [
      "self",
      "args"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "save_model_architecture_to_file": [
    "model",
    "output_dir"
  ],
  "WandbLogModel": {
    "CHECKPOINT": [],
    "END": [],
    "FALSE": [],
    "is_enabled": [
      "self"
    ],
    "_missing_": [
      "cls",
      "value"
    ]
  },
  "WandbCallback": {
    "__init__": [
      "self"
    ],
    "setup": [
      "self",
      "args",
      "state",
      "model"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "processing_class"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "logs"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ]
  },
  "TrackioCallback": {
    "SPACE_URL": [],
    "__init__": [
      "self"
    ],
    "setup": [
      "self",
      "args",
      "state",
      "model"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "processing_class"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "logs"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ],
    "on_push_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ]
  },
  "CometCallback": {
    "__init__": [
      "self"
    ],
    "setup": [
      "self",
      "args",
      "state",
      "model"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "logs"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ]
  },
  "AzureMLCallback": {
    "__init__": [
      "self",
      "azureml_run"
    ],
    "on_init_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs"
    ]
  },
  "MLflowCallback": {
    "__init__": [
      "self"
    ],
    "setup": [
      "self",
      "args",
      "state",
      "model"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs",
      "model"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "__del__": [
      "self"
    ]
  },
  "DagsHubCallback": {
    "__init__": [
      "self"
    ],
    "setup": [
      "self"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "NeptuneMissingConfiguration": {
    "__init__": [
      "self"
    ]
  },
  "NeptuneCallback": {
    "integration_version_key": [],
    "model_parameters_key": [],
    "trial_name_key": [],
    "trial_params_key": [],
    "trainer_parameters_key": [],
    "flat_metrics": [],
    "__init__": [
      "self"
    ],
    "_stop_run_if_exists": [
      "self"
    ],
    "_initialize_run": [
      "self"
    ],
    "_use_initial_run": [
      "self"
    ],
    "_ensure_run_with_monitoring": [
      "self"
    ],
    "_ensure_at_least_run_without_monitoring": [
      "self"
    ],
    "run": [
      "self"
    ],
    "_metadata_namespace": [
      "self"
    ],
    "_log_integration_version": [
      "self"
    ],
    "_log_trainer_parameters": [
      "self",
      "args"
    ],
    "_log_model_parameters": [
      "self",
      "model"
    ],
    "_log_hyper_param_search_parameters": [
      "self",
      "state"
    ],
    "_log_model_checkpoint": [
      "self",
      "source_directory",
      "checkpoint"
    ],
    "on_init_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "__del__": [
      "self"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_evaluate": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ],
    "get_run": [
      "cls",
      "trainer"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "logs"
    ]
  },
  "CodeCarbonCallback": {
    "__init__": [
      "self"
    ],
    "on_init_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "ClearMLCallback": {
    "log_suffix": [],
    "_hparams_section": [],
    "_model_config_section": [],
    "_ignore_hparams_overrides": [],
    "_ignoge_model_config_overrides": [],
    "_model_config_description": [],
    "_model_config_description_note": [],
    "_train_run_counter": [],
    "_model_connect_counter": [],
    "_task_created_in_callback": [],
    "_should_close_on_train_end": [],
    "__init__": [
      "self"
    ],
    "setup": [
      "self",
      "args",
      "state",
      "model",
      "processing_class"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "processing_class"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "processing_class",
      "logs"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "_copy_training_args_as_hparams": [
      "self",
      "training_args",
      "prefix"
    ]
  },
  "FlyteCallback": {
    "__init__": [
      "self",
      "save_log_history",
      "sync_checkpoints"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "DVCLiveCallback": {
    "__init__": [
      "self",
      "live",
      "log_model"
    ],
    "setup": [
      "self",
      "args",
      "state",
      "model"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "logs"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control"
    ]
  },
  "SwanLabCallback": {
    "__init__": [
      "self"
    ],
    "setup": [
      "self",
      "args",
      "state",
      "model"
    ],
    "on_train_begin": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_train_end": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "processing_class"
    ],
    "on_log": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "logs"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control"
    ],
    "on_predict": [
      "self",
      "args",
      "state",
      "control",
      "metrics"
    ]
  },
  "INTEGRATION_TO_CALLBACK": [],
  "get_reporting_integration_callbacks": [
    "report_to"
  ],
  "replace_with_spqr_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config"
  ],
  "AWQ_SCALES_MAPPINGS": [],
  "replace_quantization_scales": [
    "model",
    "model_type"
  ],
  "replace_with_awq_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config",
    "device_map"
  ],
  "FpQuantQuantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "missing_keys"
    ]
  },
  "FpQuantDeserialize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name",
      "missing_keys"
    ]
  },
  "adapt_fp_quant_config": [
    "config"
  ],
  "QuarkDeserialize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "missing_keys",
      "full_layer_name"
    ]
  },
  "replace_with_aqlm_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config"
  ],
  "is_kernel": [
    "attn_implementation"
  ],
  "load_and_register_attn_kernel": [
    "attn_implementation",
    "attention_wrapper"
  ],
  "lazy_load_kernel": [
    "kernel_name",
    "mapping"
  ],
  "get_kernel": [
    "kernel_name",
    "revision",
    "version"
  ],
  "use_kernelized_func": [
    "module_names"
  ],
  "TorchAoQuantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name",
      "missing_keys"
    ]
  },
  "TorchAoDeserialize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "model",
      "full_layer_name",
      "missing_keys"
    ]
  },
  "is_fsdp_managed_module": [
    "module"
  ],
  "is_fsdp_enabled": [],
  "FP4_VALUES": [],
  "on_device": [
    "dev"
  ],
  "Mxfp4Quantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "missing_keys",
      "full_layer_name"
    ]
  },
  "Mxfp4Dequantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name",
      "missing_keys"
    ]
  },
  "Mxfp4Deserialize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name",
      "missing_keys"
    ]
  },
  "quantize_to_mxfp4": [
    "w",
    "triton_kernels_hub"
  ],
  "swizzle_mxfp4": [
    "w",
    "w_scale",
    "triton_kernels_hub"
  ],
  "_convert_moe_packed_tensors": [
    "blocks",
    "scales"
  ],
  "convert_moe_packed_tensors": [
    "blocks",
    "scales"
  ],
  "Mxfp4GptOssExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "routing_data",
      "gather_idx",
      "scatter_idx"
    ]
  },
  "routing_torch_dist": [
    "logits",
    "n_expts_act"
  ],
  "mlp_forward": [
    "self",
    "hidden_states"
  ],
  "dequantize": [
    "module",
    "param_name",
    "param_value",
    "target_device",
    "dq_param_name"
  ],
  "dequantize_convertops": [
    "blocks",
    "scales"
  ],
  "load_and_swizzle_mxfp4": [
    "module",
    "param_name",
    "param_value",
    "target_device",
    "triton_kernels_hub"
  ],
  "swizzle_mxfp4_convertops": [
    "blocks",
    "scales",
    "module",
    "proj",
    "target_device",
    "triton_kernels_hub"
  ],
  "replace_with_mxfp4_linear": [
    "model",
    "quantization_config",
    "modules_to_not_convert"
  ],
  "repeat_kv": [
    "hidden_states",
    "n_rep"
  ],
  "eager_paged_attention_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "scaling"
  ],
  "Bnb4bitQuantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "full_layer_name",
      "model"
    ]
  },
  "Bnb4bitDeserialize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name"
    ]
  },
  "Bnb8bitQuantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name"
    ]
  },
  "Bnb8bitDeserialize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name"
    ]
  },
  "replace_with_bnb_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config",
    "pre_quantized"
  ],
  "dequantize_bnb_weight": [
    "weight",
    "state"
  ],
  "_create_accelerate_new_hook": [
    "old_hook"
  ],
  "dequantize_and_replace": [
    "model",
    "quantization_config",
    "dtype"
  ],
  "validate_bnb_backend_availability": [
    "raise_exception"
  ],
  "QuantoQuantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "model",
      "full_layer_name",
      "missing_keys"
    ]
  },
  "replace_with_quanto_layers": [
    "model",
    "quantization_config",
    "modules_to_not_convert"
  ],
  "convert_tiktoken_to_fast": [
    "encoding",
    "output_dir"
  ],
  "TorchExportableModuleForVLM": {
    "__init__": [
      "self",
      "model",
      "max_batch_size",
      "max_cache_len"
    ],
    "export_vision_encoder": [
      "self"
    ],
    "export_connector": [
      "self"
    ],
    "export_text_decoder": [
      "self"
    ],
    "export": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "cache_position"
    ],
    "generate": [
      "self",
      "pixel_values",
      "input_ids",
      "max_new_tokens",
      "do_sample",
      "temperature"
    ]
  },
  "TorchExportableModuleForDecoderOnlyLM": {
    "__init__": [
      "self",
      "model",
      "batch_size",
      "max_cache_len",
      "device"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_position"
    ],
    "export": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_position",
      "dynamic_shapes",
      "strict"
    ],
    "generate": [
      "exported_program",
      "tokenizer",
      "prompt",
      "max_new_tokens",
      "do_sample",
      "temperature",
      "top_k",
      "top_p",
      "device"
    ]
  },
  "TorchExportableModuleWithHybridCache": {
    "__init__": [
      "self",
      "model",
      "batch_size",
      "max_cache_len",
      "device"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_position"
    ]
  },
  "Seq2SeqLMEncoderExportableModule": {
    "__init__": [
      "self",
      "encoder_model"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "Seq2SeqLMDecoderExportableModuleWithStaticCache": {
    "__init__": [
      "self",
      "model",
      "max_static_cache_length",
      "batch_size"
    ],
    "forward": [
      "self",
      "decoder_input_ids",
      "encoder_hidden_states",
      "cache_position"
    ]
  },
  "Seq2SeqLMExportableModule": {
    "__init__": [
      "self",
      "model",
      "batch_size",
      "max_hidden_seq_length",
      "cache_implementation",
      "max_cache_length"
    ],
    "_export_encoder": [
      "self",
      "encoder_input_ids"
    ],
    "_export_decoder": [
      "self",
      "decoder_input_ids",
      "encoder_hidden_states",
      "cache_position"
    ],
    "export": [
      "self",
      "encoder_input_ids",
      "decoder_input_ids",
      "encoder_hidden_states",
      "cache_position"
    ],
    "generate": [
      "self",
      "prompt_token_ids",
      "max_new_tokens"
    ]
  },
  "export_with_dynamic_cache": [
    "model",
    "example_input_ids",
    "example_attention_mask"
  ],
  "register_dynamic_cache_export_support": [],
  "_get_cache_dict": [
    "cache"
  ],
  "_unflatten_dynamic_cache": [
    "values",
    "context"
  ],
  "EetqQuantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "full_layer_name"
    ]
  },
  "EetqLinearMMFunction": {
    "forward": [
      "ctx",
      "x",
      "weight",
      "scales",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "EetqLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "dtype",
      "bias"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "replace_with_eetq_linear": [
    "model",
    "modules_to_not_convert",
    "pre_quantized"
  ],
  "TOP_LEFT_ALIGNED_CAUSAL_MASK_MODE": [],
  "DOWN_RIGHT_ALIGNED_CAUSAL_MASK_MODE": [],
  "SPARSE_MODE": [],
  "ATTN_MASK_NPU_CACHE": [],
  "get_attn_mask_npu": [
    "device"
  ],
  "is_npu_fa2_top_left_aligned_causal_mask": [],
  "npu_flash_attn_func": [
    "q",
    "k",
    "v",
    "dropout_p",
    "softmax_scale",
    "causal"
  ],
  "npu_flash_attn_varlen_func": [
    "q",
    "k",
    "v",
    "cu_seqlens_q",
    "cu_seqlens_k",
    "max_seqlen_q",
    "max_seqlen_k",
    "dropout_p",
    "softmax_scale",
    "causal"
  ],
  "autoname_modules": [
    "model"
  ],
  "name_to_linear_tag": [
    "name"
  ],
  "get_linear_tags": [
    "model"
  ],
  "_prepare_for_hqq_linear": [
    "model",
    "patch_params",
    "has_been_replaced",
    "current_key_name"
  ],
  "prepare_for_hqq_linear": [
    "model",
    "quantization_config",
    "modules_to_not_convert",
    "has_been_replaced"
  ],
  "GGUF_CONFIG_MAPPING": [],
  "GGUF_TOKENIZER_MAPPING": [],
  "GGUF_CONFIG_DEFAULTS_MAPPING": [],
  "_gguf_parse_value": [
    "_value",
    "data_type"
  ],
  "GGUFTokenizerSkeleton": {
    "__init__": [
      "self",
      "dict_"
    ]
  },
  "GGUFLlamaConverter": {
    "__init__": [
      "self",
      "tokenizer_dict"
    ],
    "vocab": [
      "self",
      "proto"
    ],
    "merges": [
      "self",
      "proto"
    ],
    "tokenizer": [
      "self",
      "proto"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "converted": [
      "self"
    ]
  },
  "GGUFQwen2Converter": {
    "__init__": [
      "self",
      "tokenizer_dict"
    ],
    "converted": [
      "self"
    ]
  },
  "GGUFPhi3Converter": {
    "__init__": [
      "self",
      "tokenizer_dict"
    ],
    "vocab": [
      "self",
      "proto"
    ],
    "merges": [
      "self",
      "proto"
    ],
    "tokenizer": [
      "self",
      "proto"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "converted": [
      "self"
    ]
  },
  "GGUFGPTConverter": {
    "__init__": [
      "self",
      "tokenizer_dict"
    ],
    "converted": [
      "self"
    ]
  },
  "GGUFT5Converter": {
    "__init__": [
      "self",
      "tokenizer_dict"
    ],
    "vocab": [
      "self",
      "proto"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "post_processor": [
      "self"
    ],
    "converted": [
      "self"
    ]
  },
  "GGUFGemmaConverter": {
    "__init__": [
      "self",
      "tokenizer_dict"
    ],
    "vocab": [
      "self",
      "proto"
    ],
    "normalizer": [
      "self",
      "proto"
    ],
    "decoder": [
      "self",
      "replacement",
      "add_prefix_space"
    ],
    "converted": [
      "self"
    ]
  },
  "GGUF_TO_FAST_CONVERTERS": [],
  "convert_gguf_tokenizer": [
    "architecture",
    "tokenizer_dict"
  ],
  "initialize_tensor_parallelism": [
    "tp_plan",
    "tp_size",
    "device_mesh",
    "device_map"
  ],
  "replace_layer_number_by_wildcard": [
    "name"
  ],
  "_get_parameter_tp_plan": [
    "parameter_name",
    "tp_plan",
    "is_weight"
  ],
  "_blocks_to_block_sizes": [
    "total_size",
    "blocks"
  ],
  "get_packed_weights": [
    "param",
    "empty_param",
    "device_mesh",
    "rank",
    "dim"
  ],
  "repack_weights": [
    "packed_parameter",
    "sharded_dim",
    "world_size",
    "num_blocks"
  ],
  "get_tensor_shard": [
    "param",
    "empty_param",
    "device_mesh",
    "rank",
    "dim",
    "tensor_idx"
  ],
  "_split_along_last_dim": [
    "x",
    "world_size"
  ],
  "_AllReduceBackward": {
    "forward": [
      "ctx",
      "x",
      "device_mesh"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_AllReduceForward": {
    "forward": [
      "ctx",
      "x",
      "device_mesh"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_AllGather": {
    "forward": [
      "ctx",
      "x",
      "device_mesh"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_Split": {
    "forward": [
      "ctx",
      "x",
      "device_mesh"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_ReduceScatter": {
    "forward": [
      "ctx",
      "x",
      "device_mesh"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "all_reduce_backward": [
    "x",
    "device_mesh"
  ],
  "all_reduce_forward": [
    "x",
    "device_mesh"
  ],
  "all_gather": [
    "x",
    "device_mesh"
  ],
  "split": [
    "x",
    "device_mesh"
  ],
  "reduce_scatter": [
    "x",
    "device_mesh"
  ],
  "distribute_module": [
    "module",
    "device_mesh",
    "input_fn",
    "output_fn"
  ],
  "TensorParallelLayer": {
    "device_mesh": [],
    "rank": [],
    "empty_param": [],
    "__init__": [
      "self",
      "device_mesh",
      "rank",
      "empty_param"
    ],
    "_prepare_input_fn": [
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "mod",
      "outputs",
      "device_mesh"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ],
    "prepare_module_tp": [
      "self",
      "module",
      "device_mesh"
    ],
    "get_expected_sharded_shape": [
      "self",
      "full_shape"
    ]
  },
  "ColwiseParallel": {
    "__init__": [
      "self",
      "gather_output"
    ],
    "_prepare_input_fn": [
      "self",
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "self",
      "mod",
      "outputs",
      "device_mesh"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ],
    "get_expected_sharded_shape": [
      "self",
      "full_shape"
    ]
  },
  "RowwiseParallel": {
    "__init__": [
      "self",
      "split_input"
    ],
    "_prepare_input_fn": [
      "self",
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "self",
      "mod",
      "outputs",
      "device_mesh"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ],
    "get_expected_sharded_shape": [
      "self",
      "full_shape"
    ]
  },
  "PackedColwiseParallel": {
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ]
  },
  "PackedRowwiseParallel": {
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ]
  },
  "EmbeddingParallel": {
    "__init__": [
      "self"
    ],
    "_prepare_input_fn": [
      "self",
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "self",
      "mod",
      "outputs",
      "device_mesh"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ],
    "get_expected_sharded_shape": [
      "self",
      "full_shape"
    ]
  },
  "SequenceParallel": {
    "__init__": [
      "self",
      "sequence_dim",
      "use_local_output",
      "use_dtensor"
    ],
    "_prepare_input_fn": [
      "self",
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "self",
      "mod",
      "outputs",
      "device_mesh"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ]
  },
  "GroupedGemmParallel": {
    "__init__": [
      "self"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ],
    "get_expected_sharded_shape": [
      "self",
      "full_shape"
    ]
  },
  "RouterParallel": {
    "__init__": [
      "self"
    ],
    "_prepare_input_fn": [
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "mod",
      "outputs",
      "device_mesh"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ]
  },
  "MoeTensorParalellExperts": {
    "__init__": [
      "self"
    ],
    "_prepare_input_fn": [
      "mod",
      "inputs",
      "device_mesh"
    ],
    "_prepare_output_fn": [
      "mod",
      "outputs",
      "device_mesh"
    ],
    "shard_tensor": [
      "self",
      "param",
      "tensor_idx",
      "device",
      "dtype"
    ]
  },
  "ParallelInterface": {
    "_global_mapping": []
  },
  "gather_full_tensor": [
    "local_tensor",
    "shard_dim",
    "device_mesh"
  ],
  "gather_state_dict_for_save": [
    "state_dict",
    "tp_plan",
    "device_mesh",
    "tp_size"
  ],
  "add_tensor_parallel_hooks_to_module": [
    "model",
    "module",
    "tp_plan",
    "layer_name",
    "current_module_plan",
    "device_mesh",
    "parameter_name"
  ],
  "shard_and_distribute_module": [
    "model",
    "param",
    "empty_param",
    "parameter_name",
    "param_casting_dtype",
    "is_contiguous",
    "rank",
    "device_mesh"
  ],
  "verify_tp_plan": [
    "expected_keys",
    "tp_plan"
  ],
  "distribute_model": [
    "model",
    "tp_plan",
    "distributed_config",
    "device_mesh",
    "tp_size"
  ],
  "replace_with_vptq_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config"
  ],
  "VALUES_PER_ITEM": [],
  "pack_weights": [
    "quantized_weights"
  ],
  "unpack_weights": [
    "packed",
    "dtype"
  ],
  "BitLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "device",
      "dtype",
      "use_rms_norm",
      "rms_norm_eps"
    ],
    "activation_quant": [
      "self",
      "input",
      "num_bits"
    ],
    "post_quant_process": [
      "self",
      "input",
      "input_scale",
      "weight_scale"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "WeightQuant": {
    "forward": [
      "ctx",
      "weight"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "ActQuant": {
    "forward": [
      "ctx",
      "activation"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "AutoBitLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "device",
      "dtype",
      "online_quant",
      "use_rms_norm",
      "rms_norm_eps"
    ],
    "load_hook": [
      "self",
      "state_dict",
      "prefix"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "replace_with_bitnet_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config"
  ],
  "pad_to_block": [
    "tensor",
    "dims",
    "had_block_size",
    "value"
  ],
  "get_higgs_grid": [
    "p",
    "n"
  ],
  "quantize_with_higgs": [
    "weight",
    "bits",
    "p",
    "group_size",
    "hadamard_size"
  ],
  "HiggsLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "num_bits",
      "bias",
      "dtype",
      "device",
      "group_size",
      "hadamard_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "replace_with_higgs_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config"
  ],
  "dequantize_higgs": [
    "model",
    "current_key_name"
  ],
  "WrappedFlexAttention": {
    "_instance": [],
    "_is_flex_compiled": [],
    "_compiled_flex_attention": [],
    "__new__": [
      "cls"
    ],
    "__init__": [
      "self",
      "training"
    ],
    "__call__": [
      "self"
    ]
  },
  "compile_friendly_flex_attention": [
    "query",
    "key",
    "value",
    "training"
  ],
  "Offset": [],
  "make_flex_block_causal_mask": [
    "attention_mask_2d",
    "attention_chunk_size",
    "query_length",
    "key_length",
    "offsets",
    "is_causal"
  ],
  "flex_attention_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "scaling",
    "softcap",
    "s_aux"
  ],
  "_use_top_left_mask": [],
  "get_target_dtype": [
    "query",
    "module"
  ],
  "flash_attention_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "dropout",
    "scaling",
    "sliding_window",
    "softcap",
    "is_causal"
  ],
  "MIN_PEFT_VERSION": [],
  "_block_diag_3d": [],
  "PeftConcatenate": {
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns",
      "full_layer_name"
    ],
    "reverse_op": [
      "self"
    ]
  },
  "FlattenDims": {
    "__init__": [
      "self",
      "dims"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns",
      "config"
    ],
    "reverse_op": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "PermuteDims": {
    "__init__": [
      "self",
      "dims"
    ],
    "convert": [
      "self",
      "input_dict",
      "source_patterns",
      "target_patterns",
      "config"
    ],
    "reverse_op": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_build_peft_weight_mapping": [
    "weight_conversions",
    "adapter_name",
    "peft_config"
  ],
  "patch_moe_parameter_targeting": [
    "model",
    "peft_config"
  ],
  "PeftAdapterMixin": {
    "_hf_peft_config_loaded": [],
    "load_adapter": [
      "self",
      "peft_model_id",
      "adapter_name",
      "peft_config",
      "adapter_state_dict",
      "low_cpu_mem_usage",
      "is_trainable",
      "hotswap",
      "local_files_only",
      "adapter_kwargs",
      "load_config"
    ],
    "enable_peft_hotswap": [
      "self",
      "target_rank",
      "check_compiled"
    ],
    "add_adapter": [
      "self",
      "adapter_config",
      "adapter_name"
    ],
    "set_adapter": [
      "self",
      "adapter_name"
    ],
    "disable_adapters": [
      "self"
    ],
    "enable_adapters": [
      "self"
    ],
    "active_adapters": [
      "self"
    ],
    "get_adapter_state_dict": [
      "self",
      "adapter_name",
      "state_dict"
    ],
    "_dispatch_accelerate_model": [
      "self",
      "device_map",
      "max_memory",
      "offload_folder",
      "offload_index"
    ],
    "delete_adapter": [
      "self",
      "adapter_names"
    ]
  },
  "maybe_load_adapters": [
    "pretrained_model_name_or_path",
    "download_kwargs"
  ],
  "_convert_peft_config_moe": [
    "peft_config",
    "model_type"
  ],
  "convert_peft_config_for_transformers": [
    "peft_config",
    "model",
    "conversions"
  ],
  "get_module_size_with_ties": [
    "tied_params",
    "module_size",
    "module_sizes",
    "modules_to_treat"
  ],
  "check_and_set_device_map": [
    "device_map"
  ],
  "compute_module_sizes": [
    "model",
    "hf_quantizer",
    "buffers_only",
    "only_modules"
  ],
  "compute_module_total_buffer_size": [
    "model",
    "hf_quantizer"
  ],
  "get_balanced_memory": [
    "model",
    "max_memory",
    "no_split_module_classes",
    "hf_quantizer",
    "low_zero"
  ],
  "_get_device_map": [
    "model",
    "device_map",
    "max_memory",
    "hf_quantizer"
  ],
  "accelerate_dispatch": [
    "model",
    "hf_quantizer",
    "device_map",
    "offload_folder",
    "offload_index",
    "offload_buffers"
  ],
  "expand_device_map": [
    "device_map",
    "param_names"
  ],
  "get_device": [
    "device_map",
    "param_name",
    "valid_torch_device"
  ],
  "accelerate_disk_offload": [
    "model",
    "disk_offload_folder",
    "checkpoint_files",
    "device_map",
    "sharded_metadata",
    "dtype",
    "weight_mapping"
  ],
  "offload_weight": [
    "weight",
    "weight_name",
    "offload_folder",
    "offload_index"
  ],
  "load_offloaded_parameter": [
    "model",
    "param_name"
  ],
  "_init_infer_auto_device_map": [
    "model",
    "max_memory",
    "no_split_module_classes",
    "tied_parameters",
    "hf_quantizer"
  ],
  "infer_auto_device_map": [
    "model",
    "max_memory",
    "no_split_module_classes",
    "verbose",
    "clean_result",
    "offload_buffers",
    "tied_parameters",
    "hf_quantizer"
  ],
  "_get_param_device": [
    "param",
    "device_map"
  ],
  "check_tied_parameters_on_same_device": [
    "tied_params",
    "device_map"
  ],
  "_batched_linear": [
    "input",
    "weight",
    "bias",
    "is_transposed"
  ],
  "batched_mm_experts_forward": [
    "self",
    "hidden_states",
    "top_k_index",
    "top_k_weights"
  ],
  "_grouped_linear": [
    "input",
    "weight",
    "bias",
    "offs",
    "is_transposed"
  ],
  "grouped_mm_experts_forward": [
    "self",
    "hidden_states",
    "top_k_index",
    "top_k_weights"
  ],
  "ExpertsInterface": {
    "_global_mapping": [],
    "get_interface": [
      "self",
      "experts_implementation",
      "default"
    ]
  },
  "ALL_EXPERTS_FUNCTIONS": [],
  "_default_apply_gate": [
    "self",
    "gate_up_out"
  ],
  "use_experts_implementation": [
    "experts_class"
  ],
  "_is_torch_greater_or_equal_than_2_8": [],
  "_is_torch_npu_available": [],
  "use_gqa_in_sdpa": [
    "attention_mask",
    "key"
  ],
  "sdpa_attention_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "dropout",
    "scaling",
    "is_causal"
  ],
  "paged_attention_forward": [
    "module",
    "q",
    "k",
    "v",
    "attention_mask",
    "cache",
    "cu_seq_lens_q",
    "cu_seq_lens_k",
    "max_seqlen_q",
    "max_seqlen_k"
  ],
  "is_deepspeed_available": [],
  "HfDeepSpeedConfig": {
    "__init__": [
      "self",
      "config_file_or_dict"
    ]
  },
  "HfTrainerDeepSpeedConfig": {
    "__init__": [
      "self",
      "config_file_or_dict"
    ],
    "dtype": [
      "self"
    ],
    "is_auto": [
      "self",
      "ds_key_long"
    ],
    "fill_match": [
      "self",
      "ds_key_long",
      "hf_val",
      "hf_key",
      "must_match"
    ],
    "fill_only": [],
    "trainer_config_process": [
      "self",
      "args",
      "auto_find_batch_size"
    ],
    "trainer_config_finalize": [
      "self",
      "args",
      "model",
      "num_training_steps"
    ]
  },
  "_hf_deepspeed_config_weak_ref": [],
  "set_hf_deepspeed_config": [
    "hf_deepspeed_config_obj"
  ],
  "unset_hf_deepspeed_config": [],
  "is_deepspeed_zero3_enabled": [],
  "deepspeed_config": [],
  "_apply_weight_conversions_to_state_dict": [
    "model",
    "state_dict",
    "weight_mapping"
  ],
  "_load_state_dict_into_zero3_model": [
    "model_to_load",
    "state_dict",
    "load_config"
  ],
  "deepspeed_optim_sched": [
    "trainer",
    "hf_deepspeed_config",
    "args",
    "num_training_steps",
    "model_parameters"
  ],
  "deepspeed_init": [
    "trainer",
    "num_training_steps",
    "inference"
  ],
  "deepspeed_load_checkpoint": [
    "deepspeed_engine",
    "checkpoint_path",
    "load_module_strict"
  ],
  "tpu_spmd_dataloader": [
    "dataloader"
  ],
  "sdpa_attention_paged_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "dropout",
    "scaling"
  ],
  "_quantization_kernel": [],
  "_get_quantization_kernel": [],
  "_supports_cutlass": [
    "block_size",
    "output_dtype"
  ],
  "act_quant_kernel": [
    "x_ptr",
    "y_ptr",
    "s_ptr",
    "BLOCK_SIZE"
  ],
  "act_quant": [
    "x",
    "block_size"
  ],
  "_w8a8_block_fp8_matmul": [
    "A",
    "B",
    "C",
    "As",
    "Bs",
    "M",
    "N",
    "K",
    "group_n",
    "group_k",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "stride_As_m",
    "stride_As_k",
    "stride_Bs_k",
    "stride_Bs_n",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M"
  ],
  "_w8a8_block_fp8_matmul_per_tensor": [
    "A",
    "B",
    "C",
    "As",
    "Bs",
    "M",
    "N",
    "K",
    "group_n",
    "group_k",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M"
  ],
  "w8a8_block_fp8_matmul_triton": [
    "A",
    "B",
    "As",
    "Bs",
    "block_size",
    "output_dtype"
  ],
  "w8a8_block_fp8_matmul": [
    "A",
    "B",
    "As",
    "Bs",
    "block_size",
    "output_dtype"
  ],
  "w8a8_block_fp8_matmul_compile": [
    "input_q",
    "weight_q",
    "input_scale",
    "weight_scale",
    "block_size",
    "output_dtype"
  ],
  "FP8Linear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "dtype",
      "block_size",
      "activation_scheme"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "_ceil_div": [
    "a",
    "b"
  ],
  "FP8Expert": {
    "__init__": [
      "self",
      "config",
      "block_size",
      "dtype"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ],
    "linear": [
      "self",
      "input",
      "weight",
      "weight_scale_inv"
    ]
  },
  "replace_with_fp8_linear": [
    "model",
    "modules_to_not_convert",
    "quantization_config",
    "pre_quantized"
  ],
  "Fp8Quantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict"
    ]
  },
  "Fp8Dequantize": {
    "__init__": [
      "self",
      "hf_quantizer"
    ],
    "convert": [
      "self",
      "input_dict",
      "full_layer_name"
    ]
  },
  "InputDataClass": [],
  "DataCollator": [],
  "DataCollatorMixin": {
    "__call__": [
      "self",
      "features",
      "return_tensors"
    ]
  },
  "pad_without_fast_tokenizer_warning": [
    "tokenizer"
  ],
  "default_data_collator": [
    "features",
    "return_tensors"
  ],
  "DefaultDataCollator": {
    "__call__": [
      "self",
      "features",
      "return_tensors"
    ]
  },
  "torch_default_data_collator": [
    "features"
  ],
  "numpy_default_data_collator": [
    "features"
  ],
  "DataCollatorWithPadding": {
    "__call__": [
      "self",
      "features"
    ]
  },
  "DataCollatorForTokenClassification": {
    "torch_call": [
      "self",
      "features"
    ],
    "numpy_call": [
      "self",
      "features"
    ]
  },
  "_torch_collate_batch": [
    "examples",
    "tokenizer",
    "pad_to_multiple_of"
  ],
  "_numpy_collate_batch": [
    "examples",
    "tokenizer",
    "pad_to_multiple_of"
  ],
  "DataCollatorForMultipleChoice": {
    "torch_call": [
      "self",
      "examples"
    ]
  },
  "DataCollatorForSeq2Seq": {
    "__call__": [
      "self",
      "features",
      "return_tensors"
    ]
  },
  "DataCollatorForLanguageModeling": {
    "__post_init__": [
      "self"
    ],
    "get_generator": [
      "self",
      "seed"
    ],
    "create_rng": [
      "self"
    ],
    "torch_call": [
      "self",
      "examples"
    ],
    "torch_mask_tokens": [
      "self",
      "inputs",
      "special_tokens_mask",
      "offset_mapping"
    ],
    "numpy_call": [
      "self",
      "examples"
    ],
    "numpy_mask_tokens": [
      "self",
      "inputs",
      "special_tokens_mask",
      "offset_mapping"
    ],
    "_calc_word_ids_and_prob_mask": [
      "offsets",
      "special_tokens_mask"
    ],
    "_whole_word_mask": [
      "word_ids",
      "mask"
    ]
  },
  "DataCollatorForWholeWordMask": {
    "__init__": [
      "self"
    ]
  },
  "tolist": [
    "x"
  ],
  "DataCollatorForSOP": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "examples"
    ],
    "mask_tokens": [
      "self",
      "inputs"
    ]
  },
  "DataCollatorForPermutationLanguageModeling": {
    "torch_call": [
      "self",
      "examples"
    ],
    "numpy_call": [
      "self",
      "examples"
    ],
    "torch_mask_tokens": [
      "self",
      "inputs"
    ],
    "numpy_mask_tokens": [
      "self",
      "inputs"
    ]
  },
  "DataCollatorWithFlattening": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "features",
      "return_tensors",
      "separator_id"
    ]
  },
  "DEPRECATION_WARNING": [],
  "simple_accuracy": [
    "preds",
    "labels"
  ],
  "acc_and_f1": [
    "preds",
    "labels"
  ],
  "pearson_and_spearman": [
    "preds",
    "labels"
  ],
  "glue_compute_metrics": [
    "task_name",
    "preds",
    "labels"
  ],
  "xnli_compute_metrics": [
    "task_name",
    "preds",
    "labels"
  ],
  "normalize_answer": [
    "s"
  ],
  "get_tokens": [
    "s"
  ],
  "compute_exact": [
    "a_gold",
    "a_pred"
  ],
  "compute_f1": [
    "a_gold",
    "a_pred"
  ],
  "get_raw_scores": [
    "examples",
    "preds"
  ],
  "apply_no_ans_threshold": [
    "scores",
    "na_probs",
    "qid_to_has_ans",
    "na_prob_thresh"
  ],
  "make_eval_dict": [
    "exact_scores",
    "f1_scores",
    "qid_list"
  ],
  "merge_eval": [
    "main_eval",
    "new_eval",
    "prefix"
  ],
  "find_best_thresh_v2": [
    "preds",
    "scores",
    "na_probs",
    "qid_to_has_ans"
  ],
  "find_all_best_thresh_v2": [
    "main_eval",
    "preds",
    "exact_raw",
    "f1_raw",
    "na_probs",
    "qid_to_has_ans"
  ],
  "find_best_thresh": [
    "preds",
    "scores",
    "na_probs",
    "qid_to_has_ans"
  ],
  "find_all_best_thresh": [
    "main_eval",
    "preds",
    "exact_raw",
    "f1_raw",
    "na_probs",
    "qid_to_has_ans"
  ],
  "squad_evaluate": [
    "examples",
    "preds",
    "no_answer_probs",
    "no_answer_probability_threshold"
  ],
  "get_final_text": [
    "pred_text",
    "orig_text",
    "do_lower_case",
    "verbose_logging"
  ],
  "_get_best_indexes": [
    "logits",
    "n_best_size"
  ],
  "_compute_softmax": [
    "scores"
  ],
  "compute_predictions_logits": [
    "all_examples",
    "all_features",
    "all_results",
    "n_best_size",
    "max_answer_length",
    "do_lower_case",
    "output_prediction_file",
    "output_nbest_file",
    "output_null_log_odds_file",
    "verbose_logging",
    "version_2_with_negative",
    "null_score_diff_threshold",
    "tokenizer"
  ],
  "compute_predictions_log_probs": [
    "all_examples",
    "all_features",
    "all_results",
    "n_best_size",
    "max_answer_length",
    "output_prediction_file",
    "output_nbest_file",
    "output_null_log_odds_file",
    "start_n_top",
    "end_n_top",
    "version_2_with_negative",
    "tokenizer",
    "verbose_logging"
  ],
  "Split": {
    "train": [],
    "dev": [],
    "test": []
  },
  "MODEL_CONFIG_CLASSES": [],
  "MODEL_TYPES": [],
  "glue_convert_examples_to_features": [
    "examples",
    "tokenizer",
    "max_length",
    "task",
    "label_list",
    "output_mode"
  ],
  "_glue_convert_examples_to_features": [
    "examples",
    "tokenizer",
    "max_length",
    "task",
    "label_list",
    "output_mode"
  ],
  "OutputMode": {
    "classification": [],
    "regression": []
  },
  "MrpcProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "MnliProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "MnliMismatchedProcessor": {
    "__init__": [
      "self"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ]
  },
  "ColaProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "Sst2Processor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "StsbProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "QqpProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "QnliProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "RteProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "WnliProcessor": {
    "__init__": [
      "self"
    ],
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "_create_examples": [
      "self",
      "lines",
      "set_type"
    ]
  },
  "glue_tasks_num_labels": [],
  "glue_processors": [],
  "glue_output_modes": [],
  "XnliProcessor": {
    "__init__": [
      "self",
      "language",
      "train_language"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ]
  },
  "xnli_processors": [],
  "xnli_output_modes": [],
  "xnli_tasks_num_labels": [],
  "InputExample": {
    "to_json_string": [
      "self"
    ]
  },
  "InputFeatures": {
    "to_json_string": [
      "self"
    ]
  },
  "DataProcessor": {
    "get_example_from_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "get_train_examples": [
      "self",
      "data_dir"
    ],
    "get_dev_examples": [
      "self",
      "data_dir"
    ],
    "get_test_examples": [
      "self",
      "data_dir"
    ],
    "get_labels": [
      "self"
    ],
    "tfds_map": [
      "self",
      "example"
    ],
    "_read_tsv": [
      "cls",
      "input_file",
      "quotechar"
    ]
  },
  "SingleSentenceClassificationProcessor": {
    "__init__": [
      "self",
      "labels",
      "examples",
      "mode",
      "verbose"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "create_from_csv": [
      "cls",
      "file_name",
      "split_name",
      "column_label",
      "column_text",
      "column_id",
      "skip_first_row"
    ],
    "create_from_examples": [
      "cls",
      "texts_or_text_and_labels",
      "labels"
    ],
    "add_examples_from_csv": [
      "self",
      "file_name",
      "split_name",
      "column_label",
      "column_text",
      "column_id",
      "skip_first_row",
      "overwrite_labels",
      "overwrite_examples"
    ],
    "add_examples": [
      "self",
      "texts_or_text_and_labels",
      "labels",
      "ids",
      "overwrite_labels",
      "overwrite_examples"
    ],
    "get_features": [
      "self",
      "tokenizer",
      "max_length",
      "pad_on_left",
      "pad_token",
      "mask_padding_with_zero",
      "return_tensors"
    ]
  },
  "MULTI_SEP_TOKENS_TOKENIZERS_SET": [],
  "_improve_answer_span": [
    "doc_tokens",
    "input_start",
    "input_end",
    "tokenizer",
    "orig_answer_text"
  ],
  "_check_is_max_context": [
    "doc_spans",
    "cur_span_index",
    "position"
  ],
  "_new_check_is_max_context": [
    "doc_spans",
    "cur_span_index",
    "position"
  ],
  "squad_convert_example_to_features": [
    "example",
    "max_seq_length",
    "doc_stride",
    "max_query_length",
    "padding_strategy",
    "is_training"
  ],
  "squad_convert_example_to_features_init": [
    "tokenizer_for_convert"
  ],
  "squad_convert_examples_to_features": [
    "examples",
    "tokenizer",
    "max_seq_length",
    "doc_stride",
    "max_query_length",
    "is_training",
    "padding_strategy",
    "return_dataset",
    "threads",
    "tqdm_enabled"
  ],
  "SquadProcessor": {
    "train_file": [],
    "dev_file": [],
    "_get_example_from_tensor_dict": [
      "self",
      "tensor_dict",
      "evaluate"
    ],
    "get_examples_from_dataset": [
      "self",
      "dataset",
      "evaluate"
    ],
    "get_train_examples": [
      "self",
      "data_dir",
      "filename"
    ],
    "get_dev_examples": [
      "self",
      "data_dir",
      "filename"
    ],
    "_create_examples": [
      "self",
      "input_data",
      "set_type"
    ]
  },
  "SquadV1Processor": {
    "train_file": [],
    "dev_file": []
  },
  "SquadV2Processor": {
    "train_file": [],
    "dev_file": []
  },
  "SquadExample": {
    "__init__": [
      "self",
      "qas_id",
      "question_text",
      "context_text",
      "answer_text",
      "start_position_character",
      "title",
      "answers",
      "is_impossible"
    ]
  },
  "SquadFeatures": {
    "__init__": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "cls_index",
      "p_mask",
      "example_index",
      "unique_id",
      "paragraph_len",
      "token_is_max_context",
      "tokens",
      "token_to_orig_map",
      "start_position",
      "end_position",
      "is_impossible",
      "qas_id",
      "encoding"
    ]
  },
  "SquadResult": {
    "__init__": [
      "self",
      "unique_id",
      "start_logits",
      "end_logits",
      "start_top_index",
      "end_top_index",
      "cls_logits"
    ]
  },
  "TESSERACT_LOADED": [],
  "normalize_box": [
    "box",
    "width",
    "height"
  ],
  "apply_tesseract": [
    "image",
    "lang",
    "tesseract_config"
  ],
  "ModelType": {
    "LayoutLM": [],
    "LayoutLMv2andv3": [],
    "VisionEncoderDecoder": []
  },
  "DocumentQuestionAnsweringPipeline": {
    "_pipeline_calls_generate": [],
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_default_generation_config": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "padding",
      "doc_stride",
      "max_question_len",
      "lang",
      "tesseract_config",
      "max_answer_len",
      "max_seq_len",
      "top_k",
      "handle_impossible_answer",
      "timeout"
    ],
    "__call__": [
      "self",
      "image",
      "question",
      "word_boxes"
    ],
    "preprocess": [
      "self",
      "input",
      "padding",
      "doc_stride",
      "max_seq_len",
      "word_boxes",
      "lang",
      "tesseract_config",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "top_k"
    ],
    "postprocess_encoder_decoder_single": [
      "self",
      "model_outputs"
    ],
    "postprocess_extractive_qa": [
      "self",
      "model_outputs",
      "top_k",
      "handle_impossible_answer",
      "max_answer_len"
    ]
  },
  "ImageSegmentationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "preprocess": [
      "self",
      "image",
      "subtask",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "subtask",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold"
    ]
  },
  "ZeroShotObjectDetectionPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "image",
      "candidate_labels"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "preprocess": [
      "self",
      "inputs",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "threshold",
      "top_k"
    ],
    "_get_bounding_box": [
      "self",
      "box"
    ]
  },
  "MaskGenerationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "__call__": [
      "self",
      "image"
    ],
    "preprocess": [
      "self",
      "image",
      "points_per_batch",
      "crops_n_layers",
      "crop_overlap_ratio",
      "points_per_crop",
      "crop_n_points_downscale_factor",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs",
      "pred_iou_thresh",
      "stability_score_thresh",
      "mask_threshold",
      "stability_score_offset",
      "max_hole_area",
      "max_sprinkle_area"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "output_rle_mask",
      "output_bboxes_mask",
      "crops_nms_thresh"
    ]
  },
  "ZeroShotAudioClassificationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "audios"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "preprocess": [
      "self",
      "audio",
      "candidate_labels",
      "hypothesis_template"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs"
    ]
  },
  "VideoClassificationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "top_k",
      "num_frames",
      "frame_sampling_rate",
      "function_to_apply"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "preprocess": [
      "self",
      "video",
      "num_frames",
      "frame_sampling_rate"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "top_k",
      "function_to_apply"
    ]
  },
  "FillMaskPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "get_masked_index": [
      "self",
      "input_ids"
    ],
    "_ensure_exactly_one_mask_token": [
      "self",
      "input_ids"
    ],
    "ensure_exactly_one_mask_token": [
      "self",
      "model_inputs"
    ],
    "preprocess": [
      "self",
      "inputs",
      "return_tensors",
      "tokenizer_kwargs"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "top_k",
      "target_ids"
    ],
    "get_target_ids": [
      "self",
      "targets"
    ],
    "_sanitize_parameters": [
      "self",
      "top_k",
      "targets",
      "tokenizer_kwargs"
    ],
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "DepthEstimationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "_sanitize_parameters": [
      "self",
      "timeout",
      "parameters"
    ],
    "preprocess": [
      "self",
      "image",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs"
    ]
  },
  "GenericTensor": [],
  "no_collate_fn": [
    "items"
  ],
  "pad_collate_fn": [
    "tokenizer",
    "feature_extractor"
  ],
  "load_model": [
    "model",
    "config",
    "model_classes",
    "task"
  ],
  "get_default_model_and_revision": [
    "targeted_task",
    "task_options"
  ],
  "load_assistant_model": [
    "model",
    "assistant_model",
    "assistant_tokenizer"
  ],
  "PipelineException": {
    "__init__": [
      "self",
      "task",
      "model",
      "reason"
    ]
  },
  "ArgumentHandler": {
    "__call__": [
      "self"
    ]
  },
  "PipelineDataFormat": {
    "SUPPORTED_FORMATS": [],
    "__init__": [
      "self",
      "output_path",
      "input_path",
      "column",
      "overwrite"
    ],
    "__iter__": [
      "self"
    ],
    "save": [
      "self",
      "data"
    ],
    "save_binary": [
      "self",
      "data"
    ],
    "from_str": [
      "format",
      "output_path",
      "input_path",
      "column",
      "overwrite"
    ]
  },
  "CsvPipelineDataFormat": {
    "__init__": [
      "self",
      "output_path",
      "input_path",
      "column",
      "overwrite"
    ],
    "__iter__": [
      "self"
    ],
    "save": [
      "self",
      "data"
    ]
  },
  "JsonPipelineDataFormat": {
    "__init__": [
      "self",
      "output_path",
      "input_path",
      "column",
      "overwrite"
    ],
    "__iter__": [
      "self"
    ],
    "save": [
      "self",
      "data"
    ]
  },
  "PipedPipelineDataFormat": {
    "__iter__": [
      "self"
    ],
    "save": [
      "self",
      "data"
    ],
    "save_binary": [
      "self",
      "data"
    ]
  },
  "_ScikitCompat": {
    "transform": [
      "self",
      "X"
    ],
    "predict": [
      "self",
      "X"
    ]
  },
  "build_pipeline_init_args": [
    "has_tokenizer",
    "has_feature_extractor",
    "has_image_processor",
    "has_processor",
    "supports_binary_output"
  ],
  "PIPELINE_INIT_ARGS": [],
  "SUPPORTED_PEFT_TASKS": [],
  "Pipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_pipeline_calls_generate": [],
    "default_input_names": [],
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "feature_extractor",
      "image_processor",
      "processor",
      "task",
      "device",
      "binary_output"
    ],
    "__repr__": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ],
    "transform": [
      "self",
      "X"
    ],
    "predict": [
      "self",
      "X"
    ],
    "dtype": [
      "self"
    ],
    "torch_dtype": [
      "self"
    ],
    "device_placement": [
      "self"
    ],
    "ensure_tensor_on_device": [
      "self"
    ],
    "_ensure_tensor_on_device": [
      "self",
      "inputs",
      "device"
    ],
    "check_model_type": [
      "self",
      "supported_models"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "preprocess": [
      "self",
      "input_"
    ],
    "_forward": [
      "self",
      "input_tensors"
    ],
    "postprocess": [
      "self",
      "model_outputs"
    ],
    "get_inference_context": [
      "self"
    ],
    "forward": [
      "self",
      "model_inputs"
    ],
    "get_iterator": [
      "self",
      "inputs",
      "num_workers",
      "batch_size",
      "preprocess_params",
      "forward_params",
      "postprocess_params"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "run_multi": [
      "self",
      "inputs",
      "preprocess_params",
      "forward_params",
      "postprocess_params"
    ],
    "run_single": [
      "self",
      "inputs",
      "preprocess_params",
      "forward_params",
      "postprocess_params"
    ],
    "iterate": [
      "self",
      "inputs",
      "preprocess_params",
      "forward_params",
      "postprocess_params"
    ]
  },
  "ChunkPipeline": {
    "run_single": [
      "self",
      "inputs",
      "preprocess_params",
      "forward_params",
      "postprocess_params"
    ],
    "get_iterator": [
      "self",
      "inputs",
      "num_workers",
      "batch_size",
      "preprocess_params",
      "forward_params",
      "postprocess_params"
    ]
  },
  "PipelineRegistry": {
    "__init__": [
      "self",
      "supported_tasks",
      "task_aliases"
    ],
    "get_supported_tasks": [
      "self"
    ],
    "check_task": [
      "self",
      "task"
    ],
    "register_pipeline": [
      "self",
      "task",
      "pipeline_class",
      "pt_model",
      "default",
      "type"
    ],
    "to_dict": [
      "self"
    ]
  },
  "ffmpeg_read": [
    "bpayload",
    "sampling_rate"
  ],
  "AudioClassificationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "_sanitize_parameters": [
      "self",
      "top_k",
      "function_to_apply"
    ],
    "preprocess": [
      "self",
      "inputs"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "top_k",
      "function_to_apply"
    ]
  },
  "rescale_stride": [
    "stride",
    "ratio"
  ],
  "chunk_iter": [
    "inputs",
    "feature_extractor",
    "chunk_len",
    "stride_left",
    "stride_right",
    "dtype"
  ],
  "_find_longest_common_sequence": [
    "sequences",
    "tokenizer"
  ],
  "AutomaticSpeechRecognitionPipeline": {
    "_pipeline_calls_generate": [],
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_default_generation_config": [],
    "__init__": [
      "self",
      "model",
      "feature_extractor",
      "tokenizer",
      "decoder",
      "device"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "_sanitize_parameters": [
      "self",
      "chunk_length_s",
      "stride_length_s",
      "ignore_warning",
      "decoder_kwargs",
      "return_timestamps",
      "return_language"
    ],
    "_align_to": [
      "self"
    ],
    "preprocess": [
      "self",
      "inputs",
      "chunk_length_s",
      "stride_length_s"
    ],
    "_forward": [
      "self",
      "model_inputs",
      "return_timestamps"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "decoder_kwargs",
      "return_timestamps",
      "return_language"
    ]
  },
  "DEFAULT_VOCODER_ID": [],
  "AudioOutput": {},
  "TextToAudioPipeline": {
    "_pipeline_calls_generate": [],
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_default_generation_config": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "text"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "__call__": [
      "self",
      "text_inputs"
    ],
    "_sanitize_parameters": [
      "self",
      "preprocess_params",
      "forward_params",
      "generate_kwargs"
    ],
    "postprocess": [
      "self",
      "audio"
    ]
  },
  "ffmpeg_microphone": [
    "sampling_rate",
    "chunk_length_s",
    "format_for_conversion",
    "ffmpeg_input_device",
    "ffmpeg_additional_args"
  ],
  "ffmpeg_microphone_live": [
    "sampling_rate",
    "chunk_length_s",
    "stream_chunk_s",
    "stride_length_s",
    "format_for_conversion",
    "ffmpeg_input_device",
    "ffmpeg_additional_args"
  ],
  "chunk_bytes_iter": [
    "iterator",
    "chunk_len",
    "stride",
    "stream"
  ],
  "_ffmpeg_stream": [
    "ffmpeg_command",
    "buflen"
  ],
  "_get_microphone_name": [],
  "TASK_ALIASES": [],
  "SUPPORTED_TASKS": [],
  "PIPELINE_REGISTRY": [],
  "get_supported_tasks": [],
  "get_task": [
    "model",
    "token"
  ],
  "check_task": [
    "task"
  ],
  "clean_custom_task": [
    "task_info"
  ],
  "pipeline": [
    "task",
    "model",
    "config",
    "tokenizer",
    "feature_extractor",
    "image_processor",
    "processor",
    "revision",
    "use_fast",
    "token",
    "device",
    "device_map",
    "dtype",
    "trust_remote_code",
    "model_kwargs",
    "pipeline_class"
  ],
  "ImageToImagePipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "__call__": [
      "self",
      "images"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "preprocess": [
      "self",
      "image",
      "timeout"
    ],
    "postprocess": [
      "self",
      "model_outputs"
    ]
  },
  "ZeroShotImageClassificationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "image",
      "candidate_labels"
    ],
    "_sanitize_parameters": [
      "self",
      "tokenizer_kwargs"
    ],
    "preprocess": [
      "self",
      "image",
      "candidate_labels",
      "hypothesis_template",
      "timeout",
      "tokenizer_kwargs"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs"
    ]
  },
  "ObjectDetectionPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "preprocess": [
      "self",
      "image",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "threshold"
    ],
    "_get_bounding_box": [
      "self",
      "box"
    ]
  },
  "VisualQuestionAnsweringPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_pipeline_calls_generate": [],
    "_default_generation_config": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "top_k",
      "padding",
      "truncation",
      "timeout"
    ],
    "__call__": [
      "self",
      "image",
      "question"
    ],
    "preprocess": [
      "self",
      "inputs",
      "padding",
      "truncation",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "top_k"
    ]
  },
  "TokenClassificationArgumentHandler": {
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "AggregationStrategy": {
    "NONE": [],
    "SIMPLE": [],
    "FIRST": [],
    "AVERAGE": [],
    "MAX": []
  },
  "TokenClassificationPipeline": {
    "default_input_names": [],
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self",
      "args_parser"
    ],
    "_sanitize_parameters": [
      "self",
      "ignore_labels",
      "aggregation_strategy",
      "offset_mapping",
      "is_split_into_words",
      "stride",
      "delimiter"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "preprocess": [
      "self",
      "sentence",
      "offset_mapping"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "all_outputs",
      "aggregation_strategy",
      "ignore_labels"
    ],
    "aggregate_overlapping_entities": [
      "self",
      "entities"
    ],
    "gather_pre_entities": [
      "self",
      "sentence",
      "input_ids",
      "scores",
      "offset_mapping",
      "special_tokens_mask",
      "aggregation_strategy",
      "word_ids",
      "word_to_chars_map"
    ],
    "aggregate": [
      "self",
      "pre_entities",
      "aggregation_strategy"
    ],
    "aggregate_word": [
      "self",
      "entities",
      "aggregation_strategy"
    ],
    "aggregate_words": [
      "self",
      "entities",
      "aggregation_strategy"
    ],
    "group_sub_entities": [
      "self",
      "entities"
    ],
    "get_tag": [
      "self",
      "entity_name"
    ],
    "group_entities": [
      "self",
      "entities"
    ]
  },
  "NerPipeline": [],
  "ReturnType": {
    "TENSORS": [],
    "NEW_TEXT": [],
    "FULL_TEXT": []
  },
  "AnyToAnyPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_pipeline_calls_generate": [],
    "_default_generation_config": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "max_new_tokens",
      "generate_kwargs",
      "timeout",
      "return_full_text",
      "return_tensors",
      "return_type",
      "clean_up_tokenization_spaces",
      "stop_sequence",
      "continue_final_message",
      "skip_special_tokens",
      "generation_mode"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "videos",
      "audio"
    ],
    "preprocess": [
      "self",
      "inputs",
      "timeout",
      "continue_final_message"
    ],
    "_forward": [
      "self",
      "model_inputs",
      "generate_kwargs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "return_type",
      "continue_final_message",
      "skip_special_tokens"
    ]
  },
  "PipelineDataset": {
    "__init__": [
      "self",
      "dataset",
      "process",
      "params"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "i"
    ]
  },
  "PipelineIterator": {
    "__init__": [
      "self",
      "loader",
      "infer",
      "params",
      "loader_batch_size"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "loader_batch_item": [
      "self"
    ],
    "__next__": [
      "self"
    ]
  },
  "PipelineChunkIterator": {
    "__init__": [
      "self",
      "loader",
      "infer",
      "params",
      "loader_batch_size"
    ],
    "__iter__": [
      "self"
    ],
    "__next__": [
      "self"
    ]
  },
  "PipelinePackIterator": {
    "__iter__": [
      "self"
    ],
    "__next__": [
      "self"
    ]
  },
  "KeyDataset": {
    "__init__": [
      "self",
      "dataset",
      "key"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "i"
    ]
  },
  "KeyPairDataset": {
    "__init__": [
      "self",
      "dataset",
      "key1",
      "key2"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "i"
    ]
  },
  "TextGenerationPipeline": {
    "XL_PREFIX": [],
    "_pipeline_calls_generate": [],
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_default_generation_config": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "return_full_text",
      "return_tensors",
      "return_text",
      "return_type",
      "clean_up_tokenization_spaces",
      "prefix",
      "handle_long_generation",
      "stop_sequence",
      "truncation",
      "max_length",
      "continue_final_message",
      "skip_special_tokens",
      "tokenizer_encode_kwargs",
      "tools",
      "documents"
    ],
    "_parse_and_tokenize": [
      "self"
    ],
    "__call__": [
      "self",
      "text_inputs"
    ],
    "preprocess": [
      "self",
      "prompt_text",
      "prefix",
      "handle_long_generation",
      "add_special_tokens",
      "truncation",
      "padding",
      "max_length",
      "continue_final_message",
      "tokenizer_encode_kwargs",
      "tools",
      "documents"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "return_type",
      "clean_up_tokenization_spaces",
      "continue_final_message",
      "skip_special_tokens"
    ]
  },
  "ZeroShotClassificationArgumentHandler": {
    "_parse_labels": [
      "self",
      "labels"
    ],
    "__call__": [
      "self",
      "sequences",
      "labels",
      "hypothesis_template"
    ]
  },
  "ZeroShotClassificationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self",
      "args_parser"
    ],
    "entailment_id": [
      "self"
    ],
    "_parse_and_tokenize": [
      "self",
      "sequence_pairs",
      "padding",
      "add_special_tokens",
      "truncation"
    ],
    "_sanitize_parameters": [
      "self"
    ],
    "__call__": [
      "self",
      "sequences"
    ],
    "preprocess": [
      "self",
      "inputs",
      "candidate_labels",
      "hypothesis_template"
    ],
    "_forward": [
      "self",
      "inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "multi_label"
    ]
  },
  "sigmoid": [
    "_outputs"
  ],
  "softmax": [
    "_outputs"
  ],
  "ClassificationFunction": {
    "SIGMOID": [],
    "SOFTMAX": [],
    "NONE": []
  },
  "TextClassificationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "return_all_scores": [],
    "function_to_apply": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "function_to_apply",
      "top_k"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "preprocess": [
      "self",
      "inputs"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "function_to_apply",
      "top_k",
      "_legacy"
    ]
  },
  "FeatureExtractionPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_sanitize_parameters": [
      "self",
      "truncation",
      "tokenize_kwargs",
      "return_tensors"
    ],
    "preprocess": [
      "self",
      "inputs"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "return_tensors"
    ],
    "__call__": [
      "self"
    ]
  },
  "decode_spans": [
    "start",
    "end",
    "topk",
    "max_answer_len",
    "undesired_tokens"
  ],
  "select_starts_ends": [
    "start",
    "end",
    "p_mask",
    "attention_mask",
    "min_null_score",
    "top_k",
    "handle_impossible_answer",
    "max_answer_len"
  ],
  "QuestionAnsweringArgumentHandler": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "normalize": [
      "self",
      "item"
    ],
    "__call__": [
      "self"
    ]
  },
  "QuestionAnsweringPipeline": {
    "default_input_names": [],
    "handle_impossible_answer": [],
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "task"
    ],
    "create_sample": [
      "question",
      "context"
    ],
    "_sanitize_parameters": [
      "self",
      "padding",
      "top_k",
      "doc_stride",
      "max_answer_len",
      "max_seq_len",
      "max_question_len",
      "handle_impossible_answer",
      "align_to_words"
    ],
    "__call__": [
      "self"
    ],
    "preprocess": [
      "self",
      "example",
      "padding",
      "doc_stride",
      "max_question_len",
      "max_seq_len"
    ],
    "_forward": [
      "self",
      "inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "top_k",
      "handle_impossible_answer",
      "max_answer_len",
      "align_to_words"
    ],
    "get_answer": [
      "self",
      "answers",
      "target"
    ],
    "get_indices": [
      "self",
      "enc",
      "s",
      "e",
      "sequence_index",
      "align_to_words"
    ],
    "span_to_answer": [
      "self",
      "text",
      "start",
      "end"
    ]
  },
  "ImageFeatureExtractionPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_sanitize_parameters": [
      "self",
      "image_processor_kwargs",
      "return_tensors",
      "pool"
    ],
    "preprocess": [
      "self",
      "image",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "pool",
      "return_tensors"
    ],
    "__call__": [
      "self"
    ]
  },
  "Keypoint": {},
  "Match": {},
  "validate_image_pairs": [
    "images"
  ],
  "KeypointMatchingPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "threshold",
      "timeout"
    ],
    "__call__": [
      "self",
      "inputs",
      "threshold"
    ],
    "preprocess": [
      "self",
      "images",
      "timeout"
    ],
    "_forward": [
      "self",
      "preprocess_outputs"
    ],
    "postprocess": [
      "self",
      "forward_outputs",
      "threshold"
    ]
  },
  "ImageClassificationPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "top_k",
      "function_to_apply",
      "timeout"
    ],
    "__call__": [
      "self",
      "inputs"
    ],
    "preprocess": [
      "self",
      "image",
      "timeout"
    ],
    "_forward": [
      "self",
      "model_inputs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "function_to_apply",
      "top_k"
    ]
  },
  "TableQuestionAnsweringArgumentHandler": {
    "__call__": [
      "self",
      "table",
      "query"
    ]
  },
  "TableQuestionAnsweringPipeline": {
    "default_input_names": [],
    "_pipeline_calls_generate": [],
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_default_generation_config": [],
    "__init__": [
      "self",
      "args_parser"
    ],
    "batch_inference": [
      "self"
    ],
    "sequential_inference": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "sequential",
      "padding",
      "truncation"
    ],
    "preprocess": [
      "self",
      "pipeline_input",
      "padding",
      "truncation"
    ],
    "_forward": [
      "self",
      "model_inputs",
      "sequential"
    ],
    "postprocess": [
      "self",
      "model_outputs"
    ]
  },
  "IMAGE_TOKEN": [],
  "ImageTextToTextPipeline": {
    "_load_processor": [],
    "_load_image_processor": [],
    "_load_feature_extractor": [],
    "_load_tokenizer": [],
    "_pipeline_calls_generate": [],
    "_default_generation_config": [],
    "__init__": [
      "self"
    ],
    "_sanitize_parameters": [
      "self",
      "max_new_tokens",
      "generate_kwargs",
      "timeout",
      "return_full_text",
      "return_tensors",
      "return_type",
      "clean_up_tokenization_spaces",
      "stop_sequence",
      "continue_final_message",
      "skip_special_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "preprocess": [
      "self",
      "inputs",
      "timeout",
      "continue_final_message"
    ],
    "_forward": [
      "self",
      "model_inputs",
      "generate_kwargs"
    ],
    "postprocess": [
      "self",
      "model_outputs",
      "return_type",
      "continue_final_message",
      "skip_special_tokens"
    ]
  },
  "ChineseCLIPProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "ChineseCLIPTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "initializer_factor",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "ChineseCLIPVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "projection_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "ChineseCLIPConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value"
    ]
  },
  "ChineseCLIPImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "ChineseCLIPImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": []
  },
  "contrastive_loss": [
    "logits"
  ],
  "chinese_clip_loss": [
    "similarity"
  ],
  "ChineseCLIPOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "ChineseCLIPTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "ChineseCLIPVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "eager_attention_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "scaling",
    "dropout"
  ],
  "ChineseCLIPTextSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ChineseCLIPTextSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ChineseCLIPTextAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ChineseCLIPVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "ChineseCLIPTextIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChineseCLIPTextOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ChineseCLIPVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChineseCLIPTextLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "ChineseCLIPVisionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "ChineseCLIPTextPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChineseCLIPPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ChineseCLIPTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ChineseCLIPVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ChineseCLIPVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "ChineseCLIPTextModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ChineseCLIPVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "ChineseCLIPModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "TableTransformerConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "num_channels",
      "num_queries",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "init_xavier_std",
      "auxiliary_loss",
      "position_embedding_type",
      "dilation",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "mask_loss_coefficient",
      "dice_loss_coefficient",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "eos_coefficient"
    ]
  },
  "TableTransformerDecoderOutput": {},
  "TableTransformerModelOutput": {},
  "TableTransformerObjectDetectionOutput": {},
  "TableTransformerFrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "replace_batch_norm": [
    "model"
  ],
  "TableTransformerConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "TableTransformerConvModel": {
    "__init__": [
      "self",
      "conv_encoder",
      "position_embedding"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "TableTransformerSinePositionEmbedding": {
    "__init__": [
      "self",
      "embedding_dim",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "TableTransformerLearnedPositionEmbedding": {
    "__init__": [
      "self",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "build_position_encoding": [
    "config"
  ],
  "TableTransformerAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "bias"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "batch_size"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "object_queries"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "object_queries",
      "key_value_states",
      "spatial_position_embeddings",
      "output_attentions"
    ]
  },
  "TableTransformerEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "object_queries",
      "output_attentions"
    ]
  },
  "TableTransformerDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "object_queries",
      "query_position_embeddings",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "TableTransformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "TableTransformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "object_queries",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TableTransformerDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "object_queries",
      "query_position_embeddings",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TableTransformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TableTransformerForObjectDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TableTransformerMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "drop_path": [
    "input",
    "drop_prob",
    "training"
  ],
  "Florence2VisionDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Florence2VisionLearnedAbsolutePositionEmbedding2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "Florence2VisionPositionalEmbeddingCosine1D": {
    "__init__": [
      "self",
      "config"
    ],
    "get_sinusoid_embeddings": [
      "max_positions",
      "embed_dim"
    ],
    "forward": [
      "self",
      "seq_embeds"
    ]
  },
  "Florence2VisionMLP": {
    "__init__": [
      "self",
      "config",
      "stage_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2VisionConvEmbed": {
    "__init__": [
      "self",
      "config",
      "stage_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2VisionChannelAttention": {
    "__init__": [
      "self",
      "config",
      "stage_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2VisionChannelBlock": {
    "__init__": [
      "self",
      "config",
      "stage_idx",
      "drop_path_rate"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2VisionWindowAttention": {
    "__init__": [
      "self",
      "config",
      "stage_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2VisionSpatialBlock": {
    "__init__": [
      "self",
      "config",
      "stage_idx",
      "drop_path_rate"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2VisionBlock": {
    "__init__": [
      "self",
      "config",
      "stage_idx",
      "spatial_drop_path_rate",
      "channel_drop_path_rate"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2VisionPreTrainedModel": {
    "config_class": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_can_record_outputs": []
  },
  "Florence2VisionBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Florence2MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "Florence2Seq2SeqModelOutput": {},
  "Florence2Seq2SeqLMOutput": {},
  "Florence2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "config_class": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Florence2Model": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_inputs_embeds",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "get_encoder": [
      "self",
      "modality"
    ]
  },
  "shift_tokens_right": [
    "input_ids",
    "pad_token_id",
    "decoder_start_token_id"
  ],
  "Florence2ForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "_prepare_encoder_decoder_kwargs_for_generation": [
      "self",
      "inputs_tensor",
      "model_kwargs",
      "model_input_name",
      "generation_config"
    ]
  },
  "Florence2VisionConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "in_channels",
      "depths",
      "patch_size",
      "patch_stride",
      "patch_padding",
      "patch_prenorm",
      "embed_dim",
      "num_heads",
      "num_groups",
      "window_size",
      "drop_path_rate",
      "mlp_ratio",
      "qkv_bias",
      "activation_function",
      "projection_dim",
      "max_temporal_embeddings",
      "max_position_embeddings",
      "initializer_range"
    ]
  },
  "Florence2Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "is_encoder_decoder",
      "tie_word_embeddings"
    ]
  },
  "Florence2ProcessorKwargs": {
    "_defaults": []
  },
  "Florence2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "num_additional_image_tokens",
      "post_processor_config"
    ],
    "_construct_prompts": [
      "self",
      "text"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "model_input_names": [
      "self"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens"
    ],
    "post_process_generation": [
      "self",
      "text",
      "sequence",
      "task",
      "image_size"
    ]
  },
  "Florence2PostProcessor": {
    "__init__": [
      "self",
      "config",
      "tokenizer"
    ],
    "quantize": [
      "self",
      "locations",
      "size"
    ],
    "dequantize": [
      "self",
      "locations",
      "size"
    ],
    "decode_with_spans": [
      "self",
      "token_ids"
    ],
    "parse_ocr_from_text_and_spans": [
      "self",
      "text",
      "pattern",
      "image_size",
      "area_threshold"
    ],
    "parse_phrase_grounding_from_text_and_spans": [
      "self",
      "text",
      "image_size"
    ],
    "_find_matched_token_indices": [
      "self",
      "cur_span",
      "token_spans"
    ],
    "parse_description_with_bboxes_from_text_and_spans": [
      "self",
      "text",
      "image_size",
      "allow_empty_phrase"
    ],
    "parse_description_with_polygons_from_text_and_spans": [
      "self",
      "text",
      "image_size",
      "allow_empty_phrase",
      "polygon_sep_token",
      "polygon_start_token",
      "polygon_end_token",
      "with_box_at_start"
    ],
    "__call__": [
      "self",
      "text",
      "sequence",
      "image_size",
      "parse_tasks"
    ]
  },
  "Cohere2VisionConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "downsample_factor",
      "image_token_id",
      "alignment_intermediate_size",
      "tie_word_embeddings"
    ]
  },
  "Cohere2VisionMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "pixel_shuffle": [
      "self",
      "image_features"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "Cohere2VisionModelOutputWithPast": {},
  "Cohere2VisionCausalLMOutputWithPast": {},
  "Cohere2VisionPreTrainedModel": {
    "base_model_prefix": []
  },
  "Cohere2VisionModel": {
    "_checkpoint_conversion_mapping": [],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Cohere2VisionForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep",
      "image_sizes"
    ]
  },
  "get_all_supported_aspect_ratios": [
    "max_image_tiles"
  ],
  "get_optimal_tiled_canvas": [
    "original_image_size",
    "target_tile_size",
    "min_image_tiles",
    "max_image_tiles"
  ],
  "Cohere2VisionFastImageProcessorKwargs": {},
  "Cohere2VisionImageProcessorFast": {
    "size": [],
    "min_patches": [],
    "max_patches": [],
    "crop_to_patches": [],
    "patch_size": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ]
  },
  "Cohere2VisionProcessorKwargs": {
    "_defaults": []
  },
  "Cohere2VisionProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Mask2FormerImageProcessorFast": {
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation",
      "return_binary_maps"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "Mask2FormerImageProcessorKwargs": {},
  "make_pixel_mask": [
    "image",
    "output_size",
    "input_data_format"
  ],
  "binary_mask_to_rle": [
    "mask"
  ],
  "convert_segmentation_to_rle": [
    "segmentation"
  ],
  "remove_low_and_no_objects": [
    "masks",
    "scores",
    "labels",
    "object_mask_threshold",
    "num_labels"
  ],
  "check_segment_validity": [
    "mask_labels",
    "mask_probs",
    "k",
    "mask_threshold",
    "overlap_mask_area_threshold"
  ],
  "compute_segments": [
    "mask_probs",
    "pred_scores",
    "pred_labels",
    "mask_threshold",
    "overlap_mask_area_threshold",
    "label_ids_to_fuse",
    "target_size"
  ],
  "convert_segmentation_map_to_binary_masks": [
    "segmentation_map",
    "instance_id_to_semantic_id",
    "ignore_index",
    "do_reduce_labels"
  ],
  "get_mask2former_resize_output_image_size": [
    "image",
    "size",
    "max_size",
    "size_divisor",
    "default_to_square",
    "input_data_format"
  ],
  "Mask2FormerImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "num_labels",
      "pad_size"
    ],
    "to_dict": [
      "self"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "size_divisor",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "convert_segmentation_map_to_binary_masks": [
      "self",
      "segmentation_map",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_resize",
      "size",
      "size_divisor",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "images",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "encode_inputs": [
      "self",
      "pixel_values_list",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels",
      "return_tensors",
      "input_data_format",
      "pad_size"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation",
      "return_binary_maps"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "convert_segmentation_map_to_binary_masks_fast": [
    "segmentation_map",
    "instance_id_to_semantic_id",
    "ignore_index",
    "do_reduce_labels"
  ],
  "Mask2FormerConfig": {
    "model_type": [],
    "sub_configs": [],
    "backbones_supported": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "feature_size",
      "mask_feature_size",
      "hidden_dim",
      "encoder_feedforward_dim",
      "activation_function",
      "encoder_layers",
      "decoder_layers",
      "num_attention_heads",
      "dropout",
      "dim_feedforward",
      "pre_norm",
      "enforce_input_projection",
      "common_stride",
      "ignore_value",
      "num_queries",
      "no_object_weight",
      "class_weight",
      "mask_weight",
      "dice_weight",
      "train_num_points",
      "oversample_ratio",
      "importance_sample_ratio",
      "init_std",
      "init_xavier_std",
      "use_auxiliary_loss",
      "feature_strides",
      "output_auxiliary_logits"
    ]
  },
  "Mask2FormerPixelDecoderOutput": {},
  "Mask2FormerMaskedAttentionDecoderOutput": {},
  "Mask2FormerPixelLevelModuleOutput": {},
  "Mask2FormerModelOutput": {},
  "Mask2FormerForUniversalSegmentationOutput": {},
  "sample_point": [
    "input_features",
    "point_coordinates",
    "add_dim"
  ],
  "sigmoid_cross_entropy_loss": [
    "inputs",
    "labels",
    "num_masks"
  ],
  "pair_wise_dice_loss": [
    "inputs",
    "labels"
  ],
  "pair_wise_sigmoid_cross_entropy_loss": [
    "inputs",
    "labels"
  ],
  "Mask2FormerHungarianMatcher": {
    "__init__": [
      "self",
      "cost_class",
      "cost_mask",
      "cost_dice",
      "num_points"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels"
    ]
  },
  "Mask2FormerLoss": {
    "__init__": [
      "self",
      "config",
      "weight_dict"
    ],
    "_max_by_axis": [
      "self",
      "sizes"
    ],
    "_pad_images_to_max_in_batch": [
      "self",
      "tensors"
    ],
    "loss_labels": [
      "self",
      "class_queries_logits",
      "class_labels",
      "indices"
    ],
    "loss_masks": [
      "self",
      "masks_queries_logits",
      "mask_labels",
      "indices",
      "num_masks"
    ],
    "_get_predictions_permutation_indices": [
      "self",
      "indices"
    ],
    "_get_targets_permutation_indices": [
      "self",
      "indices"
    ],
    "calculate_uncertainty": [
      "self",
      "logits"
    ],
    "sample_points_using_uncertainty": [
      "self",
      "logits",
      "uncertainty_function",
      "num_points",
      "oversample_ratio",
      "importance_sample_ratio"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_predictions"
    ],
    "get_num_masks": [
      "self",
      "class_labels",
      "device"
    ]
  },
  "multi_scale_deformable_attention": [
    "value",
    "value_spatial_shapes",
    "sampling_locations",
    "attention_weights"
  ],
  "Mask2FormerSinePositionEmbedding": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "n_levels",
      "n_points"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes_list",
      "level_start_index",
      "output_attentions"
    ]
  },
  "Mask2FormerPixelDecoderEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes_list",
      "level_start_index",
      "output_attentions"
    ]
  },
  "Mask2FormerPixelDecoderEncoderOnly": {
    "__init__": [
      "self",
      "config"
    ],
    "get_reference_points": [
      "spatial_shapes_list",
      "valid_ratios",
      "device"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "position_embeddings",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Mask2FormerPixelDecoder": {
    "__init__": [
      "self",
      "config",
      "feature_channels"
    ],
    "get_valid_ratio": [
      "self",
      "mask",
      "dtype"
    ],
    "forward": [
      "self",
      "features",
      "encoder_outputs",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Mask2FormerPixelLevelModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "Mask2FormerAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "batch_size"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "key_value_states",
      "key_value_position_embeddings",
      "output_attentions"
    ]
  },
  "Mask2FormerMaskedAttentionDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "pos"
    ],
    "forward_post": [
      "self",
      "hidden_states",
      "level_index",
      "attention_mask",
      "position_embeddings",
      "query_position_embeddings",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ],
    "forward_pre": [
      "self",
      "hidden_states",
      "level_index",
      "attention_mask",
      "position_embeddings",
      "query_position_embeddings",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ],
    "forward": [
      "self",
      "hidden_states",
      "level_index",
      "attention_mask",
      "position_embeddings",
      "query_position_embeddings",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "Mask2FormerMaskedAttentionDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "multi_stage_positional_embeddings",
      "pixel_embeddings",
      "encoder_hidden_states",
      "query_position_embeddings",
      "feature_size_list",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Mask2FormerPredictionBlock": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim",
      "activation"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "Mask2FormerMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "Mask2FormerMaskPredictor": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "mask_feature_size"
    ],
    "forward": [
      "self",
      "outputs",
      "pixel_embeddings",
      "attention_mask_target_size"
    ]
  },
  "Mask2FormerTransformerModule": {
    "__init__": [
      "self",
      "in_features",
      "config"
    ],
    "forward": [
      "self",
      "multi_scale_features",
      "mask_features",
      "output_hidden_states",
      "output_attentions"
    ]
  },
  "Mask2FormerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Mask2FormerModel": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "Mask2FormerForUniversalSegmentation": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_loss_dict": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_predictions"
    ],
    "get_loss": [
      "self",
      "loss_dict"
    ],
    "get_auxiliary_logits": [
      "self",
      "classes",
      "output_masks"
    ],
    "forward": [
      "self",
      "pixel_values",
      "mask_labels",
      "class_labels",
      "pixel_mask",
      "output_hidden_states",
      "output_auxiliary_logits",
      "output_attentions",
      "return_dict"
    ]
  },
  "Pix2StructProcessorKwargs": {
    "_defaults": []
  },
  "Pix2StructProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Pix2StructTextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "d_kv",
      "d_ff",
      "num_layers",
      "num_heads",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "dropout_rate",
      "layer_norm_epsilon",
      "initializer_factor",
      "dense_act_fn",
      "decoder_start_token_id",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "Pix2StructVisionConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "patch_embed_hidden_size",
      "d_ff",
      "d_kv",
      "num_hidden_layers",
      "num_attention_heads",
      "dense_act_fn",
      "layer_norm_eps",
      "dropout_rate",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "seq_len",
      "relative_attention_num_buckets",
      "relative_attention_max_distance"
    ]
  },
  "Pix2StructConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "initializer_factor",
      "initializer_range",
      "is_vqa",
      "tie_word_embeddings",
      "is_encoder_decoder"
    ]
  },
  "torch_extract_patches": [
    "image_tensor",
    "patch_height",
    "patch_width"
  ],
  "Pix2StructImageProcessorFast": {
    "rescale_factor": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "max_patches": [],
    "is_vqa": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "_further_process_kwargs": [
      "self",
      "patch_size"
    ],
    "_validate_preprocess_kwargs": [
      "self"
    ],
    "render_header": [
      "self",
      "image",
      "header",
      "font_bytes",
      "font_path"
    ],
    "normalize": [
      "self",
      "images"
    ],
    "extract_flattened_patches": [
      "self",
      "images",
      "max_patches",
      "patch_size"
    ],
    "preprocess": [
      "self",
      "images",
      "header_text"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "header_text",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_normalize",
      "max_patches",
      "patch_size",
      "return_tensors",
      "disable_grouping"
    ]
  },
  "DEFAULT_FONT_PATH": [],
  "Pix2StructImageProcessorKwargs": {},
  "render_text": [
    "text",
    "text_size",
    "text_color",
    "background_color",
    "left_padding",
    "right_padding",
    "top_padding",
    "bottom_padding",
    "font_bytes",
    "font_path"
  ],
  "render_header": [
    "image",
    "header",
    "input_data_format"
  ],
  "Pix2StructImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_convert_rgb",
      "do_normalize",
      "patch_size",
      "max_patches",
      "is_vqa"
    ],
    "extract_flattened_patches": [
      "self",
      "image",
      "max_patches",
      "patch_size",
      "input_data_format"
    ],
    "normalize": [
      "self",
      "image",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "header_text",
      "do_convert_rgb",
      "do_normalize",
      "max_patches",
      "patch_size",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "Pix2StructLayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pix2StructVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "flattened_patches"
    ]
  },
  "Pix2StructVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "Pix2StructVisionMlp": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pix2StructVisionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Pix2StructVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Pix2StructPreTrainedModel": {
    "input_modalities": [],
    "_can_compile_fullgraph": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "Pix2StructVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "flattened_patches",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Pix2StructTextDenseGatedActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pix2StructTextLayerFF": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pix2StructTextAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_values",
      "query_length",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "Pix2StructTextLayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "Pix2StructTextLayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "query_length",
      "output_attentions",
      "cache_position"
    ]
  },
  "Pix2StructTextBlock": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "return_dict",
      "cache_position"
    ]
  },
  "Pix2StructTextModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "_tied_weights_keys": [],
    "supports_gradient_checkpointing": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "Pix2StructForConditionalGeneration": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "flattened_patches",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "labels",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ViTMSNConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias"
    ]
  },
  "ViTMSNEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "ViTMSNPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "ViTMSNSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMSNSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViTMSNAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMSNIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMSNOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViTMSNLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMSNEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMSNPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ViTMSNModel": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "ViTMSNForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "interpolate_pos_encoding"
    ]
  },
  "invert_mask": [
    "attention_mask"
  ],
  "triu_onnx": [
    "x",
    "diagonal"
  ],
  "_prepare_fsmt_decoder_inputs": [
    "config",
    "input_ids",
    "decoder_input_ids",
    "decoder_padding_mask",
    "causal_mask_dtype"
  ],
  "PretrainedFSMTModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "_make_linear_from_emb": [
    "emb"
  ],
  "_check_shapes": [
    "shape_1",
    "shape2"
  ],
  "make_padding_mask": [
    "input_ids",
    "padding_idx"
  ],
  "EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "encoder_padding_mask",
      "output_attentions"
    ]
  },
  "FSMTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "x",
      "encoder_hidden_states",
      "encoder_attn_mask",
      "layer_state",
      "causal_mask",
      "decoder_padding_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "FSMTDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "encoder_hidden_states",
      "encoder_padding_mask",
      "decoder_padding_mask",
      "decoder_causal_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "_reorder_buffer": [
    "attn_cache",
    "new_order"
  ],
  "Attention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "bias",
      "encoder_decoder_attention",
      "layer_idx"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "key_padding_mask",
      "layer_state",
      "attn_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "fill_with_neg_inf": [
    "t"
  ],
  "_get_shape": [
    "t"
  ],
  "FSMTModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "return_dict",
      "cache_position"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "value"
    ]
  },
  "FSMTForConditionalGeneration": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "value"
    ]
  },
  "SinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weight": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "make_positions": [
      "tensor",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input",
      "incremental_state",
      "timestep"
    ]
  },
  "get_pairs": [
    "word"
  ],
  "replace_unicode_punct": [
    "text"
  ],
  "remove_non_printing_char": [
    "text"
  ],
  "FSMTTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "langs",
      "src_vocab_file",
      "tgt_vocab_file",
      "merges_file",
      "do_lower_case",
      "unk_token",
      "bos_token",
      "sep_token",
      "pad_token"
    ],
    "get_vocab": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "moses_punct_norm": [
      "self",
      "text",
      "lang"
    ],
    "moses_tokenize": [
      "self",
      "text",
      "lang"
    ],
    "moses_detokenize": [
      "self",
      "tokens",
      "lang"
    ],
    "moses_pipeline": [
      "self",
      "text",
      "lang"
    ],
    "src_vocab_size": [
      "self"
    ],
    "tgt_vocab_size": [
      "self"
    ],
    "get_src_vocab": [
      "self"
    ],
    "get_tgt_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text",
      "lang",
      "bypass_tokenizer"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ]
  },
  "DecoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "bos_token_id",
      "is_encoder_decoder"
    ]
  },
  "FSMTConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "langs",
      "src_vocab_size",
      "tgt_vocab_size",
      "activation_function",
      "d_model",
      "max_length",
      "max_position_embeddings",
      "encoder_ffn_dim",
      "encoder_layers",
      "encoder_attention_heads",
      "encoder_layerdrop",
      "decoder_ffn_dim",
      "decoder_layers",
      "decoder_attention_heads",
      "decoder_layerdrop",
      "attention_dropout",
      "dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "is_encoder_decoder",
      "scale_embedding",
      "tie_word_embeddings",
      "num_beams",
      "length_penalty",
      "early_stopping",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "forced_eos_token_id"
    ]
  },
  "ElectraConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "embedding_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "summary_type",
      "summary_use_proj",
      "summary_activation",
      "summary_last_dropout",
      "pad_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "ElectraEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "ElectraSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ElectraCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "ElectraSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ElectraAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ElectraIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ElectraOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ElectraLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "ElectraEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "ElectraDiscriminatorPredictions": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "discriminator_hidden_states"
    ]
  },
  "ElectraGeneratorPredictions": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "generator_hidden_states"
    ]
  },
  "ElectraPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ElectraForPreTrainingOutput": {},
  "ElectraModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "ElectraClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "ElectraSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "ElectraForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ElectraForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ElectraForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "word_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ElectraForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ElectraForQuestionAnswering": {
    "config_class": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "ElectraForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ElectraForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "load_vocab": [
    "vocab_file"
  ],
  "WordpieceTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "unk_token",
      "max_input_chars_per_word"
    ],
    "tokenize": [
      "self",
      "token"
    ]
  },
  "CpmAntTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "add_prefix_space": [],
    "__init__": [
      "self",
      "vocab_file",
      "bod_token",
      "eod_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "unk_token",
      "line_token",
      "space_token",
      "padding_side"
    ],
    "bod_token_id": [
      "self"
    ],
    "eod_token_id": [
      "self"
    ],
    "newline_id": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_decode": [
      "self",
      "token_ids"
    ],
    "check": [
      "self",
      "token"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "CpmAntConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_attention_heads",
      "dim_head",
      "dim_ff",
      "num_hidden_layers",
      "dropout_p",
      "position_bias_num_buckets",
      "position_bias_max_distance",
      "eps",
      "init_std",
      "prompt_types",
      "prompt_length",
      "segment_types",
      "use_cache",
      "tie_word_embeddings"
    ]
  },
  "CpmAntLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CpmAntAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_q",
      "hidden_kv",
      "attention_mask",
      "position_bias",
      "output_attentions",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "CpmAntSelfAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "CpmAntDenseGatedACT": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CpmAntFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CpmAntFFNBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CpmAntTransformerBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "CpmAntEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions",
      "output_hidden_states",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "CpmAntIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CpmAntSegmentPositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "key_pos",
      "query_pos",
      "key_segment",
      "query_segment"
    ],
    "_segment_relative_position_bucket": [
      "self",
      "query_segment",
      "key_segment"
    ],
    "_position_bucket": [
      "self",
      "relative_position",
      "num_buckets",
      "max_distance"
    ]
  },
  "CpmAntOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "CpmAntPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CpmAntModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "embeddings"
    ],
    "_prepare_attention_mask": [
      "self",
      "input_ids",
      "span",
      "context",
      "length"
    ],
    "forward": [
      "self",
      "input_ids",
      "output_attentions",
      "output_hidden_states",
      "past_key_values",
      "use_cache",
      "return_dict",
      "cache_position"
    ]
  },
  "CpmAntForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict",
      "attention_mask",
      "cache_position",
      "logits_to_keep"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "embeddings"
    ]
  },
  "ListOfDict": [],
  "Wav2Vec2DecoderWithLMOutput": {},
  "Wav2Vec2ProcessorWithLM": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer",
      "decoder"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "_set_language_model_attribute": [
      "decoder",
      "attribute",
      "value"
    ],
    "language_model": [
      "self"
    ],
    "get_missing_alphabet_tokens": [
      "decoder",
      "tokenizer"
    ],
    "__call__": [
      "self"
    ],
    "pad": [
      "self"
    ],
    "batch_decode": [
      "self",
      "logits",
      "pool",
      "num_processes",
      "beam_width",
      "beam_prune_logp",
      "token_min_logp",
      "hotwords",
      "hotword_weight",
      "alpha",
      "beta",
      "unk_score_offset",
      "lm_score_boundary",
      "output_word_offsets",
      "n_best"
    ],
    "decode": [
      "self",
      "logits",
      "beam_width",
      "beam_prune_logp",
      "token_min_logp",
      "hotwords",
      "hotword_weight",
      "alpha",
      "beta",
      "unk_score_offset",
      "lm_score_boundary",
      "output_word_offsets",
      "n_best"
    ]
  },
  "NllbMoeScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "NllbMoeSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "past_key_values_length",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "NllbMoeTop2Router": {
    "__init__": [
      "self",
      "config"
    ],
    "_cast_classifier": [
      "self"
    ],
    "normalize_router_probabilities": [
      "self",
      "router_probs",
      "top_1_mask",
      "top_2_mask"
    ],
    "route_tokens": [
      "self",
      "router_logits",
      "input_dtype",
      "padding_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask"
    ]
  },
  "NllbMoeDenseActDense": {
    "__init__": [
      "self",
      "config",
      "ffn_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "NllbMoeExperts": {
    "__init__": [
      "self",
      "config",
      "ffn_dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "router_mask",
      "router_probs"
    ]
  },
  "NllbMoeSparseMLP": {
    "__init__": [
      "self",
      "config",
      "ffn_dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask"
    ]
  },
  "NllbMoeAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "cache_position"
    ]
  },
  "NllbMoeEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "is_sparse",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "NllbMoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "is_sparse",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "NllbMoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "NllbMoeEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds"
    ]
  },
  "NllbMoeDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "NllbMoeModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "load_balancing_loss_func": [
    "gate_logits",
    "num_experts",
    "top_k",
    "attention_mask"
  ],
  "NllbMoeForConditionalGeneration": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position"
    ]
  },
  "NllbMoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "router_bias",
      "router_dtype",
      "router_ignore_padding_tokens",
      "num_experts",
      "expert_capacity",
      "encoder_sparse_step",
      "decoder_sparse_step",
      "router_z_loss_coef",
      "router_aux_loss_coef",
      "second_expert_policy",
      "normalize_router_prob_before_dropping",
      "batch_prioritized_routing",
      "moe_eval_capacity_token_fraction",
      "moe_token_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "output_router_logits"
    ]
  },
  "SwinEncoderOutput": {},
  "SwinModelOutput": {},
  "SwinMaskedImageModelingOutput": {},
  "SwinImageClassifierOutput": {},
  "window_partition": [
    "input_feature",
    "window_size"
  ],
  "window_reverse": [
    "windows",
    "window_size",
    "height",
    "width"
  ],
  "SwinEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "SwinPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "maybe_pad": [
      "self",
      "pixel_values",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SwinPatchMerging": {
    "__init__": [
      "self",
      "input_resolution",
      "dim",
      "norm_layer"
    ],
    "maybe_pad": [
      "self",
      "input_feature",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "input_feature",
      "input_dimensions"
    ]
  },
  "SwinDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "SwinSelfAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "create_relative_position_index": [
      "self"
    ]
  },
  "SwinSelfOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "SwinAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SwinIntermediate": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwinOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwinLayer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "num_heads",
      "drop_path_rate",
      "shift_size"
    ],
    "set_shift_and_window_size": [
      "self",
      "input_resolution"
    ],
    "get_attn_mask": [
      "self",
      "height",
      "width",
      "dtype",
      "device"
    ],
    "maybe_pad": [
      "self",
      "hidden_states",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "always_partition"
    ]
  },
  "SwinStage": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "depth",
      "num_heads",
      "drop_path",
      "downsample"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "always_partition"
    ]
  },
  "SwinEncoder": {
    "__init__": [
      "self",
      "config",
      "grid_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "output_hidden_states",
      "output_hidden_states_before_downsampling",
      "always_partition",
      "return_dict"
    ]
  },
  "SwinPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SwinModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "use_mask_token"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "SwinForMaskedImageModeling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "SwinForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "SwinBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "SwinConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim",
      "depths",
      "num_heads",
      "window_size",
      "mlp_ratio",
      "qkv_bias",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "drop_path_rate",
      "hidden_act",
      "use_absolute_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "encoder_stride",
      "out_features",
      "out_indices"
    ]
  },
  "SegformerImageProcessorKwargs": {},
  "SegformerImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_reduce_labels"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "reduce_label": [
      "self",
      "label"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_reduce_labels",
      "do_resize",
      "do_rescale",
      "do_normalize",
      "size",
      "resample",
      "rescale_factor",
      "image_mean",
      "image_std",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_reduce_labels",
      "do_resize",
      "size",
      "input_data_format"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "SegformerImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "do_reduce_labels": [],
    "do_center_crop": [],
    "crop_size": [],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_reduce_labels",
      "interpolation",
      "do_resize",
      "do_rescale",
      "do_normalize",
      "size",
      "rescale_factor",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "SegformerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "num_encoder_blocks",
      "depths",
      "sr_ratios",
      "hidden_sizes",
      "patch_sizes",
      "strides",
      "num_attention_heads",
      "mlp_ratios",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "classifier_dropout_prob",
      "initializer_range",
      "drop_path_rate",
      "layer_norm_eps",
      "decoder_hidden_size",
      "semantic_loss_ignore_index"
    ]
  },
  "SegFormerImageClassifierOutput": {},
  "SegformerDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "SegformerOverlapPatchEmbeddings": {
    "__init__": [
      "self",
      "patch_size",
      "stride",
      "num_channels",
      "hidden_size"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SegformerEfficientSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "sequence_reduction_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "SegformerSelfOutput": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "SegformerAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "sequence_reduction_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "SegformerDWConv": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width"
    ]
  },
  "SegformerMixFFN": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "hidden_features",
      "out_features"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width"
    ]
  },
  "SegformerLayer": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "drop_path",
      "sequence_reduction_ratio",
      "mlp_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "SegformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SegformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": []
  },
  "SegformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SegformerForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SegformerMLP": {
    "__init__": [
      "self",
      "config",
      "input_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SegformerDecodeHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "SegformerForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "upcast_masked_softmax": [
    "x",
    "mask",
    "mask_value",
    "scale",
    "softmax_dtype"
  ],
  "upcast_softmax": [
    "x",
    "scale",
    "softmax_dtype"
  ],
  "masked_softmax": [
    "x",
    "mask",
    "mask_value"
  ],
  "GPTBigCodeAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTBigCodeMLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GPTBigCodeBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "layer_past",
      "attention_mask",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTBigCodePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GPTBigCodeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "GPTBigCodeForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GPTBigCodeForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTBigCodeForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTBigCodeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "n_positions",
      "n_embd",
      "n_layer",
      "n_head",
      "n_inner",
      "activation_function",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "scale_attn_weights",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "attention_softmax_in_fp32",
      "scale_attention_softmax_in_fp32",
      "multi_query",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "Gemma3ModelOutputWithPast": {},
  "Gemma3CausalLMOutputWithPast": {},
  "Gemma3TextScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "Gemma3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Gemma3RMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Gemma3RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device",
      "layer_type"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len",
      "layer_type"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "layer_type"
    ]
  },
  "rotate_half": [
    "x"
  ],
  "apply_rotary_pos_emb": [
    "q",
    "k",
    "cos",
    "sin",
    "unsqueeze_dim"
  ],
  "Gemma3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Gemma3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "Gemma3PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "_bidirectional_window_overlay": [
    "sliding_window"
  ],
  "Gemma3TextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Gemma3ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Gemma3MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_outputs"
    ]
  },
  "token_type_ids_mask_function": [
    "token_type_ids",
    "image_group_ids"
  ],
  "create_causal_mask_mapping": [
    "config",
    "input_embeds",
    "attention_mask",
    "cache_position",
    "past_key_values",
    "position_ids",
    "token_type_ids",
    "pixel_values",
    "is_training",
    "is_first_iteration"
  ],
  "Gemma3Model": {
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "token_type_ids",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache"
    ]
  },
  "Gemma3ForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "token_type_ids",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "pixel_values",
      "attention_mask",
      "token_type_ids",
      "use_cache",
      "logits_to_keep",
      "labels",
      "is_first_iteration"
    ],
    "create_masks_for_generate": [
      "config",
      "input_embeds",
      "attention_mask",
      "cache_position",
      "past_key_values",
      "position_ids",
      "token_type_ids",
      "is_first_iteration"
    ]
  },
  "Gemma3ForSequenceClassification": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "token_type_ids",
      "labels",
      "use_cache"
    ]
  },
  "Gemma3TextForSequenceClassification": {
    "input_modalities": []
  },
  "Gemma3ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "do_convert_rgb": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pan_and_scan": [],
    "pan_and_scan_min_crop_size": [],
    "pan_and_scan_max_num_crops": [],
    "pan_and_scan_min_ratio_to_activate": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "pan_and_scan_batched": [
      "self",
      "images",
      "pan_and_scan_min_crop_size",
      "pan_and_scan_max_num_crops",
      "pan_and_scan_min_ratio_to_activate"
    ],
    "_process_images_for_pan_and_scan": [
      "self",
      "images",
      "do_pan_and_scan",
      "pan_and_scan_min_crop_size",
      "pan_and_scan_max_num_crops",
      "pan_and_scan_min_ratio_to_activate"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "do_pan_and_scan",
      "pan_and_scan_min_crop_size",
      "pan_and_scan_max_num_crops",
      "pan_and_scan_min_ratio_to_activate",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "Gemma3ImageProcessorKwargs": {},
  "Gemma3ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "do_pan_and_scan",
      "pan_and_scan_min_crop_size",
      "pan_and_scan_max_num_crops",
      "pan_and_scan_min_ratio_to_activate"
    ],
    "pan_and_scan": [
      "self",
      "image",
      "pan_and_scan_min_crop_size",
      "pan_and_scan_max_num_crops",
      "pan_and_scan_min_ratio_to_activate",
      "data_format",
      "input_data_format"
    ],
    "_process_images_for_pan_and_scan": [
      "self",
      "images",
      "do_pan_and_scan",
      "pan_and_scan_min_crop_size",
      "pan_and_scan_max_num_crops",
      "pan_and_scan_min_ratio_to_activate",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format",
      "do_convert_rgb",
      "do_pan_and_scan",
      "pan_and_scan_min_crop_size",
      "pan_and_scan_max_num_crops",
      "pan_and_scan_min_ratio_to_activate"
    ]
  },
  "Gemma3ProcessorKwargs": {
    "_defaults": []
  },
  "Gemma3Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "image_seq_length"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Gemma3TextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "attention_bias",
      "attention_dropout",
      "query_pre_attn_scalar",
      "sliding_window",
      "layer_types",
      "final_logit_softcapping",
      "attn_logit_softcapping",
      "rope_parameters",
      "use_bidirectional_attention",
      "tie_word_embeddings"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "Gemma3Config": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "mm_tokens_per_image",
      "boi_token_index",
      "eoi_token_index",
      "image_token_index",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "GEMMA3_START_DOCSTRING": [],
  "add_checkpointing_args": [
    "parser"
  ],
  "add_megatron_checkpoint_args": [
    "parser"
  ],
  "add_transformers_checkpoint_args": [
    "parser"
  ],
  "megatron_to_transformers": [],
  "transformers_to_megatron": [],
  "tensor_parallel_params": [],
  "recursive_print": [
    "name",
    "val",
    "spaces"
  ],
  "megatron_to_transformers_fix_query_key_value_ordering": [
    "param",
    "checkpoint_version",
    "num_splits",
    "num_heads",
    "hidden_size"
  ],
  "transformers_to_megatron_fix_query_key_value_ordering": [
    "param",
    "checkpoint_version",
    "num_splits",
    "num_heads",
    "hidden_size"
  ],
  "merge_transformers_sharded_states": [
    "path",
    "num_checkpoints"
  ],
  "get_megatron_sharded_states": [
    "args",
    "tp_size",
    "pp_size",
    "pp_rank"
  ],
  "get_element_from_dict_by_path": [
    "d",
    "path"
  ],
  "convert_checkpoint_from_megatron_to_transformers": [
    "args"
  ],
  "convert_checkpoint_from_transformers_to_megatron": [
    "args"
  ],
  "SolarOpenConfig": {
    "model_type": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "moe_intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "n_shared_experts",
      "n_routed_experts",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "num_experts_per_tok",
      "routed_scaling_factor",
      "n_group",
      "topk_group",
      "norm_topk_prob",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ]
  },
  "SolarOpenDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "SolarOpenMoE": {},
  "SolarOpenAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "SolarOpenRMSNorm": {},
  "SolarOpenPreTrainedModel": {},
  "SolarOpenModel": {
    "_keys_to_ignore_on_load_unexpected": []
  },
  "SolarOpenForCausalLM": {},
  "SolarOpenMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SolarOpenTopkRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SolarOpenNaiveMoe": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "SolarOpenRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "DeformableDetrImageProcessorFast": {
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "top_k"
    ],
    "post_process_instance_segmentation": [
      "self"
    ],
    "post_process_semantic_segmentation": [
      "self"
    ],
    "post_process_panoptic_segmentation": [
      "self"
    ]
  },
  "DeformableDetrDecoderOutput": {},
  "DeformableDetrModelOutput": {},
  "DeformableDetrObjectDetectionOutput": {},
  "inverse_sigmoid": [
    "x",
    "eps"
  ],
  "MultiScaleDeformableAttention": {
    "forward": [
      "self",
      "value",
      "value_spatial_shapes",
      "value_spatial_shapes_list",
      "level_start_index",
      "sampling_locations",
      "attention_weights",
      "im2col_step"
    ]
  },
  "DeformableDetrConvEncoder": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DeformableDetrSinePositionEmbedding": {
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "DeformableDetrLearnedPositionEmbedding": {},
  "DeformableDetrSelfAttention": {},
  "DeformableDetrMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "config",
      "num_heads",
      "n_points"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index"
    ]
  },
  "DeformableDetrMLP": {},
  "DeformableDetrEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index"
    ]
  },
  "DeformableDetrDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "object_queries_position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "DeformableDetrPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_supports_flex_attn": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DeformableDetrEncoder": {
    "_can_record_outputs": [],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "spatial_position_embeddings",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios"
    ],
    "get_reference_points": [
      "spatial_shapes_list",
      "valid_ratios",
      "device"
    ]
  },
  "DeformableDetrDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "object_queries_position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios"
    ]
  },
  "DeformableDetrModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "get_valid_ratio": [
      "self",
      "mask",
      "dtype"
    ],
    "get_proposal_pos_embed": [
      "self",
      "proposals"
    ],
    "gen_encoder_output_proposals": [
      "self",
      "enc_output",
      "padding_mask",
      "spatial_shapes"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds"
    ]
  },
  "DeformableDetrMLPPredictionHead": {},
  "DeformableDetrForObjectDetection": {
    "_no_split_modules": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "DeformableDetrImageProcessorKwargs": {},
  "SUPPORTED_ANNOTATION_FORMATS": [],
  "normalize_annotation": [
    "annotation",
    "image_size"
  ],
  "convert_coco_poly_to_mask": [
    "segmentations",
    "height",
    "width"
  ],
  "prepare_coco_detection_annotation": [
    "image",
    "target",
    "return_segmentation_masks",
    "input_data_format"
  ],
  "masks_to_boxes": [
    "masks"
  ],
  "prepare_coco_panoptic_annotation": [
    "image",
    "target",
    "masks_path",
    "return_masks",
    "input_data_format"
  ],
  "get_segmentation_image": [
    "masks",
    "input_size",
    "target_size",
    "stuff_equiv_classes",
    "deduplicate"
  ],
  "get_mask_area": [
    "seg_img",
    "target_size",
    "n_classes"
  ],
  "score_labels_from_class_probabilities": [
    "logits"
  ],
  "post_process_panoptic_sample": [
    "out_logits",
    "masks",
    "boxes",
    "processed_size",
    "target_size",
    "is_thing_map",
    "threshold"
  ],
  "resize_annotation": [
    "annotation",
    "orig_size",
    "target_size",
    "threshold",
    "resample"
  ],
  "DeformableDetrImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "format",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_annotations",
      "do_pad",
      "pad_size"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "size",
      "resample"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "annotation",
      "constant_values",
      "data_format",
      "input_data_format",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "images",
      "annotations",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "update_bboxes",
      "pad_size"
    ],
    "preprocess": [
      "self",
      "images",
      "annotations",
      "return_segmentation_masks",
      "masks_path",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "format",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "top_k"
    ]
  },
  "DeformableDetrFrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DeformableDetrConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "num_channels",
      "num_queries",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "init_xavier_std",
      "return_intermediate",
      "auxiliary_loss",
      "position_embedding_type",
      "dilation",
      "num_feature_levels",
      "encoder_n_points",
      "decoder_n_points",
      "two_stage",
      "two_stage_num_proposals",
      "with_box_refine",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "mask_loss_coefficient",
      "dice_loss_coefficient",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "eos_coefficient",
      "focal_alpha",
      "disable_custom_kernels",
      "tie_word_embeddings"
    ]
  },
  "ResNetConvLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "activation"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "ResNetEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "ResNetShortCut": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "ResNetBasicLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "stride",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ResNetBottleNeckLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "stride",
      "activation",
      "reduction",
      "downsample_in_bottleneck"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ResNetStage": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "depth"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "ResNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ResNetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ResNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ResNetForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ResNetBackbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ResNetConfig": {
    "model_type": [],
    "layer_types": [],
    "__init__": [
      "self",
      "num_channels",
      "embedding_size",
      "hidden_sizes",
      "depths",
      "layer_type",
      "hidden_act",
      "downsample_in_first_stage",
      "downsample_in_bottleneck",
      "out_features",
      "out_indices"
    ]
  },
  "Olmo2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "rms_norm_eps"
    ]
  },
  "Olmo2RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Olmo2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Olmo2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Olmo2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Olmo2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Olmo2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Olmo2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "Olmo2ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BigBirdConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "sep_token_id",
      "attention_type",
      "use_bias",
      "rescale_embeddings",
      "block_size",
      "num_random_blocks",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "_TRIVIA_QA_MAPPING": [],
  "BigBirdEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "BigBirdSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "BigBirdBlockSparseAttention": {
    "__init__": [
      "self",
      "config",
      "seed"
    ],
    "forward": [
      "self",
      "hidden_states",
      "band_mask",
      "from_mask",
      "to_mask",
      "from_blocked_mask",
      "to_blocked_mask",
      "output_attentions"
    ],
    "torch_bmm_nd": [
      "inp_1",
      "inp_2",
      "ndim"
    ],
    "torch_bmm_nd_transpose": [
      "inp_1",
      "inp_2",
      "ndim"
    ],
    "bigbird_block_sparse_attention": [
      "self",
      "query_layer",
      "key_layer",
      "value_layer",
      "band_mask",
      "from_mask",
      "to_mask",
      "from_blocked_mask",
      "to_blocked_mask",
      "n_heads",
      "n_rand_blocks",
      "attention_head_size",
      "from_block_size",
      "to_block_size",
      "batch_size",
      "from_seq_len",
      "to_seq_len",
      "seed",
      "plan_from_length",
      "plan_num_rand_blocks",
      "output_attentions"
    ],
    "torch_gather_b2": [
      "params",
      "indices"
    ],
    "_create_rand_mask_from_inputs": [
      "from_blocked_mask",
      "to_blocked_mask",
      "rand_attn",
      "num_attention_heads",
      "num_rand_blocks",
      "batch_size",
      "from_seq_length",
      "from_block_size"
    ],
    "_get_rand_attn_plan": [
      "from_seq_length",
      "from_block_size",
      "num_rand_blocks"
    ],
    "_bigbird_block_rand_mask": [
      "self",
      "from_seq_length",
      "to_seq_length",
      "from_block_size",
      "to_block_size",
      "num_rand_blocks",
      "last_idx"
    ],
    "_bigbird_block_rand_mask_with_head": [
      "self",
      "from_seq_length",
      "to_seq_length",
      "from_block_size",
      "to_block_size",
      "num_heads",
      "plan_from_length",
      "plan_num_rand_blocks",
      "window_block_left",
      "window_block_right",
      "global_block_top",
      "global_block_bottom",
      "global_block_left",
      "global_block_right"
    ],
    "_get_single_block_row_attention": [
      "block_id",
      "to_start_block_id",
      "to_end_block_id",
      "num_rand_blocks",
      "window_block_left",
      "window_block_right",
      "global_block_left",
      "global_block_right"
    ]
  },
  "BigBirdSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BigBirdAttention": {
    "__init__": [
      "self",
      "config",
      "seed"
    ],
    "set_attention_type": [
      "self",
      "value",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "band_mask",
      "from_mask",
      "to_mask",
      "from_blocked_mask",
      "to_blocked_mask",
      "cache_position"
    ]
  },
  "BigBirdIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BigBirdOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BigBirdLayer": {
    "__init__": [
      "self",
      "config",
      "seed"
    ],
    "set_attention_type": [
      "self",
      "value",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "band_mask",
      "from_mask",
      "to_mask",
      "blocked_encoder_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "BigBirdEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "set_attention_type": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "band_mask",
      "from_mask",
      "to_mask",
      "blocked_encoder_mask",
      "return_dict",
      "cache_position"
    ]
  },
  "BigBirdPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BigBirdLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BigBirdOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "BigBirdOnlyNSPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "BigBirdPreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "BigBirdPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BigBirdForPreTrainingOutput": {},
  "BigBirdForQuestionAnsweringModelOutput": {},
  "BigBirdModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "set_attention_type": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "create_masks_for_block_sparse_attn": [
      "attention_mask",
      "block_size"
    ],
    "_pad_to_block_size": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "pad_token_id"
    ]
  },
  "BigBirdForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "next_sentence_label",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BigBirdForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BigBirdForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BigBirdClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "BigBirdForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BigBirdForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BigBirdForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BigBirdForQuestionAnsweringHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoder_output"
    ]
  },
  "BigBirdForQuestionAnswering": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "question_lengths",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "prepare_question_mask": [
      "q_lengths",
      "maxlen"
    ]
  },
  "BigBirdTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "sep_token",
      "mask_token",
      "cls_token",
      "add_prefix_space"
    ]
  },
  "_CHECKPOINT_FOR_DOC": [],
  "_CONFIG_FOR_DOC": [],
  "Exaone4Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_dropout",
      "sliding_window",
      "sliding_window_pattern",
      "layer_types"
    ]
  },
  "Exaone4RMSNorm": {},
  "Exaone4RotaryEmbedding": {},
  "Exaone4Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Exaone4MLP": {},
  "Exaone4DecoderLayer": {},
  "Exaone4PreTrainedModel": {
    "config_class": [],
    "_no_split_modules": []
  },
  "Exaone4Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Exaone4ForCausalLM": {
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Exaone4ForSequenceClassification": {},
  "Exaone4ForTokenClassification": {},
  "Exaone4ForQuestionAnswering": {},
  "Lfm2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "max_position_embeddings",
      "initializer_range",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "conv_bias",
      "conv_L_cache",
      "block_multiple_of",
      "block_ffn_dim_multiplier",
      "block_auto_adjust_ff_dim",
      "full_attn_idxs",
      "layer_types"
    ]
  },
  "Lfm2RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Lfm2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Lfm2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Lfm2HybridConvCache": {
    "max_batch_size": [],
    "is_compileable": [],
    "key_cache": [],
    "value_cache": [],
    "__init__": [
      "self",
      "config",
      "max_batch_size",
      "dtype",
      "device"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position",
      "layer_idx"
    ],
    "crop": [
      "self",
      "max_length"
    ],
    "__len__": [
      "self"
    ],
    "reset": [
      "self"
    ]
  },
  "Lfm2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "apply_mask_to_padding_states": [
    "hidden_states",
    "attention_mask"
  ],
  "kernel_modules": [],
  "is_fast_path_available": [],
  "Lfm2ShortConv": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "x",
      "past_key_values",
      "cache_position",
      "attention_mask"
    ],
    "slow_forward": [
      "self",
      "x",
      "past_key_values",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "cache_position",
      "attention_mask"
    ]
  },
  "Lfm2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "Lfm2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Lfm2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Lfm2ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BitConfig": {
    "model_type": [],
    "layer_types": [],
    "supported_padding": [],
    "__init__": [
      "self",
      "num_channels",
      "embedding_size",
      "hidden_sizes",
      "depths",
      "layer_type",
      "hidden_act",
      "global_padding",
      "num_groups",
      "drop_path_rate",
      "embedding_dynamic_padding",
      "output_stride",
      "width_factor",
      "out_features",
      "out_indices"
    ]
  },
  "BitImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "get_padding_value": [
    "padding",
    "kernel_size",
    "stride",
    "dilation"
  ],
  "WeightStandardizedConv2d": {
    "__init__": [
      "self",
      "in_channel",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "groups",
      "bias",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "BitGroupNormActivation": {
    "__init__": [
      "self",
      "config",
      "num_channels",
      "eps",
      "affine",
      "apply_activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DynamicPad2d": {
    "__init__": [
      "self",
      "kernel_size",
      "stride",
      "dilation",
      "value"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "BitMaxPool2d": {
    "__init__": [
      "self",
      "kernel_size",
      "stride",
      "dilation",
      "ceil_mode",
      "padding",
      "padding_value",
      "use_dynamic_padding"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BitEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "BitDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "make_div": [
    "value",
    "divisor"
  ],
  "BitPreActivationBottleneckLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "bottle_ratio",
      "stride",
      "dilation",
      "first_dilation",
      "groups",
      "drop_path_rate",
      "is_first_layer"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BitBottleneckLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "bottle_ratio",
      "stride",
      "dilation",
      "first_dilation",
      "groups",
      "drop_path_rate",
      "is_first_layer"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BitDownsampleConv": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "preact"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BitStage": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "dilation",
      "depth",
      "bottle_ratio",
      "layer_dropout"
    ],
    "_get_updated_hyperparameters": [
      "self",
      "layer_idx",
      "stride",
      "layer_dropout"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "BitEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_updated_hyperparameters": [
      "self",
      "stage_idx",
      "current_stride",
      "current_hidden_size",
      "dilation",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BitPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "main_input_name": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BitModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BitForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BitBackbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BitImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "rescale_factor": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": []
  },
  "PvtV2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "num_channels",
      "num_encoder_blocks",
      "depths",
      "sr_ratios",
      "hidden_sizes",
      "patch_sizes",
      "strides",
      "num_attention_heads",
      "mlp_ratios",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "drop_path_rate",
      "layer_norm_eps",
      "qkv_bias",
      "linear_attention",
      "out_features",
      "out_indices"
    ]
  },
  "PvtV2DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PvtV2OverlapPatchEmbeddings": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "PvtV2DepthWiseConv": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width"
    ]
  },
  "PvtV2SelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "spatial_reduction_ratio"
    ],
    "transpose_for_scores": [
      "self",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "PvtV2ConvFeedForwardNetwork": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "hidden_features",
      "out_features"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width"
    ]
  },
  "PvtV2BlockLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "drop_path"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "PvtV2EncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "PvtV2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PvtV2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PvtV2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PvtV2ForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PvtV2Backbone": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VitMatteConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "hidden_size",
      "batch_norm_eps",
      "initializer_range",
      "convstream_hidden_sizes",
      "fusion_hidden_sizes"
    ]
  },
  "ImageMattingOutput": {},
  "VitMattePreTrainedModel": {
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VitMatteBasicConv3x3": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "padding"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "VitMatteConvStream": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "VitMatteFusionBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "features",
      "detailed_feature_map"
    ]
  },
  "VitMatteHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "VitMatteDetailCaptureModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features",
      "pixel_values"
    ]
  },
  "VitMatteForImageMatting": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "VitMatteImageProcessorKwargs": {},
  "VitMatteImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor"
    ],
    "pad_image": [
      "self",
      "image",
      "size_divisor",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "trimaps",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "VitMatteImageProcessorFast": {
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "_pad_image": [
      "self",
      "images",
      "size_divisor"
    ],
    "preprocess": [
      "self",
      "images",
      "trimaps"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "trimaps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "trimaps",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "Sam2HieraDetConfig": {
    "base_config_key": [],
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_kernel_size",
      "patch_stride",
      "patch_padding",
      "query_stride",
      "window_positional_embedding_background_size",
      "num_query_pool_stages",
      "blocks_per_stage",
      "embed_dim_per_stage",
      "num_attention_heads_per_stage",
      "window_size_per_stage",
      "global_attention_blocks",
      "mlp_ratio",
      "hidden_act",
      "layer_norm_eps",
      "initializer_range"
    ]
  },
  "Sam2VisionConfig": {
    "base_config_key": [],
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "backbone_channel_list",
      "backbone_feature_sizes",
      "fpn_hidden_size",
      "fpn_kernel_size",
      "fpn_stride",
      "fpn_padding",
      "fpn_top_down_levels",
      "num_feature_levels",
      "hidden_act",
      "layer_norm_eps",
      "initializer_range"
    ]
  },
  "Sam2PromptEncoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "image_size",
      "patch_size",
      "mask_input_channels",
      "num_point_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "scale"
    ]
  },
  "Sam2MaskDecoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "hidden_act",
      "mlp_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "attention_downsample_rate",
      "num_multimask_outputs",
      "iou_head_depth",
      "iou_head_hidden_dim",
      "dynamic_multimask_via_stability",
      "dynamic_multimask_stability_delta",
      "dynamic_multimask_stability_thresh"
    ]
  },
  "Sam2Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "prompt_encoder_config",
      "mask_decoder_config",
      "initializer_range"
    ]
  },
  "Sam2FastImageProcessorKwargs": {},
  "Sam2ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "mask_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "valid_kwargs": [],
    "do_pad": [],
    "pad_size": [],
    "mask_pad_size": [],
    "__init__": [
      "self"
    ],
    "_preprocess": [
      "self",
      "images",
      "return_tensors"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "mask_size",
      "default_to_square",
      "image_mean",
      "image_std",
      "data_format"
    ],
    "_apply_non_overlapping_constraints": [
      "self",
      "pred_masks"
    ],
    "post_process_masks": [
      "self",
      "masks",
      "original_sizes",
      "mask_threshold",
      "binarize",
      "max_hole_area",
      "max_sprinkle_area",
      "apply_non_overlapping_constraints"
    ],
    "_get_preprocess_shape": [
      "self"
    ],
    "resize": [
      "self"
    ]
  },
  "Sam2VisionEncoderOutput": {},
  "Sam2ImageSegmentationOutput": {},
  "Sam2PatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Sam2SinePositionEmbedding": {},
  "Sam2VisionNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "do_pool": [
    "x",
    "query_stride"
  ],
  "Sam2MultiScaleAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "dim_out",
      "num_attention_heads",
      "query_stride"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam2FeedForward": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "activation",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam2MultiScaleBlock": {
    "__init__": [
      "self",
      "config",
      "stage_idx",
      "block_idx",
      "total_block_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam2HieraDetModelOutput": {},
  "Sam2PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Sam2HieraDetModel": {
    "config_class": [],
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "_get_pos_embed": [
      "self",
      "hw"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Sam2VisionModel": {
    "config_class": [],
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Sam2PositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_coords",
      "input_shape"
    ]
  },
  "Sam2MaskEmbedding": {},
  "Sam2PromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ]
  },
  "Sam2Attention": {
    "__init__": [
      "self",
      "config",
      "downsample_rate"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_similarity"
    ]
  },
  "Sam2TwoWayAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "skip_first_layer_pe"
    ]
  },
  "Sam2TwoWayTransformer": {},
  "Sam2LayerNorm": {},
  "Sam2MaskDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_stability_scores": [
      "self",
      "mask_logits"
    ],
    "_dynamic_multimask_via_stability": [
      "self",
      "all_mask_logits",
      "all_iou_scores"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "high_resolution_features",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "Sam2Model": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "image_embeddings",
      "multimask_output",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "Sam2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "target_size",
      "point_pad_value"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps",
      "input_points",
      "input_labels",
      "input_boxes",
      "original_sizes",
      "return_tensors"
    ],
    "_normalize_coordinates": [
      "self",
      "target_size",
      "coords",
      "original_size",
      "is_bounding_box"
    ],
    "_convert_to_nested_list": [
      "self",
      "data",
      "expected_depth",
      "current_depth"
    ],
    "_get_nested_dimensions": [
      "self",
      "nested_list",
      "max_dims"
    ],
    "_pad_nested_list": [
      "self",
      "nested_list",
      "target_dims",
      "current_level",
      "pad_value"
    ],
    "_create_empty_nested_structure": [
      "self",
      "dims",
      "pad_value"
    ],
    "_get_nesting_level": [
      "self",
      "input_list"
    ],
    "_validate_single_input": [
      "self",
      "data",
      "expected_depth",
      "input_name",
      "expected_format",
      "expected_coord_size"
    ],
    "_normalize_tensor_coordinates": [
      "self",
      "tensor",
      "original_sizes",
      "is_bounding_box",
      "preserve_padding"
    ],
    "post_process_masks": [
      "self",
      "masks",
      "original_sizes",
      "mask_threshold",
      "binarize",
      "max_hole_area",
      "max_sprinkle_area",
      "apply_non_overlapping_constraints"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "window_unpartition": [
    "windows",
    "window_size",
    "pad_height_width",
    "height_width"
  ],
  "_compute_stability_score": [
    "masks",
    "mask_threshold",
    "stability_score_offset"
  ],
  "_mask_to_rle": [
    "input_mask"
  ],
  "_batched_mask_to_box": [
    "masks"
  ],
  "_is_box_near_crop_edge": [
    "boxes",
    "crop_box",
    "orig_box",
    "atol"
  ],
  "_pad_masks": [
    "masks",
    "crop_box",
    "orig_height",
    "orig_width"
  ],
  "_generate_crop_boxes": [
    "image",
    "target_size",
    "crop_n_layers",
    "overlap_ratio",
    "points_per_crop",
    "crop_n_points_downscale_factor"
  ],
  "_generate_per_layer_crops": [
    "crop_n_layers",
    "overlap_ratio",
    "original_size"
  ],
  "_build_point_grid": [
    "n_per_side"
  ],
  "_generate_crop_images": [
    "crop_boxes",
    "image",
    "points_grid",
    "layer_idxs",
    "target_size",
    "original_size",
    "input_data_format"
  ],
  "_normalize_coordinates": [
    "target_size",
    "coords",
    "original_size",
    "is_bounding_box"
  ],
  "_rle_to_mask": [
    "rle"
  ],
  "_post_process_for_mask_generation": [
    "rle_masks",
    "iou_scores",
    "mask_boxes",
    "amg_crops_nms_thresh"
  ],
  "AudioFlamingo3EncoderConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "num_mel_bins",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "layerdrop",
      "activation_function",
      "hidden_size",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "initializer_range",
      "scale_embedding",
      "max_source_positions"
    ]
  },
  "AudioFlamingo3Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "audio_config",
      "text_config",
      "audio_token_id",
      "projector_hidden_act",
      "projector_bias"
    ]
  },
  "AudioFlamingo3Attention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "layer_idx",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "AudioFlamingo3EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "AudioFlamingo3PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": []
  },
  "AudioFlamingo3Encoder": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "input_features_mask"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ]
  },
  "AudioFlamingo3MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_features"
    ]
  },
  "AudioFlamingo3ForConditionalGeneration": {
    "_keep_in_fp32_modules_strict": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "get_audio_features": [
      "self",
      "input_features",
      "input_features_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "input_features_mask",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ]
  },
  "AudioFlamingo3ProcessorKwargs": {
    "_defaults": []
  },
  "AudioFlamingo3Processor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer",
      "chat_template",
      "audio_token",
      "default_transcription_prompt",
      "max_audio_len"
    ],
    "_get_audio_token_length": [
      "self",
      "audio_lengths"
    ],
    "__call__": [
      "self",
      "text",
      "audio",
      "output_labels"
    ],
    "model_input_names": [
      "self"
    ],
    "apply_transcription_request": [
      "self",
      "audio",
      "prompt"
    ],
    "batch_decode": [
      "self"
    ],
    "_strip_assistant_prefix_and_quotes": [
      "self",
      "text"
    ]
  },
  "TimesformerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "num_frames",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "qkv_bias",
      "attention_type",
      "drop_path_rate"
    ]
  },
  "TimesformerPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "TimesformerEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "TimeSformerDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "TimesformerSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "TimesformerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TimeSformerAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "TimesformerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TimesformerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TimesformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_index"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "TimesformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TimesformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "TimesformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TimesformerForVideoClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "smart_resize": [
    "height",
    "width",
    "factor",
    "min_pixels",
    "max_pixels"
  ],
  "VideoLlama3ImageProcessorFast": {
    "do_resize": [],
    "resample": [],
    "size": [],
    "do_rescale": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "min_pixels",
      "max_pixels"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "disable_grouping",
      "return_tensors"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "VideoLlama3ImageProcessorKwargs": {},
  "VideoLlama3ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "min_pixels",
      "max_pixels",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "videos",
      "do_resize",
      "size",
      "min_pixels",
      "max_pixels",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "VideoLlama3VideoProcessorInitKwargs": {},
  "VideoLlama3VideoProcessor": {
    "resample": [],
    "size": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "min_frames": [],
    "max_frames": [],
    "do_sample_frames": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "use_token_compression": [],
    "return_metadata": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "min_pixels",
      "max_pixels"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "temporal_patch_size",
      "min_frames",
      "max_frames",
      "num_frames",
      "fps"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "use_token_compression",
      "return_tensors",
      "device"
    ],
    "get_num_of_video_patches": [
      "self",
      "num_frames",
      "height",
      "width",
      "videos_kwargs"
    ],
    "_get_compression_mask": [
      "self",
      "pixel_values_videos",
      "video_grid_thw",
      "video_merge_sizes",
      "threshold",
      "min_tokens"
    ]
  },
  "VideoLlama3VisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range"
    ]
  },
  "VideoLlama3Config": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "tie_word_embeddings"
    ]
  },
  "VideoLlama3VisionRotaryEmbedding": {
    "forward": [
      "self",
      "grid_thw",
      "merge_sizes"
    ]
  },
  "VideoLlama3VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VideoLlama3VisionMLP": {},
  "VideoLlama3VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "position_embeddings"
    ]
  },
  "VideoLlama3VisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "position_embeddings"
    ]
  },
  "VideoLlama3VisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "position_embeddings"
    ]
  },
  "VideoLlama3PreTrainedModel": {
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VideoLlama3VisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "pixel_unshuffle": [
      "self",
      "hidden_states",
      "grid_thw",
      "merge_sizes"
    ],
    "forward": [
      "self",
      "pixel_values",
      "grid_thw",
      "merge_sizes"
    ]
  },
  "VideoLlama3Projector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VideoLlama3ModelOutputWithPast": {},
  "VideoLlama3Model": {
    "_checkpoint_conversion_mapping": [],
    "_can_compile_fullgraph": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_rope_index": [
      "self"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw",
      "video_merge_sizes"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw",
      "image_merge_sizes"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "pixel_values",
      "image_grid_thw",
      "image_merge_sizes",
      "pixel_values_videos",
      "video_grid_thw",
      "video_merge_sizes",
      "video_compression_mask",
      "cache_position"
    ]
  },
  "VideoLlama3CausalLMOutputWithPast": {},
  "VideoLlama3ForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_can_compile_fullgraph": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "pixel_values",
      "image_grid_thw",
      "image_merge_sizes",
      "pixel_values_videos",
      "video_grid_thw",
      "video_merge_sizes",
      "video_compression_mask",
      "cache_position"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "image_grid_thw",
      "image_merge_sizes",
      "pixel_values_videos",
      "video_grid_thw",
      "video_merge_sizes",
      "video_compression_mask",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_grid_thw",
      "image_merge_sizes",
      "video_grid_thw",
      "video_merge_sizes",
      "video_compression_mask"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "VideoLlama3ProcessorKwargs": {
    "_defaults": []
  },
  "VideoLlama3Processor": {
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ]
  },
  "apply_rotary_pos_emb_vision": [
    "q",
    "k",
    "cos",
    "sin"
  ],
  "HunYuanDenseV1RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "HunYuanDenseV1MLP": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_shared_mlp"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "HunYuanDenseV1Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "HunYuanDenseV1DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "HunYuanDenseV1PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "HunYuanDenseV1RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "HunYuanDenseV1Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "HunYuanDenseV1ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "HunYuanDenseV1ForSequenceClassification": {},
  "HunYuanDenseV1Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "eod_token_id",
      "pretraining_tp",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "head_dim"
    ]
  },
  "MimiConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "sampling_rate",
      "frame_rate",
      "audio_channels",
      "hidden_size",
      "num_filters",
      "num_residual_layers",
      "upsampling_ratios",
      "kernel_size",
      "last_kernel_size",
      "residual_kernel_size",
      "dilation_growth_rate",
      "use_causal_conv",
      "pad_mode",
      "compress",
      "trim_right_ratio",
      "codebook_size",
      "codebook_dim",
      "num_quantizers",
      "use_conv_shortcut",
      "vector_quantization_hidden_dimension",
      "num_semantic_quantizers",
      "upsample_groups",
      "num_hidden_layers",
      "intermediate_size",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "norm_eps",
      "use_cache",
      "use_streaming",
      "rope_parameters",
      "sliding_window",
      "attention_dropout",
      "layer_scale_initial_scale",
      "attention_bias",
      "tie_word_embeddings"
    ],
    "encodec_frame_rate": [
      "self"
    ],
    "num_codebooks": [
      "self"
    ],
    "frame_size": [
      "self"
    ],
    "frame_rate": [
      "self"
    ]
  },
  "MimiOutput": {},
  "MimiConv1dPaddingCache": {
    "__init__": [
      "self",
      "num_layers",
      "per_layer_padding",
      "per_layer_padding_mode",
      "per_layer_in_channels"
    ],
    "_cache_init": [
      "self",
      "hidden_states",
      "layer_idx"
    ],
    "update": [
      "self",
      "hidden_states",
      "layer_idx"
    ]
  },
  "MimiEncoderOutput": {},
  "MimiDecoderOutput": {},
  "MimiConv1d": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "dilation",
      "groups",
      "pad_mode",
      "bias",
      "layer_idx"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "_get_extra_padding_for_conv1d": [
      "self",
      "hidden_states"
    ],
    "_pad1d": [
      "hidden_states",
      "paddings",
      "mode",
      "value"
    ],
    "_get_output_length": [
      "self",
      "input_length"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_cache"
    ]
  },
  "MimiConvTranspose1d": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "bias"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MimiResnetBlock": {
    "__init__": [
      "self",
      "config",
      "dim",
      "dilations"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_cache"
    ]
  },
  "MimiEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_cache"
    ]
  },
  "MimiLayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MimiRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MimiMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MimiAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MimiFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MimiSdpaAttention": {
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MIMI_ATTENTION_CLASSES": [],
  "MimiTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MimiTransformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MimiDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MimiEuclideanCodebook": {
    "__init__": [
      "self",
      "config",
      "epsilon"
    ],
    "embed": [
      "self"
    ],
    "quantize": [
      "self",
      "hidden_states"
    ],
    "encode": [
      "self",
      "hidden_states"
    ],
    "decode": [
      "self",
      "embed_ind"
    ]
  },
  "MimiVectorQuantization": {
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "hidden_states"
    ],
    "decode": [
      "self",
      "embed_ind"
    ]
  },
  "MimiResidualVectorQuantizer": {
    "__init__": [
      "self",
      "config",
      "num_quantizers"
    ],
    "encode": [
      "self",
      "embeddings",
      "num_quantizers"
    ],
    "decode": [
      "self",
      "codes"
    ]
  },
  "MimiSplitResidualVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "embeddings",
      "num_quantizers"
    ],
    "decode": [
      "self",
      "codes"
    ]
  },
  "MimiPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MimiModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_encode_frame": [
      "self",
      "input_values",
      "num_quantizers",
      "padding_mask",
      "past_key_values",
      "padding_cache",
      "return_dict"
    ],
    "get_encoded_length": [
      "self",
      "input_length"
    ],
    "get_audio_codes_mask": [
      "self",
      "padding_mask",
      "padding_side"
    ],
    "encode": [
      "self",
      "input_values",
      "padding_mask",
      "num_quantizers",
      "encoder_past_key_values",
      "padding_cache",
      "use_streaming",
      "return_dict"
    ],
    "_decode_frame": [
      "self",
      "codes",
      "past_key_values",
      "return_dict"
    ],
    "decode": [
      "self",
      "audio_codes",
      "padding_mask",
      "decoder_past_key_values",
      "return_dict"
    ],
    "forward": [
      "self",
      "input_values",
      "padding_mask",
      "num_quantizers",
      "audio_codes",
      "encoder_past_key_values",
      "decoder_past_key_values",
      "return_dict"
    ]
  },
  "AltCLIPProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "AltCLIPTextConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "initializer_factor",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "project_dim"
    ]
  },
  "AltCLIPVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "projection_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "AltCLIPConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value"
    ]
  },
  "clip_loss": [
    "similarity"
  ],
  "AltCLIPOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "AltRobertaEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "AltRobertaSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "AltRobertaSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ALT_ROBERTA_SELF_ATTENTION_CLASSES": [],
  "AltRobertaAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "AltRobertaIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AltRobertaOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "AltRobertaLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "AltRobertaEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AltRobertaPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AltCLIPAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "AltCLIPMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AltCLIPEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "AltCLIPEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AltCLIPVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "AltCLIPPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_module": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "AltCLIPVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "interpolate_pos_encoding"
    ]
  },
  "AltCLIPVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "AltRobertaModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AltCLIPTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "return_dict",
      "output_hidden_states"
    ]
  },
  "AltCLIPModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "token_type_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "token_type_ids",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Dinov2WithRegistersConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "layerscale_value",
      "drop_path_rate",
      "use_swiglu_ffn",
      "num_register_tokens",
      "out_features",
      "out_indices",
      "apply_layernorm",
      "reshape_hidden_states"
    ]
  },
  "Dinov2WithRegistersPatchEmbeddings": {},
  "Dinov2WithRegistersEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "Dinov2WithRegistersEncoder": {},
  "Dinov2WithRegistersPreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Dinov2WithRegistersModel": {},
  "Dinov2WithRegistersForImageClassification": {
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "Dinov2WithRegistersBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "Dinov2WithRegistersSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dinov2WithRegistersSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Dinov2WithRegistersAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dinov2WithRegistersLayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Dinov2WithRegistersDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Dinov2WithRegistersMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Dinov2WithRegistersSwiGLUFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Dinov2WithRegistersLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Zamba2Config": {
    "model_type": [],
    "attribute_map": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "hidden_size",
      "num_hidden_layers",
      "layers_block_type",
      "mamba_d_state",
      "mamba_d_conv",
      "mamba_expand",
      "mamba_ngroups",
      "time_step_min",
      "time_step_max",
      "time_step_floor",
      "time_step_limit",
      "n_mamba_heads",
      "use_conv_bias",
      "chunk_size",
      "use_mem_eff_path",
      "add_bias_linear",
      "intermediate_size",
      "hidden_act",
      "num_attention_heads",
      "num_key_value_heads",
      "attention_dropout",
      "num_mem_blocks",
      "use_shared_attention_adapter",
      "adapter_rank",
      "use_mem_rope",
      "rope_parameters",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "num_logits_to_keep",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_long_context",
      "tie_word_embeddings"
    ]
  },
  "Zamba2RMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "group_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "Zamba2RMSNorm": {},
  "Zamba2HybridDynamicCache": {
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device"
    ],
    "update_conv_state": [
      "self",
      "layer_idx",
      "new_conv_state",
      "cache_position"
    ],
    "reset": [
      "self"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ]
  },
  "Zamba2RotaryEmbedding": {},
  "Zamba2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "num_fwd_mem_blocks",
      "block_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_idx",
      "attention_mask",
      "past_key_values",
      "position_embeddings"
    ]
  },
  "Zamba2MambaMixer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "attention_mask"
    ],
    "torch_forward": [
      "self",
      "input_states",
      "cache_params",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "attention_mask"
    ]
  },
  "Zamba2MLP": {
    "__init__": [
      "self",
      "config",
      "num_fwd_mem_blocks",
      "block_id"
    ],
    "forward": [
      "self",
      "hidden_state",
      "layer_idx"
    ]
  },
  "Zamba2AttentionDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "block_id",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "original_hidden_states",
      "layer_idx",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "position_embeddings"
    ]
  },
  "Zamba2MambaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Zamba2HybridLayer": {
    "__init__": [
      "self",
      "shared_transformer",
      "linear",
      "mamba"
    ],
    "forward": [
      "self",
      "hidden_states",
      "original_hidden_states",
      "layer_idx",
      "attention_mask",
      "causal_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "position_embeddings",
      "position_ids"
    ]
  },
  "Zamba2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_sdpa": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Zamba2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_layers": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Zamba2ForCausalLM": {},
  "Zamba2ForSequenceClassification": {},
  "pad_tensor_by_size": [
    "input_tensor",
    "pad_size"
  ],
  "reshape_into_chunks": [
    "input_tensor",
    "pad_size",
    "chunk_size"
  ],
  "segment_sum": [
    "input_tensor"
  ],
  "BaseModelOutputWithAttentionMask": {},
  "get_visual_bbox": [
    "image_size",
    "patch_size"
  ],
  "pad_sequence": [
    "seq",
    "target_len",
    "pad_value"
  ],
  "combine_image_text_embeddings": [
    "image_embeddings",
    "inputs_embeds",
    "bbox",
    "visual_bbox",
    "attention_mask",
    "num_patches",
    "max_len",
    "image_size",
    "patch_size"
  ],
  "UdopPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "UdopPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_can_compile_fullgraph": [],
    "_keep_in_fp32_modules": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "UdopLayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UdopDenseActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UdopDenseGatedActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UdopLayerFF": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UdopAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_values",
      "query_length",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "UdopLayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "UdopLayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "query_length",
      "output_attentions",
      "cache_position"
    ]
  },
  "UdopBlock": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "return_dict",
      "cache_position"
    ]
  },
  "UdopCellEmbeddings": {
    "__init__": [
      "self",
      "max_2d_position_embeddings",
      "hidden_size"
    ],
    "forward": [
      "self",
      "bbox"
    ]
  },
  "get_relative_position_bucket": [],
  "AUGMENTATION_RANGE": [],
  "RelativePositionBiasBase": {
    "__init__": [
      "self",
      "num_heads",
      "relative_attention_num_buckets",
      "bidirectional",
      "scaling_factor",
      "max_distance",
      "level",
      "augmentation",
      "prefix_bucket",
      "expand"
    ],
    "prepare_input": [
      "self",
      "attention_mask",
      "bbox"
    ],
    "get_bucket": [
      "self",
      "attention_mask",
      "bbox"
    ],
    "get_relative_position": [
      "self",
      "positions"
    ],
    "forward": [
      "self",
      "attention_mask",
      "bbox"
    ]
  },
  "RelativePositionBias1D": {
    "__init__": [
      "self",
      "scaling_factor",
      "max_distance"
    ],
    "prepare_input": [
      "self",
      "attention_mask",
      "bbox"
    ]
  },
  "RelativePositionBiasHorizontal": {
    "__init__": [
      "self",
      "scaling_factor",
      "max_distance"
    ],
    "prepare_input": [
      "self",
      "attention_mask",
      "bbox"
    ]
  },
  "RelativePositionBiasVertical": {
    "__init__": [
      "self",
      "scaling_factor",
      "max_distance"
    ],
    "prepare_input": [
      "self",
      "attention_mask",
      "bbox"
    ]
  },
  "RelativePositionBiasAggregated": {
    "__init__": [
      "self",
      "modules"
    ],
    "forward": [
      "self",
      "attention_mask",
      "bbox"
    ]
  },
  "BIAS_CLASSES": [],
  "create_relative_bias": [
    "config"
  ],
  "UdopStack": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_relative_bias": [
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "bbox",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "pixel_values",
      "visual_bbox",
      "image_embeddings",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "UdopModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "bbox",
      "pixel_values",
      "visual_bbox",
      "decoder_input_ids",
      "decoder_attention_mask",
      "inputs_embeds",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "UdopForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "bbox",
      "pixel_values",
      "visual_bbox",
      "decoder_input_ids",
      "decoder_attention_mask",
      "inputs_embeds",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels",
      "cache_position"
    ]
  },
  "UdopEncoderModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "pixel_values",
      "visual_bbox",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UdopConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "d_kv",
      "d_ff",
      "num_layers",
      "num_decoder_layers",
      "num_heads",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "relative_bias_args",
      "dropout_rate",
      "layer_norm_epsilon",
      "initializer_factor",
      "feed_forward_proj",
      "is_encoder_decoder",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "max_2d_position_embeddings",
      "image_size",
      "patch_size",
      "num_channels",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "UDOP_ENCODE_KWARGS_DOCSTRING": [],
  "UdopTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "eos_token",
      "sep_token",
      "unk_token",
      "pad_token",
      "sep_token_box",
      "pad_token_box",
      "pad_token_label",
      "only_label_first_subword",
      "extra_special_tokens"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "text_target",
      "text_pair_target"
    ],
    "call_boxes": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "batch_encode_plus_boxes": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_encode_plus_boxes": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus_boxes": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "encode_boxes": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "return_tensors"
    ],
    "encode_plus_boxes": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "UdopTextKwargs": {},
  "UdopProcessorKwargs": {
    "_defaults": []
  },
  "UdopProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "get_overflowing_images": [
      "self",
      "images",
      "overflow_to_sample_mapping"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "SEAMLESS_M4T_COMMON_CUSTOM_ARGS": [],
  "SeamlessM4TGenerationOutput": {},
  "_compute_new_attention_mask": [
    "hidden_states",
    "seq_lens"
  ],
  "format_speech_generation_kwargs": [
    "kwargs"
  ],
  "SeamlessM4TConformerPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TConformerRotaryPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TConformerRelPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "extend_pe": [
      "self",
      "x",
      "pe"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TConformerSamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TConformerFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TConformerFeedForward": {
    "__init__": [
      "self",
      "config",
      "act_fn",
      "dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TConformerConvolutionModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SeamlessM4TConformerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "use_position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "relative_position_embeddings",
      "output_attentions"
    ],
    "_apply_rotary_embedding": [
      "self",
      "hidden_states",
      "relative_position_embeddings"
    ],
    "_apply_relative_embeddings": [
      "self",
      "query",
      "key",
      "relative_position_embeddings"
    ]
  },
  "SeamlessM4TConformerEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "relative_position_embeddings",
      "output_attentions",
      "conv_attention_mask"
    ]
  },
  "SeamlessM4TConformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4TConformerAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_sub_sample_lengths_from_attention_mask": [
      "self",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SeamlessM4TConformerAdapter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SeamlessM4TScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "SeamlessM4TSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "past_key_values_length",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "SeamlessM4TAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "SeamlessM4TFeedForwardNetwork": {
    "__init__": [
      "self",
      "config",
      "ffn_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "encoder_ffn_dim",
      "encoder_attention_heads"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SeamlessM4TDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "SeamlessM4TPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_compute_sub_sample_lengths_from_attention_mask": [
      "self",
      "attention_mask"
    ],
    "compute_last_hidden_states_per_sample": [
      "self",
      "hidden_states",
      "beam_indices"
    ]
  },
  "SeamlessM4TSpeechEncoder": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4TEncoder": {
    "__init__": [
      "self",
      "config",
      "embed_tokens",
      "is_t2u_encoder"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4TDecoder": {
    "__init__": [
      "self",
      "config",
      "embed_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SeamlessM4TTextToUnitModel": {
    "__init__": [
      "self",
      "config",
      "embed_tokens_decoder"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SeamlessM4TTextToUnitForConditionalGeneration": {
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config",
      "embed_tokens_decoder"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "HifiGanResidualBlock": {
    "__init__": [
      "self",
      "channels",
      "kernel_size",
      "dilation",
      "leaky_relu_slope"
    ],
    "get_padding": [
      "self",
      "kernel_size",
      "dilation"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4TVariancePredictor": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4THifiGan": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_embeds"
    ]
  },
  "SeamlessM4TCodeHifiGan": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "_get_dur_output_lengths": [
      "self",
      "input_ids",
      "dur_out"
    ],
    "_get_output_hifigan_lengths": [
      "self",
      "input_lengths"
    ],
    "forward": [
      "self",
      "input_ids",
      "spkr_id",
      "lang_id"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "SeamlessM4TForTextToText": {
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_ids",
      "tgt_lang",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus"
    ]
  },
  "SeamlessM4TForSpeechToText": {
    "input_modalities": [],
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_features",
      "tgt_lang",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus"
    ]
  },
  "SeamlessM4TForTextToSpeech": {
    "output_modalities": [],
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "generate": [
      "self",
      "input_ids",
      "return_intermediate_token_ids",
      "tgt_lang",
      "spkr_id"
    ]
  },
  "SeamlessM4TForSpeechToSpeech": {
    "input_modalities": [],
    "output_modalities": [],
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_features",
      "return_intermediate_token_ids",
      "tgt_lang",
      "spkr_id"
    ]
  },
  "SeamlessM4TModel": {
    "input_modalities": [],
    "output_modalities": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config",
      "current_modality"
    ],
    "set_modality": [
      "self",
      "modality"
    ],
    "get_encoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_ids",
      "input_features",
      "return_intermediate_token_ids",
      "tgt_lang",
      "spkr_id",
      "generate_speech"
    ]
  },
  "SeamlessM4TTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "src_lang",
      "tgt_lang",
      "additional_special_tokens",
      "keep_accents",
      "vocab_file"
    ],
    "convert_from_spm_model": [
      "cls",
      "vocab"
    ],
    "src_lang": [
      "self",
      "new_src_lang"
    ],
    "tgt_lang": [
      "self",
      "new_tgt_lang"
    ],
    "_build_translation_inputs": [
      "self",
      "raw_inputs",
      "return_tensors",
      "src_lang",
      "tgt_lang"
    ],
    "prepare_seq2seq_batch": [
      "self",
      "src_texts",
      "src_lang",
      "tgt_texts",
      "tgt_lang",
      "max_length",
      "max_target_length",
      "padding",
      "return_tensors",
      "truncation"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ],
    "set_src_lang_special_tokens": [
      "self",
      "src_lang"
    ],
    "set_tgt_lang_special_tokens": [
      "self",
      "lang"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "text_target",
      "text_pair_target",
      "padding",
      "pad_to_multiple_of",
      "src_lang",
      "tgt_lang"
    ]
  },
  "SeamlessM4TConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "t2u_vocab_size",
      "hidden_size",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "max_position_embeddings",
      "is_encoder_decoder",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "activation_function",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "scale_embedding",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "decoder_start_token_id",
      "max_new_tokens",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "speech_encoder_layers",
      "speech_encoder_attention_heads",
      "speech_encoder_intermediate_size",
      "speech_encoder_hidden_act",
      "speech_encoder_dropout",
      "add_adapter",
      "speech_encoder_layerdrop",
      "feature_projection_input_dim",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "adaptor_kernel_size",
      "adaptor_stride",
      "adaptor_dropout",
      "num_adapter_layers",
      "position_embeddings_type",
      "rotary_embedding_base",
      "max_source_positions",
      "conv_depthwise_kernel_size",
      "t2u_bos_token_id",
      "t2u_pad_token_id",
      "t2u_eos_token_id",
      "t2u_decoder_start_token_id",
      "t2u_max_new_tokens",
      "t2u_encoder_layers",
      "t2u_encoder_ffn_dim",
      "t2u_encoder_attention_heads",
      "t2u_decoder_layers",
      "t2u_decoder_ffn_dim",
      "t2u_decoder_attention_heads",
      "t2u_max_position_embeddings",
      "sampling_rate",
      "upsample_initial_channel",
      "upsample_rates",
      "upsample_kernel_sizes",
      "resblock_kernel_sizes",
      "resblock_dilation_sizes",
      "leaky_relu_slope",
      "unit_hifi_gan_vocab_size",
      "unit_embed_dim",
      "lang_embed_dim",
      "spkr_embed_dim",
      "vocoder_num_langs",
      "vocoder_num_spkrs",
      "variance_predictor_kernel_size",
      "var_pred_dropout",
      "vocoder_offset",
      "tie_word_embeddings"
    ]
  },
  "SeamlessM4TTextKwargs": {},
  "SeamlessM4TProcessorKwargs": {
    "_defaults": []
  },
  "SeamlessM4TProcessor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "text",
      "audio"
    ]
  },
  "SeamlessM4TFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "num_mel_bins",
      "padding_value",
      "stride"
    ],
    "zero_mean_unit_var_norm": [
      "input_values",
      "attention_mask",
      "padding_value"
    ],
    "_extract_fbank_features": [
      "self",
      "waveform"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "padding",
      "pad_to_multiple_of",
      "max_length",
      "truncation",
      "return_tensors",
      "sampling_rate",
      "return_attention_mask",
      "do_normalize_per_mel_bins"
    ]
  },
  "MMGroundingDinoConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "text_config",
      "num_queries",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "auxiliary_loss",
      "position_embedding_type",
      "num_feature_levels",
      "encoder_n_points",
      "decoder_n_points",
      "two_stage",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "focal_alpha",
      "disable_custom_kernels",
      "max_text_len",
      "text_enhancer_dropout",
      "fusion_droppath",
      "fusion_dropout",
      "embedding_init_target",
      "query_dim",
      "positional_embedding_temperature",
      "init_std",
      "layer_norm_eps",
      "tie_word_embeddings"
    ]
  },
  "MMGroundingDinoContrastiveEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_hidden_state",
      "text_hidden_state",
      "text_token_mask"
    ]
  },
  "MMGroundingDinoLearnedPositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "MMGroundingDinoMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "config",
      "num_heads",
      "n_points"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "output_attentions"
    ]
  },
  "MMGroundingDinoBiMultiHeadAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_reshape": [
      "self",
      "tensor",
      "seq_len",
      "batch_size"
    ],
    "forward": [
      "self",
      "vision_features",
      "text_features",
      "vision_attention_mask",
      "text_attention_mask"
    ]
  },
  "MMGroundingDinoDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MMGroundingDinoFusionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_features",
      "text_features",
      "attention_mask_vision",
      "attention_mask_text"
    ]
  },
  "MMGroundingDinoPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "module",
      "value"
    ]
  },
  "MMGroundingDinoFrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MMGroundingDinoConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "MMGroundingDinoConvModel": {
    "__init__": [
      "self",
      "conv_encoder",
      "position_embedding"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "MMGroundingDinoEncoderOutput": {},
  "MMGroundingDinoMultiheadAttention": {
    "__init__": [
      "self",
      "config",
      "num_attention_heads"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "values",
      "attention_mask",
      "output_attentions"
    ]
  },
  "MMGroundingDinoTextEnhancerLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "with_pos_embed": [
      "self",
      "hidden_state",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_masks",
      "position_embeddings"
    ]
  },
  "MMGroundingDinoDeformableLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "output_attentions"
    ]
  },
  "get_sine_pos_embed": [
    "pos_tensor",
    "num_pos_feats",
    "temperature",
    "exchange_xy"
  ],
  "MMGroundingDinoEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_position_embeddings": [
      "self",
      "text_features",
      "text_position_embedding",
      "text_position_ids"
    ],
    "forward": [
      "self",
      "vision_features",
      "vision_position_embedding",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "key_padding_mask",
      "reference_points",
      "text_features",
      "text_attention_mask",
      "text_position_embedding",
      "text_self_attention_masks",
      "text_position_ids"
    ]
  },
  "MMGroundingDinoEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_reference_points": [
      "spatial_shapes_list",
      "valid_ratios",
      "device"
    ],
    "forward": [
      "self",
      "vision_features",
      "vision_attention_mask",
      "vision_position_embedding",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios",
      "text_features",
      "text_attention_mask",
      "text_position_embedding",
      "text_self_attention_masks",
      "text_position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MMGroundingDinoDecoderOutput": {},
  "MMGroundingDinoDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "vision_encoder_hidden_states",
      "vision_encoder_attention_mask",
      "text_encoder_hidden_states",
      "text_encoder_attention_mask",
      "self_attn_mask",
      "output_attentions"
    ]
  },
  "MMGroundingDinoDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "vision_encoder_hidden_states",
      "vision_encoder_attention_mask",
      "text_encoder_hidden_states",
      "text_encoder_attention_mask",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios",
      "self_attn_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MMGroundingDinoModelOutput": {},
  "MMGroundingDinoSinePositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "SPECIAL_TOKENS": [],
  "generate_masks_with_special_tokens_and_transfer_map": [
    "input_ids"
  ],
  "MMGroundingDinoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "get_valid_ratio": [
      "self",
      "mask"
    ],
    "generate_encoder_output_proposals": [
      "self",
      "enc_output",
      "padding_mask",
      "spatial_shapes_list"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "pixel_mask",
      "encoder_outputs",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MMGroundingDinoMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MMGroundingDinoObjectDetectionOutput": {},
  "build_label_maps": [
    "logits",
    "input_ids"
  ],
  "build_text_mask": [
    "logits",
    "attention_mask"
  ],
  "MMGroundingDinoForObjectDetection": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "pixel_mask",
      "encoder_outputs",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "GraniteMoeHybridAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position",
      "position_embeddings"
    ]
  },
  "HybridMambaAttentionDynamicCache": {
    "is_compileable": [],
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "layer_idx"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position",
      "layer_idx"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ]
  },
  "GraniteMoeHybridMambaLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask",
      "seq_idx"
    ],
    "torch_forward": [
      "self",
      "input_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask",
      "seq_idx"
    ]
  },
  "GraniteMoeHybridRMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "GraniteMoeHybridMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteMoeHybridRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "GraniteMoeHybridParallelExperts": {
    "__init__": [
      "self",
      "num_experts",
      "input_size",
      "output_size"
    ],
    "forward": [
      "self",
      "inputs",
      "expert_size"
    ]
  },
  "GraniteMoeHybridTopKGating": {
    "__init__": [
      "self",
      "input_size",
      "num_experts",
      "top_k"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteMoeHybridMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "layer_input"
    ]
  },
  "GraniteFlashAttentionKwargs": {},
  "GraniteMoeHybridRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GraniteMoeHybridDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GraniteMoeHybridPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GraniteMoeHybridModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ],
    "_update_mamba_mask": [
      "self",
      "attention_mask",
      "cache_position"
    ]
  },
  "GraniteMoeHybridForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "is_first_iteration"
    ]
  },
  "GraniteMoeHybridConfig": {
    "model_type": [],
    "attribute_map": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "embedding_multiplier",
      "logits_scaling",
      "residual_multiplier",
      "attention_multiplier",
      "num_local_experts",
      "num_experts_per_tok",
      "output_router_logits",
      "router_aux_loss_coef",
      "shared_intermediate_size",
      "position_embedding_type",
      "layer_types",
      "mamba_n_heads",
      "mamba_n_groups",
      "mamba_d_state",
      "mamba_d_head",
      "mamba_d_conv",
      "mamba_expand",
      "mamba_chunk_size",
      "mamba_conv_bias",
      "mamba_proj_bias",
      "time_step_min",
      "time_step_max",
      "time_step_limit"
    ],
    "layers_block_type": [
      "self"
    ]
  },
  "NanoChatConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "max_position_embeddings",
      "hidden_act",
      "attention_dropout",
      "rms_norm_eps",
      "initializer_range",
      "rope_parameters",
      "use_cache",
      "final_logit_softcapping",
      "attention_bias",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings"
    ]
  },
  "NanoChatRMSNorm": {
    "__init__": [
      "self",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "NanoChatRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "NanoChatAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "NanoChatMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "NanoChatDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "NanoChatPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "NanoChatModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "NanoChatForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "CamembertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "CamembertTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "slow_tokenizer_class": [],
    "__init__": [
      "self",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "additional_special_tokens",
      "add_prefix_space",
      "vocab_file",
      "vocab"
    ]
  },
  "CamembertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "CamembertSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "CamembertCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "CamembertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "CamembertAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "CamembertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CamembertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "CamembertLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "CamembertLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "CamembertPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CamembertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "CamembertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CamembertModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "CamembertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "CamembertClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "CamembertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "CamembertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "CamembertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "CamembertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "CamembertForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BertweetTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "normalization",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "normalizeTweet": [
      "self",
      "tweet"
    ],
    "normalizeToken": [
      "self",
      "token"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "add_from_file": [
      "self",
      "f"
    ]
  },
  "EMOTICONS": [],
  "URLS": [],
  "REGEXPS": [],
  "WORD_RE": [],
  "HANG_RE": [],
  "EMOTICON_RE": [],
  "ENT_RE": [],
  "_str_to_unicode": [
    "text",
    "encoding",
    "errors"
  ],
  "_replace_html_entities": [
    "text",
    "keep",
    "remove_illegal",
    "encoding"
  ],
  "TweetTokenizer": {
    "__init__": [
      "self",
      "preserve_case",
      "reduce_len",
      "strip_handles"
    ],
    "tokenize": [
      "self",
      "text"
    ]
  },
  "reduce_lengthening": [
    "text"
  ],
  "remove_handles": [
    "text"
  ],
  "casual_tokenize": [
    "text",
    "preserve_case",
    "reduce_len",
    "strip_handles"
  ],
  "PatchTSMixerConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "context_length",
      "patch_length",
      "num_input_channels",
      "patch_stride",
      "num_parallel_samples",
      "d_model",
      "expansion_factor",
      "num_layers",
      "dropout",
      "mode",
      "gated_attn",
      "norm_mlp",
      "self_attn",
      "self_attn_heads",
      "use_positional_encoding",
      "positional_encoding_type",
      "scaling",
      "loss",
      "init_std",
      "post_init",
      "norm_eps",
      "mask_type",
      "random_mask_ratio",
      "num_forecast_mask_patches",
      "mask_value",
      "masked_loss",
      "channel_consistent_masking",
      "unmasked_channel_indices",
      "head_dropout",
      "distribution_output",
      "prediction_length",
      "prediction_channel_indices",
      "num_targets",
      "output_range",
      "head_aggregation"
    ]
  },
  "PatchTSMixerGatedAttention": {
    "__init__": [
      "self",
      "in_size",
      "out_size"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PatchTSMixerBatchNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PatchTSMixerPositionalEncoding": {
    "__init__": [
      "self",
      "config"
    ],
    "_init_pe": [
      "config"
    ],
    "forward": [
      "self",
      "patch_input"
    ]
  },
  "PatchTSMixerNormLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PatchTSMixerMLP": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PatchTSMixerChannelFeatureMixerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PatchTSMixerAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "PatchMixerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "FeatureMixerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden"
    ]
  },
  "PatchTSMixerLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden"
    ]
  },
  "PatchTSMixerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states"
    ]
  },
  "PatchTSMixerForPredictionHead": {
    "__init__": [
      "self",
      "config",
      "distribution_output"
    ],
    "forward": [
      "self",
      "hidden_features"
    ]
  },
  "PatchTSMixerLinearHead": {
    "__init__": [
      "self",
      "config",
      "distribution_output"
    ],
    "forward": [
      "self",
      "hidden_features"
    ]
  },
  "PatchTSMixerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PatchTSMixerPretrainHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_features"
    ]
  },
  "random_masking": [
    "inputs",
    "mask_ratio",
    "unmasked_channel_indices",
    "channel_consistent_masking",
    "mask_value"
  ],
  "forecast_masking": [
    "inputs",
    "num_forecast_mask_patches",
    "unmasked_channel_indices",
    "mask_value"
  ],
  "PatchTSMixerPatchify": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values"
    ]
  },
  "PatchTSMixerMasking": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "patch_input"
    ]
  },
  "PatchTSMixerStdScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "PatchTSMixerMeanScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "PatchTSMixerNOPScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "PatchTSMixerEncoderOutput": {},
  "PatchTSMixerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PatchTSMixerModelOutput": {},
  "PatchTSMixerModel": {
    "__init__": [
      "self",
      "config",
      "mask_input"
    ],
    "forward": [
      "self",
      "past_values",
      "observed_mask",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PatchTSMixerForPreTrainingOutput": {},
  "PatchTSMixerForPretraining": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "observed_mask",
      "output_hidden_states",
      "return_loss",
      "return_dict"
    ]
  },
  "PatchTSMixerForPredictionOutput": {},
  "SamplePatchTSMixerPredictionOutput": {},
  "SamplePatchTSMixerRegressionOutput": {},
  "nll": [
    "input",
    "target"
  ],
  "weighted_average": [
    "input_tensor",
    "weights",
    "dim"
  ],
  "PatchTSMixerForPrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "observed_mask",
      "future_values",
      "output_hidden_states",
      "return_loss",
      "return_dict"
    ],
    "generate": [
      "self",
      "past_values",
      "observed_mask"
    ]
  },
  "PatchTSMixerForTimeSeriesClassificationOutput": {},
  "PatchTSMixerForTimeSeriesClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "target_values",
      "output_hidden_states",
      "return_loss",
      "return_dict"
    ]
  },
  "PatchTSMixerForRegressionOutput": {},
  "InjectScalerStatistics4D": {
    "__init__": [
      "self",
      "d_model",
      "num_patches",
      "expansion"
    ],
    "forward": [
      "self",
      "inputs",
      "loc",
      "scale"
    ]
  },
  "PatchTSMixerForRegression": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "target_values",
      "output_hidden_states",
      "return_loss",
      "return_dict"
    ],
    "generate": [
      "self",
      "past_values"
    ]
  },
  "token_time_to_note": [
    "number",
    "cutoff_time_idx",
    "current_idx"
  ],
  "token_note_to_note": [
    "number",
    "current_velocity",
    "default_velocity",
    "note_onsets_ready",
    "current_idx",
    "notes"
  ],
  "Pop2PianoConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "composer_vocab_size",
      "d_model",
      "d_kv",
      "d_ff",
      "num_layers",
      "num_decoder_layers",
      "num_heads",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "dropout_rate",
      "layer_norm_epsilon",
      "initializer_factor",
      "feed_forward_proj",
      "is_encoder_decoder",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "dense_act_fn",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "_load_pop2piano_layer_norm": [],
  "Pop2PianoLayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pop2PianoDenseActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pop2PianoDenseGatedActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pop2PianoLayerFF": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pop2PianoAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_values",
      "query_length",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "Pop2PianoLayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "Pop2PianoLayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "query_length",
      "output_attentions",
      "cache_position"
    ]
  },
  "Pop2PianoBlock": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "return_dict",
      "cache_position"
    ]
  },
  "Pop2PianoPreTrainedModel": {
    "base_model_prefix": [],
    "output_modalities": [],
    "supports_gradient_checkpointing": [],
    "_can_compile_fullgraph": [],
    "_no_split_modules": [],
    "_keep_in_fp32_modules": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "Pop2PianoStack": {
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "Pop2PianoConcatEmbeddingToMel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "feature",
      "index_value",
      "embedding_offset"
    ]
  },
  "Pop2PianoForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_mel_conditioner_outputs": [
      "self",
      "input_features",
      "composer",
      "generation_config",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "input_features",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "generate": [
      "self",
      "input_features",
      "attention_mask",
      "composer",
      "generation_config"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "GotOcr2TextKwargs": {},
  "GotOcr2ImagesKwargs": {},
  "GotOcr2ProcessorKwargs": {
    "_defaults": []
  },
  "preprocess_box_annotation": [
    "box",
    "image_size"
  ],
  "GotOcr2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "_make_list_of_inputs": [
      "self",
      "images",
      "text",
      "box",
      "color",
      "multi_page"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ]
  },
  "GotOcr2MLPBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GotOcr2VisionAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "get_rel_pos": [
      "self",
      "q_size",
      "k_size",
      "rel_pos"
    ],
    "get_decomposed_rel_pos": [
      "self",
      "query",
      "rel_pos_h",
      "rel_pos_w",
      "q_size",
      "k_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "GotOcr2VisionLayer": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "window_partition": [
      "self",
      "hidden_states",
      "window_size"
    ],
    "window_unpartition": [
      "self",
      "windows",
      "window_size",
      "padding_shape",
      "original_shape"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GotOcr2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GotOcr2VisionEncoderOutput": {},
  "GotOcr2PatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "GotOcr2LayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "GotOcr2VisionNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GotOcr2VisionEncoder": {
    "_can_record_outputs": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "GotOcr2MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_embeddings"
    ]
  },
  "GotOcr2CausalLMOutputWithPast": {},
  "GotOcr2ModelOutputWithPast": {},
  "GotOcr2Model": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "GotOcr2ForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "GotOcr2VisionConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "output_channels",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "qkv_bias",
      "use_abs_pos",
      "use_rel_pos",
      "window_size",
      "global_attn_indexes",
      "mlp_dim"
    ]
  },
  "GotOcr2Config": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "image_seq_length",
      "tie_word_embeddings"
    ]
  },
  "GotOcr2ImageProcessorKwargs": {},
  "GotOcr2ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "crop_to_patches",
      "min_patches",
      "max_patches",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_to_patches",
      "min_patches",
      "max_patches",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "crop_image_to_patches": [
      "self",
      "images",
      "min_patches",
      "max_patches",
      "use_thumbnail",
      "patch_size",
      "data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "GotOcr2ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "crop_to_patches": [],
    "min_patches": [],
    "max_patches": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "crop_image_to_patches": [
      "self",
      "images",
      "min_patches",
      "max_patches",
      "use_thumbnail",
      "patch_size",
      "interpolation"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_to_patches",
      "min_patches",
      "max_patches",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "Sam3VisionEncoderOutput": {},
  "Sam3GeometryEncoderOutput": {},
  "Sam3DETREncoderOutput": {},
  "Sam3DETRDecoderOutput": {},
  "Sam3MaskDecoderOutput": {},
  "Sam3ImageSegmentationOutput": {},
  "concat_padded_sequences": [
    "seq1",
    "mask1",
    "seq2",
    "mask2",
    "return_index"
  ],
  "box_cxcywh_to_xyxy": [
    "x"
  ],
  "Sam3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam3Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ]
  },
  "Sam3ViTRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "end_x",
      "end_y",
      "scale"
    ],
    "forward": [
      "self"
    ]
  },
  "rotate_pairwise": [
    "x"
  ],
  "apply_rotary_pos_emb_2d": [
    "q",
    "k",
    "cos",
    "sin"
  ],
  "Sam3ViTRoPEAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings"
    ]
  },
  "Sam3ViTPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Sam3ViTEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "_tile_position_embeddings": [
      "self",
      "position_embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "Sam3ViTLayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Sam3ViTLayer": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam3PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Sam3ViTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Sam3SinePositionEmbedding": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "encode_1d_positions": [
      "self",
      "x",
      "y"
    ],
    "encode_boxes": [
      "self",
      "boxes"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "Sam3FPNLayer": {
    "__init__": [
      "self",
      "in_channels",
      "fpn_dim",
      "scale_factor"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam3VisionNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam3VisionModel": {
    "config_class": [],
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Sam3GeometryEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "prompt_feats",
      "vision_feats",
      "vision_pos_encoding",
      "prompt_mask"
    ]
  },
  "Sam3GeometryEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_encode_box_coordinates": [
      "self",
      "center_x",
      "center_y",
      "width",
      "height"
    ],
    "_encode_boxes": [
      "self",
      "boxes",
      "boxes_mask",
      "boxes_labels",
      "vision_features"
    ],
    "forward": [
      "self",
      "box_embeddings",
      "box_mask",
      "box_labels",
      "img_feats",
      "img_pos_embeds"
    ]
  },
  "Sam3DetrEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_feats",
      "prompt_feats",
      "vision_pos_encoding",
      "prompt_cross_attn_mask"
    ]
  },
  "Sam3DetrEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "_prepare_multilevel_features": [
      "self",
      "vision_features",
      "vision_pos_embeds"
    ],
    "forward": [
      "self",
      "vision_features",
      "text_features",
      "vision_pos_embeds",
      "text_mask",
      "spatial_sizes"
    ]
  },
  "Sam3DecoderMLP": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Sam3DetrDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "query_pos",
      "text_features",
      "vision_features",
      "vision_pos_encoding",
      "text_cross_attn_mask",
      "vision_cross_attn_mask"
    ]
  },
  "Sam3DetrDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "_get_coords": [
      "self",
      "height",
      "width",
      "dtype",
      "device"
    ],
    "_get_rpb_matrix": [
      "self",
      "reference_boxes",
      "spatial_shape"
    ],
    "forward": [
      "self",
      "vision_features",
      "text_features",
      "vision_pos_encoding",
      "text_mask",
      "spatial_shapes"
    ]
  },
  "Sam3DotProductScoring": {
    "__init__": [
      "self",
      "config"
    ],
    "_pool_text_features": [
      "self",
      "text_features",
      "text_mask"
    ],
    "forward": [
      "self",
      "decoder_hidden_states",
      "text_features",
      "text_mask"
    ]
  },
  "Sam3MaskEmbedder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "queries"
    ]
  },
  "Sam3PixelDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "backbone_features"
    ]
  },
  "Sam3MaskDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "decoder_queries",
      "backbone_features",
      "encoder_hidden_states",
      "prompt_features",
      "prompt_mask"
    ],
    "_embed_pixels": [
      "self",
      "backbone_features",
      "encoder_hidden_states"
    ]
  },
  "Sam3Model": {
    "input_modalities": [],
    "_checkpoint_conversion_mapping": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_vision_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "pixel_values",
      "vision_embeds",
      "input_ids",
      "attention_mask",
      "text_embeds",
      "input_boxes",
      "input_boxes_labels"
    ]
  },
  "Sam3FastImageProcessorKwargs": {},
  "_scale_boxes": [
    "boxes",
    "target_sizes"
  ],
  "Sam3ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "mask_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "valid_kwargs": [],
    "do_pad": [],
    "pad_size": [],
    "mask_pad_size": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "mask_size",
      "default_to_square",
      "image_mean",
      "image_std",
      "data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "return_tensors"
    ],
    "generate_crop_boxes": [
      "self",
      "image",
      "target_size",
      "crop_n_layers",
      "overlap_ratio",
      "points_per_crop",
      "crop_n_points_downscale_factor",
      "device"
    ],
    "filter_masks": [
      "self",
      "masks",
      "iou_scores",
      "original_size",
      "cropped_box_image",
      "pred_iou_thresh",
      "stability_score_thresh",
      "mask_threshold",
      "stability_score_offset"
    ],
    "post_process_masks": [
      "self",
      "masks",
      "original_sizes",
      "mask_threshold",
      "binarize",
      "max_hole_area",
      "max_sprinkle_area",
      "apply_non_overlapping_constraints"
    ],
    "post_process_for_mask_generation": [
      "self",
      "all_masks",
      "all_scores",
      "all_boxes",
      "crops_nms_thresh"
    ],
    "_apply_non_overlapping_constraints": [
      "self",
      "pred_masks"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "target_sizes"
    ]
  },
  "Sam3ViTConfig": {
    "base_config_key": [],
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "rope_theta",
      "window_size",
      "global_attn_indexes",
      "layer_scale_init_value",
      "pretrain_image_size",
      "hidden_dropout",
      "initializer_range"
    ]
  },
  "Sam3VisionConfig": {
    "base_config_key": [],
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "fpn_hidden_size",
      "backbone_feature_sizes",
      "scale_factors",
      "hidden_act",
      "layer_norm_eps",
      "initializer_range"
    ],
    "image_size": [
      "self",
      "value"
    ]
  },
  "Sam3GeometryEncoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_layers",
      "num_attention_heads",
      "intermediate_size",
      "dropout",
      "hidden_act",
      "hidden_dropout",
      "layer_norm_eps",
      "roi_size",
      "initializer_range"
    ]
  },
  "Sam3DETREncoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_layers",
      "num_attention_heads",
      "intermediate_size",
      "dropout",
      "hidden_act",
      "hidden_dropout",
      "layer_norm_eps",
      "initializer_range"
    ]
  },
  "Sam3DETRDecoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_layers",
      "num_queries",
      "num_attention_heads",
      "intermediate_size",
      "dropout",
      "hidden_act",
      "hidden_dropout",
      "layer_norm_eps",
      "initializer_range"
    ]
  },
  "Sam3MaskDecoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_upsampling_stages",
      "layer_norm_eps",
      "dropout",
      "num_attention_heads",
      "initializer_range"
    ]
  },
  "Sam3Config": {
    "model_type": [],
    "is_composition": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "geometry_encoder_config",
      "detr_encoder_config",
      "detr_decoder_config",
      "mask_decoder_config",
      "initializer_range"
    ],
    "image_size": [
      "self",
      "value"
    ]
  },
  "box_cxcywh_to_xywh": [
    "x"
  ],
  "box_xywh_to_xyxy": [
    "x"
  ],
  "box_xywh_to_cxcywh": [
    "x"
  ],
  "box_xyxy_to_xywh": [
    "x"
  ],
  "box_xyxy_to_cxcywh": [
    "x"
  ],
  "Sam3Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "target_size",
      "point_pad_value"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "segmentation_maps",
      "input_boxes",
      "input_boxes_labels",
      "original_sizes",
      "return_tensors"
    ],
    "_normalize_coordinates": [
      "self",
      "coords",
      "original_size",
      "is_bounding_box"
    ],
    "_convert_to_nested_list": [
      "self",
      "data",
      "expected_depth",
      "current_depth"
    ],
    "_resolve_text_prompts": [
      "self",
      "text",
      "input_boxes"
    ],
    "_get_nested_dimensions": [
      "self",
      "nested_list",
      "max_dims"
    ],
    "_pad_nested_list": [
      "self",
      "nested_list",
      "target_dims",
      "current_level",
      "pad_value"
    ],
    "_create_empty_nested_structure": [
      "self",
      "dims",
      "pad_value"
    ],
    "_get_nesting_level": [
      "self",
      "input_list"
    ],
    "_validate_single_input": [
      "self",
      "data",
      "expected_depth",
      "input_name",
      "expected_format",
      "expected_coord_size"
    ],
    "_normalize_tensor_coordinates": [
      "self",
      "tensor",
      "original_sizes",
      "is_bounding_box",
      "preserve_padding"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "target_sizes"
    ]
  },
  "MgpstrTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "MgpstrConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "max_token_length",
      "num_character_labels",
      "num_bpe_labels",
      "num_wordpiece_labels",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "qkv_bias",
      "distilled",
      "layer_norm_eps",
      "drop_rate",
      "attn_drop_rate",
      "drop_path_rate",
      "output_a3_attentions",
      "initializer_range"
    ]
  },
  "MgpstrDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MgpstrModelOutput": {},
  "MgpstrEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "MgpstrMlp": {
    "__init__": [
      "self",
      "config",
      "hidden_features"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MgpstrAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MgpstrLayer": {
    "__init__": [
      "self",
      "config",
      "drop_path"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MgpstrEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MgpstrA3Module": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MgpstrPreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MgpstrModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MgpstrForSceneTextRecognition": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_a3_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DecodeType": {
    "CHARACTER": [],
    "BPE": [],
    "WORDPIECE": []
  },
  "MgpstrProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "return_tensors"
    ],
    "batch_decode": [
      "self",
      "sequences"
    ],
    "_decode_helper": [
      "self",
      "pred_logits",
      "format"
    ],
    "char_decode": [
      "self",
      "sequences"
    ],
    "bpe_decode": [
      "self",
      "sequences"
    ],
    "wp_decode": [
      "self",
      "sequences"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "VJEPA2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "patch_size",
      "crop_size",
      "frames_per_clip",
      "tubelet_size",
      "hidden_size",
      "in_chans",
      "num_attention_heads",
      "num_hidden_layers",
      "drop_path_rate",
      "mlp_ratio",
      "layer_norm_eps",
      "qkv_bias",
      "attention_probs_dropout_prob",
      "hidden_act",
      "initializer_range",
      "attention_dropout",
      "num_pooler_layers",
      "pred_hidden_size",
      "pred_num_attention_heads",
      "pred_num_hidden_layers",
      "pred_num_mask_tokens",
      "pred_zero_init_mask_tokens",
      "pred_mlp_ratio"
    ]
  },
  "VJEPA2VideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "crop_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_center_crop": [],
    "do_normalize": [],
    "__init__": [
      "self"
    ]
  },
  "VJEPA2WithMaskedInputPredictorOutput": {},
  "VJEPA2WithMaskedInputModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "VJEPA2PatchEmbeddings3D": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "num_patches": [
      "config"
    ],
    "forward": [
      "self",
      "pixel_values_videos"
    ]
  },
  "VJEPA2Embeddings": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "forward": [
      "self",
      "pixel_values_videos"
    ]
  },
  "rotate_queries_or_keys": [
    "x",
    "pos"
  ],
  "VJEPA2RopeAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads"
    ],
    "_get_frame_pos": [
      "self",
      "ids"
    ],
    "_get_height_pos": [
      "self",
      "ids"
    ],
    "get_position_ids": [
      "self",
      "x",
      "masks"
    ],
    "apply_rotary_embeddings": [
      "self",
      "qk",
      "pos_ids"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_mask",
      "output_attentions"
    ]
  },
  "VJEPA2DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "VJEPA2MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "mlp_ratio"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "VJEPA2Layer": {
    "__init__": [
      "self",
      "config",
      "drop_path_rate",
      "hidden_size",
      "num_attention_heads",
      "mlp_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_mask",
      "output_attentions"
    ]
  },
  "VJEPA2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values_videos",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "apply_masks": [
    "tensor",
    "masks"
  ],
  "VJEPA2PredictorEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "num_patches": [
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "context_mask",
      "target_mask",
      "mask_index"
    ]
  },
  "VJEPA2Predictor": {
    "__init__": [
      "self",
      "config"
    ],
    "sort_tokens": [
      "self",
      "hidden_states",
      "position_masks",
      "argsort"
    ],
    "unsort_tokens": [
      "self",
      "hidden_states",
      "argsort"
    ],
    "forward": [
      "self",
      "encoder_hidden_states",
      "context_mask",
      "target_mask",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "VJEPA2PoolerSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "VJEPA2PoolerCrossAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "values",
      "attention_mask",
      "output_attentions"
    ]
  },
  "VJEPA2PoolerSelfAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "VJEPA2PoolerCrossAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "queries",
      "hidden_state",
      "attention_mask",
      "output_attentions"
    ]
  },
  "VJEPA2AttentivePooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "VJEPA2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VJEPA2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values_videos",
      "context_mask",
      "target_mask",
      "skip_predictor",
      "output_attentions",
      "output_hidden_states"
    ],
    "get_vision_features": [
      "self",
      "pixel_values_videos"
    ]
  },
  "VJEPA2ForVideoClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values_videos",
      "labels",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "UnivNetConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "model_in_channels",
      "model_hidden_channels",
      "num_mel_bins",
      "resblock_kernel_sizes",
      "resblock_stride_sizes",
      "resblock_dilation_sizes",
      "kernel_predictor_num_blocks",
      "kernel_predictor_hidden_channels",
      "kernel_predictor_conv_size",
      "kernel_predictor_dropout",
      "initializer_range",
      "leaky_relu_slope"
    ]
  },
  "UnivNetFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "do_normalize",
      "num_mel_bins",
      "hop_length",
      "win_length",
      "win_function",
      "filter_length",
      "max_length_s",
      "fmin",
      "fmax",
      "mel_floor",
      "center",
      "compression_factor",
      "compression_clip_val",
      "normalize_min",
      "normalize_max",
      "model_in_channels",
      "pad_end_length",
      "return_attention_mask"
    ],
    "normalize": [
      "self",
      "spectrogram"
    ],
    "denormalize": [
      "self",
      "spectrogram"
    ],
    "mel_spectrogram": [
      "self",
      "waveform"
    ],
    "generate_noise": [
      "self",
      "noise_length",
      "generator"
    ],
    "batch_decode": [
      "self",
      "waveforms",
      "waveform_lengths"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "sampling_rate",
      "padding",
      "max_length",
      "truncation",
      "pad_to_multiple_of",
      "return_noise",
      "generator",
      "pad_end",
      "pad_length",
      "do_normalize",
      "return_attention_mask",
      "return_tensors"
    ],
    "to_dict": [
      "self"
    ]
  },
  "UnivNetModelOutput": {},
  "UnivNetKernelPredictorResidualBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "UnivNetKernelPredictor": {
    "__init__": [
      "self",
      "config",
      "conv_kernel_size",
      "conv_layers"
    ],
    "forward": [
      "self",
      "spectrogram"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "UnivNetLvcResidualBlock": {
    "__init__": [
      "self",
      "config",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "kernel",
      "bias",
      "hop_size"
    ],
    "location_variable_convolution": [
      "self",
      "hidden_states",
      "kernel",
      "bias",
      "dilation",
      "hop_size"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "UnivNetLvcBlock": {
    "__init__": [
      "self",
      "config",
      "layer_id",
      "lvc_hop_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "spectrogram"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "UnivNetModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "noise_sequence",
      "padding_mask",
      "generator",
      "return_dict"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "HeliumRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "HeliumRotaryEmbedding": {},
  "HeliumMLP": {},
  "HeliumAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "HeliumDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "HeliumPreTrainedModel": {},
  "HeliumModel": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "HeliumForCausalLM": {},
  "HeliumForSequenceClassification": {},
  "HeliumForTokenClassification": {},
  "HeliumConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "attention_dropout",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "attention_bias",
      "mlp_bias"
    ]
  },
  "load_vocab_and_emoji": [
    "vocab_file",
    "emoji_file"
  ],
  "GPTNeoXJapaneseTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "emoji_file",
      "unk_token",
      "pad_token",
      "bos_token",
      "eos_token",
      "do_clean_text"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "SubWordJapaneseTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "ids_to_tokens",
      "emoji"
    ],
    "__len__": [
      "self"
    ],
    "clean_text": [
      "self",
      "content"
    ],
    "tokenize": [
      "self",
      "text",
      "clean"
    ],
    "convert_id_to_token": [
      "self",
      "index",
      "breakline"
    ]
  },
  "GPTNeoXJapanesePreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GPTNeoXJapaneseRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "GPTNeoXJapaneseAttention": {
    "__init__": [
      "self",
      "config",
      "use_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position",
      "position_embeddings"
    ],
    "_split_heads": [
      "cls",
      "tensor",
      "num_attention_heads",
      "attn_head_size"
    ],
    "_merge_heads": [
      "cls",
      "tensor",
      "num_attention_heads",
      "attn_head_size"
    ],
    "_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ]
  },
  "bias_dropout_add": [
    "x",
    "bias",
    "residual",
    "prob",
    "training"
  ],
  "GPTNeoXJapaneseMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GPTNeoXJapaneseLayer": {
    "__init__": [
      "self",
      "config",
      "layer_number"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "use_cache",
      "layer_past",
      "output_attentions",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GPTNeoXJapaneseModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "GPTNeoXJapaneseForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GPTNeoXJapaneseConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_multiple_size",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "rope_parameters",
      "attention_dropout",
      "hidden_dropout",
      "is_decoder",
      "pad_token_id",
      "tie_word_embeddings"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "MambaCache": {
    "is_compileable": [],
    "__init__": [
      "self",
      "config",
      "max_batch_size",
      "dtype",
      "device"
    ],
    "update_conv_state": [
      "self",
      "layer_idx",
      "new_conv_state",
      "cache_position"
    ],
    "update_ssm_state": [
      "self",
      "layer_idx",
      "new_ssm_state"
    ],
    "reset": [
      "self"
    ]
  },
  "MambaMixer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "warn_slow_implementation": [
      "self"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "slow_forward": [
      "self",
      "input_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "MambaRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MambaBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "MambaPreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MambaOutput": {},
  "MambaCausalLMOutput": {},
  "MambaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "load_hook": [
      "self",
      "state_dict",
      "prefix"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "use_cache",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "attention_mask"
    ]
  },
  "MambaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "num_new_tokens"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "inputs_embeds",
      "use_cache",
      "cache_params",
      "cache_position",
      "attention_mask",
      "is_first_iteration"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "cache_params",
      "labels",
      "output_hidden_states",
      "return_dict",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MambaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "state_size",
      "num_hidden_layers",
      "layer_norm_epsilon",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "expand",
      "conv_kernel",
      "use_bias",
      "use_conv_bias",
      "hidden_act",
      "initializer_range",
      "residual_in_fp32",
      "time_step_rank",
      "time_step_scale",
      "time_step_min",
      "time_step_max",
      "time_step_init_scheme",
      "time_step_floor",
      "rescale_prenorm_residual",
      "use_cache",
      "use_mambapy",
      "tie_word_embeddings"
    ]
  },
  "build_mpt_alibi_tensor": [
    "num_heads",
    "sequence_length",
    "alibi_bias_max",
    "device"
  ],
  "MptAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_bias",
      "past_key_values",
      "attention_mask",
      "cache_position"
    ]
  },
  "MptMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual"
    ]
  },
  "MptBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_bias",
      "attention_mask",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "MptPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": []
  },
  "MptModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "build_mpt_alibi_tensor": [
      "self",
      "num_heads",
      "sequence_length",
      "alibi_bias_max",
      "device"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MptForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MptForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MptForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MptForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MptAttentionConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "attn_type",
      "attn_pdrop",
      "attn_impl",
      "clip_qkv",
      "softmax_scale",
      "prefix_lm",
      "qk_ln",
      "attn_uses_sequence_id",
      "alibi",
      "alibi_bias_max"
    ]
  },
  "MptConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "d_model",
      "n_heads",
      "n_layers",
      "expansion_ratio",
      "max_seq_len",
      "vocab_size",
      "resid_pdrop",
      "layer_norm_epsilon",
      "emb_pdrop",
      "learned_pos_emb",
      "attn_config",
      "init_device",
      "logit_scale",
      "no_bias",
      "embedding_fraction",
      "norm_type",
      "use_cache",
      "initializer_range",
      "tie_word_embeddings",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "EomtDinov3Config": {
    "model_type": [],
    "default_theta": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "layerscale_value",
      "drop_path_rate",
      "num_upscale_blocks",
      "attention_dropout",
      "num_blocks",
      "no_object_weight",
      "class_weight",
      "mask_weight",
      "dice_weight",
      "train_num_points",
      "oversample_ratio",
      "importance_sample_ratio",
      "num_queries",
      "num_register_tokens",
      "rope_parameters",
      "query_bias",
      "key_bias",
      "value_bias",
      "proj_bias",
      "mlp_bias",
      "use_gated_mlp",
      "pos_embed_shift",
      "pos_embed_jitter",
      "pos_embed_rescale"
    ]
  },
  "EomtDinov3Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "EomtDinov3Embeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "EomtDinov3DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "EomtDinov3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EomtDinov3GatedMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EomtDinov3Layer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "EomtDinov3LayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "get_patches_center_coordinates": [
    "num_patches_h",
    "num_patches_w",
    "dtype",
    "device"
  ],
  "augment_patches_center_coordinates": [
    "coords",
    "shift",
    "jitter",
    "rescale"
  ],
  "EomtDinov3RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "forward": [
      "self",
      "pixel_values"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ]
  },
  "EomtDinov3HungarianMatcher": {
    "__init__": [
      "self",
      "cost_class",
      "cost_mask",
      "cost_dice",
      "num_points"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels"
    ]
  },
  "EomtDinov3Loss": {
    "__init__": [
      "self",
      "config",
      "weight_dict"
    ],
    "_max_by_axis": [
      "self",
      "sizes"
    ],
    "_pad_images_to_max_in_batch": [
      "self",
      "tensors"
    ],
    "loss_labels": [
      "self",
      "class_queries_logits",
      "class_labels",
      "indices"
    ],
    "loss_masks": [
      "self",
      "masks_queries_logits",
      "mask_labels",
      "indices",
      "num_masks"
    ],
    "_get_predictions_permutation_indices": [
      "self",
      "indices"
    ],
    "_get_targets_permutation_indices": [
      "self",
      "indices"
    ],
    "calculate_uncertainty": [
      "self",
      "logits"
    ],
    "sample_points_using_uncertainty": [
      "self",
      "logits",
      "uncertainty_function",
      "num_points",
      "oversample_ratio",
      "importance_sample_ratio"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_predictions"
    ],
    "get_num_masks": [
      "self",
      "class_labels",
      "device"
    ]
  },
  "EomtDinov3ForUniversalSegmentationOutput": {},
  "EomtDinov3PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_can_record_outputs": [],
    "config_class": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EomtDinov3LayerNorm2d": {
    "__init__": [
      "self",
      "num_channels",
      "eps",
      "affine"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "EomtDinov3ScaleLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EomtDinov3ScaleBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EomtDinov3MaskHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EomtDinov3ForUniversalSegmentation": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_loss_dict": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_predictions"
    ],
    "get_loss": [
      "self",
      "loss_dict"
    ],
    "forward": [
      "self",
      "pixel_values",
      "mask_labels",
      "class_labels",
      "patch_offsets"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "predict": [
      "self",
      "logits"
    ],
    "_disable_attention_mask": [
      "attn_mask",
      "prob",
      "num_query_tokens",
      "encoder_start_tokens",
      "device"
    ]
  },
  "OlmoLayerNorm": {
    "__init__": [
      "self",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "OlmoMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OlmoRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "OlmoAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "OlmoDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "OlmoPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "OlmoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "OlmoForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "OlmoConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "clip_qkv"
    ]
  },
  "GPT2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "n_positions",
      "n_embd",
      "n_layer",
      "n_head",
      "n_inner",
      "activation_function",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "summary_type",
      "summary_use_proj",
      "summary_activation",
      "summary_proj_to_labels",
      "summary_first_dropout",
      "scale_attn_weights",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "scale_attn_by_inverse_layer_idx",
      "reorder_and_upcast_attn",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "GPT2Attention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "layer_idx"
    ],
    "_upcast_and_reordered_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "GPT2MLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GPT2Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions"
    ]
  },
  "GPT2SequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "GPT2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_attention_backend": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GPT2DoubleHeadsModelOutput": {},
  "GPT2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPT2LMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "GPT2DoubleHeadsModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "mc_token_ids",
      "labels",
      "mc_labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPT2ForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPT2ForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPT2ForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPT2Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "errors",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "add_prefix_space"
    ]
  },
  "PersimmonRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PersimmonMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PersimmonAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_split_heads": [
      "self",
      "fused_qkv"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PersimmonDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PersimmonPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_can_compile_fullgraph": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": []
  },
  "PersimmonModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "PersimmonForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "PersimmonForSequenceClassification": {},
  "PersimmonForTokenClassification": {},
  "PersimmonConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "qk_layernorm",
      "hidden_dropout",
      "attention_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "FAIRSEQ_LANGUAGE_CODES": [],
  "MBart50Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "src_lang",
      "tgt_lang",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token"
    ],
    "_build_language_code_mappings": [
      "self"
    ],
    "_post_init": [
      "self"
    ],
    "src_lang": [
      "self",
      "new_src_lang"
    ],
    "prepare_seq2seq_batch": [
      "self",
      "src_texts",
      "src_lang",
      "tgt_texts",
      "tgt_lang"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ],
    "set_src_lang_special_tokens": [
      "self",
      "src_lang"
    ],
    "set_tgt_lang_special_tokens": [
      "self",
      "tgt_lang"
    ],
    "_build_translation_inputs": [
      "self",
      "raw_inputs",
      "return_tensors",
      "src_lang",
      "tgt_lang"
    ]
  },
  "MBart50TokenizerFast": [],
  "JetMoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_key_value_heads",
      "kv_channels",
      "intermediate_size",
      "max_position_embeddings",
      "activation_function",
      "num_local_experts",
      "num_experts_per_tok",
      "output_router_logits",
      "aux_loss_coef",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "rms_norm_eps",
      "initializer_range",
      "attention_dropout"
    ]
  },
  "JetMoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "JetMoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "JetMoeParallelExperts": {
    "__init__": [
      "self",
      "num_experts",
      "input_size",
      "output_size"
    ],
    "forward": [
      "self",
      "inputs",
      "expert_size"
    ]
  },
  "JetMoeTopKGating": {
    "__init__": [
      "self",
      "input_size",
      "num_experts",
      "top_k"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JetMoeMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "layer_input"
    ]
  },
  "JetMoeMoA": {
    "__init__": [
      "self",
      "config"
    ],
    "map": [
      "self",
      "layer_input"
    ],
    "reduce": [
      "self",
      "layer_input",
      "topo_info"
    ],
    "forward": [
      "self",
      "layer_input"
    ]
  },
  "JetMoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "past_key_values",
      "cache_position"
    ]
  },
  "JetMoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "JetMoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "JetMoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "JetMoeForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep",
      "output_router_logits"
    ]
  },
  "JetMoeForSequenceClassification": {},
  "ByT5Tokenizer": {
    "model_input_names": [],
    "__init__": [
      "self",
      "eos_token",
      "unk_token",
      "pad_token",
      "extra_ids",
      "additional_special_tokens"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "_add_eos_if_not_present": [
      "self",
      "token_ids"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "TimmWrapperModelOutput": {},
  "_create_timm_model_with_error_handling": [
    "config"
  ],
  "TimmWrapperPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "model_tags": [],
    "accepts_loss_kwargs": [],
    "post_init": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_timm_model_supports_gradient_checkpointing": [
      "self"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "enable"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ]
  },
  "TimmWrapperModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "do_pooling",
      "use_cache"
    ]
  },
  "TimmWrapperForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TimmWrapperConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "architecture",
      "initializer_range",
      "do_pooling",
      "model_args"
    ],
    "from_dict": [
      "cls",
      "config_dict"
    ],
    "to_dict": [
      "self"
    ]
  },
  "ConvNextImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "crop_pct": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "crop_pct",
      "interpolation"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_pct",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "ConvNextDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ConvNextLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "ConvNextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "ConvNextLayer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "drop_path"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "ConvNextStage": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "depth",
      "drop_path_rates"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "ConvNextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states"
    ]
  },
  "ConvNextPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ConvNextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "ConvNextForImageClassification": {
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "ConvNextBackbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "ConvNextImageProcessorKwargs": {},
  "ConvNextImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "crop_pct",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "crop_pct",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_pct",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "ConvNextConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "patch_size",
      "num_stages",
      "hidden_sizes",
      "depths",
      "hidden_act",
      "initializer_range",
      "layer_norm_eps",
      "layer_scale_init_value",
      "drop_path_rate",
      "image_size",
      "out_features",
      "out_indices"
    ]
  },
  "CLIPSegProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "visual_prompt",
      "return_tensors"
    ]
  },
  "CLIPSegTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "CLIPSegVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "CLIPSegConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value",
      "extract_layers",
      "reduce_dim",
      "decoder_num_attention_heads",
      "decoder_attention_dropout",
      "decoder_hidden_act",
      "decoder_intermediate_size",
      "conditional_layer",
      "use_complex_transposed_convolution"
    ]
  },
  "clipseg_loss": [
    "similarity"
  ],
  "CLIPSegOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "CLIPSegDecoderOutput": {},
  "CLIPSegImageSegmentationOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "CLIPSegVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "CLIPSegTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "CLIPSegAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "CLIPSegMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CLIPSegEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "CLIPSegPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CLIPSegEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CLIPSegTextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CLIPSegTextModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CLIPSegVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "interpolate_pos_encoding"
    ]
  },
  "CLIPSegVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "CLIPSegModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "CLIPSegDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "CLIPSegDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "conditional_embeddings",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CLIPSegForImageSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "get_conditional_embeddings": [
      "self",
      "batch_size",
      "input_ids",
      "attention_mask",
      "position_ids",
      "conditional_pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "conditional_pixel_values",
      "conditional_embeddings",
      "attention_mask",
      "position_ids",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "SmolLM3RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "SmolLM3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "SmolLM3RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "SmolLM3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SmolLM3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "SmolLM3PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "SmolLM3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "SmolLM3ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "SmolLM3ForSequenceClassification": {},
  "SmolLM3ForTokenClassification": {},
  "SmolLM3ForQuestionAnswering": {
    "base_model_prefix": []
  },
  "SmolLM3Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "rope_parameters",
      "use_sliding_window",
      "sliding_window",
      "no_rope_layers",
      "no_rope_layer_interval",
      "layer_types",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "tie_word_embeddings"
    ]
  },
  "convert_to_unicode": [
    "text"
  ],
  "FlaubertTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "do_lowercase",
      "unk_token",
      "bos_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "additional_special_tokens",
      "lang2id",
      "id2lang"
    ],
    "do_lower_case": [
      "self"
    ],
    "moses_punct_norm": [
      "self",
      "text",
      "lang"
    ],
    "moses_tokenize": [
      "self",
      "text",
      "lang"
    ],
    "moses_pipeline": [
      "self",
      "text",
      "lang"
    ],
    "ja_tokenize": [
      "self",
      "text"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "preprocess_text": [
      "self",
      "text"
    ],
    "_tokenize": [
      "self",
      "text",
      "bypass_tokenizer"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ]
  },
  "create_sinusoidal_embeddings": [
    "n_pos",
    "dim",
    "out"
  ],
  "get_masks": [
    "slen",
    "lengths",
    "causal",
    "padding_mask"
  ],
  "MultiHeadAttention": {
    "__init__": [
      "self",
      "n_heads",
      "dim",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "input",
      "mask",
      "kv",
      "cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "TransformerFFN": {
    "__init__": [
      "self",
      "in_dim",
      "dim_hidden",
      "out_dim",
      "config"
    ],
    "forward": [
      "self",
      "input"
    ],
    "ff_chunk": [
      "self",
      "input"
    ]
  },
  "FlaubertPredLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "y"
    ]
  },
  "FlaubertSquadHeadOutput": {},
  "FlaubertPoolerStartLogits": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "p_mask"
    ]
  },
  "FlaubertPoolerEndLogits": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_states",
      "start_positions",
      "p_mask"
    ]
  },
  "FlaubertPoolerAnswerClass": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_states",
      "start_positions",
      "cls_index"
    ]
  },
  "FlaubertSQuADHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_positions",
      "end_positions",
      "cls_index",
      "is_impossible",
      "p_mask",
      "return_dict"
    ]
  },
  "FlaubertSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "FlaubertPreTrainedModel": {
    "base_model_prefix": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "FlaubertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "FlaubertWithLMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlaubertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlaubertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlaubertForQuestionAnsweringSimple": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlaubertForQuestionAnsweringOutput": {},
  "FlaubertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "is_impossible",
      "cls_index",
      "p_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlaubertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlaubertConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "pre_norm",
      "layerdrop",
      "vocab_size",
      "emb_dim",
      "n_layers",
      "n_heads",
      "dropout",
      "attention_dropout",
      "gelu_activation",
      "sinusoidal_embeddings",
      "causal",
      "asm",
      "n_langs",
      "use_lang_emb",
      "max_position_embeddings",
      "embed_init_std",
      "layer_norm_eps",
      "init_std",
      "bos_index",
      "eos_index",
      "pad_index",
      "unk_index",
      "mask_index",
      "is_encoder",
      "summary_type",
      "summary_use_proj",
      "summary_activation",
      "summary_proj_to_labels",
      "summary_first_dropout",
      "start_n_top",
      "end_n_top",
      "mask_token_id",
      "lang_id",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "BertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "whitespace_tokenize": [
    "text"
  ],
  "BertTokenizerLegacy": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "do_lower_case",
      "do_basic_tokenize",
      "never_split",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "tokenize_chinese_chars",
      "strip_accents",
      "clean_up_tokenization_spaces"
    ],
    "do_lower_case": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text",
      "split_special_tokens"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "BasicTokenizer": {
    "__init__": [
      "self",
      "do_lower_case",
      "never_split",
      "tokenize_chinese_chars",
      "strip_accents",
      "do_split_on_punc"
    ],
    "tokenize": [
      "self",
      "text",
      "never_split"
    ],
    "_run_strip_accents": [
      "self",
      "text"
    ],
    "_run_split_on_punc": [
      "self",
      "text",
      "never_split"
    ],
    "_tokenize_chinese_chars": [
      "self",
      "text"
    ],
    "_is_chinese_char": [
      "self",
      "cp"
    ],
    "_clean_text": [
      "self",
      "text"
    ]
  },
  "BertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "BertSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BertCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "BertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BertAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BertLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "BertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "BertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "BertOnlyNSPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "BertPreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "BertPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BertForPreTrainingOutput": {},
  "BertModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "BertForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "next_sentence_label"
    ]
  },
  "BertLMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "BertForNextSentencePrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "BertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "BertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "BertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "BertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "BertTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "tokenize_chinese_chars",
      "strip_accents"
    ]
  },
  "BertTokenizerFast": [],
  "EnglishNumberNormalizer": {
    "__init__": [
      "self"
    ],
    "spell_number": [
      "self",
      "num"
    ],
    "convert": [
      "self",
      "number"
    ],
    "__call__": [
      "self",
      "text"
    ]
  },
  "SpeechT5Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "is_fast": [],
    "__init__": [
      "self",
      "vocab_file",
      "bos_token",
      "eos_token",
      "unk_token",
      "pad_token",
      "normalize",
      "sp_model_kwargs"
    ],
    "prepare_for_tokenization": [
      "self",
      "text",
      "is_split_into_words"
    ],
    "normalizer": [
      "self",
      "value"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "_HIDDEN_STATES_START_POSITION": [],
  "shift_spectrograms_right": [
    "input_values",
    "reduction_factor",
    "attention_mask"
  ],
  "_compute_mask_indices": [
    "shape",
    "mask_prob",
    "mask_length",
    "attention_mask",
    "min_masks"
  ],
  "SpeechT5NoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5LayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5GroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5SinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length"
    ],
    "create_position_ids_from_input_ids": [
      "self",
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "SpeechT5PositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5ScaledPositionalEncoding": {
    "__init__": [
      "self",
      "dropout",
      "dim",
      "max_len"
    ],
    "forward": [
      "self",
      "emb"
    ]
  },
  "SpeechT5RelativePositionalEncoding": {
    "__init__": [
      "self",
      "dim",
      "max_length"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5SamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5FeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "SpeechT5FeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5SpeechEncoderPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ]
  },
  "SpeechT5SpeechDecoderPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "_consistent_dropout": [
      "self",
      "inputs_embeds",
      "p"
    ],
    "forward": [
      "self",
      "input_values",
      "speaker_embeddings"
    ]
  },
  "SpeechT5BatchNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5SpeechDecoderPostnet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "postnet": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5TextEncoderPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "SpeechT5TextDecoderPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values"
    ]
  },
  "SpeechT5TextDecoderPostnet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ]
  },
  "SpeechT5Attention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "position_bias",
      "output_attentions",
      "cache_position"
    ]
  },
  "SpeechT5FeedForward": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SpeechT5EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "SpeechT5DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "SpeechT5PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SpeechT5Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SpeechT5EncoderWithSpeechPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SpeechT5EncoderWithTextPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SpeechT5EncoderWithoutPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SpeechT5Decoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SpeechT5DecoderWithSpeechPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "speaker_embeddings",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SpeechT5DecoderWithTextPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SpeechT5DecoderWithoutPrenet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SpeechT5GuidedMultiheadAttentionLoss": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "attentions",
      "input_masks",
      "output_masks"
    ],
    "_make_guided_attention_masks": [
      "self",
      "input_masks",
      "output_masks",
      "device"
    ],
    "_make_guided_attention_mask": [
      "input_length",
      "output_length",
      "sigma",
      "device"
    ]
  },
  "SpeechT5SpectrogramLoss": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "attention_mask",
      "outputs_before_postnet",
      "outputs_after_postnet",
      "logits",
      "labels",
      "cross_attentions"
    ]
  },
  "SpeechT5Model": {
    "__init__": [
      "self",
      "config",
      "encoder",
      "decoder"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "decoder_input_values",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "use_cache",
      "speaker_embeddings",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SpeechT5ForSpeechToText": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels",
      "cache_position"
    ]
  },
  "_generate_speech": [
    "model",
    "input_values",
    "speaker_embeddings",
    "attention_mask",
    "threshold",
    "minlenratio",
    "maxlenratio",
    "vocoder",
    "output_cross_attentions",
    "return_output_lengths"
  ],
  "SpeechT5ForTextToSpeech": {
    "input_modalities": [],
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "can_generate": [
      "cls"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_values",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "speaker_embeddings",
      "labels",
      "stop_labels",
      "cache_position"
    ],
    "generate": [
      "self",
      "input_ids",
      "attention_mask",
      "speaker_embeddings",
      "threshold",
      "minlenratio",
      "maxlenratio",
      "vocoder",
      "output_cross_attentions",
      "return_output_lengths"
    ],
    "generate_speech": [
      "self",
      "input_ids",
      "speaker_embeddings",
      "attention_mask",
      "threshold",
      "minlenratio",
      "maxlenratio",
      "vocoder",
      "output_cross_attentions",
      "return_output_lengths"
    ]
  },
  "SpeechT5ForSpeechToSpeech": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "decoder_input_values",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "speaker_embeddings",
      "labels",
      "stop_labels",
      "cache_position"
    ],
    "generate_speech": [
      "self",
      "input_values",
      "speaker_embeddings",
      "attention_mask",
      "threshold",
      "minlenratio",
      "maxlenratio",
      "vocoder",
      "output_cross_attentions",
      "return_output_lengths"
    ]
  },
  "SpeechT5HifiGan": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "forward": [
      "self",
      "spectrogram"
    ]
  },
  "SpeechT5Config": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "encoder_layers",
      "encoder_attention_heads",
      "encoder_ffn_dim",
      "encoder_layerdrop",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "decoder_layerdrop",
      "hidden_act",
      "positional_dropout",
      "hidden_dropout",
      "attention_dropout",
      "activation_dropout",
      "initializer_range",
      "layer_norm_eps",
      "scale_embedding",
      "feat_extract_norm",
      "feat_proj_dropout",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "decoder_start_token_id",
      "num_mel_bins",
      "speech_decoder_prenet_layers",
      "speech_decoder_prenet_units",
      "speech_decoder_prenet_dropout",
      "speaker_embedding_dim",
      "speech_decoder_postnet_layers",
      "speech_decoder_postnet_units",
      "speech_decoder_postnet_kernel",
      "speech_decoder_postnet_dropout",
      "reduction_factor",
      "max_speech_positions",
      "max_text_positions",
      "encoder_max_relative_position",
      "use_guided_attention_loss",
      "guided_attention_loss_num_heads",
      "guided_attention_loss_sigma",
      "guided_attention_loss_scale",
      "use_cache",
      "is_encoder_decoder",
      "tie_word_embeddings"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "SpeechT5HifiGanConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "model_in_dim",
      "sampling_rate",
      "upsample_initial_channel",
      "upsample_rates",
      "upsample_kernel_sizes",
      "resblock_kernel_sizes",
      "resblock_dilation_sizes",
      "initializer_range",
      "leaky_relu_slope",
      "normalize_before"
    ]
  },
  "SpeechT5Processor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self"
    ],
    "pad": [
      "self"
    ]
  },
  "SpeechT5FeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "do_normalize",
      "num_mel_bins",
      "hop_length",
      "win_length",
      "win_function",
      "fmin",
      "fmax",
      "mel_floor",
      "return_attention_mask"
    ],
    "zero_mean_unit_var_norm": [
      "input_values",
      "attention_mask",
      "padding_value"
    ],
    "_extract_mel_features": [
      "self",
      "one_waveform"
    ],
    "__call__": [
      "self",
      "audio",
      "audio_target",
      "padding",
      "max_length",
      "truncation",
      "pad_to_multiple_of",
      "return_attention_mask",
      "return_tensors",
      "sampling_rate"
    ],
    "_process_audio": [
      "self",
      "speech",
      "is_target",
      "padding",
      "max_length",
      "truncation",
      "pad_to_multiple_of",
      "return_attention_mask",
      "return_tensors"
    ],
    "to_dict": [
      "self"
    ]
  },
  "PerceptionLMImageProcessorKwargs": {},
  "PerceptionLMImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "vision_input_type": [],
    "tile_size": [],
    "max_num_tiles": [],
    "size": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_factors": [
      "n"
    ],
    "_find_supported_aspect_ratios": [
      "self"
    ],
    "_get_image_height_width": [
      "self",
      "image_width",
      "image_height",
      "target_width",
      "target_height"
    ],
    "_fit_image_to_canvas": [
      "self",
      "img_width",
      "img_height",
      "tile_size"
    ],
    "_find_closest_aspect_ratio": [
      "self",
      "img_width",
      "img_height",
      "tile_size"
    ],
    "_split": [
      "self",
      "image",
      "ncw",
      "nch"
    ],
    "resize": [
      "self",
      "image",
      "tile_size",
      "max_num_tiles",
      "resample",
      "input_data_format"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "vision_input_type",
      "tile_size",
      "max_num_tiles",
      "return_tensors",
      "disable_grouping"
    ]
  },
  "PerceptionLMVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": []
  },
  "PerceptionLMAdaptiveAvgPooling": {
    "__init__": [
      "self",
      "pooling_ratio"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PerceptionLMMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "PerceptionLMPreTrainedModel": {
    "base_model_prefix": []
  },
  "PerceptionLMModelOutputWithPast": {},
  "PerceptionLMCausalLMOutputWithPast": {},
  "PerceptionLMModel": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_values_videos",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "PerceptionLMForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_values_videos",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_values_videos",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ],
    "get_image_features": [
      "self"
    ]
  },
  "PerceptionLMProcessorKwargs": {
    "_defaults": []
  },
  "PerceptionLMProcessor": {
    "__init__": [
      "self",
      "video_processor",
      "image_processor",
      "tokenizer",
      "patch_size",
      "chat_template",
      "pooling_ratio"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_expand_media_tokens": [
      "self",
      "sample",
      "media_token",
      "media_iter"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "PerceptionLMConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "vision_use_cls_token",
      "projector_pooling_ratio",
      "image_token_id",
      "video_token_id"
    ]
  },
  "YoutuConfig": {
    "model_type": [],
    "base_model_tp_plan": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "kv_lora_rank",
      "q_lora_rank",
      "qk_rope_head_dim",
      "v_head_dim",
      "qk_nope_head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "embedding_initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "rope_interleave",
      "attention_bias",
      "attention_dropout"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "YoutuRMSNorm": {},
  "YoutuRotaryEmbedding": {},
  "YoutuMLP": {},
  "YoutuAttention": {},
  "YoutuDecoderLayer": {},
  "YoutuPreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "YoutuModel": {},
  "YoutuForCausalLM": {},
  "apply_rotary_pos_emb_interleave": [
    "q",
    "k",
    "cos",
    "sin",
    "position_ids",
    "unsqueeze_dim"
  ],
  "yarn_get_mscale": [
    "scale",
    "mscale"
  ],
  "Sam3VideoProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "video_processor",
      "tokenizer",
      "target_size"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps",
      "original_sizes",
      "return_tensors"
    ],
    "add_text_prompt": [
      "self",
      "inference_session",
      "text"
    ],
    "init_video_session": [
      "self",
      "video",
      "inference_device",
      "inference_state_device",
      "processing_device",
      "video_storage_device",
      "max_vision_features_cache_size",
      "dtype"
    ],
    "_apply_non_overlapping_constraints": [
      "self",
      "pred_masks"
    ],
    "_apply_object_wise_non_overlapping_constraints": [
      "self",
      "pred_masks",
      "obj_scores",
      "background_value",
      "prompt_ids"
    ],
    "_apply_object_wise_non_overlapping_constraints_impl": [
      "self",
      "pred_masks",
      "obj_scores",
      "background_value"
    ],
    "postprocess_outputs": [
      "self",
      "inference_session",
      "model_outputs",
      "original_sizes"
    ]
  },
  "Sam3VideoConfig": {
    "model_type": [],
    "is_composition": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "detector_config",
      "tracker_config",
      "initializer_range",
      "low_res_mask_size",
      "score_threshold_detection",
      "det_nms_thresh",
      "assoc_iou_thresh",
      "trk_assoc_iou_thresh",
      "new_det_thresh",
      "recondition_on_trk_masks",
      "hotstart_delay",
      "hotstart_unmatch_thresh",
      "hotstart_dup_thresh",
      "suppress_unmatched_only_within_hotstart",
      "init_trk_keep_alive",
      "max_trk_keep_alive",
      "min_trk_keep_alive",
      "suppress_overlapping_based_on_recent_occlusion_threshold",
      "decrease_trk_keep_alive_for_empty_masklets",
      "fill_hole_area",
      "max_num_objects",
      "recondition_every_nth_frame",
      "high_conf_thresh",
      "high_iou_thresh"
    ],
    "image_size": [
      "self",
      "value"
    ]
  },
  "cv_utils_kernel": [],
  "_load_cv_utils_kernel_once": [],
  "Sam3VideoInferenceCache": {
    "__init__": [
      "self",
      "inference_device",
      "inference_state_device",
      "max_vision_features_cache_size"
    ],
    "cache_vision_features": [
      "self",
      "frame_idx",
      "features"
    ],
    "get_vision_features": [
      "self",
      "frame_idx"
    ],
    "clear_all": [
      "self"
    ]
  },
  "Sam3VideoInferenceSession": {
    "__init__": [
      "self",
      "video",
      "video_height",
      "video_width",
      "inference_device",
      "inference_state_device",
      "video_storage_device",
      "dtype",
      "max_vision_features_cache_size"
    ],
    "num_frames": [
      "self"
    ],
    "add_prompt": [
      "self",
      "prompt_text"
    ],
    "obj_id_to_idx": [
      "self",
      "obj_id"
    ],
    "obj_idx_to_id": [
      "self",
      "obj_idx"
    ],
    "get_obj_num": [
      "self"
    ],
    "add_mask_inputs": [
      "self",
      "obj_idx",
      "frame_idx",
      "inputs"
    ],
    "remove_mask_inputs": [
      "self",
      "obj_idx",
      "frame_idx"
    ],
    "remove_object": [
      "self",
      "obj_id",
      "strict"
    ],
    "store_output": [
      "self",
      "obj_idx",
      "frame_idx",
      "output_key",
      "output_value",
      "is_conditioning_frame"
    ],
    "get_output": [
      "self",
      "obj_idx",
      "frame_idx",
      "output_key",
      "is_conditioning_frame"
    ],
    "add_new_frame": [
      "self",
      "pixel_values",
      "frame_idx"
    ],
    "get_frame": [
      "self",
      "frame_idx"
    ],
    "reset_tracking_data": [
      "self"
    ],
    "reset_inference_session": [
      "self"
    ],
    "reset_state": [
      "self"
    ]
  },
  "Sam3VideoSegmentationOutput": {},
  "Sam3VideoPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": []
  },
  "Sam3VideoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_vision_features_for_tracker": [
      "self",
      "vision_embeds"
    ],
    "run_detection": [
      "self",
      "inference_session",
      "vision_embeds"
    ],
    "run_tracker_propagation": [
      "self",
      "inference_session",
      "frame_idx",
      "reverse"
    ],
    "_associate_det_trk": [
      "self",
      "det_masks",
      "det_scores",
      "trk_masks",
      "trk_obj_ids",
      "det_prompt_ids",
      "trk_prompt_ids"
    ],
    "_process_hotstart": [
      "self",
      "inference_session",
      "frame_idx",
      "reverse",
      "det_to_matched_trk_obj_ids",
      "new_det_obj_ids",
      "empty_trk_obj_ids",
      "unmatched_trk_obj_ids",
      "extra_metadata",
      "streaming"
    ],
    "run_memory_encoder": [
      "self",
      "inference_session",
      "frame_idx",
      "high_res_masks",
      "object_score_logits"
    ],
    "_prepare_recondition_masks": [
      "self",
      "inference_session",
      "frame_idx",
      "det_out",
      "trk_masks",
      "trk_id_to_max_iou_high_conf_det",
      "tracker_obj_scores_global"
    ],
    "_get_objects_to_suppress_based_on_most_recently_occluded": [
      "self",
      "binary_low_res_masks",
      "last_occluded",
      "obj_ids",
      "reverse"
    ],
    "_suppress_overlapping_based_on_recent_occlusion": [
      "self",
      "inference_session",
      "frame_idx",
      "tracker_low_res_masks_global",
      "tracker_metadata_new",
      "obj_ids_newly_removed",
      "reverse"
    ],
    "_apply_non_overlapping_constraints": [
      "self",
      "pred_masks"
    ],
    "_suppress_shrinked_masks": [
      "self",
      "pred_masks",
      "new_pred_masks",
      "shrink_threshold"
    ],
    "_suppress_object_pw_area_shrinkage": [
      "self",
      "pred_masks",
      "prompt_ids"
    ],
    "_suppress_object_pw_area_shrinkage_impl": [
      "self",
      "pred_masks"
    ],
    "_tracker_update_memories": [
      "self",
      "inference_session",
      "frame_idx",
      "low_res_masks",
      "reconditioned_masks"
    ],
    "run_tracker_update_planning_phase": [
      "self",
      "inference_session",
      "frame_idx",
      "reverse",
      "det_out",
      "tracker_low_res_masks_global",
      "tracker_obj_scores_global",
      "det_idx_to_prompt_id",
      "streaming"
    ],
    "_tracker_add_new_objects": [
      "self",
      "inference_session",
      "frame_idx",
      "new_obj_ids",
      "new_obj_masks",
      "reverse"
    ],
    "run_tracker_update_execution_phase": [
      "self",
      "inference_session",
      "frame_idx",
      "det_out",
      "tracker_update_plan",
      "reverse"
    ],
    "build_outputs": [
      "self",
      "inference_session",
      "det_out",
      "tracker_low_res_masks_global",
      "tracker_update_plan",
      "reconditioned_obj_ids"
    ],
    "_merge_detections_from_prompts": [
      "self",
      "all_detections",
      "inference_session"
    ],
    "_det_track_one_frame": [
      "self",
      "inference_session",
      "frame_idx",
      "reverse",
      "streaming"
    ],
    "forward": [
      "self",
      "inference_session",
      "frame_idx",
      "frame",
      "reverse"
    ],
    "_get_processing_order": [
      "self",
      "inference_session",
      "start_frame_idx",
      "max_frame_num_to_track",
      "reverse"
    ],
    "propagate_in_video_iterator": [
      "self",
      "inference_session",
      "start_frame_idx",
      "max_frame_num_to_track",
      "reverse",
      "show_progress_bar"
    ]
  },
  "fast_diag_box_iou": [
    "boxes1",
    "boxes2"
  ],
  "mask_iou": [
    "pred_masks",
    "gt_masks"
  ],
  "nms_masks": [
    "pred_probs",
    "pred_masks",
    "prob_threshold",
    "iou_threshold"
  ],
  "fill_holes_in_mask_scores": [
    "mask",
    "max_area",
    "fill_holes",
    "remove_sprinkles"
  ],
  "_get_connected_components_with_padding": [
    "mask"
  ],
  "BigBirdPegasusConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "classifier_dropout",
      "scale_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "attention_type",
      "block_size",
      "num_random_blocks",
      "use_bias",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "_EXPECTED_OUTPUT_SHAPE": [],
  "BigBirdPegasusLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "BigBirdPegasusScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "BigBirdPegasusSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "BigBirdPegasusBlockSparseAttention": {
    "__init__": [
      "self",
      "config",
      "seed"
    ],
    "forward": [
      "self",
      "hidden_states",
      "band_mask",
      "from_mask",
      "to_mask",
      "from_blocked_mask",
      "to_blocked_mask",
      "output_attentions"
    ],
    "torch_bmm_nd": [
      "inp_1",
      "inp_2",
      "ndim"
    ],
    "torch_bmm_nd_transpose": [
      "inp_1",
      "inp_2",
      "ndim"
    ],
    "bigbird_block_sparse_attention": [
      "self",
      "query_layer",
      "key_layer",
      "value_layer",
      "band_mask",
      "from_mask",
      "to_mask",
      "from_blocked_mask",
      "to_blocked_mask",
      "n_heads",
      "n_rand_blocks",
      "attention_head_size",
      "from_block_size",
      "to_block_size",
      "batch_size",
      "from_seq_len",
      "to_seq_len",
      "seed",
      "plan_from_length",
      "plan_num_rand_blocks",
      "output_attentions"
    ],
    "torch_gather_b2": [
      "params",
      "indices"
    ],
    "_create_rand_mask_from_inputs": [
      "from_blocked_mask",
      "to_blocked_mask",
      "rand_attn",
      "num_attention_heads",
      "num_rand_blocks",
      "batch_size",
      "from_seq_length",
      "from_block_size"
    ],
    "_get_rand_attn_plan": [
      "from_seq_length",
      "from_block_size",
      "num_rand_blocks"
    ],
    "_bigbird_block_rand_mask": [
      "self",
      "from_seq_length",
      "to_seq_length",
      "from_block_size",
      "to_block_size",
      "num_rand_blocks",
      "last_idx"
    ],
    "_bigbird_block_rand_mask_with_head": [
      "self",
      "from_seq_length",
      "to_seq_length",
      "from_block_size",
      "to_block_size",
      "num_heads",
      "plan_from_length",
      "plan_num_rand_blocks",
      "window_block_left",
      "window_block_right",
      "global_block_top",
      "global_block_bottom",
      "global_block_left",
      "global_block_right"
    ],
    "_get_single_block_row_attention": [
      "block_id",
      "to_start_block_id",
      "to_end_block_id",
      "num_rand_blocks",
      "window_block_left",
      "window_block_right",
      "global_block_left",
      "global_block_right"
    ]
  },
  "BigBirdPegasusEncoderAttention": {
    "__init__": [
      "self",
      "config",
      "seed"
    ],
    "set_attention_type": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "band_mask",
      "from_mask",
      "to_mask",
      "from_blocked_mask",
      "to_blocked_mask"
    ]
  },
  "BigBirdPegasusDecoderAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "BigBirdPegasusEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "seed"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "band_mask",
      "from_mask",
      "to_mask",
      "from_blocked_mask",
      "to_blocked_mask",
      "output_attentions"
    ],
    "set_attention_type": [
      "self",
      "value"
    ]
  },
  "BigBirdPegasusDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "BigBirdPegasusClassificationHead": {
    "__init__": [
      "self",
      "input_dim",
      "inner_dim",
      "num_classes",
      "pooler_dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BigBirdPegasusPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "BigBirdPegasusEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "set_attention_type": [
      "self",
      "value"
    ],
    "create_masks_for_block_sparse_attn": [
      "attention_mask",
      "block_size"
    ],
    "_pad_to_block_size": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "BigBirdPegasusDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BigBirdPegasusModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BigBirdPegasusForConditionalGeneration": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_missing": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "BigBirdPegasusForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BigBirdPegasusForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BigBirdPegasusDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "BigBirdPegasusForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "FlavaMaskingGenerator": {
    "__init__": [
      "self",
      "input_size",
      "total_mask_patches",
      "mask_group_max_patches",
      "mask_group_min_patches",
      "mask_group_min_aspect_ratio",
      "mask_group_max_aspect_ratio"
    ],
    "__repr__": [
      "self"
    ],
    "get_shape": [
      "self"
    ],
    "_mask": [
      "self",
      "mask",
      "max_mask_patches"
    ],
    "__call__": [
      "self"
    ]
  },
  "FlavaImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "return_image_mask": [],
    "input_size_patches": [],
    "total_mask_patches": [],
    "mask_group_min_patches": [],
    "mask_group_max_patches": [],
    "mask_group_min_aspect_ratio": [],
    "mask_group_max_aspect_ratio": [],
    "return_codebook_pixels": [],
    "codebook_do_resize": [],
    "codebook_size": [],
    "codebook_resample": [],
    "codebook_do_center_crop": [],
    "codebook_crop_size": [],
    "codebook_do_rescale": [],
    "codebook_rescale_factor": [],
    "codebook_do_map_pixels": [],
    "codebook_do_normalize": [],
    "codebook_image_mean": [],
    "codebook_image_std": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "from_dict": [
      "cls",
      "image_processor_dict"
    ],
    "masking_generator": [
      "self",
      "input_size_patches",
      "total_mask_patches",
      "mask_group_min_patches",
      "mask_group_max_patches",
      "mask_group_min_aspect_ratio",
      "mask_group_max_aspect_ratio"
    ],
    "map_pixels": [
      "self",
      "image"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "crop_size",
      "default_to_square",
      "image_mean",
      "image_std",
      "codebook_size",
      "codebook_crop_size",
      "codebook_image_mean",
      "codebook_image_std",
      "codebook_resample",
      "data_format"
    ],
    "_preprocess_image": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_map_pixels",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_image_mask",
      "input_size_patches",
      "total_mask_patches",
      "mask_group_min_patches",
      "mask_group_max_patches",
      "mask_group_min_aspect_ratio",
      "mask_group_max_aspect_ratio",
      "return_codebook_pixels",
      "codebook_do_resize",
      "codebook_size",
      "codebook_interpolation",
      "codebook_do_center_crop",
      "codebook_crop_size",
      "codebook_do_rescale",
      "codebook_rescale_factor",
      "codebook_do_map_pixels",
      "codebook_do_normalize",
      "codebook_image_mean",
      "codebook_image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "_CHECKPOINT_FOR_CODEBOOK_DOC": [],
  "LOGIT_SCALE_CLAMP_MIN": [],
  "LOGIT_SCALE_CLAMP_MAX": [],
  "FlavaPossibleConfigs": [],
  "FlavaModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "FlavaLosses": {
    "all_none": [
      "self"
    ]
  },
  "FlavaForPreTrainingOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "FlavaImageEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "PatchEmbeddings": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "FlavaTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids"
    ]
  },
  "FlavaSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "FlavaSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "FlavaAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "FlavaIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FlavaOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "FlavaLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "FlavaEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlavaPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FlavaPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "FlavaImageModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlavaTextModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlavaMultimodalModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlavaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "token_type_ids",
      "bool_masked_pos",
      "position_ids",
      "image_attention_mask",
      "skip_multimodal_encoder",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FlavaImageCodebookResPath": {
    "__init__": [
      "self",
      "in_size",
      "out_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FlavaImageCodebookBlock": {
    "__init__": [
      "self",
      "in_size",
      "out_size",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FlavaImageCodebookLayerGroup": {
    "__init__": [
      "self",
      "num_blocks",
      "num_layers",
      "in_size",
      "out_size",
      "use_pool"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FlavaImageCodebook": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_codebook_indices": [
      "self",
      "pixel_values"
    ],
    "get_codebook_probs": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "FlavaPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FlavaMaskedPredictionHead": {
    "__init__": [
      "self",
      "config",
      "weight"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FlavaITMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FlavaGlobalContrastiveHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "text_embeddings",
      "logit_scale"
    ]
  },
  "FlavaForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config",
      "image_codebook"
    ],
    "_resize_to_2d": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_ids_masked",
      "pixel_values",
      "codebook_pixel_values",
      "attention_mask",
      "token_type_ids",
      "bool_masked_pos",
      "position_ids",
      "image_attention_mask",
      "skip_unmasked_multimodal_encoder",
      "mlm_labels",
      "mim_labels",
      "itm_labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "return_loss"
    ]
  },
  "FLAVA_IMAGE_MEAN": [],
  "FLAVA_IMAGE_STD": [],
  "FLAVA_CODEBOOK_MEAN": [],
  "FLAVA_CODEBOOK_STD": [],
  "FlavaImageProcessorKwargs": {},
  "FlavaImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_image_mask",
      "input_size_patches",
      "total_mask_patches",
      "mask_group_min_patches",
      "mask_group_max_patches",
      "mask_group_min_aspect_ratio",
      "mask_group_max_aspect_ratio",
      "return_codebook_pixels",
      "codebook_do_resize",
      "codebook_size",
      "codebook_resample",
      "codebook_do_center_crop",
      "codebook_crop_size",
      "codebook_do_rescale",
      "codebook_rescale_factor",
      "codebook_do_map_pixels",
      "codebook_do_normalize",
      "codebook_image_mean",
      "codebook_image_std"
    ],
    "from_dict": [
      "cls",
      "image_processor_dict"
    ],
    "masking_generator": [
      "self",
      "input_size_patches",
      "total_mask_patches",
      "mask_group_min_patches",
      "mask_group_max_patches",
      "mask_group_min_aspect_ratio",
      "mask_group_max_aspect_ratio"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "map_pixels": [
      "self",
      "image"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_map_pixels",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_image_mask",
      "input_size_patches",
      "total_mask_patches",
      "mask_group_min_patches",
      "mask_group_max_patches",
      "mask_group_min_aspect_ratio",
      "mask_group_max_aspect_ratio",
      "return_codebook_pixels",
      "codebook_do_resize",
      "codebook_size",
      "codebook_resample",
      "codebook_do_center_crop",
      "codebook_crop_size",
      "codebook_do_rescale",
      "codebook_rescale_factor",
      "codebook_do_map_pixels",
      "codebook_do_normalize",
      "codebook_image_mean",
      "codebook_image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "FlavaImageConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "mask_token",
      "vocab_size"
    ]
  },
  "FlavaTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "type_vocab_size",
      "max_position_embeddings",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "qkv_bias"
    ]
  },
  "FlavaMultimodalConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "qkv_bias",
      "use_cls_token"
    ]
  },
  "FlavaImageCodebookConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "num_groups",
      "input_channels",
      "num_blocks_per_group",
      "hidden_size",
      "vocab_size",
      "freeze",
      "initializer_range"
    ]
  },
  "FlavaConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "image_config",
      "text_config",
      "multimodal_config",
      "image_codebook_config",
      "hidden_size",
      "layer_norm_eps",
      "projection_dim",
      "init_codebook",
      "logit_scale_init_value",
      "initializer_range",
      "ce_ignore_index",
      "mim_weight",
      "mlm_weight",
      "global_contrastive_weight",
      "itm_weight",
      "mmm_image_weight",
      "mmm_text_weight",
      "global_backprop_contrastive",
      "skip_unmasked_multimodal_encoder",
      "return_loss",
      "tie_word_embeddings"
    ]
  },
  "FlavaProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "TimesFmConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "is_encoder_decoder": [],
    "__init__": [
      "self",
      "patch_length",
      "context_length",
      "horizon_length",
      "freq_size",
      "num_hidden_layers",
      "hidden_size",
      "intermediate_size",
      "head_dim",
      "num_attention_heads",
      "tolerance",
      "rms_norm_eps",
      "quantiles",
      "pad_val",
      "attention_dropout",
      "use_positional_embedding",
      "initializer_range",
      "min_timescale",
      "max_timescale"
    ]
  },
  "TimesFmOutput": {},
  "TimesFmOutputForPrediction": {},
  "TimesFmMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "paddings"
    ]
  },
  "TimesFmResidualBlock": {
    "__init__": [
      "self",
      "input_dims",
      "hidden_dims",
      "output_dims"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "TimesFmRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "TimesFmPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "seq_length",
      "position"
    ]
  },
  "simple_eager_attention_forward": [
    "module",
    "query_states",
    "key_states",
    "value_states",
    "attention_mask",
    "scaling",
    "dropout"
  ],
  "TimesFmAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_scale_query": [
      "self",
      "query"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "TimesFmDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "paddings",
      "output_attentions"
    ]
  },
  "TimesFmPreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "TimesFmModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_forward_transform": [
      "self",
      "inputs",
      "patched_pads"
    ],
    "forward": [
      "self",
      "past_values",
      "past_values_padding",
      "freq",
      "output_attentions",
      "output_hidden_states"
    ],
    "_prepare_4d_attention_mask": [
      "attention_mask",
      "sequence_length",
      "dtype",
      "device",
      "is_causal"
    ],
    "_timesfm_masked_mean_std": [
      "inputs",
      "padding"
    ],
    "_timesfm_shift_padded_seq": [
      "mask",
      "seq"
    ]
  },
  "TimesFmModelForPrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "_preprocess": [
      "self",
      "inputs",
      "freq"
    ],
    "_postprocess_output": [
      "self",
      "model_output",
      "stats"
    ],
    "_quantile_loss": [
      "self",
      "predictions",
      "targets"
    ],
    "forward": [
      "self",
      "past_values",
      "freq",
      "window_size",
      "future_values",
      "forecast_context_len",
      "return_forecast_on_context",
      "truncate_negative",
      "output_attentions",
      "output_hidden_states"
    ],
    "_timesfm_moving_average": [
      "arr",
      "window_size"
    ]
  },
  "AutoFormerDecoderOutput": {},
  "AutoformerModelOutput": {},
  "AutoformerFeatureEmbedder": {
    "__init__": [
      "self",
      "cardinalities",
      "embedding_dims"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "AutoformerStdScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "AutoformerMeanScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "AutoformerNOPScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "AutoformerSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "create_weight": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "AutoformerValueEmbedding": {
    "__init__": [
      "self",
      "feature_size",
      "d_model"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "AutoformerSeriesDecompositionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "AutoformerLayernorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "AutoformerAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "autocorrelation_factor",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "AutoformerEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "AutoformerDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "AutoformerPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_update_full_mask": [
      "self",
      "attention_mask",
      "inputs_embeds"
    ]
  },
  "AutoformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AutoformerDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "trend",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "AutoformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_past_length": [
      "self"
    ],
    "get_lagged_subsequences": [
      "self",
      "sequence",
      "subsequences_length",
      "shift"
    ],
    "create_network_inputs": [
      "self",
      "past_values",
      "past_time_features",
      "static_categorical_features",
      "static_real_features",
      "past_observed_mask",
      "future_values",
      "future_time_features"
    ],
    "forward": [
      "self",
      "past_values",
      "past_time_features",
      "past_observed_mask",
      "static_categorical_features",
      "static_real_features",
      "future_values",
      "future_time_features",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "output_hidden_states",
      "output_attentions",
      "use_cache",
      "return_dict",
      "cache_position"
    ]
  },
  "AutoformerForPrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "output_params": [
      "self",
      "decoder_output"
    ],
    "output_distribution": [
      "self",
      "params",
      "loc",
      "scale",
      "trailing_n"
    ],
    "forward": [
      "self",
      "past_values",
      "past_time_features",
      "past_observed_mask",
      "static_categorical_features",
      "static_real_features",
      "future_values",
      "future_time_features",
      "future_observed_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "output_hidden_states",
      "output_attentions",
      "use_cache",
      "return_dict"
    ],
    "generate": [
      "self",
      "past_values",
      "past_time_features",
      "future_time_features",
      "past_observed_mask",
      "static_categorical_features",
      "static_real_features",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "AutoformerConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "prediction_length",
      "context_length",
      "distribution_output",
      "loss",
      "input_size",
      "lags_sequence",
      "scaling",
      "num_time_features",
      "num_dynamic_real_features",
      "num_static_categorical_features",
      "num_static_real_features",
      "cardinality",
      "embedding_dimension",
      "d_model",
      "encoder_attention_heads",
      "decoder_attention_heads",
      "encoder_layers",
      "decoder_layers",
      "encoder_ffn_dim",
      "decoder_ffn_dim",
      "activation_function",
      "dropout",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "attention_dropout",
      "activation_dropout",
      "num_parallel_samples",
      "init_std",
      "use_cache",
      "is_encoder_decoder",
      "label_length",
      "moving_average",
      "autocorrelation_factor"
    ],
    "_number_of_features": [
      "self"
    ]
  },
  "create_position_ids_from_input_ids": [
    "input_ids",
    "padding_idx"
  ],
  "EvollaSaProtEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ]
  },
  "rotate_half_esm": [
    "x"
  ],
  "apply_rotary_pos_emb_esm": [
    "x",
    "cos",
    "sin"
  ],
  "EvollaSaProtRotaryEmbedding": {
    "__init__": [
      "self",
      "dim"
    ],
    "_update_cos_sin_tables": [
      "self",
      "x",
      "seq_dimension"
    ],
    "forward": [
      "self",
      "q",
      "k"
    ]
  },
  "EvollaSaProtSelfAttention": {
    "__init__": [
      "self",
      "config",
      "position_embedding_type",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "EvollaSaProtSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "EvollaSaProtAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "EvollaSaProtIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EvollaSaProtOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "EvollaSaProtLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "EvollaSaProtEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "EvollaSaProtPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EvollaSaProtPreTrainedModel": {
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EvollaSaProtProteinEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask"
    ]
  },
  "EvollaSequenceCompressorAttention": {
    "__init__": [
      "self",
      "dim",
      "dim_head",
      "heads"
    ],
    "forward": [
      "self",
      "x",
      "latents",
      "mask"
    ]
  },
  "EvollaFeedForward": {
    "__init__": [
      "self",
      "dim",
      "mult"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EvollaSequenceCompressorResampler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "embeds",
      "mask"
    ]
  },
  "EvollaProteinEncoderModelOutput": {},
  "EvollaProteinEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask"
    ]
  },
  "EvollaSequenceAlignerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "protein_encoder_dim",
      "structure_encoder_dim",
      "msa_encoder_dim"
    ],
    "cross_attention": [
      "self",
      "query_states",
      "protein_key_value_states",
      "structure_key_value_states",
      "msa_key_value_states",
      "query_attn_mask",
      "protein_kv_attn_mask",
      "structure_kv_attn_mask",
      "msa_kv_attn_mask"
    ],
    "forward": [
      "self",
      "query_states",
      "protein_kv_states",
      "structure_kv_states",
      "msa_kv_states",
      "query_attn_mask",
      "protein_kv_attn_mask",
      "structure_kv_attn_mask",
      "msa_kv_attn_mask",
      "protein_batch_mask",
      "structure_batch_mask",
      "msa_batch_mask",
      "past_key_values"
    ]
  },
  "EvollaRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "EvollaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "EvollaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EvollaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "EvollaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "protein_kv_states",
      "structure_kv_states",
      "msa_kv_states",
      "protein_batch_mask",
      "structure_batch_mask",
      "msa_batch_mask",
      "query_attn_mask"
    ]
  },
  "EvollaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EvollaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "protein_input_ids",
      "protein_attention_mask",
      "structure_feats",
      "msa_feats",
      "structure_batch_mask",
      "msa_batch_mask"
    ]
  },
  "EvollaForProteinText2Text": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "protein_input_ids",
      "protein_attention_mask",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "SaProtConfig": {
    "__init__": [
      "self",
      "vocab_size",
      "mask_token_id",
      "pad_token_id",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "position_embedding_type",
      "emb_layer_norm_before",
      "token_dropout",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "EvollaConfig": {
    "model_type": [],
    "sub_configs": [],
    "default_theta": [],
    "__init__": [
      "self",
      "protein_encoder_config",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "rms_norm_eps",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "aligner_ffn_mult",
      "aligner_enable_bias",
      "aligner_attention_probs_dropout_prob",
      "aligner_num_add_layers",
      "resampler_depth",
      "resampler_dim_head",
      "resampler_heads",
      "resampler_num_latents",
      "resampler_ff_mult",
      "initializer_range",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "tie_word_embeddings",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "PROTEIN_VALID_KEYS": [],
  "EvollaProcessor": {
    "__init__": [
      "self",
      "protein_tokenizer",
      "tokenizer",
      "protein_max_length",
      "text_max_length"
    ],
    "process_proteins": [
      "self",
      "proteins",
      "protein_max_length"
    ],
    "process_text": [
      "self",
      "texts",
      "text_max_length"
    ],
    "__call__": [
      "self",
      "proteins",
      "messages_list",
      "protein_max_length",
      "text_max_length"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "protein_batch_decode": [
      "self"
    ],
    "protein_decode": [
      "self"
    ]
  },
  "Gemma2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "query_pre_attn_scalar",
      "sliding_window",
      "layer_types",
      "final_logit_softcapping",
      "attn_logit_softcapping",
      "use_bidirectional_attention"
    ]
  },
  "Gemma2RMSNorm": {},
  "Gemma2MLP": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Gemma2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Gemma2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Gemma2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "Gemma2PreTrainedModel": {},
  "Gemma2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Gemma2ForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Gemma2ForSequenceClassification": {},
  "Gemma2ForTokenClassification": {},
  "XLMRobertaTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "add_prefix_space",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token"
    ]
  },
  "XLMRobertaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "XLMRobertaEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "XLMRobertaSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "XLMRobertaCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "XLMRobertaSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "XLMRobertaAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "XLMRobertaIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XLMRobertaOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "XLMRobertaLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "XLMRobertaLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "XLMRobertaPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "XLMRobertaEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "XLMRobertaPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XLMRobertaModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "XLMRobertaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "XLMRobertaForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "XLMRobertaClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "XLMRobertaForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "XLMRobertaForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "XLMRobertaForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "XLMRobertaForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "GeLU": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LxmertModelOutput": {},
  "LxmertForQuestionAnsweringOutput": {},
  "LxmertForPreTrainingOutput": {},
  "LxmertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "inputs_embeds"
    ]
  },
  "LxmertAttention": {
    "__init__": [
      "self",
      "config",
      "ctx_dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "context",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LxmertAttentionOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LxmertCrossAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "ctx_tensor",
      "ctx_att_mask",
      "output_attentions"
    ]
  },
  "LxmertSelfAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LxmertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LxmertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LxmertLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LxmertXLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "cross_att": [
      "self",
      "lang_input",
      "lang_attention_mask",
      "visual_input",
      "visual_attention_mask",
      "output_x_attentions"
    ],
    "self_att": [
      "self",
      "lang_input",
      "lang_attention_mask",
      "visual_input",
      "visual_attention_mask"
    ],
    "output_fc": [
      "self",
      "lang_input",
      "visual_input"
    ],
    "forward": [
      "self",
      "lang_feats",
      "lang_attention_mask",
      "visual_feats",
      "visual_attention_mask",
      "output_attentions"
    ]
  },
  "LxmertVisualFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "visual_feats",
      "visual_pos"
    ]
  },
  "LxmertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "lang_feats",
      "lang_attention_mask",
      "visual_feats",
      "visual_pos",
      "visual_attention_mask",
      "output_attentions"
    ]
  },
  "LxmertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LxmertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LxmertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LxmertVisualAnswerHead": {
    "__init__": [
      "self",
      "config",
      "num_labels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LxmertVisualObjHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LxmertPreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "LxmertPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LxmertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "visual_feats",
      "visual_pos",
      "attention_mask",
      "visual_attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LxmertForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_bias": [
      "self",
      "bias",
      "new_num_tokens"
    ],
    "resize_num_qa_labels": [
      "self",
      "num_labels"
    ],
    "_resize_qa_labels": [
      "self",
      "num_labels"
    ],
    "get_qa_logit_layer": [
      "self"
    ],
    "_set_qa_logit_layer": [
      "self",
      "qa_logit_layer"
    ],
    "_get_resized_qa_labels": [
      "self",
      "cur_qa_logit_layer",
      "num_labels"
    ],
    "forward": [
      "self",
      "input_ids",
      "visual_feats",
      "visual_pos",
      "attention_mask",
      "visual_attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "obj_labels",
      "matched_label",
      "ans",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LxmertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "resize_num_qa_labels": [
      "self",
      "num_labels"
    ],
    "_resize_qa_labels": [
      "self",
      "num_labels"
    ],
    "get_qa_logit_layer": [
      "self"
    ],
    "_set_qa_logit_layer": [
      "self",
      "qa_logit_layer"
    ],
    "_get_resized_qa_labels": [
      "self",
      "cur_qa_logit_layer",
      "num_labels"
    ],
    "forward": [
      "self",
      "input_ids",
      "visual_feats",
      "visual_pos",
      "attention_mask",
      "visual_attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LxmertConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_attention_heads",
      "num_qa_labels",
      "num_object_labels",
      "num_attr_labels",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "l_layers",
      "x_layers",
      "r_layers",
      "visual_feat_dim",
      "visual_pos_dim",
      "visual_loss_normalizer",
      "task_matched",
      "task_mask_lm",
      "task_obj_predict",
      "task_qa",
      "visual_obj_loss",
      "visual_attr_loss",
      "visual_feat_loss",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "MiniMaxConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "sliding_window",
      "attention_dropout",
      "num_experts_per_tok",
      "num_local_experts",
      "output_router_logits",
      "router_aux_loss_coef",
      "router_jitter_noise",
      "rope_parameters",
      "layer_types",
      "block_size",
      "full_attn_alpha_factor",
      "full_attn_beta_factor",
      "linear_attn_alpha_factor",
      "linear_attn_beta_factor",
      "mlp_alpha_factor",
      "mlp_beta_factor"
    ]
  },
  "MiniMaxRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MiniMaxCache": {
    "__init__": [
      "self"
    ],
    "set_linear_cache": [
      "self",
      "layer_idx",
      "linear_cache"
    ],
    "get_linear_cache": [
      "self",
      "layer_idx"
    ],
    "__len__": [
      "self"
    ],
    "batch_repeat_interleave": [
      "self",
      "repeats"
    ],
    "batch_select_indices": [
      "self",
      "indices"
    ],
    "crop": [
      "self",
      "max_length"
    ]
  },
  "MiniMaxLightningAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "get_slope_rate": [
      "self"
    ],
    "decay_factors": [
      "self",
      "slope_rate"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "MiniMaxRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MiniMaxAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "MiniMaxTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MiniMaxExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "MiniMaxSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MiniMaxDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "MiniMaxPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MiniMaxModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "MiniMaxForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MiniMaxForSequenceClassification": {},
  "MiniMaxForTokenClassification": {},
  "MiniMaxForQuestionAnswering": {},
  "_prepare_cross_attention_mask": [
    "cross_attention_mask",
    "num_vision_tokens",
    "dtype"
  ],
  "_prepare_aspect_ratio_attention_mask": [
    "aspect_ratio_mask",
    "num_patches",
    "target_length",
    "dtype"
  ],
  "MllamaPrecomputedAspectRatioEmbedding": {
    "__init__": [
      "self",
      "config",
      "is_gated"
    ],
    "forward": [
      "self",
      "hidden_state",
      "aspect_ratio_ids"
    ]
  },
  "MllamaPrecomputedPositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "aspect_ratio_ids"
    ]
  },
  "MllamaVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MllamaVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "MllamaVisionEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "is_gated"
    ],
    "forward": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "MllamaVisionEncoder": {
    "__init__": [
      "self",
      "config",
      "num_layers",
      "is_gated"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "MllamaTextRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MllamaTextCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cross_attention_states",
      "past_key_values",
      "attention_mask",
      "use_cache",
      "cache_position"
    ]
  },
  "MllamaTextSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "past_key_values",
      "cache_position"
    ]
  },
  "MllamaTextMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MllamaSelfAttentionDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cross_attention_states",
      "cross_attention_mask",
      "attention_mask",
      "full_text_row_masked_out_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "MllamaCrossAttentionDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cross_attention_states",
      "cross_attention_mask",
      "attention_mask",
      "full_text_row_masked_out_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "MllamaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MllamaPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_can_compile_fullgraph": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "MllamaVisionModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "apply_class_embedding": [
      "self",
      "hidden_state"
    ],
    "forward": [
      "self",
      "pixel_values",
      "aspect_ratio_ids",
      "aspect_ratio_mask"
    ]
  },
  "MllamaTextModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "cross_attention_states",
      "cross_attention_mask",
      "full_text_row_masked_out_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "MllamaForCausalLM": {
    "_can_compile_fullgraph": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "cross_attention_states",
      "cross_attention_mask",
      "full_text_row_masked_out_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MllamaModel": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "aspect_ratio_mask",
      "aspect_ratio_ids",
      "attention_mask",
      "cross_attention_mask",
      "cross_attention_states",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "MllamaForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "aspect_ratio_mask",
      "aspect_ratio_ids",
      "attention_mask",
      "cross_attention_mask",
      "cross_attention_states",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "inputs_embeds",
      "attention_mask",
      "position_ids",
      "pixel_values",
      "aspect_ratio_ids",
      "aspect_ratio_mask",
      "cross_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder"
    ]
  },
  "MllamaImageProcessorKwargs": {},
  "get_image_size_fit_to_canvas": [
    "image_height",
    "image_width",
    "canvas_height",
    "canvas_width",
    "tile_size"
  ],
  "build_aspect_ratio_mask": [
    "aspect_ratios",
    "max_image_tiles"
  ],
  "pack_images": [
    "batch_images",
    "max_image_tiles"
  ],
  "convert_aspect_ratios_to_ids": [
    "aspect_ratios",
    "max_image_tiles"
  ],
  "_validate_size": [
    "size"
  ],
  "_validate_mllama_preprocess_arguments": [
    "do_resize",
    "size",
    "do_pad",
    "max_image_tiles"
  ],
  "MllamaImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_convert_rgb",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "max_image_tiles"
    ],
    "preprocess": [
      "self",
      "images",
      "do_convert_rgb",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "max_image_tiles",
      "input_data_format",
      "return_tensors"
    ],
    "pad": [
      "self",
      "image",
      "size",
      "aspect_ratio",
      "data_format",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "max_image_tiles",
      "resample",
      "data_format",
      "input_data_format"
    ]
  },
  "pad_batches_and_tiles": [
    "batch_images",
    "max_image_tiles"
  ],
  "MllamaImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_pad": [],
    "max_image_tiles": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_prepare_images_structure": [
      "self",
      "images",
      "expected_ndims"
    ],
    "convert_to_rgb": [
      "self",
      "image"
    ],
    "pad": [
      "self",
      "image",
      "size",
      "aspect_ratio"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "max_image_tiles",
      "interpolation",
      "antialias"
    ],
    "_preprocess": [
      "self",
      "images",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "max_image_tiles",
      "return_tensors",
      "disable_grouping"
    ]
  },
  "MllamaProcessorKwargs": {
    "_defaults": []
  },
  "get_cross_attention_token_mask": [
    "input_ids",
    "image_token_id"
  ],
  "convert_sparse_cross_attention_mask_to_dense": [
    "cross_attention_token_mask",
    "num_tiles",
    "max_num_tiles",
    "length"
  ],
  "build_string_from_input": [
    "prompt",
    "bos_token",
    "image_token"
  ],
  "MllamaProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "MllamaVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "hidden_act",
      "num_hidden_layers",
      "num_global_layers",
      "num_attention_heads",
      "num_channels",
      "intermediate_size",
      "vision_output_dim",
      "image_size",
      "patch_size",
      "norm_eps",
      "max_num_tiles",
      "intermediate_layers_indices",
      "supported_aspect_ratios",
      "initializer_range"
    ],
    "max_aspect_ratio_id": [
      "self"
    ]
  },
  "MllamaTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "hidden_act",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "intermediate_size",
      "rope_parameters",
      "rms_norm_eps",
      "max_position_embeddings",
      "initializer_range",
      "use_cache",
      "tie_word_embeddings",
      "cross_attention_layers",
      "dropout",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ]
  },
  "MllamaConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index"
    ]
  },
  "PRETRAINED_VOCAB_FILES_MAP": [],
  "DEFAULT_SYSTEM_PROMPT": [],
  "DEFAULT_RAG_PREAMBLE": [],
  "CohereTokenizer": {
    "vocab_files_names": [],
    "pretrained_vocab_files_map": [],
    "padding_side": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "errors",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "cls_token",
      "sep_token",
      "mask_token",
      "use_default_system_prompt",
      "add_prefix_space"
    ],
    "apply_tool_use_template": [
      "self",
      "conversation",
      "tools"
    ],
    "apply_grounded_generation_template": [
      "self",
      "conversation",
      "documents",
      "citation_mode"
    ]
  },
  "CohereLayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CohereRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "CohereMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CohereAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "CohereDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "CoherePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "CohereModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "CohereForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "CohereConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "logit_scale",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "use_qk_norm"
    ]
  },
  "Wav2Vec2PhonemeCTCTokenizerOutput": {},
  "Wav2Vec2PhonemeCTCTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "bos_token",
      "eos_token",
      "unk_token",
      "pad_token",
      "phone_delimiter_token",
      "word_delimiter_token",
      "do_phonemize",
      "phonemizer_lang",
      "phonemizer_backend"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "init_backend": [
      "self",
      "phonemizer_lang"
    ],
    "prepare_for_tokenization": [
      "self",
      "text",
      "is_split_into_words",
      "phonemizer_lang",
      "do_phonemize"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "phonemize": [
      "self",
      "text",
      "phonemizer_lang"
    ],
    "word_delimiter_token": [
      "self",
      "value"
    ],
    "word_delimiter_token_id": [
      "self",
      "value"
    ],
    "phone_delimiter_token": [
      "self",
      "value"
    ],
    "phone_delimiter_token_id": [
      "self",
      "value"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens",
      "group_tokens",
      "spaces_between_special_tokens",
      "filter_word_delimiter_token",
      "output_char_offsets"
    ],
    "_compute_offsets": [
      "char_repetitions",
      "chars",
      "ctc_token",
      "word_delimiter_token"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "group_tokens",
      "filter_word_delimiter_token",
      "spaces_between_special_tokens",
      "output_char_offsets"
    ],
    "decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "output_char_offsets"
    ],
    "batch_decode": [
      "self",
      "sequences",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "output_char_offsets"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "CodeGenTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "add_prefix_space",
      "return_token_type_ids"
    ],
    "decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "truncate_before_pattern"
    ],
    "truncate": [
      "self",
      "completion",
      "truncate_before_pattern"
    ]
  },
  "create_sinusoidal_positions": [
    "num_pos",
    "dim"
  ],
  "rotate_every_two": [
    "x"
  ],
  "CodeGenAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_split_heads": [
      "self",
      "x",
      "n_head",
      "dim_head",
      "mp_num"
    ],
    "_merge_heads": [
      "self",
      "tensor",
      "num_attention_heads",
      "attn_head_size"
    ],
    "_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "position_ids",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "CodeGenMLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CodeGenBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "position_ids",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "CodeGenPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CodeGenModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "CodeGenForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "CodeGenConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "n_positions",
      "n_ctx",
      "n_embd",
      "n_layer",
      "n_head",
      "rotary_dim",
      "n_inner",
      "activation_function",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "Cohere2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Cohere2LayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Cohere2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Cohere2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Cohere2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "Cohere2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Cohere2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Cohere2ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Cohere2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "logit_scale",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "sliding_window",
      "layer_types"
    ]
  },
  "GPTNeoSelfAttention": {
    "__init__": [
      "self",
      "config",
      "attention_type",
      "layer_id"
    ],
    "_split_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "_merge_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTNeoFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPT_NEO_ATTENTION_CLASSES": [],
  "GPTNeoAttention": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTNeoMLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GPTNeoBlock": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTNeoPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GPTNeoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "GPTNeoForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GPTNeoForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTNeoForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTNeoForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTNeoConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "hidden_size",
      "num_layers",
      "attention_types",
      "num_heads",
      "intermediate_size",
      "window_size",
      "activation_function",
      "resid_dropout",
      "embed_dropout",
      "attention_dropout",
      "classifier_dropout",
      "layer_norm_epsilon",
      "initializer_range",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings"
    ],
    "expand_attention_types_params": [
      "attention_types"
    ]
  },
  "custom_unfold": [
    "input",
    "dimension",
    "size",
    "step"
  ],
  "custom_get_block_length_and_num_blocks": [
    "seq_length",
    "window_size"
  ],
  "SqueezeBertTokenizer": [],
  "SqueezeBertTokenizerFast": [],
  "SqueezeBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "MatMulWrapper": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "mat1",
      "mat2"
    ]
  },
  "SqueezeBertLayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConvDropoutLayerNorm": {
    "__init__": [
      "self",
      "cin",
      "cout",
      "groups",
      "dropout_prob"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ConvActivation": {
    "__init__": [
      "self",
      "cin",
      "cout",
      "groups",
      "act"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SqueezeBertSelfAttention": {
    "__init__": [
      "self",
      "config",
      "cin",
      "q_groups",
      "k_groups",
      "v_groups"
    ],
    "transpose_for_scores": [
      "self",
      "x"
    ],
    "transpose_key_for_scores": [
      "self",
      "x"
    ],
    "transpose_output": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SqueezeBertModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SqueezeBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SqueezeBertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SqueezeBertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SqueezeBertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SqueezeBertOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "SqueezeBertPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SqueezeBertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SqueezeBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SqueezeBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SqueezeBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SqueezeBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SqueezeBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SqueezeBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "embedding_size",
      "q_groups",
      "k_groups",
      "v_groups",
      "post_attention_groups",
      "intermediate_groups",
      "output_groups",
      "tie_word_embeddings"
    ]
  },
  "LayoutLMLayerNorm": [],
  "LayoutLMEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "LayoutLMSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LayoutLMSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LayoutLMAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LayoutLMIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayoutLMOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LayoutLMLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "LayoutLMEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayoutLMPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayoutLMLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayoutLMOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "LayoutLMPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LayoutLMModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMForQuestionAnswering": {
    "__init__": [
      "self",
      "config",
      "has_visual_segment_embedding"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "use_cache",
      "max_2d_position_embeddings",
      "tie_word_embeddings"
    ]
  },
  "DeepseekV2Config": {
    "base_model_tp_plan": [],
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "first_k_dense_replace",
      "kv_lora_rank",
      "q_lora_rank",
      "n_group",
      "n_routed_experts",
      "n_shared_experts",
      "qk_nope_head_dim",
      "qk_rope_head_dim",
      "routed_scaling_factor",
      "topk_group",
      "topk_method",
      "norm_topk_prob",
      "v_head_dim",
      "num_experts_per_tok",
      "moe_intermediate_size"
    ]
  },
  "apply_rotary_emb": [
    "xq",
    "xk",
    "freqs_cis"
  ],
  "DeepseekV2Experts": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DeepseekV2Moe": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeepseekV2MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ]
  },
  "DeepseekV2RMSNorm": {},
  "DeepseekV2RotaryEmbedding": {
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "DeepseekV2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position",
      "position_embeddings"
    ]
  },
  "DeepseekV2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "DeepseekV2PreTrainedModel": {
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DeepseekV2Model": {},
  "DeepseekV2ForCausalLM": {},
  "DeepseekV2ForSequenceClassification": {},
  "ACT_FNS": [],
  "MLP": {
    "__init__": [
      "self",
      "n_state",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OpenAIGPTSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "OpenAIGPTPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "OpenAIGPTDoubleHeadsModelOutput": {},
  "OpenAIGPTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OpenAIGPTLMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids"
    ]
  },
  "OpenAIGPTDoubleHeadsModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "mc_token_ids",
      "labels",
      "mc_labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OpenAIGPTForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OpenAIGPTConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "n_positions",
      "n_embd",
      "n_layer",
      "n_head",
      "afn",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "summary_type",
      "summary_use_proj",
      "summary_activation",
      "summary_proj_to_labels",
      "summary_first_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "OpenAIGPTTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token"
    ],
    "do_lower_case": [
      "self"
    ]
  },
  "ConditionalDetrConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "num_channels",
      "num_queries",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "init_xavier_std",
      "auxiliary_loss",
      "position_embedding_type",
      "dilation",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "mask_loss_coefficient",
      "dice_loss_coefficient",
      "cls_loss_coefficient",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "focal_alpha"
    ]
  },
  "ConditionalDetrImageProcessorKwargs": {},
  "ConditionalDetrImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "format",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_annotations",
      "do_pad",
      "pad_size"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "size",
      "resample"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "annotation",
      "constant_values",
      "data_format",
      "input_data_format",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "images",
      "annotations",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "update_bboxes",
      "pad_size"
    ],
    "preprocess": [
      "self",
      "images",
      "annotations",
      "return_segmentation_masks",
      "masks_path",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "format",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "top_k"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "ConditionalDetrDecoderOutput": {},
  "ConditionalDetrModelOutput": {},
  "ConditionalDetrObjectDetectionOutput": {},
  "ConditionalDetrSegmentationOutput": {},
  "ConditionalDetrFrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConditionalDetrConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "ConditionalDetrSinePositionEmbedding": {
    "__init__": [
      "self",
      "num_position_features",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "ConditionalDetrLearnedPositionEmbedding": {
    "__init__": [
      "self",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "ConditionalDetrSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "ConditionalDetrDecoderSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout"
    ],
    "forward": [
      "self",
      "hidden_states",
      "query_position_embeddings",
      "attention_mask"
    ]
  },
  "ConditionalDetrDecoderCrossAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "query_sine_embed",
      "encoder_position_embeddings",
      "query_position_embeddings",
      "attention_mask"
    ]
  },
  "ConditionalDetrMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ConditionalDetrEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings"
    ]
  },
  "ConditionalDetrDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings",
      "query_position_embeddings",
      "query_sine_embed",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "is_first"
    ]
  },
  "ConditionalDetrMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConditionalDetrConvBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "activation"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConditionalDetrFPNFusionStage": {
    "__init__": [
      "self",
      "fpn_channels",
      "current_channels",
      "output_channels",
      "activation"
    ],
    "forward": [
      "self",
      "features",
      "fpn_features"
    ]
  },
  "ConditionalDetrMaskHeadSmallConv": {
    "__init__": [
      "self",
      "input_channels",
      "fpn_channels",
      "hidden_size",
      "activation_function"
    ],
    "forward": [
      "self",
      "features",
      "attention_masks",
      "fpn_features"
    ]
  },
  "ConditionalDetrMHAttentionMap": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "query_states",
      "key_states",
      "attention_mask"
    ]
  },
  "ConditionalDetrPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_supports_flex_attn": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ConditionalDetrEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "spatial_position_embeddings"
    ]
  },
  "gen_sine_position_embeddings": [
    "pos_tensor",
    "d_model"
  ],
  "ConditionalDetrDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "spatial_position_embeddings",
      "object_queries_position_embeddings"
    ]
  },
  "ConditionalDetrModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds"
    ]
  },
  "ConditionalDetrForObjectDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ],
    "_set_aux_loss": [
      "self",
      "outputs_class",
      "outputs_coord"
    ]
  },
  "ConditionalDetrForSegmentation": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "ConditionalDetrImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "format": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "size": [],
    "default_to_square": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "target_size",
      "threshold",
      "interpolation"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "annotation",
      "update_bboxes",
      "fill"
    ],
    "_preprocess": [
      "self",
      "images",
      "annotations",
      "masks_path",
      "return_segmentation_masks",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "format",
      "return_tensors"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "top_k"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "BaseModelOutputWithDeepstackFeatures": {},
  "Qwen3VLVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen3VLVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3VLVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Qwen3VLVisionPatchMerger": {
    "__init__": [
      "self",
      "config",
      "use_postshuffle_norm"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen3VLVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen3VLVisionBlock": {
    "__init__": [
      "self",
      "config",
      "attn_implementation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen3VLTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ],
    "apply_interleaved_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ]
  },
  "Qwen3VLTextRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Qwen3VLTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Qwen3VLTextMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen3VLTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "Qwen3VLModelOutputWithPast": {},
  "Qwen3VLPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen3VLVisionModel": {
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "fast_pos_embed_interpolate": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Qwen3VLTextModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "visual_pos_masks",
      "deepstack_visual_embeds"
    ],
    "_deepstack_process": [
      "self",
      "hidden_states",
      "visual_pos_masks",
      "visual_embeds"
    ]
  },
  "Qwen3VLModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position"
    ]
  },
  "Qwen3VLCausalLMOutputWithPast": {},
  "Qwen3VLForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "Qwen3VLVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "intermediate_size",
      "num_heads",
      "in_channels",
      "patch_size",
      "spatial_merge_size",
      "temporal_patch_size",
      "out_hidden_size",
      "num_position_embeddings",
      "deepstack_visual_indexes",
      "initializer_range"
    ]
  },
  "Qwen3VLTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "pad_token_id"
    ]
  },
  "Qwen3VLConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "vision_start_token_id",
      "vision_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "Qwen3VLProcessorKwargs": {
    "_defaults": []
  },
  "Qwen3VLProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_calculate_timestamps": [
      "self",
      "indices",
      "video_fps",
      "merge_size"
    ]
  },
  "Qwen3VLVideoProcessorInitKwargs": {},
  "Qwen3VLVideoProcessor": {
    "resample": [],
    "size": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "fps": [],
    "min_frames": [],
    "max_frames": [],
    "do_sample_frames": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "num_frames",
      "fps"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "return_tensors"
    ]
  },
  "FalconH1Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "tie_word_embeddings",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "num_logits_to_keep",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_position_embeddings",
      "attention_dropout",
      "mamba_d_ssm",
      "mamba_n_heads",
      "mamba_d_head",
      "mamba_n_groups",
      "mamba_d_state",
      "mamba_d_conv",
      "mamba_expand",
      "mamba_chunk_size",
      "mamba_conv_bias",
      "mamba_proj_bias",
      "mamba_norm_before_gate",
      "mamba_rms_norm",
      "time_step_min",
      "time_step_max",
      "time_step_limit",
      "projectors_bias",
      "rope_parameters",
      "lm_head_multiplier",
      "embedding_multiplier",
      "mlp_multipliers",
      "key_multiplier",
      "attention_out_multiplier",
      "attention_in_multiplier",
      "ssm_multipliers",
      "ssm_in_multiplier",
      "ssm_out_multiplier"
    ],
    "layers_block_type": [
      "self"
    ]
  },
  "FalconHybridMambaAttentionDynamicCache": {
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "devices"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "update_conv_state": [
      "self",
      "layer_idx",
      "new_conv_state",
      "cache_position"
    ],
    "reset": [
      "self"
    ]
  },
  "FalconH1RotaryEmbedding": {},
  "FalconH1Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "FalconH1RMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps",
      "n_groups",
      "norm_before_gate"
    ],
    "forward": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "FalconH1Mixer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "torch_forward": [
      "self",
      "input_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "FalconH1MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FalconH1RMSNorm": {},
  "FalconH1DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "mamba_attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "FalconH1PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "compute_mup_vector": [
    "config"
  ],
  "FalconH1Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ],
    "_update_mamba_mask": [
      "self",
      "attention_mask",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "FalconH1ForCausalLM": {
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "is_first_iteration"
    ]
  },
  "RoFormerTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "tokenize_chinese_chars",
      "strip_accents"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "legacy_format",
      "filename_prefix",
      "push_to_hub"
    ]
  },
  "RoFormerTokenizerFast": [],
  "RoFormerSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "create_weight": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "RoFormerEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "inputs_embeds"
    ]
  },
  "RoFormerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "sinusoidal_pos",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ],
    "apply_rotary_position_embeddings": [
      "sinusoidal_pos",
      "query_layer",
      "key_layer",
      "value_layer"
    ]
  },
  "RoFormerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RoFormerAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "sinusoidal_pos",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "RoFormerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RoFormerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RoFormerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "sinusoidal_pos",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "RoFormerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "RoFormerSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "RoFormerPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RoFormerLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RoFormerOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "RoFormerPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RoFormerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "RoFormerForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RoFormerForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "RoFormerClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "RoFormerForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RoFormerForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RoFormerForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RoFormerForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "JiebaPreTokenizer": {
    "__init__": [
      "self",
      "vocab"
    ],
    "jieba_split": [
      "self",
      "i",
      "normalized_string"
    ],
    "pre_tokenize": [
      "self",
      "pretok"
    ]
  },
  "RoFormerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "embedding_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "rotary_value",
      "use_cache",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "ColQwen2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ColQwen2ForRetrievalOutput": {},
  "ColQwen2ForRetrieval": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "labels",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "pixel_values",
      "image_grid_thw",
      "cache_position"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ]
  },
  "ColQwen2ProcessorKwargs": {
    "_defaults": []
  },
  "ColQwen2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "visual_prompt_prefix",
      "query_prefix"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "model_input_names": [
      "self"
    ],
    "query_augmentation_token": [
      "self"
    ],
    "process_images": [
      "self",
      "images"
    ],
    "process_queries": [
      "self",
      "text"
    ],
    "score_retrieval": [
      "self",
      "query_embeddings",
      "passage_embeddings",
      "batch_size",
      "output_dtype",
      "output_device"
    ]
  },
  "ColQwen2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "vlm_config",
      "embedding_dim",
      "initializer_range"
    ],
    "get_text_config": [
      "self"
    ]
  },
  "create_fb_matrix": [
    "n_freqs",
    "f_min",
    "f_max",
    "n_mels",
    "sample_rate",
    "fft_length",
    "norm"
  ],
  "_unfold": [
    "array",
    "dimension",
    "size",
    "step"
  ],
  "Gemma3nAudioFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "return_attention_mask",
      "frame_length_ms",
      "hop_length_ms",
      "min_frequency",
      "max_frequency",
      "preemphasis",
      "preemphasis_htk_flavor",
      "fft_overdrive",
      "dither",
      "input_scale_factor",
      "mel_floor",
      "per_bin_mean",
      "per_bin_stddev"
    ],
    "_extract_spectrogram": [
      "self",
      "waveform",
      "attention_mask"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "padding",
      "max_length",
      "truncation",
      "pad_to_multiple_of",
      "return_tensors",
      "return_attention_mask"
    ]
  },
  "Gemma3nAudioEncoderModelOutput": {},
  "Gemma3nModelOutputWithPast": {},
  "Gemma3nCausalLMOutputWithPast": {},
  "Gemma3nRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps",
      "with_scale"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Gemma3nAudioRelativePositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_timing_signal_1d_pos": [
      "self",
      "position",
      "dtype"
    ],
    "_relative_shift": [
      "self",
      "term_bd_before_shift",
      "batch_size",
      "num_heads",
      "num_query_blocks",
      "query_block_size",
      "key_context_size",
      "max_span_plus_1"
    ],
    "forward": [
      "self",
      "queries",
      "keys"
    ]
  },
  "Gemma3nAudioAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "create_local_causal_valid_mask": [
      "self"
    ],
    "_pad_dim1": [
      "self",
      "x",
      "pad_left",
      "pad_right"
    ],
    "_convert_to_block": [
      "self",
      "hidden_states"
    ],
    "_extract_block_context": [
      "self",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask"
    ]
  },
  "Gemma3nAudioCumulativeGroupNorm": {
    "__init__": [
      "self",
      "num_channels",
      "feature_dims",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Gemma3nAudioSSCPConvBlock": {
    "__init__": [
      "self",
      "config",
      "idx",
      "input_freq_dim",
      "manual_padding"
    ],
    "forward": [
      "self",
      "audio_encodings"
    ]
  },
  "Gemma3nAudioSubSampleConvProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_encodings"
    ]
  },
  "Gemma3nAudioConformerAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_encodings",
      "audio_mel_mask"
    ]
  },
  "Gemma3nAudioConformerFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_encodings"
    ]
  },
  "Gemma3nAudioConformerLightConv1d": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_encodings"
    ]
  },
  "Gemma3nAudioConformerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_encodings",
      "audio_mel_mask"
    ]
  },
  "Gemma3nAudioEncoder": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_mel",
      "audio_mel_mask"
    ]
  },
  "Gemma3nTextScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "Gemma3nTextLaurelBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Gemma3nTextMLP": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "_gaussian_topk": [
      "self",
      "inputs"
    ]
  },
  "Gemma3nTextAltUp": {
    "__init__": [
      "self",
      "config"
    ],
    "compute_router_modalities": [
      "self",
      "x"
    ],
    "predict": [
      "self",
      "hidden_states"
    ],
    "correct": [
      "self",
      "predictions",
      "activated"
    ],
    "forward": [
      "self",
      "corrected"
    ],
    "scale_corrected_output": [
      "self",
      "corrected"
    ]
  },
  "Gemma3nTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Gemma3nTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "per_layer_input",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "Gemma3nPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Gemma3nRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device",
      "layer_type"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len",
      "layer_type"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "layer_type"
    ]
  },
  "Gemma3nTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "per_layer_inputs",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ],
    "get_per_layer_inputs": [
      "self",
      "input_ids"
    ],
    "project_per_layer_inputs": [
      "self",
      "inputs_embeds",
      "per_layer_inputs"
    ]
  },
  "Gemma3nForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Gemma3nMultimodalEmbedder": {
    "__init__": [
      "self",
      "multimodal_config",
      "text_config"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds"
    ]
  },
  "Gemma3nModel": {
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "audio_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "input_features",
      "attention_mask",
      "input_features_mask",
      "position_ids",
      "past_key_values",
      "token_type_ids",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states"
    ],
    "get_audio_features": [
      "self",
      "input_features",
      "input_features_mask"
    ]
  },
  "Gemma3nForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "input_features",
      "attention_mask",
      "input_features_mask",
      "position_ids",
      "past_key_values",
      "token_type_ids",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "pixel_values",
      "input_features",
      "attention_mask",
      "input_features_mask",
      "token_type_ids",
      "use_cache",
      "logits_to_keep",
      "labels",
      "is_first_iteration"
    ]
  },
  "Gemma3nTextConfig": {
    "model_type": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "vocab_size_per_layer_input",
      "hidden_size",
      "hidden_size_per_layer_input",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "sliding_window",
      "layer_types",
      "final_logit_softcapping",
      "altup_active_idx",
      "altup_coef_clip",
      "altup_correct_scale",
      "altup_num_inputs",
      "num_kv_shared_layers",
      "laurel_rank",
      "activation_sparsity_pattern",
      "tie_word_embeddings"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "Gemma3nAudioConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "vocab_offset",
      "input_feat_size",
      "hidden_size",
      "rms_norm_eps",
      "gradient_clipping",
      "conf_attention_chunk_size",
      "conf_attention_context_left",
      "conf_attention_context_right",
      "conf_attention_logit_cap",
      "conf_num_attention_heads",
      "conf_num_hidden_layers",
      "conf_conv_kernel_size",
      "conf_reduction_factor",
      "conf_residual_weight",
      "sscp_conv_channel_size",
      "sscp_conv_group_norm_eps",
      "sscp_conv_kernel_size",
      "sscp_conv_stride_size"
    ]
  },
  "Gemma3nVisionConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "initializer_range",
      "do_pooling",
      "architecture",
      "hidden_size",
      "vocab_size",
      "vocab_offset",
      "rms_norm_eps",
      "model_args"
    ]
  },
  "Gemma3nConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "audio_config",
      "audio_soft_tokens_per_image",
      "vision_soft_tokens_per_image",
      "boi_token_id",
      "eoi_token_id",
      "image_token_id",
      "boa_token_id",
      "eoa_token_id",
      "audio_token_id",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "Gemma3nProcessorKwargs": {
    "_defaults": []
  },
  "Gemma3nProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "image_processor",
      "tokenizer",
      "chat_template",
      "audio_seq_length",
      "image_seq_length"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "audio"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "ViTImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format",
      "do_convert_rgb"
    ]
  },
  "ViTEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "ViTPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "ViTSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViTLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ViTModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "use_mask_token"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "ViTPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTForMaskedImageModeling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "ViTForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "interpolate_pos_encoding"
    ]
  },
  "ViTConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "encoder_stride",
      "pooler_output_size",
      "pooler_act"
    ]
  },
  "ViTImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": []
  },
  "FastSpeech2ConformerConfig": {
    "model_type": [],
    "base_config_key": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "hidden_size",
      "vocab_size",
      "num_mel_bins",
      "encoder_num_attention_heads",
      "encoder_layers",
      "encoder_linear_units",
      "decoder_layers",
      "decoder_num_attention_heads",
      "decoder_linear_units",
      "speech_decoder_postnet_layers",
      "speech_decoder_postnet_units",
      "speech_decoder_postnet_kernel",
      "positionwise_conv_kernel_size",
      "encoder_normalize_before",
      "decoder_normalize_before",
      "encoder_concat_after",
      "decoder_concat_after",
      "reduction_factor",
      "speaking_speed",
      "use_macaron_style_in_conformer",
      "use_cnn_in_conformer",
      "encoder_kernel_size",
      "decoder_kernel_size",
      "duration_predictor_layers",
      "duration_predictor_channels",
      "duration_predictor_kernel_size",
      "energy_predictor_layers",
      "energy_predictor_channels",
      "energy_predictor_kernel_size",
      "energy_predictor_dropout",
      "energy_embed_kernel_size",
      "energy_embed_dropout",
      "stop_gradient_from_energy_predictor",
      "pitch_predictor_layers",
      "pitch_predictor_channels",
      "pitch_predictor_kernel_size",
      "pitch_predictor_dropout",
      "pitch_embed_kernel_size",
      "pitch_embed_dropout",
      "stop_gradient_from_pitch_predictor",
      "encoder_dropout_rate",
      "encoder_positional_dropout_rate",
      "encoder_attention_dropout_rate",
      "decoder_dropout_rate",
      "decoder_positional_dropout_rate",
      "decoder_attention_dropout_rate",
      "duration_predictor_dropout_rate",
      "speech_decoder_postnet_dropout",
      "max_source_positions",
      "use_masking",
      "use_weighted_masking",
      "num_speakers",
      "num_languages",
      "speaker_embed_dim",
      "is_encoder_decoder",
      "convolution_bias"
    ]
  },
  "FastSpeech2ConformerHifiGanConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "model_in_dim",
      "upsample_initial_channel",
      "upsample_rates",
      "upsample_kernel_sizes",
      "resblock_kernel_sizes",
      "resblock_dilation_sizes",
      "initializer_range",
      "leaky_relu_slope",
      "normalize_before"
    ]
  },
  "FastSpeech2ConformerWithHifiGanConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "model_config",
      "vocoder_config"
    ]
  },
  "FastSpeech2ConformerModelOutput": {},
  "FastSpeech2ConformerWithHifiGanOutput": {},
  "length_regulator": [
    "encoded_embeddings",
    "duration_labels",
    "speaking_speed"
  ],
  "FastSpeech2ConformerDurationPredictor": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "FastSpeech2ConformerBatchNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FastSpeech2ConformerSpeechDecoderPostnet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FastSpeech2ConformerPredictorLayer": {
    "__init__": [
      "self",
      "input_channels",
      "num_chans",
      "kernel_size",
      "dropout_rate"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FastSpeech2ConformerVariancePredictor": {
    "__init__": [
      "self",
      "config",
      "num_layers",
      "num_chans",
      "kernel_size",
      "dropout_rate"
    ],
    "forward": [
      "self",
      "encoder_hidden_states",
      "padding_masks"
    ]
  },
  "FastSpeech2ConformerVarianceEmbedding": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "padding",
      "dropout_rate"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FastSpeech2ConformerAttention": {
    "__init__": [
      "self",
      "config",
      "module_config"
    ],
    "shift_relative_position_tensor": [
      "self",
      "pos_tensor"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "pos_emb",
      "output_attentions"
    ]
  },
  "FastSpeech2ConformerConvolutionModule": {
    "__init__": [
      "self",
      "config",
      "module_config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "FastSpeech2ConformerEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "module_config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "pos_emb",
      "attention_mask",
      "output_attentions"
    ]
  },
  "FastSpeech2ConformerMultiLayeredConv1d": {
    "__init__": [
      "self",
      "config",
      "module_config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FastSpeech2ConformerRelPositionalEncoding": {
    "__init__": [
      "self",
      "config",
      "module_config"
    ],
    "extend_pos_enc": [
      "self",
      "x",
      "pos_enc"
    ],
    "forward": [
      "self",
      "feature_representation"
    ]
  },
  "FastSpeech2ConformerEncoder": {
    "__init__": [
      "self",
      "config",
      "module_config",
      "use_encoder_input_layer"
    ],
    "forward": [
      "self",
      "input_tensor",
      "attention_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "FastSpeech2ConformerLoss": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "outputs_after_postnet",
      "outputs_before_postnet",
      "duration_outputs",
      "pitch_outputs",
      "energy_outputs",
      "spectrogram_labels",
      "duration_labels",
      "pitch_labels",
      "energy_labels",
      "duration_mask",
      "spectrogram_mask"
    ]
  },
  "FastSpeech2ConformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "module",
      "value"
    ]
  },
  "FastSpeech2ConformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "spectrogram_labels",
      "duration_labels",
      "pitch_labels",
      "energy_labels",
      "speaker_ids",
      "lang_ids",
      "speaker_embedding",
      "return_dict",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "FastSpeech2ConformerHifiGan": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "forward": [
      "self",
      "spectrogram"
    ]
  },
  "FastSpeech2ConformerWithHifiGan": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "spectrogram_labels",
      "duration_labels",
      "pitch_labels",
      "energy_labels",
      "speaker_ids",
      "lang_ids",
      "speaker_embedding",
      "return_dict",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "FastSpeech2ConformerTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "bos_token",
      "eos_token",
      "pad_token",
      "unk_token",
      "should_strip_spaces"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "prepare_for_tokenization": [
      "self",
      "text",
      "is_split_into_words"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "decode": [
      "self",
      "token_ids"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ]
  },
  "AlbertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "AlbertAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "AlbertLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ],
    "ff_chunk": [
      "self",
      "attention_output"
    ]
  },
  "AlbertLayerGroup": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "AlbertTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "AlbertPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "AlbertForPreTrainingOutput": {},
  "AlbertModel": {
    "config_class": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "AlbertForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "sentence_order_label"
    ]
  },
  "AlbertMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AlbertSOPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "AlbertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "AlbertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "AlbertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "AlbertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "AlbertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "AlbertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "embedding_size",
      "hidden_size",
      "num_hidden_layers",
      "num_hidden_groups",
      "num_attention_heads",
      "intermediate_size",
      "inner_group_num",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "classifier_dropout_prob",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "AlbertTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "keep_accents",
      "bos_token",
      "eos_token",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "add_prefix_space",
      "trim_offsets"
    ]
  },
  "TimmBackbone": {
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "__init__": [
      "self",
      "config"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "freeze_batch_norm_2d": [
      "self"
    ],
    "unfreeze_batch_norm_2d": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TimmBackboneConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "backbone",
      "num_channels",
      "features_only",
      "out_indices",
      "freeze_batch_norm_2d",
      "output_stride"
    ],
    "out_indices": [
      "self",
      "out_indices"
    ],
    "out_features": [
      "self",
      "out_features"
    ]
  },
  "T5Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "d_kv",
      "d_ff",
      "num_layers",
      "num_decoder_layers",
      "num_heads",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "dropout_rate",
      "layer_norm_epsilon",
      "initializer_factor",
      "feed_forward_proj",
      "is_encoder_decoder",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "classifier_dropout",
      "tie_word_embeddings",
      "is_decoder"
    ]
  },
  "T5LayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5DenseActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5DenseGatedActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5LayerFF": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5Attention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_values",
      "query_length",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "T5LayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "T5LayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "query_length",
      "output_attentions",
      "cache_position"
    ]
  },
  "T5Block": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "return_dict",
      "cache_position"
    ]
  },
  "T5ClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_can_compile_fullgraph": [],
    "_no_split_modules": [],
    "_keep_in_fp32_modules": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "T5Stack": {
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "T5Model": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "T5ForConditionalGeneration": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "T5EncoderModel": {
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "T5ForSequenceClassification": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "T5ForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "T5ForQuestionAnswering": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "T5Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "eos_token",
      "unk_token",
      "pad_token",
      "extra_ids",
      "additional_special_tokens"
    ],
    "get_sentinel_tokens": [
      "self"
    ],
    "get_sentinel_token_ids": [
      "self"
    ]
  },
  "Phi4MultimodalImageProcessorKwargs": {},
  "Phi4MultimodalImageProcessorFast": {
    "resample": [],
    "size": [],
    "patch_size": [],
    "dynamic_hd": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "find_closest_aspect_ratio": [
      "self",
      "aspect_ratio",
      "target_ratios",
      "width",
      "height",
      "image_size"
    ],
    "dynamic_preprocess": [
      "self",
      "image",
      "image_size",
      "patch_size",
      "mask_size",
      "max_num",
      "min_num"
    ],
    "pad_to_max_num_crops": [
      "self",
      "images",
      "max_crops"
    ],
    "pad_mask_to_max_num_crops": [
      "self",
      "masks",
      "max_crops"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "size",
      "interpolation",
      "patch_size",
      "dynamic_hd",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors"
    ]
  },
  "Phi4MultimodalVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "crop_size",
      "image_token_id",
      "feature_layer"
    ]
  },
  "Phi4MultimodalAudioConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_blocks",
      "num_attention_heads",
      "activation",
      "chunk_size",
      "left_chunk",
      "dropout_rate",
      "ext_pw_out_channel",
      "depthwise_separable_out_channel",
      "depthwise_multiplier",
      "kernel_size",
      "conv_activation",
      "input_size",
      "conv_glu_type",
      "time_reduction",
      "bias_max_distance",
      "bias_symmetric",
      "nemo_activation",
      "nemo_conv_channels",
      "downsample_rate",
      "initializer_range",
      "audio_token_id",
      "feature_layer"
    ]
  },
  "Phi4MultimodalConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "resid_pdrop",
      "embd_pdrop",
      "attention_dropout",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "original_max_position_embeddings",
      "sliding_window",
      "vision_config",
      "audio_config"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "default_theta",
      "ignore_keys"
    ],
    "validate_rope": [
      "self",
      "ignore_keys"
    ]
  },
  "Phi4MultimodalVisionMLP": {},
  "Phi4MultimodalVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Phi4MultimodalVisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Phi4MultimodalVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Phi4MultimodalVisionPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Phi4MultimodalVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "patch_attention_mask"
    ]
  },
  "Phi4MultimodalVisionMultiheadAttentionPoolingHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "Phi4MultimodalVisionModel": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "patch_attention_mask"
    ]
  },
  "Phi4MultimodalImageEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "get_img_features": [
      "self",
      "img_embeds",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_pixel_values",
      "image_sizes",
      "image_attention_mask"
    ]
  },
  "Phi4MultimodalAudioMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Phi4MultimodalAudioAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Phi4MultimodalAudioDepthWiseSeparableConv1d": {
    "__init__": [
      "self",
      "config",
      "padding"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Phi4MultimodalAudioGluPointWiseConv": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Phi4MultimodalAudioConvModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Phi4MultimodalAudioConformerEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Phi4MultimodalAudioNemoConvSubsampling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask"
    ]
  },
  "Phi4MultimodalAudioRelativeAttentionBias": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Phi4MultimodalAudioMeanVarianceNormLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Phi4MultimodalAudioPreTrainedModel": {
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Phi4MultimodalAudioModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_streaming_mask": [
      "self",
      "seq_len",
      "batch_size",
      "chunk_size",
      "left_chunk"
    ],
    "forward_embeddings": [
      "self",
      "hidden_states",
      "masks"
    ],
    "calculate_hs_mask": [
      "self",
      "hidden_states",
      "device",
      "mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask"
    ]
  },
  "unfold_tensor": [
    "tensor",
    "max_seq_len"
  ],
  "adaptive_enc_mask": [
    "x_len",
    "chunk_start_idx",
    "left_window",
    "right_window"
  ],
  "Phi4MultimodalAudioEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "audio_input_features",
      "audio_embed_sizes",
      "audio_attention_mask",
      "audio_projection_mode"
    ]
  },
  "Phi4MultimodalRMSNorm": {},
  "Phi4MultimodalDecoderLayer": {},
  "Phi4MultimodalFeatureEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_pixel_values",
      "audio_input_features",
      "image_sizes",
      "image_attention_mask",
      "audio_embed_sizes",
      "audio_attention_mask"
    ]
  },
  "Phi4MultimodalPreTrainedModel": {
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Phi4MultimodalModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "image_pixel_values",
      "image_sizes",
      "image_attention_mask",
      "audio_input_features",
      "audio_embed_sizes",
      "audio_attention_mask",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "Phi4MultimodalForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "image_pixel_values",
      "image_sizes",
      "image_attention_mask",
      "audio_input_features",
      "audio_embed_sizes",
      "audio_attention_mask",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "image_pixel_values",
      "image_sizes",
      "image_attention_mask",
      "audio_input_features",
      "audio_embed_sizes",
      "audio_attention_mask",
      "cache_position",
      "position_ids",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "Phi4MultimodalFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "hop_length",
      "n_fft",
      "win_length",
      "preemphasis",
      "padding_value",
      "audio_compression_rate",
      "audio_downsample_rate",
      "audio_feat_stride",
      "mel_min_frequency",
      "mel_max_frequency"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "sampling_rate",
      "pad_to_multiple_of",
      "padding",
      "max_length",
      "truncation",
      "return_tensors",
      "return_attention_mask",
      "device"
    ],
    "_torch_extract_fbank_features": [
      "self",
      "waveform",
      "audio_lengths",
      "device"
    ],
    "_compute_audio_embed_size": [
      "self",
      "audio_frames"
    ]
  },
  "Phi4MultimodalMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Phi4MultimodalAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Phi4MultimodalRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Phi4MultimodalProcessorKwargs": {
    "_defaults": []
  },
  "Phi4MultimodalProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "audio_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "audio"
    ]
  },
  "UniSpeechSatForPreTrainingOutput": {},
  "UniSpeechSatSamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatNoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatGroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "UniSpeechSatFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "UniSpeechSatFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "UniSpeechSatEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechSatAttnAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatEncoderLayerStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "UniSpeechSatEncoderStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechSatGumbelVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_perplexity": [
      "probs",
      "mask"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask"
    ]
  },
  "UniSpeechSatBaseModelOutput": [],
  "UniSpeechSatModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechSatForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "set_gumbel_temperature": [
      "self",
      "temperature"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "compute_contrastive_logits": [
      "target_features",
      "negative_features",
      "predicted_features",
      "temperature"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechSatForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "tie_weights": [
      "self"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "UniSpeechSatForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "UniSpeechSatForAudioFrameClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AMSoftmaxLoss": {
    "__init__": [
      "self",
      "input_dim",
      "num_labels",
      "scale",
      "margin"
    ],
    "forward": [
      "self",
      "hidden_states",
      "labels"
    ]
  },
  "TDNNLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechSatForXVector": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "_get_tdnn_output_lengths": [
      "self",
      "input_lengths"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "UniSpeechSatConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "feat_quantizer_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "do_stable_layer_norm",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "num_codevectors_per_group",
      "num_codevector_groups",
      "contrastive_logits_temperature",
      "num_negatives",
      "codevector_dim",
      "proj_codevector_dim",
      "diversity_loss_weight",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "tdnn_dim",
      "tdnn_kernel",
      "tdnn_dilation",
      "xvector_output_dim",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "num_clusters"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "BltLocalEncoderConfig": {
    "model_type": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "cross_attn_all_layers",
      "cross_attn_k",
      "hidden_size_global",
      "hidden_size",
      "num_attention_heads",
      "num_key_value_heads",
      "num_hidden_layers",
      "rms_norm_eps",
      "dropout",
      "max_position_embeddings",
      "rope_parameters",
      "hidden_act",
      "intermediate_size",
      "initializer_range"
    ]
  },
  "BltLocalDecoderConfig": {
    "model_type": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "cross_attn_all_layers",
      "cross_attn_k",
      "hidden_size_global",
      "hidden_size",
      "num_attention_heads",
      "num_key_value_heads",
      "num_hidden_layers",
      "rms_norm_eps",
      "dropout",
      "max_position_embeddings",
      "rope_parameters",
      "hidden_act",
      "intermediate_size",
      "initializer_range",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "BltGlobalTransformerConfig": {
    "model_type": [],
    "default_theta": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "num_key_value_heads",
      "num_hidden_layers",
      "rms_norm_eps",
      "dropout",
      "max_position_embeddings",
      "rope_parameters",
      "hidden_act",
      "intermediate_size",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "BltPatcherConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "max_position_embeddings",
      "rms_norm_eps",
      "dropout",
      "intermediate_size",
      "rope_parameters",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "BltConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "patch_in_forward",
      "patch_size",
      "patching_mode",
      "patching_threshold",
      "patching_batch_size",
      "max_patch_length",
      "cross_attn_k",
      "encoder_hash_byte_group_size",
      "encoder_hash_byte_group_vocab",
      "encoder_hash_byte_group_nb_functions",
      "patcher_config",
      "encoder_config",
      "decoder_config",
      "global_config",
      "tie_word_embeddings",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "initializer_range",
      "rope_parameters"
    ]
  },
  "rolling_polynomial_hash": [
    "token_tensor",
    "prime"
  ],
  "byte_group_hash_function": [
    "token_ids",
    "group_size",
    "prime",
    "max_hash"
  ],
  "compute_hash_embeddings": [
    "local_encoder_tokens",
    "local_encoder",
    "encoder_hash_tok_embedding",
    "encoder_hash_byte_group_nb_functions",
    "encoder_hash_byte_group_size",
    "encoder_hash_byte_group_vocab"
  ],
  "_prepare_patch_cross_attention_mask": [
    "patch_ids",
    "num_patches",
    "sequence_length",
    "patches_as_queries",
    "cross_attn_k",
    "dtype"
  ],
  "process_patch_lengths": [
    "patch_lengths",
    "max_patch_length"
  ],
  "BltMLP": {},
  "BltRMSNorm": {},
  "BltRotaryEmbedding": {
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "BltTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "BltSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "BltCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cross_attention_states",
      "attention_mask"
    ]
  },
  "BltPreTrainedModel": {
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_update_causal_mask": [
      "self",
      "module"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "self",
      "module"
    ]
  },
  "BltLocalEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "patch_embeds",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "encoder_attention_mask",
      "num_patches",
      "patch_ids"
    ],
    "patch_reduce": [
      "self",
      "hidden_states",
      "max_num_patches",
      "patch_ids"
    ]
  },
  "BltLocalDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "patch_embeds",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "encoder_attention_mask"
    ]
  },
  "BltGlobalTransformer": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_embeds",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "BltPatcher": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "patch_size",
      "threshold",
      "max_patch_length"
    ],
    "patch_lengths_from_entropies": [
      "entropies",
      "sequence_length",
      "patch_size",
      "threshold"
    ]
  },
  "BltModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "patch_lengths",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "_patch_ids_from_lengths": [
      "self",
      "patch_lengths",
      "seq_len"
    ]
  },
  "BltForCausalLM": {
    "_can_compile_fullgraph": [],
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "cross_attention_states",
      "cross_attention_mask",
      "full_text_row_masked_out_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "DabDetrConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "num_queries",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "is_encoder_decoder",
      "activation_function",
      "hidden_size",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "init_xavier_std",
      "auxiliary_loss",
      "dilation",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "cls_loss_coefficient",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "focal_alpha",
      "temperature_height",
      "temperature_width",
      "query_dim",
      "random_refpoints_xy",
      "keep_query_pos",
      "num_patterns",
      "normalize_before",
      "sine_position_embedding_scale",
      "initializer_bias_prior_prob",
      "tie_word_embeddings"
    ]
  },
  "DabDetrDecoderOutput": {},
  "DabDetrModelOutput": {},
  "DabDetrObjectDetectionOutput": {},
  "DabDetrFrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DabDetrConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "DabDetrConvModel": {
    "__init__": [
      "self",
      "conv_encoder",
      "position_embedding"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "DabDetrSinePositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "DetrAttention": {
    "__init__": [
      "self",
      "config",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "object_queries",
      "key_value_states",
      "output_attentions"
    ]
  },
  "DabDetrAttention": {
    "__init__": [
      "self",
      "config",
      "bias",
      "is_cross"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "key_states",
      "value_states",
      "output_attentions"
    ]
  },
  "DabDetrDecoderLayerSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "query_position_embeddings",
      "attention_mask",
      "output_attentions"
    ]
  },
  "DabDetrDecoderLayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_first"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "query_position_embeddings",
      "object_queries",
      "encoder_attention_mask",
      "query_sine_embed",
      "output_attentions"
    ]
  },
  "DabDetrDecoderLayerFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DabDetrEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "object_queries",
      "output_attentions"
    ]
  },
  "DabDetrDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "is_first"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "object_queries",
      "query_position_embeddings",
      "query_sine_embed",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "DabDetrMLP": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "input_tensor"
    ]
  },
  "DabDetrPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DabDetrEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "object_queries",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DabDetrDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "encoder_hidden_states",
      "memory_key_padding_mask",
      "object_queries",
      "query_position_embeddings",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DabDetrModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DabDetrMHAttentionMap": {
    "__init__": [
      "self",
      "query_dim",
      "hidden_dim",
      "num_heads",
      "dropout",
      "bias",
      "std"
    ],
    "forward": [
      "self",
      "q",
      "k",
      "mask"
    ]
  },
  "DabDetrForObjectDetection": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "_set_aux_loss": [
      "self",
      "outputs_class",
      "outputs_coord"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PixioConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "n_cls_tokens",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "drop_path_rate",
      "out_features",
      "out_indices",
      "apply_layernorm",
      "reshape_hidden_states"
    ]
  },
  "PixioPatchEmbeddings": {},
  "PixioEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "PixioAttention": {},
  "PixioDropPath": {},
  "PixioMLP": {},
  "PixioLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PixioEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states"
    ]
  },
  "PixioPreTrainedModel": {},
  "PixioModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "PixioBackbone": {
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "PixioSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PixioSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Sam3TrackerPromptEncoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "image_size",
      "patch_size",
      "mask_input_channels",
      "num_point_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "scale"
    ]
  },
  "Sam3TrackerMaskDecoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "hidden_act",
      "mlp_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "attention_downsample_rate",
      "num_multimask_outputs",
      "iou_head_depth",
      "iou_head_hidden_dim",
      "dynamic_multimask_via_stability",
      "dynamic_multimask_stability_delta",
      "dynamic_multimask_stability_thresh"
    ]
  },
  "Sam3TrackerConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "prompt_encoder_config",
      "mask_decoder_config",
      "initializer_range"
    ]
  },
  "Sam3TrackerProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "target_size",
      "point_pad_value"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps",
      "input_points",
      "input_labels",
      "input_boxes",
      "original_sizes",
      "return_tensors"
    ],
    "_normalize_coordinates": [
      "self",
      "target_size",
      "coords",
      "original_size",
      "is_bounding_box"
    ],
    "_convert_to_nested_list": [
      "self",
      "data",
      "expected_depth",
      "current_depth"
    ],
    "_get_nested_dimensions": [
      "self",
      "nested_list",
      "max_dims"
    ],
    "_pad_nested_list": [
      "self",
      "nested_list",
      "target_dims",
      "current_level",
      "pad_value"
    ],
    "_create_empty_nested_structure": [
      "self",
      "dims",
      "pad_value"
    ],
    "_get_nesting_level": [
      "self",
      "input_list"
    ],
    "_validate_single_input": [
      "self",
      "data",
      "expected_depth",
      "input_name",
      "expected_format",
      "expected_coord_size"
    ],
    "_normalize_tensor_coordinates": [
      "self",
      "tensor",
      "original_sizes",
      "is_bounding_box",
      "preserve_padding"
    ],
    "post_process_masks": [
      "self",
      "masks",
      "original_sizes",
      "mask_threshold",
      "binarize",
      "max_hole_area",
      "max_sprinkle_area",
      "apply_non_overlapping_constraints"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Sam3TrackerImageSegmentationOutput": {},
  "Sam3TrackerFeedForward": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "activation",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam3TrackerPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Sam3TrackerPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_coords",
      "input_shape"
    ]
  },
  "Sam3TrackerMaskEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "masks"
    ]
  },
  "Sam3TrackerPromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ],
    "forward": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ]
  },
  "Sam3TrackerAttention": {
    "__init__": [
      "self",
      "config",
      "downsample_rate"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_similarity"
    ]
  },
  "Sam3TrackerTwoWayAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "skip_first_layer_pe"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "query_point_embedding",
      "key_point_embedding",
      "attention_similarity"
    ]
  },
  "Sam3TrackerTwoWayTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "point_embeddings",
      "image_embeddings",
      "image_positional_embeddings",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "Sam3TrackerLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "Sam3TrackerMaskDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "high_resolution_features",
      "attention_similarity",
      "target_embedding"
    ],
    "_get_stability_scores": [
      "self",
      "mask_logits"
    ],
    "_dynamic_multimask_via_stability": [
      "self",
      "all_mask_logits",
      "all_iou_scores"
    ]
  },
  "Sam3TrackerVisionEncoderOutput": {},
  "Sam3TrackerModel": {
    "input_modalities": [],
    "_can_record_outputs": [],
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_prompt_embeddings": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "image_embeddings",
      "multimask_output",
      "attention_similarity",
      "target_embedding"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ]
  },
  "Qwen2_5OmniVisionEncoderConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "intermediate_size",
      "num_heads",
      "in_channels",
      "patch_size",
      "spatial_merge_size",
      "temporal_patch_size",
      "window_size",
      "out_hidden_size",
      "fullatt_block_indexes",
      "initializer_range"
    ]
  },
  "Qwen2_5OmniAudioEncoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_mel_bins",
      "encoder_layers",
      "encoder_attention_heads",
      "encoder_ffn_dim",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_function",
      "activation_dropout",
      "scale_embedding",
      "initializer_range",
      "max_source_positions",
      "n_window",
      "output_dim"
    ]
  },
  "Qwen2_5OmniTextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "rope_parameters",
      "use_sliding_window",
      "sliding_window",
      "max_window_layers",
      "layer_types",
      "attention_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Qwen2_5OmniThinkerConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "audio_config",
      "vision_config",
      "text_config",
      "audio_token_index",
      "image_token_index",
      "video_token_index",
      "position_id_per_seconds",
      "seconds_per_chunk",
      "audio_start_token_id",
      "audio_end_token_id",
      "user_token_id",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "Qwen2_5OmniTalkerConfig": {
    "model_type": [],
    "default_theta": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "audio_token_index",
      "image_token_index",
      "video_token_index",
      "vocab_size",
      "tts_text_start_token_id",
      "tts_text_end_token_id",
      "tts_text_pad_token_id",
      "tts_codec_start_token_id",
      "tts_codec_end_token_id",
      "tts_codec_pad_token_id",
      "tts_codec_mask_token_id",
      "vision_start_token_id",
      "vision_end_token_id",
      "embedding_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "rms_norm_eps",
      "head_dim",
      "use_cache",
      "tie_word_embeddings",
      "use_sliding_window",
      "sliding_window",
      "max_window_layers",
      "attention_dropout",
      "rope_parameters",
      "position_id_per_seconds",
      "seconds_per_chunk",
      "audio_start_token_id",
      "audio_end_token_id",
      "initializer_range",
      "spatial_merge_size",
      "layer_types",
      "pad_token_id"
    ]
  },
  "Qwen2_5OmniDiTConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "ff_mult",
      "emb_dim",
      "head_dim",
      "rope_parameters",
      "max_position_embeddings",
      "block_size",
      "look_ahead_layers",
      "look_backward_layers",
      "repeats",
      "num_embeds",
      "mel_dim",
      "dropout",
      "enc_emb_dim",
      "enc_dim",
      "enc_channels",
      "enc_kernel_sizes",
      "enc_dilations",
      "enc_attention_channels",
      "enc_res2net_scale",
      "enc_se_channels"
    ]
  },
  "Qwen2_5OmniBigVGANConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "mel_dim",
      "upsample_initial_channel",
      "resblock_kernel_sizes",
      "resblock_dilation_sizes",
      "upsample_rates",
      "upsample_kernel_sizes"
    ]
  },
  "Qwen2_5OmniToken2WavConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "dit_config",
      "bigvgan_config"
    ]
  },
  "Qwen2_5OmniConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "thinker_config",
      "talker_config",
      "token2wav_config",
      "enable_audio_output"
    ],
    "get_text_config": [
      "self"
    ]
  },
  "Qwen2_5_OmniVideosKwargs": {},
  "Qwen2_5OmniProcessorKwargs": {
    "_defaults": []
  },
  "Qwen2_5OmniProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "video_processor",
      "feature_extractor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "videos",
      "audio"
    ],
    "replace_multimodal_special_tokens": [
      "self",
      "text",
      "audio_lengths",
      "image_grid_thw",
      "video_grid_thw",
      "video_second_per_grid",
      "use_audio_in_video",
      "position_id_per_seconds",
      "seconds_per_chunk"
    ],
    "get_chunked_index": [
      "self",
      "token_indices",
      "tokens_per_chunk"
    ],
    "apply_chat_template": [
      "self",
      "conversations",
      "chat_template"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens"
    ],
    "post_process_multimodal_output": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "generation_mode"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "kaiser_sinc_filter1d": [
    "cutoff",
    "half_width",
    "kernel_size"
  ],
  "Qwen2_5OmniPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen2_5OmniPreTrainedModelForConditionalGeneration": {
    "input_modalities": [],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "self",
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "device",
      "min_dtype",
      "cache_position",
      "batch_size"
    ],
    "get_llm_pos_ids_for_vision": [
      "self",
      "start_idx",
      "vision_idx",
      "spatial_merge_size",
      "t_index",
      "grid_hs",
      "grid_ws"
    ],
    "get_chunked_index": [
      "self",
      "token_indices",
      "tokens_per_chunk",
      "remove_index"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask",
      "use_audio_in_video",
      "audio_seqlens",
      "second_per_grids"
    ]
  },
  "Qwen2_5OmniThinkerCausalLMOutputWithPast": {},
  "Qwen2_5OmniAudioAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "attention_mask"
    ]
  },
  "Qwen2_5OmniAudioEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "attention_mask"
    ]
  },
  "SinusoidsPositionEmbedding": {
    "__init__": [
      "self",
      "length",
      "channels",
      "max_timescale"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Qwen2_5OmniAudioEncoder": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "_prepare_attention_mask": [
      "self",
      "inputs_tensor",
      "cu_seqlens"
    ],
    "forward": [
      "self",
      "input_features",
      "feature_lens",
      "aftercnn_lens"
    ],
    "padded_and_mask_function": [
      "self",
      "tensor_list",
      "tensor_len",
      "padding_value",
      "padding_side"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ]
  },
  "Qwen2_5OmniVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "Qwen2_5OmniMLP": {
    "__init__": [
      "self",
      "config",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen2_5OmniVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "Qwen2_5_VisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Qwen2_5_VisionPatchEmbed": {
    "__init__": [
      "self",
      "patch_size",
      "temporal_patch_size",
      "in_channels",
      "embed_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen2_5OmniPatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "spatial_merge_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen2_5OmniVisionEncoder": {
    "_no_split_modules": [],
    "_input_embed_layer": [],
    "_can_record_outputs": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "get_window_index": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Qwen2_5OmniRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "apply_multimodal_rotary_pos_emb": [
    "q",
    "k",
    "cos",
    "sin",
    "mrope_section",
    "unsqueeze_dim"
  ],
  "Qwen2_5OmniAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen2MLP": {
    "__init__": [
      "self",
      "config",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen2_5OmniDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen2_5OmniThinkerTextModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Qwen2_5OmniThinkerForConditionalGeneration": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_audio_features": [
      "self",
      "input_features",
      "feature_attention_mask",
      "audio_feature_lengths"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask",
      "feature_attention_mask",
      "audio_feature_lengths",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "rope_deltas",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "use_audio_in_video",
      "cache_position",
      "video_second_per_grid"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "input_features",
      "feature_attention_mask",
      "use_audio_in_video",
      "video_second_per_grid",
      "is_first_iteration"
    ]
  },
  "Qwen2_5OmniTalkerCausalLMOutputWithPast": {},
  "Qwen2_5OmniTalkerModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Qwen2_5OmniTalkerForConditionalGeneration": {
    "base_model_prefix": [],
    "output_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "thinker_reply_part",
      "inputs_embeds",
      "rope_deltas",
      "use_cache",
      "cache_position",
      "input_text_ids",
      "image_grid_thw",
      "video_grid_thw",
      "use_audio_in_video",
      "audio_feature_lengths",
      "video_second_per_grid",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "_get_initial_cache_position": [
      "self",
      "seq_length",
      "device",
      "model_kwargs"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "input_text_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "thinker_reply_part",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "input_audio_features",
      "audio_feature_attention_mask",
      "audio_feature_lengths",
      "use_audio_in_video",
      "video_second_per_grid"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ]
  },
  "Qwen2_5OmniDiTRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "TimeDelayNetBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Res2NetBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "scale",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SqueezeExcitationBlock": {
    "__init__": [
      "self",
      "in_channels",
      "se_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AttentiveStatisticsPooling": {
    "__init__": [
      "self",
      "channels",
      "attention_channels"
    ],
    "_length_to_mask": [
      "self",
      "length",
      "max_len",
      "dtype",
      "device"
    ],
    "_compute_statistics": [
      "self",
      "x",
      "m",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SqueezeExcitationRes2NetBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "res2net_scale",
      "se_channels",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ECAPA_TimeDelayNet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DiTInputEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "speaker_embedding",
      "condition_vector",
      "code_embed",
      "drop_audio_cond",
      "code_embed_uncond",
      "apply_cfg"
    ]
  },
  "DiTCodecEmbedding": {
    "__init__": [
      "self",
      "codec_num_embeds",
      "codec_dim",
      "repeats"
    ],
    "forward": [
      "self",
      "code",
      "drop_code"
    ]
  },
  "Qwen2_5_OmniAdaLayerNormZero": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "emb"
    ]
  },
  "Qwen2_5_OmniAdaLayerNormZero_Final": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "emb"
    ]
  },
  "DiTMLP": {
    "__init__": [
      "self",
      "dim",
      "mult",
      "dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DiTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "SinusPositionEmbedding": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "scale"
    ]
  },
  "DiTTimestepEmbedding": {
    "__init__": [
      "self",
      "dim",
      "freq_embed_dim"
    ],
    "forward": [
      "self",
      "timestep"
    ]
  },
  "DiTDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "look_ahead_block",
      "look_backward_block"
    ],
    "forward": [
      "self",
      "hidden_states",
      "timestep",
      "position_embeddings",
      "block_diff"
    ]
  },
  "SnakeBeta": {
    "__init__": [
      "self",
      "in_features",
      "alpha"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UpSample1d": {
    "__init__": [
      "self",
      "ratio",
      "kernel_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DownSample1d": {
    "__init__": [
      "self",
      "ratio",
      "kernel_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TorchActivation1d": {
    "__init__": [
      "self",
      "activation",
      "up_ratio",
      "down_ratio",
      "up_kernel_size",
      "down_kernel_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AMPBlock": {
    "__init__": [
      "self",
      "channels",
      "kernel_size",
      "dilation"
    ],
    "_get_padding": [
      "self",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen2_5OmniToken2WavBigVGANModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "normalize_spectrogram": [
      "self",
      "spectrogram",
      "max_value",
      "min_db"
    ],
    "amplitude_to_db": [
      "self",
      "amplitude",
      "min_db_level"
    ],
    "process_mel_spectrogram": [
      "self",
      "mel_spectrogram"
    ],
    "forward": [
      "self",
      "mel_spectrogram"
    ]
  },
  "RungeKutta4ODESolver": {
    "__init__": [
      "self",
      "function",
      "initial_value"
    ],
    "_rk4_step": [
      "self",
      "function",
      "time_start",
      "time_step",
      "time_end",
      "value_start",
      "function_value_start"
    ],
    "_compute_step": [
      "self",
      "function",
      "time_start",
      "time_step",
      "time_end",
      "value_start"
    ],
    "_linear_interpolation": [
      "self",
      "time_start",
      "time_end",
      "value_start",
      "value_end",
      "time_point"
    ],
    "integrate": [
      "self",
      "time_points"
    ]
  },
  "Qwen2_5OmniToken2WavDiTModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "_create_block_diff": [
      "self",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states",
      "condition_vector",
      "speaker_embedding",
      "quantized_code",
      "time_step",
      "drop_audio_conditioning",
      "drop_code",
      "apply_cfg"
    ],
    "sample": [
      "self",
      "conditioning_vector",
      "reference_mel_spectrogram",
      "quantized_code",
      "num_steps",
      "guidance_scale",
      "sway_coefficient"
    ]
  },
  "Qwen2_5OmniToken2WavModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "code",
      "conditioning",
      "reference_mel",
      "num_steps",
      "guidance_scale",
      "sway_coefficient"
    ]
  },
  "Qwen2_5OmniForConditionalGeneration": {
    "output_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "enable_talker": [
      "self"
    ],
    "load_speakers": [
      "self",
      "path"
    ],
    "disable_talker": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "generate": [
      "self",
      "input_ids",
      "speaker",
      "use_audio_in_video",
      "thinker_max_new_tokens",
      "talker_max_new_tokens",
      "talker_do_sample",
      "talker_top_k",
      "talker_top_p",
      "talker_temperature",
      "talker_eos_token_id",
      "talker_repetition_penalty"
    ]
  },
  "_two_dim_matmul": [
    "x",
    "matrix_dim_one",
    "matrix_dim_two"
  ],
  "two_dim_matmul": [
    "x",
    "matrix_dim_one",
    "matrix_dim_two"
  ],
  "fftn": [
    "x"
  ],
  "FNetEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "FNetBasicFourierTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "_init_fourier_transform": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FNetBasicOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "FNetFourierTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FNetIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FNetOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "FNetLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "feed_forward_chunk": [
      "self",
      "fourier_output"
    ]
  },
  "FNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FNetPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FNetLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FNetOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "FNetOnlyNSPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "FNetPreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "FNetPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "FNetForPreTrainingOutput": {},
  "FNetModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "next_sentence_label",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetForNextSentencePrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FNetConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "use_tpu_fourier_optimizations",
      "tpu_short_seq_length",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "FNetTokenizer": {
    "model_input_names": []
  },
  "FNetTokenizerFast": [],
  "PaddleOCRProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features",
      "image_grid_thw"
    ]
  },
  "PaddleOCRVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "PaddleOCRRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PaddleOCRMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PaddleOCRAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PaddleOCRRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PaddleOCRDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PaddleOCRVLPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PaddleOCRTextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "PaddleOCRVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ]
  },
  "PaddleOCRVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "position_embeddings"
    ]
  },
  "PaddleOCRVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PaddleOCRVisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "position_embeddings"
    ]
  },
  "PaddleOCRVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "cu_seqlens",
      "attention_mask",
      "image_grid_thw"
    ]
  },
  "PaddleOCRVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "cu_seqlens",
      "attention_mask",
      "image_grid_thw"
    ]
  },
  "PaddleOCRVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "cu_seqlens",
      "image_grid_thw"
    ]
  },
  "PaddleOCRVLModelOutputWithPast": {},
  "PaddleOCRVLCausalLMOutputWithPast": {},
  "PaddleOCRVLModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "pixel_values",
      "image_grid_thw",
      "rope_deltas",
      "cache_position"
    ]
  },
  "PaddleOCRVLForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "pixel_values",
      "image_grid_thw",
      "rope_deltas",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "PaddleOCRVLImageProcessorFast": {
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "min_pixels",
      "max_pixels",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ]
  },
  "PaddleOCRVLProcessorKwargs": {
    "_defaults": []
  },
  "PaddleOCRVLProcessor": {
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ]
  },
  "PaddleOCRVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "spatial_merge_size"
    ]
  },
  "PaddleOCRTextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "use_bias",
      "head_dim"
    ]
  },
  "PaddleOCRVLConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "vision_start_token_id",
      "vision_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "PaddleOCRVLImageProcessorKwargs": {},
  "PaddleOCRVLImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "min_pixels",
      "max_pixels",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "min_pixels",
      "max_pixels",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "XmodEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "XmodSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "XmodCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "XmodSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "XmodAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "XmodIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XmodAdapter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XmodOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor",
      "lang_ids"
    ],
    "lang_adapter": [
      "self",
      "lang_ids",
      "hidden_states"
    ]
  },
  "XmodLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "lang_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "XmodEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "lang_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "XmodPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XmodPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "set_default_language": [
      "self",
      "language"
    ],
    "freeze_embeddings_and_language_adapters": [
      "self"
    ]
  },
  "XmodModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "lang_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "XmodForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "lang_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "XmodForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "lang_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "XmodLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "XmodForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "lang_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "XmodForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "lang_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "XmodForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "lang_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "XmodClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "XmodForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "lang_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "XmodConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "classifier_dropout",
      "pre_norm",
      "adapter_reduction_factor",
      "adapter_layer_norm",
      "adapter_reuse_layer_norm",
      "ln_before_adapter",
      "languages",
      "default_language",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "LlavaNextProcessorKwargs": {
    "_defaults": []
  },
  "LlavaNextProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "patch_size",
      "vision_feature_select_strategy",
      "chat_template",
      "image_token",
      "num_additional_image_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_number_of_features": [
      "self",
      "orig_height",
      "orig_width",
      "height",
      "width"
    ],
    "_get_unpadded_features": [
      "self",
      "height",
      "width",
      "patches_height",
      "patches_width",
      "scale_height",
      "scale_width"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "LlavaNextImageProcessorFast": {
    "model_input_names": [],
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_pad": [],
    "image_grid_pinpoints": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_resize_for_patching": [
      "self",
      "image",
      "target_resolution",
      "interpolation",
      "input_data_format"
    ],
    "_get_padding_size": [
      "self",
      "original_resolution",
      "target_resolution"
    ],
    "_pad_for_patching": [
      "self",
      "image",
      "target_resolution",
      "input_data_format"
    ],
    "_get_image_patches": [
      "self",
      "image",
      "grid_pinpoints",
      "size",
      "patch_size",
      "interpolation"
    ],
    "_pad_for_batching": [
      "self",
      "pixel_values"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "image_grid_pinpoints",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "LlavaNextImageProcessorKwargs": {},
  "expand_to_square": [
    "image",
    "background_color",
    "input_data_format"
  ],
  "LlavaNextImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "image_grid_pinpoints",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "image",
      "padding",
      "mode",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_resize_for_patching": [
      "self",
      "image",
      "target_resolution",
      "resample",
      "input_data_format"
    ],
    "_get_padding_size": [
      "self",
      "original_resolution",
      "target_resolution"
    ],
    "_pad_for_patching": [
      "self",
      "image",
      "target_resolution",
      "input_data_format"
    ],
    "get_image_patches": [
      "self",
      "image",
      "grid_pinpoints",
      "size",
      "patch_size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_pad_for_batching": [
      "self",
      "pixel_values",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "image_grid_pinpoints",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "LlavaNextConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "projector_hidden_act",
      "vision_feature_select_strategy",
      "vision_feature_layer",
      "image_grid_pinpoints",
      "tie_word_embeddings",
      "image_seq_length",
      "multimodal_projector_bias"
    ]
  },
  "get_anyres_image_grid_shape": [
    "image_size",
    "grid_pinpoints",
    "patch_size"
  ],
  "image_size_to_num_patches": [
    "image_size",
    "grid_pinpoints",
    "patch_size"
  ],
  "unpad_image": [
    "tensor",
    "original_size"
  ],
  "LlavaNextModelOutputWithPast": {},
  "LlavaNextCausalLMOutputWithPast": {},
  "LlavaNextMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "LlavaNextPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LlavaNextModel": {
    "_checkpoint_conversion_mapping": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "pack_image_features": [
      "self",
      "image_features",
      "image_sizes",
      "vision_feature_select_strategy",
      "image_newline"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "output_hidden_states"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "LlavaNextForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "pack_image_features": [
      "self",
      "image_features",
      "image_sizes",
      "vision_feature_select_strategy",
      "image_newline"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer",
      "vision_feature_select_strategy"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "image_sizes",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "GraniteMoeSharedMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteMoeSharedRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GraniteMoeSharedParallelExperts": {
    "__init__": [
      "self",
      "num_experts",
      "input_size",
      "output_size"
    ],
    "forward": [
      "self",
      "inputs",
      "expert_size"
    ]
  },
  "GraniteMoeSharedTopKGating": {
    "__init__": [
      "self",
      "input_size",
      "num_experts",
      "top_k"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteMoeSharedMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "layer_input"
    ]
  },
  "GraniteMoeSharedAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "GraniteMoeSharedDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GraniteMoeSharedPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GraniteMoeSharedRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "GraniteMoeSharedModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "GraniteMoeSharedForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GraniteMoeSharedConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "embedding_multiplier",
      "logits_scaling",
      "residual_multiplier",
      "attention_multiplier",
      "num_local_experts",
      "num_experts_per_tok",
      "output_router_logits",
      "router_aux_loss_coef",
      "shared_intermediate_size"
    ]
  },
  "MLCDVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_groups",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "MLCDMLP": {},
  "MLCDRotaryEmbedding": {
    "forward": [
      "self",
      "num_patches_height",
      "num_patches_width"
    ]
  },
  "MLCDVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "MLCDAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "MLCDEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "MLCDEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "MLCDPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "accepts_loss_kwargs": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MLCDVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "MLCDVisionModel": {
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "LSHSelfAttentionOutput": [],
  "LocalSelfAttentionOutput": [],
  "AttentionOutput": [],
  "ReformerOutput": [],
  "ReformerBackwardOutput": [],
  "ReformerEncoderOutput": [],
  "ReformerDynamicCache": {
    "__init__": [
      "self",
      "_distributed_cache_data"
    ],
    "__len__": [
      "self"
    ],
    "update": [
      "self",
      "buckets",
      "states",
      "layer_idx",
      "cache_kwargs"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "get_start_idx": [
      "self"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ]
  },
  "_stable_argsort": [
    "vector",
    "dim"
  ],
  "_get_least_common_mult_chunk_len": [
    "config"
  ],
  "_get_min_chunk_len": [
    "config"
  ],
  "AxialPositionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "position_ids"
    ]
  },
  "PositionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "position_ids"
    ]
  },
  "ReformerEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds",
      "start_idx_pos_encodings"
    ]
  },
  "EfficientAttentionMixin": {
    "_look_adjacent": [
      "self",
      "vectors",
      "num_chunks_before",
      "num_chunks_after"
    ],
    "_split_hidden_size_dim": [
      "self",
      "x",
      "num_attn_heads",
      "attn_head_size"
    ],
    "_merge_hidden_size_dims": [
      "self",
      "x",
      "num_attn_heads",
      "attn_head_size"
    ],
    "_split_seq_length_dim_to": [
      "self",
      "vectors",
      "dim_factor_1",
      "dim_factor_2",
      "num_attn_heads",
      "attn_head_size"
    ]
  },
  "LSHSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "num_hashes",
      "buckets",
      "past_buckets_states",
      "use_cache",
      "output_attentions",
      "cache_position"
    ],
    "_query_per_attn_head": [
      "self",
      "hidden_states"
    ],
    "_value_per_attn_head": [
      "self",
      "hidden_states"
    ],
    "_hash_vectors": [
      "self",
      "vectors",
      "num_hashes",
      "attention_mask",
      "increase_num_buckets"
    ],
    "_get_sorted_bucket_idx_and_undo_sorted_bucket_idx": [
      "self",
      "sequence_length",
      "buckets",
      "num_hashes"
    ],
    "_set_num_buckets": [
      "self",
      "sequence_length"
    ],
    "_attend": [
      "self",
      "query_vectors",
      "key_vectors",
      "value_vectors",
      "sorted_bucket_idx_per_hash",
      "attention_mask",
      "do_standard_self_attention",
      "use_cache"
    ],
    "_compute_attn_mask": [
      "self",
      "query_indices",
      "key_indices",
      "attention_mask",
      "query_key_dot_shape",
      "do_standard_self_attention"
    ],
    "_get_relevant_hid_states_and_buckets": [
      "self",
      "query_vectors",
      "attention_mask",
      "num_hashes",
      "hidden_states",
      "past_states",
      "past_buckets"
    ],
    "_expand_to_indices_in_relevant_chunk": [
      "self",
      "indices",
      "sequence_length"
    ],
    "_len_and_dim_norm": [
      "self",
      "vectors",
      "sqrt_num"
    ],
    "_len_norm": [
      "self",
      "x",
      "epsilon"
    ],
    "_gather_by_expansion": [
      "self",
      "vectors",
      "idxs",
      "num_hashes"
    ]
  },
  "ReverseSort": {
    "forward": [
      "ctx",
      "out_vectors",
      "logits",
      "sorted_bucket_idx",
      "undo_sorted_bucket_idx"
    ],
    "backward": [
      "ctx",
      "grad_out_vectors",
      "grad_logits"
    ]
  },
  "LocalSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_buckets_states",
      "use_cache",
      "output_attentions"
    ],
    "_compute_attn_mask": [
      "self",
      "query_indices",
      "key_indices",
      "attention_mask",
      "query_key_dots_shape",
      "do_standard_self_attention"
    ],
    "_retrieve_relevant_hidden_states": [
      "previous_hidden_states",
      "chunk_length",
      "num_chunks_before"
    ]
  },
  "ReformerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ReformerAttention": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "num_hashes",
      "past_buckets_states",
      "use_cache",
      "orig_sequence_length",
      "output_attentions",
      "buckets",
      "cache_position"
    ]
  },
  "ReformerFeedForwardDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ReformerFeedForwardOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChunkReformerFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "attention_output"
    ],
    "forward_chunk": [
      "self",
      "hidden_states"
    ]
  },
  "ReformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "_init_attention_seed": [
      "self"
    ],
    "_init_feed_forward_seed": [
      "self"
    ],
    "forward": [
      "self",
      "prev_attn_output",
      "hidden_states",
      "attention_mask",
      "num_hashes",
      "past_buckets_states",
      "use_cache",
      "orig_sequence_length",
      "output_attentions"
    ],
    "backward_pass": [
      "self",
      "next_attn_output",
      "hidden_states",
      "grad_attn_output",
      "grad_hidden_states",
      "attention_mask",
      "buckets"
    ]
  },
  "_ReversibleFunction": {
    "forward": [
      "ctx",
      "hidden_states",
      "layers",
      "attention_mask",
      "num_hashes",
      "all_hidden_states",
      "all_attentions",
      "past_buckets_states",
      "use_cache",
      "orig_sequence_length",
      "output_hidden_states",
      "output_attentions"
    ],
    "backward": [
      "ctx",
      "grad_hidden_states"
    ]
  },
  "ReformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "num_hashes",
      "past_buckets_states",
      "use_cache",
      "orig_sequence_length",
      "output_hidden_states",
      "output_attentions"
    ]
  },
  "ReformerOnlyLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "forward_chunk": [
      "self",
      "hidden_states"
    ]
  },
  "ReformerPreTrainedModel": {
    "base_model_prefix": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ReformerModelOutput": {},
  "ReformerModelWithLMHeadOutput": {},
  "ReformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "num_hashes",
      "past_buckets_states",
      "use_cache",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ],
    "_pad_to_mult_of_chunk_length": [
      "self",
      "input_ids",
      "inputs_embeds",
      "attention_mask",
      "position_ids",
      "input_shape",
      "padding_length",
      "padded_seq_length",
      "device"
    ]
  },
  "ReformerModelWithLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "inputs_embeds",
      "num_hashes",
      "past_buckets_states",
      "use_cache",
      "output_hidden_states",
      "output_attentions",
      "return_dict",
      "labels",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "use_cache",
      "num_hashes",
      "is_first_iteration"
    ]
  },
  "ReformerForMaskedLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "inputs_embeds",
      "num_hashes",
      "labels",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "ReformerForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "inputs_embeds",
      "num_hashes",
      "labels",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "ReformerClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ReformerForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "inputs_embeds",
      "num_hashes",
      "start_positions",
      "end_positions",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "ReformerConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "attention_head_size",
      "attn_layers",
      "axial_norm_std",
      "axial_pos_embds",
      "axial_pos_shape",
      "axial_pos_embds_dim",
      "chunk_size_lm_head",
      "eos_token_id",
      "feed_forward_size",
      "hash_seed",
      "hidden_act",
      "hidden_dropout_prob",
      "hidden_size",
      "initializer_range",
      "is_decoder",
      "layer_norm_eps",
      "local_num_chunks_before",
      "local_num_chunks_after",
      "local_attention_probs_dropout_prob",
      "local_attn_chunk_length",
      "lsh_attn_chunk_length",
      "lsh_attention_probs_dropout_prob",
      "lsh_num_chunks_before",
      "lsh_num_chunks_after",
      "max_position_embeddings",
      "num_attention_heads",
      "num_buckets",
      "num_hashes",
      "pad_token_id",
      "vocab_size",
      "tie_word_embeddings",
      "use_cache",
      "classifier_dropout",
      "bos_token_id"
    ]
  },
  "ReformerTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "eos_token",
      "unk_token",
      "additional_special_tokens"
    ]
  },
  "LwDetrViTConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "hidden_act",
      "dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "pretrain_image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "window_block_indices",
      "use_absolute_position_embeddings",
      "out_features",
      "out_indices",
      "cae_init_values",
      "num_windows"
    ]
  },
  "LwDetrConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "projector_scale_factors",
      "hidden_expansion",
      "c2f_num_blocks",
      "activation_function",
      "batch_norm_eps",
      "d_model",
      "dropout",
      "decoder_ffn_dim",
      "decoder_n_points",
      "decoder_layers",
      "decoder_self_attention_heads",
      "decoder_cross_attention_heads",
      "decoder_activation_function",
      "num_queries",
      "attention_bias",
      "attention_dropout",
      "activation_dropout",
      "group_detr",
      "init_std",
      "disable_custom_kernels",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "mask_loss_coefficient",
      "dice_loss_coefficient",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "eos_coefficient",
      "focal_alpha",
      "auxiliary_loss"
    ]
  },
  "LwDetrViTSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrViTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrViTMlp": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "hidden_features"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LwDetrViTLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrViTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrViTEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "get_absolute_positions": [
      "self",
      "abs_pos_embeddings",
      "has_cls_token",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "LwDetrViTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LwDetrViTBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "LwDetrConvNormLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LwDetrRepVggBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LwDetrC2FLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "LwDetrSamplingLayer": {
    "__init__": [
      "self",
      "config",
      "channel_size",
      "scale"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrScaleProjector": {
    "__init__": [
      "self",
      "config",
      "scale"
    ],
    "forward": [
      "self",
      "hidden_states_tuple"
    ]
  },
  "LwDetrMultiScaleProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "LwDetrAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings"
    ]
  },
  "LwDetrMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "config",
      "num_heads",
      "n_points"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index"
    ]
  },
  "LwDetrMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LwDetrDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "LwDetrPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LwDetrDecoderOutput": {},
  "LwDetrDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_reference": [
      "self",
      "reference_points",
      "valid_ratios"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "LwDetrModelOutput": {},
  "refine_bboxes": [
    "reference_points",
    "deltas"
  ],
  "LwDetrModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "get_valid_ratio": [
      "self",
      "mask",
      "dtype"
    ],
    "get_proposal_pos_embed": [
      "self",
      "proposals"
    ],
    "gen_encoder_output_proposals": [
      "self",
      "enc_output",
      "padding_mask",
      "spatial_shapes"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "LwDetrMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LwDetrObjectDetectionOutput": {},
  "LwDetrForObjectDetection": {
    "_no_split_modules": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "labels"
    ]
  },
  "DPTImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "rescale_factor": [],
    "ensure_multiple_of": [],
    "keep_aspect_ratio": [],
    "crop_size": [],
    "do_center_crop": [],
    "do_reduce_labels": [],
    "valid_kwargs": [],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias",
      "ensure_multiple_of",
      "keep_aspect_ratio"
    ],
    "pad_image": [
      "self",
      "image",
      "size_divisor"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_reduce_labels",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "do_pad",
      "size_divisor",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "BaseModelOutputWithIntermediateActivations": {},
  "BaseModelOutputWithPoolingAndIntermediateActivations": {},
  "DPTViTHybridEmbeddings": {
    "__init__": [
      "self",
      "config",
      "feature_size"
    ],
    "_resize_pos_embed": [
      "self",
      "posemb",
      "grid_size_height",
      "grid_size_width",
      "start_index"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "DPTViTEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "_resize_pos_embed": [
      "self",
      "posemb",
      "grid_size_height",
      "grid_size_width",
      "start_index"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DPTViTPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DPTSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTViTSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "DPTViTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTViTIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTViTOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "DPTViTLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTViTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states"
    ]
  },
  "DPTReassembleStage": {
    "__init__": [
      "self",
      "config"
    ],
    "_init_reassemble_dpt_hybrid": [
      "self",
      "config"
    ],
    "_init_reassemble_dpt": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "_get_backbone_hidden_size": [
    "config"
  ],
  "DPTReassembleLayer": {
    "__init__": [
      "self",
      "config",
      "channels",
      "factor"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DPTFeatureFusionStage": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTPreActResidualLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DPTFeatureFusionLayer": {
    "__init__": [
      "self",
      "config",
      "align_corners"
    ],
    "forward": [
      "self",
      "hidden_state",
      "residual"
    ]
  },
  "DPTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DPTModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "DPTViTPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "DPTDepthEstimationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTForDepthEstimation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states"
    ]
  },
  "DPTSemanticSegmentationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTAuxiliaryHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DPTForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states"
    ]
  },
  "DPTImageProcessorKwargs": {},
  "DPTImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "do_reduce_labels"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "pad_image": [
      "self",
      "image",
      "size_divisor",
      "data_format",
      "input_data_format"
    ],
    "reduce_label": [
      "self",
      "label"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_reduce_labels",
      "do_resize",
      "size",
      "resample",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_segmentation_map": [
      "self",
      "segmentation_map",
      "do_resize",
      "size",
      "resample",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "do_reduce_labels",
      "input_data_format"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "do_resize",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "DPTConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "is_hybrid",
      "qkv_bias",
      "backbone_out_indices",
      "readout_type",
      "reassemble_factors",
      "neck_hidden_sizes",
      "fusion_hidden_size",
      "head_in_index",
      "use_batch_norm_in_fusion_residual",
      "use_bias_in_fusion_residual",
      "add_projection",
      "use_auxiliary_head",
      "auxiliary_loss_weight",
      "semantic_loss_ignore_index",
      "semantic_classifier_dropout",
      "backbone_featmap_shape",
      "neck_ignore_stages",
      "backbone_config",
      "pooler_output_size",
      "pooler_act"
    ]
  },
  "DPRContextEncoderTokenizerFast": {
    "vocab_files_names": [],
    "slow_tokenizer_class": []
  },
  "DPRQuestionEncoderTokenizerFast": {
    "vocab_files_names": [],
    "slow_tokenizer_class": []
  },
  "DPRSpanPrediction": [],
  "DPRReaderOutput": [],
  "CUSTOM_DPR_READER_DOCSTRING": [],
  "CustomDPRReaderTokenizerMixin": {
    "__call__": [
      "self",
      "questions",
      "titles",
      "texts",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "return_attention_mask"
    ],
    "decode_best_spans": [
      "self",
      "reader_input",
      "reader_output",
      "num_spans",
      "max_answer_length",
      "num_spans_per_passage"
    ],
    "_get_best_spans": [
      "self",
      "start_logits",
      "end_logits",
      "max_answer_length",
      "top_spans"
    ]
  },
  "DPRReaderTokenizerFast": {
    "vocab_files_names": [],
    "model_input_names": [],
    "slow_tokenizer_class": []
  },
  "DPRConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "projection_dim",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "DPRContextEncoderTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self"
    ]
  },
  "DPRQuestionEncoderTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self"
    ]
  },
  "DPRReaderTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ]
  },
  "DPRContextEncoderOutput": {},
  "DPRQuestionEncoderOutput": {},
  "DPRPreTrainedModel": {
    "_supports_sdpa": []
  },
  "DPREncoder": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "embeddings_size": [
      "self"
    ]
  },
  "DPRSpanPredictor": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DPRPretrainedContextEncoder": {
    "base_model_prefix": []
  },
  "DPRPretrainedQuestionEncoder": {
    "base_model_prefix": []
  },
  "DPRPretrainedReader": {
    "base_model_prefix": []
  },
  "DPRContextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DPRQuestionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DPRReader": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DebertaTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "errors",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space"
    ],
    "mask_token": [
      "self",
      "value"
    ]
  },
  "DebertaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "relative_attention",
      "max_relative_positions",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "position_biased_input",
      "pos_att_type",
      "pooler_dropout",
      "pooler_hidden_act",
      "legacy",
      "tie_word_embeddings"
    ]
  },
  "DebertaLayerNorm": {
    "__init__": [
      "self",
      "size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DebertaSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "build_relative_position": [
    "query_layer",
    "key_layer"
  ],
  "c2p_dynamic_expand": [
    "c2p_pos",
    "query_layer",
    "relative_pos"
  ],
  "p2c_dynamic_expand": [
    "c2p_pos",
    "query_layer",
    "key_layer"
  ],
  "pos_dynamic_expand": [
    "pos_index",
    "p2c_att",
    "key_layer"
  ],
  "scaled_size_sqrt": [
    "query_layer",
    "scale_factor"
  ],
  "build_rpos": [
    "query_layer",
    "key_layer",
    "relative_pos"
  ],
  "compute_attention_span": [
    "query_layer",
    "key_layer",
    "max_relative_positions"
  ],
  "uneven_size_corrected": [
    "p2c_att",
    "query_layer",
    "key_layer",
    "relative_pos"
  ],
  "DisentangledSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "transpose_for_scores": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "query_states",
      "relative_pos",
      "rel_embeddings"
    ],
    "disentangled_att_bias": [
      "self",
      "query_layer",
      "key_layer",
      "relative_pos",
      "rel_embeddings",
      "scale_factor"
    ]
  },
  "DebertaEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "mask",
      "inputs_embeds"
    ]
  },
  "DebertaAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "query_states",
      "relative_pos",
      "rel_embeddings"
    ]
  },
  "DebertaIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DebertaOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "DebertaLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "query_states",
      "relative_pos",
      "rel_embeddings",
      "output_attentions"
    ]
  },
  "DebertaEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_rel_embedding": [
      "self"
    ],
    "get_attention_mask": [
      "self",
      "attention_mask"
    ],
    "get_rel_pos": [
      "self",
      "hidden_states",
      "query_states",
      "relative_pos"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_hidden_states",
      "output_attentions",
      "query_states",
      "relative_pos",
      "return_dict"
    ]
  },
  "DebertaPreTrainedModel": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DebertaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LegacyDebertaPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LegacyDebertaLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LegacyDebertaOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "DebertaLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "word_embeddings"
    ]
  },
  "DebertaOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "word_embeddings"
    ]
  },
  "DebertaForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ContextPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "output_dim": [
      "self"
    ]
  },
  "DebertaForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DebertaForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DebertaForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTSw3Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "is_fast": [],
    "__init__": [
      "self",
      "vocab_file",
      "do_lower_case",
      "remove_space",
      "keep_accents",
      "pad_token",
      "unk_token",
      "eos_token",
      "bos_token",
      "sp_model_kwargs"
    ],
    "preprocess_text": [
      "self",
      "text"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "encode_fast": [
      "self",
      "text",
      "return_tensors"
    ],
    "decode_fast": [
      "self",
      "token_ids"
    ]
  },
  "BlipImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ]
  },
  "BlipProcessorKwargs": {
    "_defaults": []
  },
  "BlipProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "BlipTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "BlipTextSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "layer_idx"
    ],
    "save_attn_gradients": [
      "self",
      "attn_gradients"
    ],
    "get_attn_gradients": [
      "self"
    ],
    "save_attention_map": [
      "self",
      "attention_map"
    ],
    "get_attention_map": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "BlipTextSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BlipTextAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "BlipTextIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BlipTextOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BlipTextLayer": {
    "__init__": [
      "self",
      "config",
      "layer_num"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "BlipTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BlipTextPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BlipTextPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BlipTextLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BlipTextOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "BlipTextPreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BlipTextModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_extended_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "device",
      "is_decoder"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "encoder_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "is_decoder",
      "cache_position"
    ]
  },
  "BlipTextLMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "return_logits",
      "is_decoder",
      "reduction",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask"
    ]
  },
  "BlipTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "encoder_hidden_size",
      "intermediate_size",
      "projection_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "sep_token_id",
      "is_decoder",
      "use_cache",
      "label_smoothing"
    ]
  },
  "BlipVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "projection_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range"
    ]
  },
  "BlipConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value",
      "image_text_hidden_size",
      "label_smoothing",
      "tie_word_embeddings"
    ]
  },
  "BlipImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": []
  },
  "blip_loss": [
    "similarity"
  ],
  "BlipForConditionalGenerationModelOutput": {},
  "BlipTextVisionModelOutput": {},
  "BlipImageTextMatchingModelOutput": {},
  "BlipOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "BlipVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "BlipAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BlipMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BlipEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BlipPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BlipEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds"
    ]
  },
  "BlipVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_input_embeddings": [
      "self"
    ]
  },
  "BlipModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_multimodal_features": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "interpolate_pos_encoding"
    ]
  },
  "BlipForConditionalGeneration": {
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "attention_mask",
      "labels",
      "interpolate_pos_encoding",
      "logits_to_keep"
    ],
    "generate": [
      "self",
      "pixel_values",
      "input_ids",
      "attention_mask",
      "interpolate_pos_encoding"
    ]
  },
  "BlipForQuestionAnswering": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "decoder_input_ids",
      "decoder_attention_mask",
      "attention_mask",
      "labels",
      "interpolate_pos_encoding"
    ],
    "generate": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "interpolate_pos_encoding"
    ]
  },
  "BlipForImageTextRetrieval": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "use_itm_head",
      "attention_mask",
      "interpolate_pos_encoding"
    ]
  },
  "MPNetPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MPNetEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ]
  },
  "MPNetSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "MPNetAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "MPNetIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MPNetOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MPNetLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "MPNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "compute_position_bias": [
      "self",
      "x",
      "position_ids",
      "num_buckets"
    ],
    "relative_position_bucket": [
      "relative_position",
      "num_buckets",
      "max_distance"
    ]
  },
  "MPNetPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MPNetModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MPNetForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MPNetLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MPNetForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MPNetForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MPNetForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MPNetClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MPNetForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MPNetTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "tokenize_chinese_chars",
      "strip_accents"
    ],
    "mask_token": [
      "self",
      "value"
    ]
  },
  "MPNetConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "relative_attention_num_buckets",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "PhimoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "layer_type"
    ]
  },
  "PhimoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "PhimoeMultiplier": {
    "forward": [
      "ctx",
      "scores",
      "multiplier",
      "selected_experts",
      "masked_gates",
      "mask_for_one"
    ],
    "backward": [
      "ctx",
      "grad_at_output"
    ]
  },
  "PhimoeExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "sparsemixer": [
    "scores",
    "jitter_eps",
    "training",
    "top_k"
  ],
  "PhimoeTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PhimoeSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PhimoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "PhimoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PhimoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "PhimoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "PhimoeForSequenceClassification": {},
  "PhimoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "sliding_window",
      "attention_dropout",
      "num_experts_per_tok",
      "num_local_experts",
      "output_router_logits",
      "router_aux_loss_coef",
      "router_jitter_noise",
      "input_jitter_noise",
      "attention_bias",
      "lm_head_bias"
    ],
    "validate_rope": [
      "self",
      "ignore_keys"
    ]
  },
  "InstructBlipVideoVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "qkv_bias"
    ]
  },
  "InstructBlipVideoQFormerConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "cross_attention_frequency",
      "encoder_hidden_size"
    ]
  },
  "InstructBlipVideoConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "qformer_config",
      "text_config",
      "num_query_tokens",
      "video_token_index"
    ]
  },
  "InstructBlipVideoPreTrainedModel": {
    "input_modalities": []
  },
  "InstructBlipVideoVisionModel": {
    "input_modalities": []
  },
  "InstructBlipVideoQFormerModel": {},
  "InstructBlipVideoForConditionalGenerationModelOutput": {},
  "InstructBlipVideoModel": {
    "forward": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "interpolate_pos_encoding",
      "use_cache"
    ]
  },
  "InstructBlipVideoForConditionalGeneration": {
    "get_video_features": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "interpolate_pos_encoding"
    ],
    "get_image_features": [],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict",
      "interpolate_pos_encoding",
      "use_cache"
    ],
    "generate": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "interpolate_pos_encoding"
    ]
  },
  "InstructBlipVideoVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": [],
    "model_input_names": [],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors"
    ]
  },
  "InstructBlipVideoProcessor": {
    "__init__": [
      "self",
      "video_processor",
      "tokenizer",
      "qformer_tokenizer",
      "num_query_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_token_type_ids",
      "return_length",
      "verbose",
      "return_tensors"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "InstructBlipVideoVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "InstructBlipVideoQFormerEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "query_embeds",
      "past_key_values_length"
    ]
  },
  "InstructBlipVideoAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipVideoMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipVideoEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipVideoEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds"
    ]
  },
  "InstructBlipVideoQFormerMultiHeadAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention"
    ],
    "save_attn_gradients": [
      "self",
      "attn_gradients"
    ],
    "get_attn_gradients": [
      "self"
    ],
    "save_attention_map": [
      "self",
      "attention_map"
    ],
    "get_attention_map": [
      "self"
    ],
    "transpose_for_scores": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "InstructBlipVideoQFormerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "InstructBlipVideoQFormerAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "InstructBlipVideoQFormerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipVideoQFormerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "InstructBlipVideoQFormerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "query_length"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ],
    "feed_forward_chunk_query": [
      "self",
      "attention_output"
    ]
  },
  "InstructBlipVideoQFormerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "query_length"
    ]
  },
  "BaseModelOutputWithVisionQformerOutputs": {},
  "LayoutLMv3Config": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_2d_position_embeddings",
      "coordinate_size",
      "shape_size",
      "has_relative_attention_bias",
      "rel_pos_bins",
      "max_rel_pos",
      "rel_2d_pos_bins",
      "max_rel_2d_pos",
      "has_spatial_attention_bias",
      "text_embed",
      "visual_embed",
      "input_size",
      "num_channels",
      "patch_size",
      "classifier_dropout"
    ]
  },
  "LayoutLMv3ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "apply_ocr": [],
    "ocr_lang": [],
    "tesseract_config": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "apply_ocr",
      "ocr_lang",
      "tesseract_config",
      "return_tensors",
      "disable_grouping"
    ]
  },
  "LAYOUTLMV3_ENCODE_KWARGS_DOCSTRING": [],
  "LAYOUTLMV3_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING": [],
  "LayoutLMv3Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "errors",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space",
      "cls_token_box",
      "sep_token_box",
      "pad_token_box",
      "pad_token_label",
      "only_label_first_subword",
      "vocab",
      "merges"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "encode_plus": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "LayoutLMv3TokenizerFast": [],
  "LayoutLMv3ImageProcessorKwargs": {},
  "LayoutLMv3ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "apply_ocr",
      "ocr_lang",
      "tesseract_config"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "apply_ocr",
      "ocr_lang",
      "tesseract_config",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "LayoutLMv3Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "return_tensors"
    ],
    "get_overflowing_images": [
      "self",
      "images",
      "overflow_to_sample_mapping"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "LayoutLMv3PatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "position_embedding"
    ]
  },
  "LayoutLMv3TextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "calculate_spatial_position_embeddings": [
      "self",
      "bbox"
    ],
    "create_position_ids_from_input_ids": [
      "self",
      "input_ids",
      "padding_idx"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "LayoutLMv3PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LayoutLMv3SelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "cogview_attention": [
      "self",
      "attention_scores",
      "alpha"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "rel_pos",
      "rel_2d_pos"
    ]
  },
  "LayoutLMv3SelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LayoutLMv3Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "rel_pos",
      "rel_2d_pos"
    ]
  },
  "LayoutLMv3Layer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "rel_pos",
      "rel_2d_pos"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "LayoutLMv3Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "relative_position_bucket": [
      "self",
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "_cal_1d_pos_emb": [
      "self",
      "position_ids"
    ],
    "_cal_2d_pos_emb": [
      "self",
      "bbox"
    ],
    "forward": [
      "self",
      "hidden_states",
      "bbox",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "position_ids",
      "patch_height",
      "patch_width"
    ]
  },
  "LayoutLMv3Intermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayoutLMv3Output": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LayoutLMv3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "create_visual_bbox": [
      "self",
      "image_size",
      "max_len"
    ],
    "calculate_visual_bbox": [
      "self",
      "device",
      "dtype",
      "batch_size"
    ],
    "forward_image": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMv3ClassificationHead": {
    "__init__": [
      "self",
      "config",
      "pool_feature"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LayoutLMv3ForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "pixel_values"
    ]
  },
  "LayoutLMv3ForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "bbox",
      "pixel_values"
    ]
  },
  "LayoutLMv3ForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "bbox",
      "pixel_values"
    ]
  },
  "GLPNImageProcessorKwargs": {},
  "GLPNImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor"
    ],
    "resize": [
      "self",
      "image",
      "size_divisor",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "GLPNConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "num_encoder_blocks",
      "depths",
      "sr_ratios",
      "hidden_sizes",
      "patch_sizes",
      "strides",
      "num_attention_heads",
      "mlp_ratios",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "drop_path_rate",
      "layer_norm_eps",
      "decoder_hidden_size",
      "max_depth",
      "head_in_index"
    ]
  },
  "GLPNDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GLPNOverlapPatchEmbeddings": {
    "__init__": [
      "self",
      "patch_size",
      "stride",
      "num_channels",
      "hidden_size"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "GLPNEfficientSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "sequence_reduction_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "GLPNSelfOutput": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "GLPNAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "sequence_reduction_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "GLPNDWConv": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width"
    ]
  },
  "GLPNMixFFN": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "hidden_features",
      "out_features"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width"
    ]
  },
  "GLPNLayer": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "drop_path",
      "sequence_reduction_ratio",
      "mlp_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "GLPNEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GLPNPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": []
  },
  "GLPNModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GLPNSelectiveFeatureFusion": {
    "__init__": [
      "self",
      "in_channel"
    ],
    "forward": [
      "self",
      "local_features",
      "global_features"
    ]
  },
  "GLPNDecoderStage": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "hidden_state",
      "residual"
    ]
  },
  "GLPNDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SiLogLoss": {
    "__init__": [
      "self",
      "lambd"
    ],
    "forward": [
      "self",
      "pred",
      "target"
    ]
  },
  "GLPNDepthEstimationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GLPNForDepthEstimation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GLPNImageProcessorFast": {
    "do_resize": [],
    "do_rescale": [],
    "rescale_factor": [],
    "resample": [],
    "size_divisor": [],
    "valid_kwargs": [],
    "_validate_preprocess_kwargs": [
      "self"
    ],
    "resize": [
      "self",
      "image",
      "size_divisor",
      "interpolation",
      "antialias"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size_divisor",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors",
      "resample"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "_prepare_4d_attention_mask_inverted": [
    "mask",
    "dtype",
    "tgt_len"
  ],
  "LEDLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length"
    ]
  },
  "LEDEncoderSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "is_index_masked",
      "is_index_global_attn",
      "is_global_attn",
      "output_attentions"
    ],
    "_pad_and_transpose_last_two_dims": [
      "hidden_states_padded",
      "padding"
    ],
    "_pad_and_diagonalize": [
      "chunked_hidden_states"
    ],
    "_chunk": [
      "hidden_states",
      "window_overlap",
      "onnx_export"
    ],
    "_mask_invalid_locations": [
      "input_tensor",
      "affected_seq_len"
    ],
    "_sliding_chunks_query_key_matmul": [
      "self",
      "query",
      "key",
      "window_overlap"
    ],
    "_sliding_chunks_matmul_attn_probs_value": [
      "self",
      "attn_probs",
      "value",
      "window_overlap"
    ],
    "_get_global_attn_indices": [
      "is_index_global_attn"
    ],
    "_concat_with_global_key_attn_probs": [
      "self",
      "key_vectors",
      "query_vectors",
      "max_num_global_attn_indices",
      "is_index_global_attn_nonzero",
      "is_local_index_global_attn_nonzero",
      "is_local_index_no_global_attn_nonzero"
    ],
    "_compute_attn_output_with_global_indices": [
      "self",
      "value_vectors",
      "attn_probs",
      "max_num_global_attn_indices",
      "is_index_global_attn_nonzero",
      "is_local_index_global_attn_nonzero"
    ],
    "_compute_global_attn_output_from_hidden": [
      "self",
      "hidden_states",
      "max_num_global_attn_indices",
      "is_local_index_global_attn_nonzero",
      "is_index_global_attn_nonzero",
      "is_local_index_no_global_attn_nonzero",
      "is_index_masked"
    ]
  },
  "LEDEncoderAttention": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "is_index_masked",
      "is_index_global_attn",
      "is_global_attn",
      "output_attentions"
    ]
  },
  "LEDDecoderAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "LEDEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "is_index_masked",
      "is_index_global_attn",
      "is_global_attn",
      "output_attentions"
    ]
  },
  "LEDDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "LEDClassificationHead": {
    "__init__": [
      "self",
      "input_dim",
      "inner_dim",
      "num_classes",
      "pooler_dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LEDPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LEDEncoderBaseModelOutput": {},
  "LEDSeq2SeqModelOutput": {},
  "LEDSeq2SeqLMOutput": {},
  "LEDSeq2SeqSequenceClassifierOutput": {},
  "LEDSeq2SeqQuestionAnsweringModelOutput": {},
  "LEDEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_merge_to_attention_mask": [
      "self",
      "attention_mask",
      "global_attention_mask"
    ],
    "_pad_to_window_size": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "pad_token_id"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "global_attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LEDDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "global_attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "LEDModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "global_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "LEDForConditionalGeneration": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "global_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "LEDForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "global_attention_mask",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LEDConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_encoder_position_embeddings",
      "max_decoder_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "classifier_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "attention_window",
      "tie_word_embeddings"
    ]
  },
  "OmDetTurboConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "text_config",
      "backbone_config",
      "apply_layernorm_after_vision_backbone",
      "image_size",
      "disable_custom_kernels",
      "layer_norm_eps",
      "batch_norm_eps",
      "init_std",
      "text_projection_in_dim",
      "text_projection_out_dim",
      "task_encoder_hidden_dim",
      "class_embed_dim",
      "class_distance_type",
      "num_queries",
      "csp_activation",
      "conv_norm_activation",
      "encoder_feedforward_activation",
      "encoder_feedforward_dropout",
      "encoder_dropout",
      "hidden_expansion",
      "vision_features_channels",
      "encoder_hidden_dim",
      "encoder_in_channels",
      "encoder_projection_indices",
      "encoder_attention_heads",
      "encoder_dim_feedforward",
      "encoder_layers",
      "positional_encoding_temperature",
      "num_feature_levels",
      "decoder_hidden_dim",
      "decoder_num_heads",
      "decoder_num_layers",
      "decoder_activation",
      "decoder_dim_feedforward",
      "decoder_num_points",
      "decoder_dropout",
      "eval_size",
      "learn_initial_query",
      "cache_size",
      "is_encoder_decoder"
    ]
  },
  "OmDetTurboEncoderOutput": {},
  "OmDetTurboDecoderOutput": {},
  "OmDetTurboObjectDetectionOutput": {},
  "OmDetTurboLRUCache": {
    "__init__": [
      "self",
      "capacity"
    ],
    "has": [
      "self",
      "key"
    ],
    "get": [
      "self",
      "key"
    ],
    "put": [
      "self",
      "key",
      "value"
    ]
  },
  "OmDetTurboLanguageBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "encode_type"
    ]
  },
  "OmDetTurboVisionBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "OmDetTurboMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "config",
      "num_heads",
      "n_points"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index"
    ]
  },
  "OmDetTurboConvNormLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "OmDetTurboRepVggBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OmDetTurboCSPRepLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "OmDetTurboMultiheadAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "values",
      "attention_mask",
      "output_attentions"
    ]
  },
  "OmDetTurboEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "with_pos_embed": [
      "tensor",
      "pos_embed"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "output_attentions"
    ]
  },
  "OmDetTurboEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "src",
      "src_mask",
      "pos_embed",
      "output_attentions"
    ]
  },
  "OmDetTurboHybridEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "build_2d_sincos_position_embedding": [
      "width",
      "height",
      "embed_dim",
      "temperature",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "inputs_embeddings",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OmDetTurboMLPWithDropout": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OmDetTurboMLP": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OmDetTurboResidualLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "y"
    ]
  },
  "OmDetTurboTaskEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OmDetTurboDeformableTransformerDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "with_pos_embed": [
      "tensor",
      "pos"
    ],
    "forward": [
      "self",
      "decoder_embeddings",
      "task_features",
      "reference_points",
      "vision_features",
      "vision_shapes",
      "vision_shapes_list",
      "level_start_index",
      "attention_mask",
      "padding_mask",
      "query_position",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "OmDetTurboPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "module",
      "value"
    ],
    "_get_cache_key_at_index": [
      "input_ids",
      "attention_mask",
      "index"
    ],
    "get_cached_class_embeddings": [
      "self",
      "classes_input_ids",
      "classes_attention_mask"
    ],
    "get_cached_task_embeddings": [
      "self",
      "tasks_input_ids",
      "tasks_attention_mask"
    ],
    "get_language_embedding": [
      "self",
      "classes_input_ids",
      "classes_attention_mask",
      "tasks_input_ids",
      "tasks_attention_mask",
      "classes_structure"
    ]
  },
  "_cosine_similarity_scaled": [
    "a",
    "b",
    "logit_scale"
  ],
  "get_class_similarity": [
    "class_distance_type",
    "cls_feature",
    "class_proj"
  ],
  "_inverse_sigmoid": [
    "x",
    "eps"
  ],
  "OmDetTurboDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "generate_anchors": [
      "self",
      "spatial_shapes",
      "grid_size",
      "device",
      "dtype"
    ],
    "_get_encoder_input": [
      "self",
      "vision_features"
    ],
    "_get_decoder_input": [
      "self",
      "vision_features",
      "vision_shapes",
      "class_features",
      "denoise_embeddings",
      "denoise_bboxes"
    ],
    "forward": [
      "self",
      "vision_features",
      "class_features",
      "task_features",
      "task_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OmDetTurboForObjectDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "forward": [
      "self",
      "pixel_values",
      "classes_input_ids",
      "classes_attention_mask",
      "tasks_input_ids",
      "tasks_attention_mask",
      "classes_structure",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OmDetTurboTextKwargs": {},
  "OmDetTurboProcessorKwargs": {
    "_defaults": []
  },
  "clip_boxes": [
    "box",
    "box_size"
  ],
  "compute_score": [
    "boxes"
  ],
  "_post_process_boxes_for_image": [
    "boxes",
    "scores",
    "labels",
    "image_num_classes",
    "image_size",
    "threshold",
    "nms_threshold",
    "max_num_det"
  ],
  "OmDetTurboProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "model_input_names": [
      "self"
    ],
    "_get_default_image_size": [
      "self"
    ],
    "post_process_grounded_object_detection": [
      "self",
      "outputs",
      "text_labels",
      "threshold",
      "nms_threshold",
      "target_sizes",
      "max_num_det"
    ]
  },
  "ConvNextV2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "patch_size",
      "num_stages",
      "hidden_sizes",
      "depths",
      "hidden_act",
      "initializer_range",
      "layer_norm_eps",
      "drop_path_rate",
      "image_size",
      "out_features",
      "out_indices"
    ]
  },
  "ConvNextV2DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ConvNextV2GRN": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ConvNextV2LayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "ConvNextV2Embeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "ConvNextV2Layer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "drop_path"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "ConvNextV2Stage": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "depth",
      "drop_path_rates"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "ConvNextV2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states"
    ]
  },
  "ConvNextV2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ConvNextV2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "ConvNextV2ForImageClassification": {
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "ConvNextV2Backbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "VisionTextDualEncoderProcessorKwargs": {
    "_defaults": []
  },
  "VisionTextDualEncoderProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "VisionTextDualEncoderModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "__init__": [
      "self",
      "config",
      "vision_model",
      "text_model"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "token_type_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "token_type_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "from_vision_text_pretrained": [
      "cls",
      "vision_model_name_or_path",
      "text_model_name_or_path"
    ]
  },
  "VISION_MODEL_CONFIGS": [],
  "VisionTextDualEncoderConfig": {
    "model_type": [],
    "sub_configs": [],
    "has_no_defaults_at_init": [],
    "__init__": [
      "self",
      "projection_dim",
      "logit_scale_init_value"
    ],
    "from_vision_text_configs": [
      "cls",
      "vision_config",
      "text_config"
    ]
  },
  "EntitySpan": [],
  "EntitySpanInput": [],
  "Entity": [],
  "EntityInput": [],
  "LukeTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "entity_vocab",
      "errors",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space",
      "task",
      "max_entity_length",
      "max_mention_length",
      "entity_token_1",
      "entity_token_2",
      "entity_unk_token",
      "entity_pad_token",
      "entity_mask_token",
      "entity_mask2_token"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "entity_spans",
      "entity_spans_pair",
      "entities",
      "entities_pair",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "max_entity_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "entity_spans",
      "entity_spans_pair",
      "entities",
      "entities_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "max_entity_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "batch_entity_spans_or_entity_spans_pairs",
      "batch_entities_or_entities_pairs",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "max_entity_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_check_entity_input_format": [
      "self",
      "entities",
      "entity_spans"
    ],
    "_create_input_sequence": [
      "self",
      "text",
      "text_pair",
      "entities",
      "entities_pair",
      "entity_spans",
      "entity_spans_pair"
    ],
    "_batch_prepare_for_model": [
      "self",
      "batch_ids_pairs",
      "batch_entity_ids_pairs",
      "batch_entity_token_spans_pairs",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "max_entity_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose"
    ],
    "prepare_for_model": [
      "self",
      "ids",
      "pair_ids",
      "entity_ids",
      "pair_entity_ids",
      "entity_token_spans",
      "pair_entity_token_spans",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "max_entity_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "prepend_batch_axis"
    ],
    "pad": [
      "self",
      "encoded_inputs",
      "padding",
      "max_length",
      "max_entity_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask",
      "return_tensors",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "max_entity_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ]
  },
  "BaseLukeModelOutputWithPooling": {},
  "BaseLukeModelOutput": {},
  "LukeMaskedLMOutput": {},
  "EntityClassificationOutput": {},
  "EntityPairClassificationOutput": {},
  "EntitySpanClassificationOutput": {},
  "LukeSequenceClassifierOutput": {},
  "LukeTokenClassifierOutput": {},
  "LukeQuestionAnsweringModelOutput": {},
  "LukeMultipleChoiceModelOutput": {},
  "LukeEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ]
  },
  "LukeEntityEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "entity_ids",
      "position_ids",
      "token_type_ids"
    ]
  },
  "LukeSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "transpose_for_scores": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "word_hidden_states",
      "entity_hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LukeSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LukeAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "word_hidden_states",
      "entity_hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LukeIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LukeOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LukeLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "word_hidden_states",
      "entity_hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "LukeEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "word_hidden_states",
      "entity_hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukePooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EntityPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EntityPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LukePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LukeModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_entity_embeddings": [
      "self"
    ],
    "set_entity_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "get_extended_attention_mask": [
      "self",
      "word_attention_mask",
      "entity_attention_mask"
    ]
  },
  "LukeLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "LukeForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "labels",
      "entity_labels",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeForEntityClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeForEntityPairClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeForEntitySpanClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "entity_start_positions",
      "entity_end_positions",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "entity_ids",
      "entity_attention_mask",
      "entity_token_type_ids",
      "entity_position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LukeConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "entity_vocab_size",
      "hidden_size",
      "entity_emb_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "use_entity_aware_attention",
      "classifier_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "ImageGPTImageProcessorKwargs": {},
  "squared_euclidean_distance": [
    "a",
    "b"
  ],
  "color_quantize": [
    "x",
    "clusters"
  ],
  "ImageGPTImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "clusters",
      "do_resize",
      "size",
      "resample",
      "do_normalize",
      "do_color_quantize"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "normalize": [
      "self",
      "image",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_normalize",
      "do_color_quantize",
      "clusters",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "to_dict": [
      "self"
    ]
  },
  "ImageGPTLayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "tensor"
    ]
  },
  "ImageGPTAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "layer_idx"
    ],
    "_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "_upcast_and_reordered_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "_split_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "_merge_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "ImageGPTMLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ImageGPTBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "ImageGPTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ImageGPTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ImageGPTForCausalImageModeling": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ImageGPTForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ImageGPTConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "n_positions",
      "n_embd",
      "n_layer",
      "n_head",
      "n_inner",
      "activation_function",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "scale_attn_weights",
      "use_cache",
      "tie_word_embeddings",
      "scale_attn_by_inverse_layer_idx",
      "reorder_and_upcast_attn",
      "add_cross_attention",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "squared_euclidean_distance_torch": [
    "a",
    "b"
  ],
  "color_quantize_torch": [
    "x",
    "clusters"
  ],
  "ImageGPTImageProcessorFast": {
    "model_input_names": [],
    "resample": [],
    "do_color_quantize": [],
    "clusters": [],
    "image_mean": [],
    "image_std": [],
    "do_rescale": [],
    "do_normalize": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "clusters"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_color_quantize",
      "clusters",
      "disable_grouping",
      "return_tensors"
    ],
    "to_dict": [
      "self"
    ]
  },
  "GlmImageImageProcessorFast": {
    "do_resize": [],
    "resample": [],
    "size": [],
    "do_rescale": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "min_pixels",
      "max_pixels"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "disable_grouping",
      "return_tensors"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "GlmImageImagesKwargs": {},
  "GlmImageProcessorKwargs": {
    "_defaults": []
  },
  "GlmImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_build_prompt_with_target_shape": [
      "self",
      "prompt",
      "height",
      "width",
      "is_text_to_image"
    ],
    "_build_target_image_grid_thw": [
      "token_h",
      "token_w",
      "prev_token_h",
      "prev_token_w",
      "is_text_to_image"
    ]
  },
  "GlmImageVQVAEConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "embed_dim",
      "num_embeddings",
      "latent_channels",
      "in_channels",
      "initializer_range"
    ]
  },
  "GlmImageVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "attention_bias",
      "attention_dropout",
      "num_heads",
      "in_channels",
      "image_size",
      "patch_size",
      "layer_norm_eps",
      "spatial_merge_size",
      "intermediate_size",
      "initializer_range"
    ]
  },
  "GlmImageTextConfig": {
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "vision_vocab_size",
      "attention_bias",
      "pad_token_id",
      "eos_token_id"
    ]
  },
  "GlmImageConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "vq_config",
      "image_token_id",
      "image_start_token_id",
      "image_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "GlmImageVisionMLP": {},
  "GlmImageVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens"
    ]
  },
  "GlmImageVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GlmImageVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "GlmImageVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens"
    ]
  },
  "GlmImageTextAttention": {},
  "GlmImagePreTrainedModel": {
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GlmImageModelOutputWithPast": {},
  "GlmImageVQVAEVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "GlmImageVQVAEModelOutput": {},
  "GlmImageVQVAE": {
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "hidden_states"
    ]
  },
  "GlmImageVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "pixel_values",
      "grid_thw"
    ]
  },
  "GlmImageTextModel": {},
  "GlmImageModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "images_per_sample",
      "attention_mask"
    ],
    "get_image_tokens": [
      "self",
      "hidden_states",
      "image_grid_thw"
    ],
    "get_video_features": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "image_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "image_grid_thw",
      "images_per_sample",
      "rope_deltas",
      "cache_position"
    ]
  },
  "GlmImageCausalLMOutputWithPast": {},
  "GlmImageForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_image_tokens": [
      "self",
      "hidden_states",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "pixel_values",
      "image_grid_thw",
      "images_per_sample",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "image_grid_thw",
      "images_per_sample",
      "is_first_iteration"
    ],
    "_get_image_nums": [
      "self",
      "input_ids"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "GlmImageImageProcessor": {
    "model_input_names": []
  },
  "GlmImageImageProcessorKwargs": {},
  "GlmImageRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GlmImageTextMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GlmImageTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "GlmImageTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ],
    "apply_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ]
  },
  "Starcoder2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Starcoder2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Starcoder2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Starcoder2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Starcoder2ForCausalLM": {},
  "Starcoder2ForSequenceClassification": {},
  "Starcoder2ForTokenClassification": {},
  "Starcoder2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Starcoder2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Starcoder2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "norm_epsilon",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "rope_parameters",
      "sliding_window",
      "attention_dropout",
      "residual_dropout",
      "embedding_dropout",
      "use_bias",
      "tie_word_embeddings"
    ]
  },
  "VideoMAEVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": [],
    "model_input_names": [],
    "preprocess": [
      "self",
      "videos"
    ]
  },
  "VideoMAEConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "num_frames",
      "tubelet_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "qkv_bias",
      "use_mean_pooling",
      "decoder_num_attention_heads",
      "decoder_hidden_size",
      "decoder_num_hidden_layers",
      "decoder_intermediate_size",
      "norm_pix_loss"
    ]
  },
  "VideoMAEDecoderOutput": {},
  "VideoMAEForPreTrainingOutput": {},
  "get_sinusoid_encoding_table": [
    "n_position",
    "d_hid"
  ],
  "VideoMAEEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "VideoMAEPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "VideoMAESelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VideoMAESelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "VideoMAEAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VideoMAEIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VideoMAEOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "VideoMAELayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VideoMAEEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VideoMAEPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "VideoMAEModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "VideoMAEDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "return_token_num"
    ]
  },
  "VideoMAEForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "VideoMAEForVideoClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "make_batched": [
    "videos"
  ],
  "VideoMAEImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "videos",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "PeVideoEncoderConfig": {
    "model_type": [],
    "sub_configs": [],
    "base_config_key": [],
    "_default_vision_config_kwargs": [],
    "__init__": [
      "self",
      "vision_config",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "rope_parameters",
      "attention_bias",
      "attention_dropout"
    ]
  },
  "PeVideoConfig": {
    "model_type": [],
    "sub_configs": [],
    "base_config_key": [],
    "_default_text_config_kwargs": [],
    "__init__": [
      "self",
      "text_config",
      "video_config"
    ]
  },
  "PeVideoVideoProcessor": {
    "resample": [],
    "sample_frames": [
      "self",
      "metadata",
      "num_frames",
      "fps"
    ],
    "_preprocess": [
      "self",
      "videos"
    ]
  },
  "PeVideoOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "PeVideoContrastiveHead": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PeVideoMaskedGroupNorm": {
    "forward": [
      "self",
      "x",
      "padding_mask"
    ]
  },
  "PeVideoConvBlock1d": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "padding_mask"
    ]
  },
  "PeVideoResnetBlock1d": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask"
    ]
  },
  "PeVideoEncoderPatchEmbedder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "padding_mask"
    ]
  },
  "PeVideoEncoderEmbedder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values_videos",
      "padding_mask"
    ]
  },
  "stack_freqs": [
    "cos",
    "sin"
  ],
  "PeVideoEncoderRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PeVideoEncoderAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "PeVideoEncoderMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PeVideoEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PeVideoPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "main_input_name": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PeVideoEncoderRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PeVideoEncoder": {
    "main_input_name": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values_videos",
      "padding_mask_videos"
    ]
  },
  "PeVideoModel": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values_videos",
      "attention_mask",
      "padding_mask_videos",
      "return_loss"
    ]
  },
  "PeVideoProcessor": {
    "attributes": [],
    "video_processor_class": [],
    "tokenizer_class": []
  },
  "Olmo3RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Olmo3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Olmo3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Olmo3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Olmo3RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Olmo3PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Olmo3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "Olmo3ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Olmo3Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "rms_norm_eps",
      "sliding_window",
      "layer_types"
    ]
  },
  "SegGptEncoderOutput": {},
  "SegGptImageSegmentationOutput": {},
  "SegGptPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SegGptEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "prompt_pixel_values",
      "bool_masked_pos",
      "embedding_type"
    ]
  },
  "SegGptAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "get_rel_pos": [
      "self",
      "q_size",
      "k_size",
      "rel_pos"
    ],
    "add_decomposed_rel_pos": [
      "self",
      "attn",
      "query",
      "rel_pos_h",
      "rel_pos_w",
      "q_size",
      "k_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "SegGptMlp": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SegGptDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "SegGptLayer": {
    "__init__": [
      "self",
      "config",
      "drop_path_rate"
    ],
    "forward": [
      "self",
      "hidden_states",
      "ensemble_cond",
      "feature_ensemble",
      "output_attentions"
    ]
  },
  "SegGptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "feature_ensemble",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SegGptLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "SegGptDecoderHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SegGptDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_reshape_hidden_states": [
      "self",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SegGptPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SegGptModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "prompt_pixel_values",
      "prompt_masks",
      "bool_masked_pos",
      "feature_ensemble",
      "embedding_type",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "patchify": [
    "tensor",
    "patch_size"
  ],
  "unpatchify": [
    "tensor",
    "patch_height",
    "patch_width"
  ],
  "SegGptLoss": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "prompt_masks",
      "pred_masks",
      "labels",
      "bool_masked_pos"
    ]
  },
  "SegGptForImageSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "prompt_pixel_values",
      "prompt_masks",
      "bool_masked_pos",
      "feature_ensemble",
      "embedding_type",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "build_palette": [
    "num_labels"
  ],
  "mask_to_rgb": [
    "mask",
    "palette",
    "data_format"
  ],
  "SegGptImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "get_palette": [
      "self",
      "num_labels"
    ],
    "mask_to_rgb": [
      "self",
      "image",
      "palette",
      "data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_step": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format",
      "do_convert_rgb",
      "num_labels"
    ],
    "preprocess": [
      "self",
      "images",
      "prompt_images",
      "prompt_masks",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "num_labels",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "num_labels"
    ]
  },
  "SegGptConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_act",
      "hidden_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "mlp_dim",
      "drop_path_rate",
      "pretrain_image_size",
      "decoder_hidden_size",
      "use_relative_position_embeddings",
      "merge_index",
      "intermediate_hidden_state_indices",
      "beta"
    ]
  },
  "Qwen3MoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Qwen3MoeMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen3MoeExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Qwen3MoeTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3MoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Qwen3MoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen3MoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen3MoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Qwen3MoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Qwen3MoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Qwen3MoeForSequenceClassification": {},
  "Qwen3MoeForTokenClassification": {},
  "Qwen3MoeForQuestionAnswering": {
    "base_model_prefix": []
  },
  "Qwen3MoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "use_sliding_window",
      "sliding_window",
      "attention_dropout",
      "decoder_sparse_step",
      "moe_intermediate_size",
      "num_experts_per_tok",
      "num_experts",
      "norm_topk_prob",
      "output_router_logits",
      "router_aux_loss_coef",
      "mlp_only_layers",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "EdgeTamLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "EdgeTamVisionEncoderOutput": {},
  "EdgeTamAttention": {
    "__init__": [
      "self",
      "config",
      "downsample_rate"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_similarity"
    ]
  },
  "EdgeTamTwoWayAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "skip_first_layer_pe"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "query_point_embedding",
      "key_point_embedding",
      "attention_similarity"
    ]
  },
  "EdgeTamFeedForward": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "activation",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EdgeTamPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EdgeTamSinePositionEmbedding": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "EdgeTamVisionNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EdgeTamVisionModel": {
    "config_class": [],
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "EdgeTamImageSegmentationOutput": {},
  "EdgeTamPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_coords",
      "input_shape"
    ]
  },
  "EdgeTamMaskEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "masks"
    ]
  },
  "EdgeTamPromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ],
    "forward": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ]
  },
  "EdgeTamTwoWayTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "point_embeddings",
      "image_embeddings",
      "image_positional_embeddings",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "EdgeTamMaskDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "high_resolution_features",
      "attention_similarity",
      "target_embedding"
    ],
    "_get_stability_scores": [
      "self",
      "mask_logits"
    ],
    "_dynamic_multimask_via_stability": [
      "self",
      "all_mask_logits",
      "all_iou_scores"
    ]
  },
  "EdgeTamModel": {
    "input_modalities": [],
    "_can_record_outputs": [],
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_prompt_embeddings": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "image_embeddings",
      "multimask_output",
      "attention_similarity",
      "target_embedding"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ]
  },
  "EdgeTamVisionConfig": {
    "base_config_key": [],
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "backbone_channel_list",
      "backbone_feature_sizes",
      "fpn_hidden_size",
      "fpn_kernel_size",
      "fpn_stride",
      "fpn_padding",
      "fpn_top_down_levels",
      "num_feature_levels",
      "hidden_act",
      "layer_norm_eps",
      "initializer_range"
    ]
  },
  "EdgeTamPromptEncoderConfig": {},
  "EdgeTamMaskDecoderConfig": {},
  "EdgeTamConfig": {},
  "PeAudioDacEncoderBlock": {
    "__init__": [
      "self",
      "config",
      "stride",
      "stride_index"
    ]
  },
  "PeAudioDacEncoder": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "PeAudioEncoderEmbedder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "padding_mask"
    ]
  },
  "PeAudioContrastiveHead": {},
  "PeAudioPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PeAudioEncoderOutput": {},
  "PeAudioEncoder": {
    "base_model_prefix": [],
    "forward": [
      "self",
      "input_values",
      "padding_mask"
    ]
  },
  "PeAudioOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "PeAudioModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_audio_embeds": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_audio_embeds": [
      "self",
      "input_values",
      "padding_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_values",
      "attention_mask",
      "padding_mask",
      "return_loss"
    ]
  },
  "PeAudioFrameLevelModel": {
    "get_audio_embeds": [
      "self",
      "input_values",
      "padding_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_values",
      "attention_mask",
      "padding_mask",
      "return_loss"
    ]
  },
  "PeAudioEncoderConfig": {
    "model_type": [],
    "sub_configs": [],
    "base_config_key": [],
    "_default_dac_config_kwargs": [],
    "__init__": [
      "self",
      "dac_config",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "rope_parameters",
      "attention_bias",
      "attention_dropout"
    ]
  },
  "PeAudioConfig": {
    "model_type": [],
    "sub_configs": [],
    "base_config_key": [],
    "_default_text_config_kwargs": [],
    "__init__": [
      "self",
      "text_config",
      "audio_config"
    ]
  },
  "Snake1d": {
    "__init__": [
      "self",
      "hidden_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PeAudioDacResidualUnit": {
    "__init__": [
      "self",
      "dimension",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "PeAudioMaskedGroupNorm": {
    "forward": [
      "self",
      "x",
      "padding_mask"
    ]
  },
  "PeAudioConvBlock1d": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "padding_mask"
    ]
  },
  "PeAudioResnetBlock1d": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask"
    ]
  },
  "PeAudioEncoderPatchEmbedder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "padding_mask"
    ]
  },
  "PeAudioEncoderRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PeAudioEncoderAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "PeAudioEncoderMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PeAudioEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PeAudioEncoderRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PeAudioFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "hop_length"
    ],
    "_reflect_pad": [
      "self",
      "wav"
    ],
    "__call__": [
      "self",
      "raw_audio",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "sampling_rate"
    ]
  },
  "PeAudioProcessor": {
    "attributes": [],
    "feature_extractor_class": [],
    "tokenizer_class": []
  },
  "KyutaiSpeechToTextProcessorKwargs": {
    "_defaults": []
  },
  "KyutaiSpeechToTextProcessor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ]
  },
  "KyutaiSpeechToTextFeatureExtractor": {
    "__init__": [
      "self",
      "audio_delay_seconds",
      "audio_silence_prefix_seconds"
    ],
    "__call__": [
      "self",
      "raw_audio",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "sampling_rate"
    ]
  },
  "KyutaiSpeechToTextPreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "KyutaiSpeechToTextConv1dPaddingCache": {},
  "KyutaiSpeechToTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "KyutaiSpeechToTextModel": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "KyutaiSpeechToTextForConditionalGeneration": {
    "_tied_weights_keys": [],
    "_keep_in_fp32_modules_strict": [],
    "output_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ],
    "_prepare_generation_config": [
      "self"
    ],
    "_prepare_model_inputs": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ],
    "save_pretrained": [
      "self"
    ],
    "generate": [
      "self"
    ]
  },
  "KyutaiSpeechToTextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "codebook_vocab_size",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "max_position_embeddings",
      "rope_parameters",
      "hidden_act",
      "head_dim",
      "initializer_range",
      "use_cache",
      "sliding_window",
      "attention_dropout",
      "ffn_dim",
      "rms_norm_eps",
      "num_codebooks",
      "audio_bos_token_id",
      "audio_pad_token_id",
      "tie_word_embeddings",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "codec_config"
    ]
  },
  "KyutaiSpeechToTextFlexibleLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size",
      "num_layers"
    ],
    "forward": [
      "self",
      "x",
      "layer_idx"
    ]
  },
  "KyutaiSpeechToTextRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "KyutaiSpeechToTextLinear": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "num_codebooks",
      "use_flexible_linear"
    ],
    "forward": [
      "self",
      "x",
      "layer_idx"
    ]
  },
  "KyutaiSpeechToTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "KyutaiSpeechToTextGatingMLP": {
    "__init__": [
      "self",
      "config",
      "use_flexible_linear"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_idx"
    ]
  },
  "KyutaiSpeechToTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "use_flexible_linear",
      "use_rope"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "KyutaiSpeechToTextFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "KyutaiSpeechToTextSdpaAttention": {
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "KYUTAI_SPEECH_TO_TEXT_ATTENTION_CLASSES": [],
  "KyutaiSpeechToTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "use_flexible_linear",
      "use_rope"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MT5LayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MT5DenseActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MT5DenseGatedActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MT5LayerFF": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MT5Attention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_values",
      "query_length",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "MT5LayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "MT5LayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "query_length",
      "output_attentions",
      "cache_position"
    ]
  },
  "MT5Block": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "return_dict",
      "cache_position"
    ]
  },
  "MT5ClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MT5PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_can_compile_fullgraph": [],
    "_no_split_modules": [],
    "_keep_in_fp32_modules": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "MT5Stack": {
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MT5Model": {
    "model_type": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MT5ForConditionalGeneration": {
    "model_type": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "MT5EncoderModel": {
    "model_type": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MT5ForSequenceClassification": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MT5ForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MT5ForQuestionAnswering": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MT5Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "d_kv",
      "d_ff",
      "num_layers",
      "num_decoder_layers",
      "num_heads",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "dropout_rate",
      "layer_norm_epsilon",
      "initializer_factor",
      "feed_forward_proj",
      "is_encoder_decoder",
      "use_cache",
      "tokenizer_class",
      "tie_word_embeddings",
      "bos_token_id",
      "pad_token_id",
      "eos_token_id",
      "decoder_start_token_id",
      "classifier_dropout",
      "is_decoder"
    ]
  },
  "GemmaRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GemmaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GemmaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "GemmaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "GemmaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GemmaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GemmaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "GemmaForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GemmaForSequenceClassification": {},
  "GemmaForTokenClassification": {},
  "GemmaTokenizer": {
    "vocab_files_names": [],
    "padding_side": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "mask_token"
    ],
    "_unk_id": [
      "self"
    ]
  },
  "GemmaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "use_bidirectional_attention"
    ]
  },
  "Data2VecVisionConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "use_mask_token",
      "use_absolute_position_embeddings",
      "use_relative_position_bias",
      "use_shared_relative_position_bias",
      "layer_scale_init_value",
      "drop_path_rate",
      "use_mean_pooling",
      "out_indices",
      "pool_scales",
      "use_auxiliary_head",
      "auxiliary_loss_weight",
      "auxiliary_channels",
      "auxiliary_num_convs",
      "auxiliary_concat_input",
      "semantic_loss_ignore_index"
    ]
  },
  "Data2VecAudioConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embedding_groups",
      "conv_pos_kernel_size",
      "num_conv_pos_embeddings",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "tdnn_dim",
      "tdnn_kernel",
      "tdnn_dilation",
      "xvector_output_dim",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_adapter",
      "adapter_kernel_size",
      "adapter_stride",
      "num_adapter_layers",
      "output_hidden_size"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "Data2VecTextEmbeddings": {},
  "Data2VecTextSelfAttention": {},
  "Data2VecTextCrossAttention": {},
  "Data2VecTextLayer": {},
  "Data2VecTextPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Data2VecTextModel": {},
  "Data2VecTextLMHead": {},
  "Data2VecTextClassificationHead": {},
  "Data2VecTextForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Data2VecTextForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "Data2VecTextForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "Data2VecTextForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "Data2VecTextForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "Data2VecTextForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "Data2VecAudioConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioPadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioPositionalConvLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "Data2VecAudioFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Data2VecAudioFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Data2VecAudioEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Data2VecAudioAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioAdapter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecAudioPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths",
      "add_adapter"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask",
      "add_adapter"
    ]
  },
  "Data2VecAudioBaseModelOutput": [],
  "Data2VecAudioModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Data2VecAudioForCTC": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Data2VecAudioForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Data2VecAudioForAudioFrameClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Data2VecAudioForXVector": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "_get_tdnn_output_lengths": [
      "self",
      "input_lengths"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Data2VecTextSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Data2VecTextAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Data2VecTextIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecTextOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Data2VecTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "Data2VecTextPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecVisionModelOutputWithPooling": {},
  "Data2VecVisionDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Data2VecVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "Data2VecVisionPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Data2VecVisionSelfAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "Data2VecVisionSdpaSelfAttention": {
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "Data2VecVisionSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor",
      "gamma"
    ]
  },
  "DATA2VEC_VISION_SELF_ATTENTION_CLASSES": [],
  "Data2VecVisionAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "Data2VecVisionIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecVisionOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecVisionLayer": {
    "__init__": [
      "self",
      "config",
      "window_size",
      "drop_path_rate"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "Data2VecVisionRelativePositionBias": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "generate_relative_position_index": [
      "self",
      "window_size"
    ],
    "forward": [
      "self",
      "window_size",
      "interpolate_pos_encoding",
      "dim_size"
    ]
  },
  "Data2VecVisionEncoder": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "resolution",
      "return_dict"
    ]
  },
  "Data2VecVisionPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Data2VecVisionModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Data2VecVisionPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Data2VecVisionForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Data2VecVisionConvModule": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "padding",
      "bias",
      "dilation"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "Data2VecVisionPyramidPoolingBlock": {
    "__init__": [
      "self",
      "pool_scale",
      "in_channels",
      "channels"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "Data2VecVisionPyramidPoolingModule": {
    "__init__": [
      "self",
      "pool_scales",
      "in_channels",
      "channels",
      "align_corners"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Data2VecVisionUperHead": {
    "__init__": [
      "self",
      "config"
    ],
    "psp_forward": [
      "self",
      "inputs"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "Data2VecVisionFCNHead": {
    "__init__": [
      "self",
      "config",
      "in_index",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "Data2VecVisionForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "compute_loss": [
      "self",
      "logits",
      "auxiliary_logits",
      "labels"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Data2VecTextConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "EPSILON_ZERO_DIVISION": [],
  "CLOSE_ENOUGH_TO_LOG_ZERO": [],
  "TableQuestionAnsweringOutput": {},
  "TapasEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "TapasSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "TapasSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "TapasAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "TapasIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TapasOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "TapasLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "TapasEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "TapasPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TapasPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TapasLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TapasOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "TapasPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "TapasModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TapasForMaskedLM": {
    "_tied_weights_keys": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TapasForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "table_mask",
      "labels",
      "aggregation_labels",
      "float_answer",
      "numeric_values",
      "numeric_values_scale",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TapasForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AverageApproximationFunction": {
    "RATIO": [],
    "FIRST_ORDER": [],
    "SECOND_ORDER": []
  },
  "IndexMap": {
    "__init__": [
      "self",
      "indices",
      "num_segments",
      "batch_dims"
    ],
    "batch_shape": [
      "self"
    ]
  },
  "ProductIndexMap": {
    "__init__": [
      "self",
      "outer_index",
      "inner_index"
    ],
    "project_outer": [
      "self",
      "index"
    ],
    "project_inner": [
      "self",
      "index"
    ]
  },
  "gather": [
    "values",
    "index",
    "name"
  ],
  "range_index_map": [
    "batch_shape",
    "num_segments",
    "name"
  ],
  "_segment_reduce": [
    "values",
    "index",
    "segment_reduce_fn",
    "name"
  ],
  "reduce_sum": [
    "values",
    "index",
    "name"
  ],
  "reduce_mean": [
    "values",
    "index",
    "name"
  ],
  "reduce_max": [
    "values",
    "index",
    "name"
  ],
  "reduce_min": [
    "values",
    "index",
    "name"
  ],
  "compute_column_logits": [
    "sequence_output",
    "column_output_weights",
    "column_output_bias",
    "cell_index",
    "cell_mask",
    "allow_empty_column_selection"
  ],
  "_single_column_cell_selection_loss": [
    "token_logits",
    "column_logits",
    "labels",
    "cell_index",
    "col_index",
    "cell_mask"
  ],
  "compute_token_logits": [
    "sequence_output",
    "temperature",
    "output_weights",
    "output_bias"
  ],
  "_calculate_aggregate_mask": [
    "answer",
    "pooled_output",
    "cell_selection_preference",
    "labels",
    "aggregation_classifier"
  ],
  "_calculate_aggregation_loss_known": [
    "logits_aggregation",
    "aggregate_mask",
    "aggregation_labels",
    "use_answer_as_supervision",
    "num_aggregation_labels"
  ],
  "_calculate_aggregation_loss_unknown": [
    "logits_aggregation",
    "aggregate_mask"
  ],
  "_calculate_aggregation_loss": [
    "logits_aggregation",
    "aggregate_mask",
    "aggregation_labels",
    "use_answer_as_supervision",
    "num_aggregation_labels",
    "aggregation_loss_weight"
  ],
  "_calculate_expected_result": [
    "dist_per_cell",
    "numeric_values",
    "numeric_values_scale",
    "input_mask_float",
    "logits_aggregation",
    "config"
  ],
  "huber_loss": [
    "input",
    "target",
    "delta"
  ],
  "_calculate_regression_loss": [
    "answer",
    "aggregate_mask",
    "dist_per_cell",
    "numeric_values",
    "numeric_values_scale",
    "input_mask_float",
    "logits_aggregation",
    "config"
  ],
  "TapasTruncationStrategy": {
    "DROP_ROWS_TO_FIT": [],
    "DO_NOT_TRUNCATE": []
  },
  "TableValue": [],
  "TokenCoordinates": {},
  "TokenizedTable": {},
  "SerializedExample": {},
  "_is_inner_wordpiece": [
    "token"
  ],
  "TAPAS_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING": [],
  "TapasTokenizer": {
    "model_input_names": [],
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "do_lower_case",
      "do_basic_tokenize",
      "never_split",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "empty_token",
      "tokenize_chinese_chars",
      "strip_accents",
      "cell_trim_length",
      "max_column_id",
      "max_row_id",
      "strip_column_names",
      "update_answer_coordinates",
      "min_question_length",
      "max_question_length",
      "model_max_length",
      "additional_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "do_lower_case": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "create_attention_mask_from_sequences": [
      "self",
      "query_ids",
      "table_values"
    ],
    "create_segment_token_type_ids_from_sequences": [
      "self",
      "query_ids",
      "table_values"
    ],
    "create_column_token_type_ids_from_sequences": [
      "self",
      "query_ids",
      "table_values"
    ],
    "create_row_token_type_ids_from_sequences": [
      "self",
      "query_ids",
      "table_values"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "__call__": [
      "self",
      "table",
      "queries",
      "answer_coordinates",
      "answer_text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "batch_encode_plus": [
      "self",
      "table",
      "queries",
      "answer_coordinates",
      "answer_text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_get_question_tokens": [
      "self",
      "query"
    ],
    "_batch_encode_plus": [
      "self",
      "table",
      "queries",
      "answer_coordinates",
      "answer_text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_prepare_for_model": [
      "self",
      "raw_table",
      "raw_queries",
      "tokenized_table",
      "queries_tokens",
      "answer_coordinates",
      "answer_text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "prepend_batch_axis"
    ],
    "encode": [
      "self",
      "table",
      "query",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "return_tensors"
    ],
    "encode_plus": [
      "self",
      "table",
      "query",
      "answer_coordinates",
      "answer_text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "table",
      "query",
      "answer_coordinates",
      "answer_text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "prepare_for_model": [
      "self",
      "raw_table",
      "raw_query",
      "tokenized_table",
      "query_tokens",
      "answer_coordinates",
      "answer_text",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "prepend_batch_axis"
    ],
    "_get_truncated_table_rows": [
      "self",
      "query_tokens",
      "tokenized_table",
      "num_rows",
      "num_columns",
      "max_length",
      "truncation_strategy"
    ],
    "_tokenize_table": [
      "self",
      "table"
    ],
    "_question_encoding_cost": [
      "self",
      "question_tokens"
    ],
    "_get_token_budget": [
      "self",
      "question_tokens",
      "max_length"
    ],
    "_get_table_values": [
      "self",
      "table",
      "num_columns",
      "num_rows",
      "num_tokens"
    ],
    "_get_table_boundaries": [
      "self",
      "table"
    ],
    "_get_table_cost": [
      "self",
      "table",
      "num_columns",
      "num_rows",
      "num_tokens"
    ],
    "_get_max_num_tokens": [
      "self",
      "question_tokens",
      "tokenized_table",
      "num_columns",
      "num_rows",
      "max_length"
    ],
    "_get_num_columns": [
      "self",
      "table"
    ],
    "_get_num_rows": [
      "self",
      "table",
      "drop_rows_to_fit"
    ],
    "_serialize_text": [
      "self",
      "question_tokens"
    ],
    "_serialize": [
      "self",
      "question_tokens",
      "table",
      "num_columns",
      "num_rows",
      "num_tokens"
    ],
    "_get_column_values": [
      "self",
      "table",
      "col_index"
    ],
    "_get_cell_token_indexes": [
      "self",
      "column_ids",
      "row_ids",
      "column_id",
      "row_id"
    ],
    "_get_numeric_column_ranks": [
      "self",
      "column_ids",
      "row_ids",
      "table"
    ],
    "_get_numeric_sort_key_fn": [
      "self",
      "table_numeric_values",
      "value"
    ],
    "_get_numeric_relations": [
      "self",
      "question",
      "column_ids",
      "row_ids",
      "table"
    ],
    "_get_numeric_values": [
      "self",
      "table",
      "column_ids",
      "row_ids"
    ],
    "_get_numeric_values_scale": [
      "self",
      "table",
      "column_ids",
      "row_ids"
    ],
    "_pad_to_seq_length": [
      "self",
      "inputs"
    ],
    "_get_all_answer_ids_from_coordinates": [
      "self",
      "column_ids",
      "row_ids",
      "answers_list"
    ],
    "_get_all_answer_ids": [
      "self",
      "column_ids",
      "row_ids",
      "answer_coordinates"
    ],
    "_find_tokens": [
      "self",
      "text",
      "segment"
    ],
    "_find_answer_coordinates_from_answer_text": [
      "self",
      "tokenized_table",
      "answer_text"
    ],
    "_find_answer_ids_from_answer_texts": [
      "self",
      "column_ids",
      "row_ids",
      "tokenized_table",
      "answer_texts"
    ],
    "_get_answer_ids": [
      "self",
      "column_ids",
      "row_ids",
      "answer_coordinates"
    ],
    "get_answer_ids": [
      "self",
      "column_ids",
      "row_ids",
      "tokenized_table",
      "answer_texts_question",
      "answer_coordinates_question"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "_get_cell_token_probs": [
      "self",
      "probabilities",
      "segment_ids",
      "row_ids",
      "column_ids"
    ],
    "_get_mean_cell_probs": [
      "self",
      "probabilities",
      "segment_ids",
      "row_ids",
      "column_ids"
    ],
    "convert_logits_to_predictions": [
      "self",
      "data",
      "logits",
      "logits_agg",
      "cell_classification_threshold"
    ]
  },
  "Relation": {
    "HEADER_TO_CELL": [],
    "CELL_TO_HEADER": [],
    "QUERY_TO_HEADER": [],
    "QUERY_TO_CELL": [],
    "ROW_TO_CELL": [],
    "CELL_TO_ROW": [],
    "EQ": [],
    "LT": [],
    "GT": []
  },
  "Date": {},
  "NumericValue": {},
  "NumericValueSpan": {},
  "Cell": {},
  "Question": {},
  "_DateMask": [],
  "_YEAR": [],
  "_YEAR_MONTH": [],
  "_YEAR_MONTH_DAY": [],
  "_MONTH": [],
  "_MONTH_DAY": [],
  "_DATE_PATTERNS": [],
  "_FIELD_TO_REGEX": [],
  "_process_date_pattern": [
    "dp"
  ],
  "_process_date_patterns": [],
  "_PROCESSED_DATE_PATTERNS": [],
  "_MAX_DATE_NGRAM_SIZE": [],
  "_NUMBER_WORDS": [],
  "_ORDINAL_WORDS": [],
  "_ORDINAL_SUFFIXES": [],
  "_NUMBER_PATTERN": [],
  "_MIN_YEAR": [],
  "_MAX_YEAR": [],
  "_INF": [],
  "_get_numeric_value_from_date": [
    "date",
    "mask"
  ],
  "_get_span_length_key": [
    "span"
  ],
  "_get_numeric_value_from_float": [
    "value"
  ],
  "_parse_date": [
    "text"
  ],
  "_parse_number": [
    "text"
  ],
  "get_all_spans": [
    "text",
    "max_ngram_length"
  ],
  "normalize_for_match": [
    "text"
  ],
  "format_text": [
    "text"
  ],
  "parse_text": [
    "text"
  ],
  "_PrimitiveNumericValue": [],
  "_SortKeyFn": [],
  "_DATE_TUPLE_SIZE": [],
  "EMPTY_TEXT": [],
  "NUMBER_TYPE": [],
  "DATE_TYPE": [],
  "_get_value_type": [
    "numeric_value"
  ],
  "_get_value_as_primitive_value": [
    "numeric_value"
  ],
  "_get_all_types": [
    "numeric_values"
  ],
  "get_numeric_sort_key_fn": [
    "numeric_values"
  ],
  "_consolidate_numeric_values": [
    "row_index_to_values",
    "min_consolidation_fraction",
    "debug_info"
  ],
  "_get_numeric_values": [
    "text"
  ],
  "_get_column_values": [
    "table",
    "col_index"
  ],
  "get_numeric_relation": [
    "value",
    "other_value",
    "sort_key_fn"
  ],
  "add_numeric_values_to_question": [
    "question"
  ],
  "filter_invalid_unicode": [
    "text"
  ],
  "filter_invalid_unicode_from_table": [
    "table"
  ],
  "add_numeric_table_values": [
    "table",
    "min_consolidation_fraction",
    "debug_info"
  ],
  "TapasConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_sizes",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "positive_label_weight",
      "num_aggregation_labels",
      "aggregation_loss_weight",
      "use_answer_as_supervision",
      "answer_loss_importance",
      "use_normalized_answer_loss",
      "huber_loss_delta",
      "temperature",
      "aggregation_temperature",
      "use_gumbel_for_cells",
      "use_gumbel_for_aggregation",
      "average_approximation_function",
      "cell_selection_preference",
      "answer_loss_cutoff",
      "max_num_rows",
      "max_num_columns",
      "average_logits_per_cell",
      "select_one_column",
      "allow_empty_column_selection",
      "init_cell_selection_weights_to_zero",
      "reset_position_index_per_cell",
      "disable_per_token_loss",
      "aggregation_labels",
      "no_aggregation_label_index",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "SpeechEncoderDecoderConfig": {
    "model_type": [],
    "sub_configs": [],
    "has_no_defaults_at_init": [],
    "__init__": [
      "self"
    ],
    "from_encoder_decoder_configs": [
      "cls",
      "encoder_config",
      "decoder_config"
    ]
  },
  "SpeechEncoderDecoderModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "__init__": [
      "self",
      "config",
      "encoder",
      "decoder"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "from_encoder_decoder_pretrained": [
      "cls",
      "encoder_pretrained_model_name_or_path",
      "decoder_pretrained_model_name_or_path"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "input_values",
      "input_features",
      "return_dict"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ],
    "resize_token_embeddings": [
      "self"
    ]
  },
  "GroupViTTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "dropout",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "GroupViTVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "depths",
      "num_hidden_layers",
      "num_group_tokens",
      "num_output_groups",
      "num_attention_heads",
      "image_size",
      "patch_size",
      "num_channels",
      "hidden_act",
      "layer_norm_eps",
      "dropout",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "assign_eps",
      "assign_mlp_ratio"
    ]
  },
  "GroupViTConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "projection_intermediate_dim",
      "logit_scale_init_value"
    ]
  },
  "groupvit_loss": [
    "similarity"
  ],
  "hard_softmax": [
    "logits",
    "dim"
  ],
  "gumbel_softmax": [
    "logits",
    "tau",
    "hard",
    "dim"
  ],
  "resize_attention_map": [
    "attentions",
    "height",
    "width",
    "align_corners"
  ],
  "get_grouping_from_attentions": [
    "attentions",
    "hw_shape"
  ],
  "GroupViTCrossAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query",
      "key"
    ]
  },
  "GroupViTAssignAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "get_attn": [
      "self",
      "attn",
      "gumbel",
      "hard"
    ],
    "forward": [
      "self",
      "query",
      "key"
    ]
  },
  "GroupViTTokenAssign": {
    "__init__": [
      "self",
      "config",
      "num_group_token",
      "num_output_group"
    ],
    "project_group_token": [
      "self",
      "group_tokens"
    ],
    "forward": [
      "self",
      "image_tokens",
      "group_tokens"
    ]
  },
  "GroupViTModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "GroupViTPatchEmbeddings": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "GroupViTVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "GroupViTTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "GroupViTStage": {
    "__init__": [
      "self",
      "config",
      "depth",
      "num_prev_group_token",
      "num_group_token",
      "num_output_group"
    ],
    "with_group_token": [
      "self"
    ],
    "split_x": [
      "self",
      "x"
    ],
    "concat_x": [
      "self",
      "x",
      "group_token"
    ],
    "forward": [
      "self",
      "hidden_states",
      "prev_group_token",
      "output_attentions"
    ]
  },
  "GroupViTMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size",
      "output_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GroupViTMixerMLP": {
    "forward": [
      "self",
      "x"
    ]
  },
  "GroupViTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "encoder_hidden_states",
      "output_attentions"
    ]
  },
  "GroupViTEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "GroupViTPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GroupViTVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "GroupViTTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GroupViTTextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GroupViTTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GroupViTVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "GroupViTVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GroupViTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "output_segmentation",
      "return_dict"
    ]
  },
  "PegasusConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "pad_token_id",
      "eos_token_id",
      "forced_eos_token_id",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "PegasusSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "create_weight": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "PegasusAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "PegasusEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "PegasusDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "PegasusPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PegasusEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PegasusDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "PegasusModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "PegasusForConditionalGeneration": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "PegasusDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "PegasusForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "PegasusTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "pad_token",
      "eos_token",
      "unk_token",
      "mask_token",
      "mask_token_sent",
      "additional_special_tokens",
      "offset"
    ]
  },
  "RTDetrV2Config": {
    "model_type": [],
    "sub_configs": [],
    "layer_types": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "initializer_range",
      "initializer_bias_prior_prob",
      "layer_norm_eps",
      "batch_norm_eps",
      "backbone_config",
      "freeze_backbone_batch_norms",
      "encoder_hidden_dim",
      "encoder_in_channels",
      "feat_strides",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "dropout",
      "activation_dropout",
      "encode_proj_layers",
      "positional_encoding_temperature",
      "encoder_activation_function",
      "activation_function",
      "eval_size",
      "normalize_before",
      "hidden_expansion",
      "d_model",
      "num_queries",
      "decoder_in_channels",
      "decoder_ffn_dim",
      "num_feature_levels",
      "decoder_n_points",
      "decoder_layers",
      "decoder_attention_heads",
      "decoder_activation_function",
      "attention_dropout",
      "num_denoising",
      "label_noise_ratio",
      "box_noise_scale",
      "learn_initial_query",
      "anchor_image_size",
      "with_box_refine",
      "is_encoder_decoder",
      "matcher_alpha",
      "matcher_gamma",
      "matcher_class_cost",
      "matcher_bbox_cost",
      "matcher_giou_cost",
      "use_focal_loss",
      "auxiliary_loss",
      "focal_loss_alpha",
      "focal_loss_gamma",
      "weight_loss_vfl",
      "weight_loss_bbox",
      "weight_loss_giou",
      "eos_coefficient",
      "decoder_n_levels",
      "decoder_offset_scale",
      "decoder_method",
      "tie_word_embeddings"
    ]
  },
  "multi_scale_deformable_attention_v2": [
    "value",
    "value_spatial_shapes",
    "sampling_locations",
    "attention_weights",
    "num_points_list",
    "method"
  ],
  "RTDetrV2MultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index"
    ]
  },
  "RTDetrV2DecoderLayer": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "RTDetrV2PreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RTDetrV2Decoder": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "RTDetrV2Model": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "RTDetrV2MLPPredictionHead": {},
  "RTDetrV2ForObjectDetection": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ]
  },
  "RTDetrV2MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size",
      "activation_function"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RTDetrV2SelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "RTDetrV2DecoderOutput": {},
  "RTDetrV2ModelOutput": {},
  "RTDetrV2FrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "RTDetrV2ConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "RTDetrV2ConvNormLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RTDetrV2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings"
    ]
  },
  "RTDetrV2RepVggBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "RTDetrV2CSPRepLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RTDetrV2SinePositionEmbedding": {
    "__init__": [
      "self",
      "embed_dim",
      "temperature"
    ],
    "forward": [
      "self",
      "width",
      "height",
      "device",
      "dtype"
    ]
  },
  "RTDetrV2AIFILayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RTDetrV2HybridEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds"
    ]
  },
  "get_contrastive_denoising_training_group": [
    "targets",
    "num_classes",
    "num_queries",
    "class_embed",
    "num_denoising_queries",
    "label_noise_ratio",
    "box_noise_scale"
  ],
  "RTDetrV2ObjectDetectionOutput": {},
  "ViTMAEConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "decoder_num_attention_heads",
      "decoder_hidden_size",
      "decoder_num_hidden_layers",
      "decoder_intermediate_size",
      "mask_ratio",
      "norm_pix_loss"
    ]
  },
  "ViTMAEModelOutput": {},
  "ViTMAEDecoderOutput": {},
  "ViTMAEForPreTrainingOutput": {},
  "get_2d_sincos_pos_embed": [
    "embed_dim",
    "grid_size",
    "add_cls_token"
  ],
  "get_2d_sincos_pos_embed_from_grid": [
    "embed_dim",
    "grid"
  ],
  "get_1d_sincos_pos_embed_from_grid": [
    "embed_dim",
    "pos"
  ],
  "ViTMAEEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "initialize_weights": [
      "self"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "random_masking": [
      "self",
      "sequence",
      "noise"
    ],
    "forward": [
      "self",
      "pixel_values",
      "noise",
      "interpolate_pos_encoding"
    ]
  },
  "ViTMAEPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "ViTMAESelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMAESelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViTMAEAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMAEIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMAEOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViTMAELayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMAEEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViTMAEPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ViTMAEModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "noise",
      "interpolate_pos_encoding"
    ]
  },
  "ViTMAEDecoder": {
    "__init__": [
      "self",
      "config",
      "num_patches"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings"
    ],
    "initialize_weights": [
      "self",
      "num_patches"
    ],
    "forward": [
      "self",
      "hidden_states",
      "ids_restore",
      "interpolate_pos_encoding"
    ]
  },
  "ViTMAEForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "patchify": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "unpatchify": [
      "self",
      "patchified_pixel_values",
      "original_image_size"
    ],
    "forward_loss": [
      "self",
      "pixel_values",
      "pred",
      "mask",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "pixel_values",
      "noise",
      "interpolate_pos_encoding"
    ]
  },
  "DecisionTransformerConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "state_dim",
      "act_dim",
      "hidden_size",
      "max_ep_len",
      "action_tanh",
      "vocab_size",
      "n_positions",
      "n_layer",
      "n_head",
      "n_inner",
      "activation_function",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "scale_attn_weights",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "scale_attn_by_inverse_layer_idx",
      "reorder_and_upcast_attn",
      "add_cross_attention"
    ]
  },
  "DecisionTransformerGPT2Attention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "layer_idx"
    ],
    "_upcast_and_reordered_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "DecisionTransformerGPT2MLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DecisionTransformerGPT2Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions"
    ]
  },
  "DecisionTransformerGPT2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DecisionTransformerGPT2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "cache_position",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DecisionTransformerOutput": {},
  "DecisionTransformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": []
  },
  "DecisionTransformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "states",
      "actions",
      "rewards",
      "returns_to_go",
      "timesteps",
      "attention_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "_create_sinusoidal_embeddings": [
    "n_pos",
    "dim",
    "out"
  ],
  "Embeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_embeds",
      "position_ids"
    ]
  },
  "DistilBertSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "FFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input"
    ],
    "ff_chunk": [
      "self",
      "input"
    ]
  },
  "TransformerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Transformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "DistilBertPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DistilBertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "position_ids"
    ]
  },
  "DistilBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "position_ids"
    ]
  },
  "DistilBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "position_ids"
    ]
  },
  "DistilBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "position_ids"
    ]
  },
  "DistilBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "position_ids"
    ]
  },
  "DistilBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "position_ids"
    ]
  },
  "DistilBertTokenizer": {
    "model_input_names": [],
    "__init__": [
      "self"
    ]
  },
  "DistilBertTokenizerFast": [],
  "DistilBertConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "sinusoidal_pos_embds",
      "n_layers",
      "n_heads",
      "dim",
      "hidden_dim",
      "dropout",
      "attention_dropout",
      "activation",
      "initializer_range",
      "qa_dropout",
      "seq_classif_dropout",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings"
    ]
  },
  "Idefics2ImageProcessorKwargs": {},
  "Idefics2ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_convert_rgb",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_image_splitting"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "images",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "_crop": [
      "self",
      "im",
      "w1",
      "h1",
      "w2",
      "h2",
      "input_data_format"
    ],
    "split_image": [
      "self",
      "image",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_convert_rgb",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_image_splitting",
      "return_tensors",
      "input_data_format",
      "data_format"
    ]
  },
  "is_url": [
    "val"
  ],
  "is_image_or_image_url": [
    "elem"
  ],
  "Idefics2ProcessorKwargs": {
    "_defaults": []
  },
  "Idefics2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "image_seq_len",
      "chat_template"
    ],
    "_extract_images_from_prompts": [
      "self",
      "prompts"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ]
  },
  "Idefics2VisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range"
    ]
  },
  "Idefics2PerceiverConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_act",
      "hidden_size",
      "rms_norm_eps",
      "resampler_n_latents",
      "resampler_depth",
      "resampler_n_heads",
      "resampler_head_dim",
      "num_key_value_heads",
      "attention_dropout",
      "initializer_range"
    ]
  },
  "Idefics2Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "use_cache",
      "image_token_id",
      "tie_word_embeddings",
      "vision_config",
      "perceiver_config",
      "text_config"
    ]
  },
  "Idefics2ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "do_convert_rgb": [],
    "do_image_splitting": [],
    "size": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "convert_to_rgb": [
      "self",
      "image"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ],
    "_prepare_images_structure": [
      "self",
      "images",
      "expected_ndims"
    ],
    "split_images": [
      "self",
      "images"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "fill"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_image_splitting",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "Idefics2BaseModelOutputWithPast": {},
  "Idefics2CausalLMOutputWithPast": {},
  "Idefics2VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "patch_attention_mask"
    ]
  },
  "Idefics2VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Idefics2VisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Idefics2MLP": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "output_size",
      "hidden_act"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Idefics2MultiheadAttentionPoolingHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Idefics2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Idefics2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "Idefics2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Idefics2VisionTransformer": {
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "patch_attention_mask"
    ]
  },
  "Idefics2RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Idefics2PerceiverAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "latents",
      "context",
      "attention_mask",
      "position_ids",
      "past_key_values"
    ]
  },
  "Idefics2PerceiverLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "latents",
      "context",
      "attention_mask",
      "position_ids",
      "past_key_values"
    ]
  },
  "Idefics2PerceiverResampler": {
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "context",
      "attention_mask"
    ]
  },
  "Idefics2Connector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_hidden_states",
      "attention_mask"
    ]
  },
  "Idefics2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "inputs_merger": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_hidden_states"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_attention_mask",
      "image_hidden_states",
      "use_cache",
      "cache_position"
    ]
  },
  "Idefics2ForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_attention_mask",
      "image_hidden_states",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "pixel_values",
      "pixel_attention_mask",
      "image_hidden_states",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "AlignVisionModelOutput": {},
  "AlignTextModelOutput": {},
  "AlignOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "align_loss": [
    "similarity"
  ],
  "round_filters": [
    "config",
    "num_channels"
  ],
  "correct_pad": [
    "kernel_size",
    "adjust"
  ],
  "AlignVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "AlignVisionDepthwiseConv2d": {
    "__init__": [
      "self",
      "in_channels",
      "depth_multiplier",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "bias",
      "padding_mode"
    ]
  },
  "AlignVisionExpansionLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "out_dim",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AlignVisionDepthwiseLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "stride",
      "kernel_size",
      "adjust_padding"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AlignVisionSqueezeExciteLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "expand_dim",
      "expand"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AlignVisionFinalBlockLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "out_dim",
      "stride",
      "drop_rate",
      "id_skip"
    ],
    "forward": [
      "self",
      "embeddings",
      "hidden_states"
    ]
  },
  "AlignVisionBlock": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "out_dim",
      "stride",
      "expand_ratio",
      "kernel_size",
      "drop_rate",
      "id_skip",
      "adjust_padding"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AlignVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AlignTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "AlignTextSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "AlignTextSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "AlignTextAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "AlignTextIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AlignTextOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "AlignTextLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "AlignTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AlignTextPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AlignPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "AlignTextModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AlignVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_input_embed_layer": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AlignModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "AlignProcessorKwargs": {
    "_defaults": []
  },
  "AlignProcessor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "AlignTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "AlignVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "num_channels",
      "image_size",
      "width_coefficient",
      "depth_coefficient",
      "depth_divisor",
      "kernel_sizes",
      "in_channels",
      "out_channels",
      "depthwise_padding",
      "strides",
      "num_block_repeats",
      "expand_ratios",
      "squeeze_expansion_ratio",
      "hidden_act",
      "hidden_dim",
      "pooling_type",
      "initializer_range",
      "batch_norm_eps",
      "batch_norm_momentum",
      "drop_connect_rate"
    ]
  },
  "AlignConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "temperature_init_value",
      "initializer_range"
    ]
  },
  "JambaRMSNorm": {},
  "JambaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "JambaMambaMixer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "attention_mask"
    ],
    "slow_forward": [
      "self",
      "input_states",
      "cache_params",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "attention_mask"
    ]
  },
  "JambaMLP": {},
  "JambaExperts": {},
  "JambaSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "hidden_states",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JambaAttentionDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "JambaMambaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values"
    ]
  },
  "ALL_DECODER_LAYER_TYPES": [],
  "JambaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_is_stateful": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "JambaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ],
    "_update_mamba_mask": [
      "self",
      "attention_mask",
      "cache_position"
    ]
  },
  "JambaForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "JambaForSequenceClassification": {},
  "JambaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "tie_word_embeddings",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "output_router_logits",
      "router_aux_loss_coef",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_position_embeddings",
      "attention_dropout",
      "num_experts_per_tok",
      "num_experts",
      "expert_layer_period",
      "expert_layer_offset",
      "attn_layer_period",
      "attn_layer_offset",
      "use_mamba_kernels",
      "mamba_d_state",
      "mamba_d_conv",
      "mamba_expand",
      "mamba_dt_rank",
      "mamba_conv_bias",
      "mamba_proj_bias"
    ],
    "layers_block_type": [
      "self"
    ],
    "layers_num_experts": [
      "self"
    ],
    "_check_supported_offset": [
      "self",
      "property_",
      "period",
      "offset"
    ]
  },
  "BeitConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "use_mask_token",
      "use_absolute_position_embeddings",
      "use_relative_position_bias",
      "use_shared_relative_position_bias",
      "layer_scale_init_value",
      "drop_path_rate",
      "use_mean_pooling",
      "pool_scales",
      "use_auxiliary_head",
      "auxiliary_loss_weight",
      "auxiliary_channels",
      "auxiliary_num_convs",
      "auxiliary_concat_input",
      "semantic_loss_ignore_index",
      "out_features",
      "out_indices",
      "add_fpn",
      "reshape_hidden_states"
    ]
  },
  "BeitModelOutputWithPooling": {},
  "BeitDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "BeitEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "BeitPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "BeitSelfAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "BeitSdpaSelfAttention": {
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "BeitSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor",
      "gamma"
    ]
  },
  "BEIT_SELF_ATTENTION_CLASSES": [],
  "BeitAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "BeitIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BeitOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BeitLayer": {
    "__init__": [
      "self",
      "config",
      "window_size",
      "drop_path_rate"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "relative_position_bias",
      "interpolate_pos_encoding",
      "resolution"
    ]
  },
  "BeitRelativePositionBias": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "generate_relative_position_index": [
      "self",
      "window_size"
    ],
    "forward": [
      "self",
      "window_size",
      "interpolate_pos_encoding",
      "dim_size"
    ]
  },
  "BeitEncoder": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "resolution",
      "return_dict"
    ]
  },
  "BeitPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BeitModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "BeitPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BeitForMaskedImageModeling": {
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "BeitForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "BeitConvModule": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "padding",
      "bias",
      "dilation"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "BeitPyramidPoolingBlock": {
    "__init__": [
      "self",
      "pool_scale",
      "in_channels",
      "channels"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "BeitPyramidPoolingModule": {
    "__init__": [
      "self",
      "pool_scales",
      "in_channels",
      "channels",
      "align_corners"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BeitUperHead": {
    "__init__": [
      "self",
      "config"
    ],
    "psp_forward": [
      "self",
      "inputs"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "BeitFCNHead": {
    "__init__": [
      "self",
      "config",
      "in_index",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "BeitForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "compute_loss": [
      "self",
      "logits",
      "auxiliary_logits",
      "labels"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "BeitBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "BeitImageProcessorKwargs": {},
  "BeitImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "rescale_factor",
      "do_rescale",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_reduce_labels"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "reduce_label": [
      "self",
      "label"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_reduce_labels",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_segmentation_map": [
      "self",
      "segmentation_map",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_reduce_labels",
      "input_data_format"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "BeitImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_reduce_labels": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "reduce_label": [
      "self",
      "labels"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_reduce_labels",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "ModernBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds"
    ]
  },
  "ModernBertMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ModernBertRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len",
      "layer_type"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "layer_type"
    ]
  },
  "ModernBertAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "ModernBertEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "ModernBertPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_check_and_adjust_attn_implementation": [
      "self",
      "attn_implementation",
      "is_init_check"
    ]
  },
  "ModernBertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "ModernBertPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ModernBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ModernBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ModernBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ModernBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "start_positions",
      "end_positions"
    ]
  },
  "ModernBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ModernBertConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "initializer_cutoff_factor",
      "norm_eps",
      "norm_bias",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "cls_token_id",
      "sep_token_id",
      "attention_bias",
      "attention_dropout",
      "layer_types",
      "rope_parameters",
      "local_attention",
      "embedding_dropout",
      "mlp_bias",
      "mlp_dropout",
      "decoder_bias",
      "classifier_pooling",
      "classifier_dropout",
      "classifier_bias",
      "classifier_activation",
      "deterministic_flash_attn",
      "sparse_prediction",
      "sparse_pred_ignore_index",
      "reference_compile",
      "tie_word_embeddings"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ],
    "to_dict": [
      "self"
    ],
    "sliding_window": [
      "self",
      "value"
    ]
  },
  "GlmConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "attention_dropout",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "attention_bias"
    ]
  },
  "GlmMLP": {},
  "GlmRotaryEmbedding": {
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ]
  },
  "GlmAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "GlmForCausalLM": {},
  "GlmForSequenceClassification": {},
  "GlmForTokenClassification": {},
  "GlmRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GlmDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GlmPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "GlmModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "DINOv3ViTImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "DINOv3ViTEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "DINOv3ViTRopePositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DINOv3ViTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "DINOv3ViTLayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DINOv3ViTDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "DINOv3ViTMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DINOv3ViTGatedMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DINOv3ViTLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "DINOv3ViTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DINOv3ViTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "DINOv3ViTBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "DINOv3ViTConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "patch_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_act",
      "attention_dropout",
      "initializer_range",
      "layer_norm_eps",
      "rope_theta",
      "image_size",
      "num_channels",
      "query_bias",
      "key_bias",
      "value_bias",
      "proj_bias",
      "mlp_bias",
      "layerscale_value",
      "drop_path_rate",
      "use_gated_mlp",
      "num_register_tokens",
      "pos_embed_shift",
      "pos_embed_jitter",
      "pos_embed_rescale",
      "out_features",
      "out_indices",
      "apply_layernorm",
      "reshape_hidden_states"
    ]
  },
  "MoshiConditionalGenerationGenerateOutput": {},
  "MoshiCausalLMOutputWithPast": {},
  "MoshiConditionalGenerationOutputWithPast": {},
  "MoshiUnconditionalInput": {},
  "MoshiRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MoshiFlexibleLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size",
      "num_layers"
    ],
    "forward": [
      "self",
      "x",
      "layer_idx"
    ]
  },
  "MoshiLinear": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "num_codebooks",
      "use_flexible_linear"
    ],
    "forward": [
      "self",
      "x",
      "layer_idx"
    ]
  },
  "MoshiRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MoshiGatingMLP": {
    "__init__": [
      "self",
      "config",
      "use_flexible_linear"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_idx"
    ]
  },
  "MoshiAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "use_flexible_linear",
      "use_rope"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MoshiFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MoshiSdpaAttention": {
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MOSHI_ATTENTION_CLASSES": [],
  "MoshiDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "use_flexible_linear",
      "use_rope"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MoshiPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "main_input_name": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MoshiDepthDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "last_hidden_state",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "position_ids",
      "labels",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size",
      "config",
      "past_key_values"
    ]
  },
  "MoshiModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size",
      "config",
      "past_key_values"
    ]
  },
  "MoshiForCausalLM": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "labels",
      "logits_to_keep"
    ]
  },
  "MoshiForConditionalGeneration": {
    "output_modalities": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_depth_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "user_input_values",
      "user_audio_codes",
      "moshi_input_values",
      "moshi_audio_codes",
      "past_key_values",
      "inputs_embeds",
      "text_labels",
      "audio_labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "_prepare_attention_mask_for_generation": [
      "self",
      "input_ids",
      "generation_config",
      "kwargs"
    ],
    "_prepare_inputs_embeds_for_generation": [
      "self",
      "input_ids",
      "user_input_values",
      "user_audio_codes",
      "moshi_input_values",
      "moshi_audio_codes",
      "inputs_embeds",
      "attention_mask",
      "generation_config",
      "apply_delay_pattern_mask",
      "concat_unconditional_inputs"
    ],
    "generate": [
      "self",
      "input_ids",
      "user_input_values",
      "user_audio_codes",
      "moshi_input_values",
      "moshi_audio_codes",
      "inputs_embeds",
      "return_audio_waveforms",
      "return_audio_codes",
      "concat_unconditional_inputs"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "logits_to_keep",
      "user_delay_pattern_mask",
      "moshi_delay_pattern_mask",
      "kwargs_depth_decoder",
      "is_first_iteration",
      "blank_user_audio_codes"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "freeze_audio_encoder": [
      "self"
    ],
    "freeze_depth_decoder": [
      "self"
    ],
    "apply_delay_pattern_mask": [
      "input_ids",
      "decoder_pad_token_mask"
    ],
    "build_delay_pattern_mask": [
      "self",
      "input_ids",
      "bos_token_id",
      "pad_token_id",
      "max_length"
    ],
    "get_unconditional_inputs": [
      "self",
      "num_samples"
    ],
    "_check_and_maybe_initialize_inputs": [
      "self",
      "input_ids",
      "user_input_values",
      "user_audio_codes",
      "moshi_input_values",
      "moshi_audio_codes",
      "inputs_embeds",
      "concat_unconditional_inputs"
    ]
  },
  "MoshiDepthConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "input_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "audio_vocab_size",
      "max_position_embeddings",
      "hidden_act",
      "head_dim",
      "initializer_range",
      "use_cache",
      "sliding_window",
      "attention_dropout",
      "ffn_dim",
      "rms_norm_eps",
      "num_codebooks",
      "tie_word_embeddings",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "MoshiConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "audio_vocab_size",
      "max_position_embeddings",
      "rope_parameters",
      "hidden_act",
      "head_dim",
      "initializer_range",
      "use_cache",
      "sliding_window",
      "attention_dropout",
      "ffn_dim",
      "rms_norm_eps",
      "num_codebooks",
      "tie_word_embeddings",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ],
    "sampling_rate": [
      "self"
    ],
    "from_audio_encoder_config": [
      "cls",
      "audio_encoder_config"
    ]
  },
  "MarkupLMFeatureExtractor": {
    "__init__": [
      "self"
    ],
    "xpath_soup": [
      "self",
      "element"
    ],
    "get_three_from_single": [
      "self",
      "html_string"
    ],
    "construct_xpath": [
      "self",
      "xpath_tags",
      "xpath_subscripts"
    ],
    "__call__": [
      "self",
      "html_strings"
    ]
  },
  "MARKUPLM_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING": [],
  "MarkupLMTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "tags_dict",
      "vocab",
      "merges",
      "errors",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space",
      "max_depth",
      "max_width",
      "pad_width",
      "pad_token_label",
      "only_label_first_subword",
      "trim_offsets"
    ],
    "get_xpath_seq": [
      "self",
      "xpath"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "xpaths",
      "node_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "xpaths",
      "node_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "encode_plus": [
      "self",
      "text",
      "text_pair",
      "xpaths",
      "node_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "xpaths",
      "node_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "xpaths",
      "node_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "MarkupLMTokenizerFast": [],
  "XPathEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "xpath_tags_seq",
      "xpath_subs_seq"
    ]
  },
  "MarkupLMEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ],
    "forward": [
      "self",
      "input_ids",
      "xpath_tags_seq",
      "xpath_subs_seq",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "MarkupLMSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MarkupLMIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MarkupLMOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MarkupLMPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MarkupLMPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MarkupLMLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MarkupLMOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "MarkupLMSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "MarkupLMAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "MarkupLMLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "MarkupLMEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MarkupLMPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MarkupLMModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "xpath_tags_seq",
      "xpath_subs_seq",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MarkupLMForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "xpath_tags_seq",
      "xpath_subs_seq",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MarkupLMForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "xpath_tags_seq",
      "xpath_subs_seq",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MarkupLMForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "xpath_tags_seq",
      "xpath_subs_seq",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MarkupLMProcessor": {
    "parse_html": [],
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "html_strings",
      "nodes",
      "xpaths",
      "node_labels",
      "questions",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "return_tensors"
    ]
  },
  "MarkupLMConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_xpath_tag_unit_embeddings",
      "max_xpath_subs_unit_embeddings",
      "tag_pad_id",
      "subs_pad_id",
      "xpath_unit_hidden_size",
      "max_depth",
      "use_cache",
      "classifier_dropout"
    ]
  },
  "YolosImageProcessorKwargs": {},
  "YolosImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "format",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_annotations",
      "do_pad",
      "pad_size"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "size",
      "resample"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "annotation",
      "constant_values",
      "data_format",
      "input_data_format",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "images",
      "annotations",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "update_bboxes",
      "pad_size"
    ],
    "preprocess": [
      "self",
      "images",
      "annotations",
      "return_segmentation_masks",
      "masks_path",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_annotations",
      "do_pad",
      "format",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ]
  },
  "YolosConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "num_detection_tokens",
      "use_mid_position_embeddings",
      "auxiliary_loss",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "eos_coefficient"
    ]
  },
  "YolosImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "format": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "size": [],
    "default_to_square": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "target_size",
      "threshold",
      "interpolation"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "annotation",
      "update_bboxes",
      "fill"
    ],
    "_preprocess": [
      "self",
      "images",
      "annotations",
      "masks_path",
      "return_segmentation_masks",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "format",
      "return_tensors"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ]
  },
  "YolosObjectDetectionOutput": {},
  "YolosEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "InterpolateInitialPositionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pos_embed",
      "img_size"
    ]
  },
  "InterpolateMidPositionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pos_embed",
      "img_size"
    ]
  },
  "YolosPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "YolosSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YolosSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "YolosAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YolosIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YolosOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "YolosLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YolosEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width"
    ]
  },
  "YolosPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "YolosModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "YolosPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YolosMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "YolosForObjectDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "_set_aux_loss": [
      "self",
      "outputs_class",
      "outputs_coord"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "ConvBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "ConvBertPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SeparableConv1D": {
    "__init__": [
      "self",
      "config",
      "input_filters",
      "output_filters",
      "kernel_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ConvBertSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "output_attentions"
    ]
  },
  "ConvBertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ConvBertAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "output_attentions"
    ]
  },
  "GroupedLinearLayer": {
    "__init__": [
      "self",
      "input_size",
      "output_size",
      "num_groups"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ConvBertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ConvBertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ConvBertLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "ConvBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ConvBertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ConvBertSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "ConvBertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ConvBertGeneratorPredictions": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "generator_hidden_states"
    ]
  },
  "ConvBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "word_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ConvBertClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ConvBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ConvBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ConvBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ConvBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ConvBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "embedding_size",
      "head_ratio",
      "conv_kernel_size",
      "num_groups",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "ConvBertTokenizer": {},
  "_get_vector_norm": [
    "tensor"
  ],
  "CLIPVisionModelOutput": {},
  "CLIPTextModelOutput": {},
  "CLIPOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "CLIPVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "CLIPTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "CLIPAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "CLIPMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CLIPEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "CLIPPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CLIPEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "CLIPTextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "CLIPTextModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "CLIPVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "CLIPVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "CLIPModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "interpolate_pos_encoding"
    ]
  },
  "CLIPTextModelWithProjection": {
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "CLIPVisionModelWithProjection": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "CLIPForImageClassification": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "CLIPTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "projection_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "CLIPVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "projection_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "CLIPConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value"
    ]
  },
  "CLIPTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token"
    ],
    "_wrap_decode_method_backend_tokenizer": [
      "self"
    ]
  },
  "CLIPProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "CLIPImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "__init__": [
      "self"
    ]
  },
  "CLIPImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "PhobertTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "add_from_file": [
      "self",
      "f"
    ]
  },
  "NestedList": [],
  "SamImagesKwargs": {},
  "SamProcessorKwargs": {
    "_defaults": []
  },
  "SamProcessor": {
    "__init__": [
      "self",
      "image_processor"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_normalize_and_convert": [
      "self",
      "encoding_image_processor",
      "original_sizes",
      "input_points",
      "input_labels",
      "input_boxes",
      "return_tensors",
      "point_pad_value"
    ],
    "_pad_points_and_labels": [
      "self",
      "input_points",
      "input_labels",
      "point_pad_value"
    ],
    "_normalize_coordinates": [
      "self",
      "target_size",
      "coords",
      "original_size",
      "is_bounding_box"
    ],
    "_check_and_preprocess_points": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes"
    ],
    "model_input_names": [
      "self"
    ],
    "post_process_masks": [
      "self"
    ]
  },
  "SamVisionEncoderOutput": {},
  "SamImageSegmentationOutput": {},
  "SamPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SamMLPBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "SamAttention": {
    "__init__": [
      "self",
      "config",
      "downsample_rate"
    ],
    "_separate_heads": [
      "self",
      "hidden_states",
      "num_attention_heads"
    ],
    "_recombine_heads": [
      "self",
      "hidden_states",
      "point_batch_size"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_similarity"
    ]
  },
  "SamTwoWayAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "attention_downsample_rate",
      "skip_first_layer_pe"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "query_point_embedding",
      "key_point_embedding",
      "attention_similarity"
    ]
  },
  "SamTwoWayTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "point_embeddings",
      "image_embeddings",
      "image_positional_embeddings",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "SamFeedForward": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamMaskDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "SamPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_coords",
      "input_shape"
    ]
  },
  "SamMaskEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "masks"
    ]
  },
  "SamPromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ],
    "forward": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ]
  },
  "SamVisionAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "get_rel_pos": [
      "self",
      "q_size",
      "k_size",
      "rel_pos"
    ],
    "get_decomposed_rel_pos": [
      "self",
      "query",
      "rel_pos_h",
      "rel_pos_w",
      "q_size",
      "k_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "SamVisionSdpaAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "SAM_VISION_ATTENTION_CLASSES": [],
  "SamVisionLayer": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "window_partition": [
      "self",
      "hidden_states",
      "window_size"
    ],
    "window_unpartition": [
      "self",
      "windows",
      "window_size",
      "padding_shape",
      "original_shape"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamVisionNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SamVisionEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SamVisionModel": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SamModel": {
    "input_modalities": [],
    "_can_record_outputs": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_prompt_embeddings": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "image_embeddings",
      "multimask_output",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "SamImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "mask_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "valid_kwargs": [],
    "do_pad": [],
    "pad_size": [],
    "mask_pad_size": [],
    "__init__": [
      "self"
    ],
    "_get_preprocess_shape": [
      "self",
      "old_shape",
      "longest_edge"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "pad_size",
      "mask_size",
      "mask_pad_size",
      "default_to_square",
      "image_mean",
      "image_std",
      "data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "disable_grouping",
      "return_tensors"
    ],
    "generate_crop_boxes": [
      "self",
      "image",
      "target_size",
      "crop_n_layers",
      "overlap_ratio",
      "points_per_crop",
      "crop_n_points_downscale_factor",
      "device"
    ],
    "filter_masks": [
      "self",
      "masks",
      "iou_scores",
      "original_size",
      "cropped_box_image",
      "pred_iou_thresh",
      "stability_score_thresh",
      "mask_threshold",
      "stability_score_offset"
    ],
    "post_process_masks": [
      "self",
      "masks",
      "original_sizes",
      "reshaped_input_sizes",
      "mask_threshold",
      "binarize",
      "pad_size"
    ],
    "post_process_for_mask_generation": [
      "self",
      "all_masks",
      "all_scores",
      "all_boxes",
      "crops_nms_thresh"
    ]
  },
  "SamImageProcessorKwargs": {},
  "SamImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "mask_size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "mask_pad_size",
      "do_convert_rgb"
    ],
    "pad_image": [
      "self",
      "image",
      "pad_size",
      "data_format",
      "input_data_format"
    ],
    "_get_preprocess_shape": [
      "self",
      "old_shape",
      "longest_edge"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_resize",
      "do_rescale",
      "do_normalize",
      "size",
      "resample",
      "rescale_factor",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_resize",
      "mask_size",
      "do_pad",
      "mask_pad_size",
      "input_data_format"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "do_resize",
      "size",
      "mask_size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "mask_pad_size",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_masks": [
      "self",
      "masks",
      "original_sizes",
      "reshaped_input_sizes",
      "mask_threshold",
      "binarize",
      "pad_size",
      "return_tensors"
    ],
    "_post_process_masks_pt": [
      "self",
      "masks",
      "original_sizes",
      "reshaped_input_sizes",
      "mask_threshold",
      "binarize",
      "pad_size"
    ],
    "post_process_for_mask_generation": [
      "self",
      "all_masks",
      "all_scores",
      "all_boxes",
      "crops_nms_thresh",
      "return_tensors"
    ],
    "generate_crop_boxes": [
      "self",
      "image",
      "target_size",
      "crop_n_layers",
      "overlap_ratio",
      "points_per_crop",
      "crop_n_points_downscale_factor",
      "device",
      "input_data_format",
      "return_tensors"
    ],
    "filter_masks": [
      "self",
      "masks",
      "iou_scores",
      "original_size",
      "cropped_box_image",
      "pred_iou_thresh",
      "stability_score_thresh",
      "mask_threshold",
      "stability_score_offset",
      "return_tensors"
    ],
    "_filter_masks_pt": [
      "self",
      "masks",
      "iou_scores",
      "original_size",
      "cropped_box_image",
      "pred_iou_thresh",
      "stability_score_thresh",
      "mask_threshold",
      "stability_score_offset"
    ]
  },
  "_compute_stability_score_pt": [
    "masks",
    "mask_threshold",
    "stability_score_offset"
  ],
  "_mask_to_rle_pytorch": [
    "input_mask"
  ],
  "_postprocess_for_mg": [
    "rle_masks",
    "iou_scores",
    "mask_boxes",
    "amg_crops_nms_thresh"
  ],
  "SamPromptEncoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "image_size",
      "patch_size",
      "mask_input_channels",
      "num_point_embeddings",
      "hidden_act",
      "layer_norm_eps"
    ]
  },
  "SamMaskDecoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "hidden_act",
      "mlp_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "attention_downsample_rate",
      "num_multimask_outputs",
      "iou_head_depth",
      "iou_head_hidden_dim",
      "layer_norm_eps"
    ]
  },
  "SamVisionConfig": {
    "base_config_key": [],
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "output_channels",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "qkv_bias",
      "mlp_ratio",
      "use_abs_pos",
      "use_rel_pos",
      "window_size",
      "global_attn_indexes",
      "num_pos_feats",
      "mlp_dim"
    ]
  },
  "SamConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "prompt_encoder_config",
      "mask_decoder_config",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "MiniMaxM2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "attribute_map": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "attention_dropout",
      "num_experts_per_tok",
      "num_local_experts",
      "output_router_logits",
      "router_aux_loss_coef",
      "router_jitter_noise",
      "rope_parameters"
    ]
  },
  "MiniMaxM2TopKRouter": {
    "forward": [
      "self",
      "hidden_states",
      "e_score_correction_bias"
    ]
  },
  "MiniMaxM2Experts": {},
  "MiniMaxM2SparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MiniMaxM2RMSNorm": {},
  "MiniMaxM2RotaryEmbedding": {},
  "MiniMaxM2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "MiniMaxM2PreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MiniMaxM2Model": {
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "MiniMaxM2ForCausalLM": {},
  "MiniMaxM2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "InternVLVisionRMSNorm": {},
  "InternVLVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "InternVLVisionModelOutputWithPooling": {},
  "InternVLVisionPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "InternVLVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "InternVLVisionMLP": {},
  "NORM2FN": [],
  "InternVLVisionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InternVLVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InternVLVisionPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "InternVLVisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "InternVLPreTrainedModel": {
    "input_modalities": []
  },
  "INTERNVL_INPUTS_DOCSTRING": [],
  "InternVLMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "InternVLModelOutputWithPast": {},
  "InternVLModel": {
    "pixel_shuffle": [
      "self",
      "vision_features",
      "scale_factor"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "cache_position"
    ]
  },
  "InternVLCausalLMOutputWithPast": {},
  "InternVLForConditionalGeneration": {
    "forward": []
  },
  "InternVLVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "attention_bias",
      "use_qk_norm",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_dropout",
      "projection_dropout",
      "initializer_range",
      "norm_type",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "use_mask_token",
      "use_absolute_position_embeddings",
      "layer_scale_init_value",
      "use_mean_pooling"
    ]
  },
  "InternVLConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_id",
      "image_seq_length",
      "downsample_ratio",
      "projector_hidden_act",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "tie_word_embeddings"
    ]
  },
  "InternVLVideoProcessorInitKwargs": {},
  "InternVLVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "initial_shift": [],
    "do_sample_frames": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "num_frames",
      "fps",
      "initial_shift"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors"
    ]
  },
  "InternVLProcessorKwargs": {
    "_defaults": []
  },
  "InternVLProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "image_seq_length",
      "chat_template"
    ],
    "_insert_media_placeholders": [
      "self",
      "text",
      "image_pixel_values",
      "video_pixel_values",
      "image_num_patches",
      "video_num_patches",
      "image_num_patches_indices",
      "video_num_patches_indices",
      "video_patch_indices"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "RobertaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "RobertaEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "RobertaSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "RobertaCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "RobertaSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RobertaAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "RobertaIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RobertaOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RobertaLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "RobertaPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RobertaEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "RobertaPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RobertaModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "RobertaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "RobertaForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "RobertaLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "RobertaForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "RobertaForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "RobertaForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "RobertaClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "RobertaForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "RobertaTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "errors",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space",
      "trim_offsets"
    ]
  },
  "RobertaTokenizerFast": {
    "vocab_files_names": [],
    "model_input_names": [],
    "slow_tokenizer_class": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "tokenizer_file",
      "errors",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space",
      "trim_offsets"
    ],
    "mask_token": [
      "self",
      "value"
    ],
    "_batch_encode_plus": [
      "self"
    ],
    "_encode_plus": [
      "self"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "EncodecOutput": {},
  "EncodecEncoderOutput": {},
  "EncodecDecoderOutput": {},
  "EncodecConv1d": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "dilation"
    ],
    "_get_extra_padding_for_conv1d": [
      "self",
      "hidden_states"
    ],
    "_pad1d": [
      "hidden_states",
      "paddings",
      "mode",
      "value"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EncodecConvTranspose1d": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EncodecLSTM": {
    "__init__": [
      "self",
      "config",
      "dimension"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EncodecResnetBlock": {
    "__init__": [
      "self",
      "config",
      "dim",
      "dilations"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EncodecEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EncodecDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EncodecEuclideanCodebook": {
    "__init__": [
      "self",
      "config"
    ],
    "quantize": [
      "self",
      "hidden_states"
    ],
    "encode": [
      "self",
      "hidden_states"
    ],
    "decode": [
      "self",
      "embed_ind"
    ]
  },
  "EncodecVectorQuantization": {
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "hidden_states"
    ],
    "decode": [
      "self",
      "embed_ind"
    ]
  },
  "EncodecResidualVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "get_num_quantizers_for_bandwidth": [
      "self",
      "bandwidth"
    ],
    "encode": [
      "self",
      "embeddings",
      "bandwidth"
    ],
    "decode": [
      "self",
      "codes"
    ]
  },
  "EncodecPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EncodecModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_encode_frame": [
      "self",
      "input_values",
      "bandwidth"
    ],
    "encode": [
      "self",
      "input_values",
      "padding_mask",
      "bandwidth",
      "return_dict"
    ],
    "_linear_overlap_add": [
      "frames",
      "stride"
    ],
    "_decode_frame": [
      "self",
      "codes",
      "scale"
    ],
    "decode": [
      "self",
      "audio_codes",
      "audio_scales",
      "padding_mask",
      "return_dict",
      "last_frame_pad_length"
    ],
    "forward": [
      "self",
      "input_values",
      "padding_mask",
      "bandwidth",
      "audio_codes",
      "audio_scales",
      "return_dict",
      "last_frame_pad_length"
    ]
  },
  "EncodecFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "chunk_length_s",
      "overlap"
    ],
    "chunk_length": [
      "self"
    ],
    "chunk_stride": [
      "self"
    ],
    "__call__": [
      "self",
      "raw_audio",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "sampling_rate"
    ]
  },
  "EncodecConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "target_bandwidths",
      "sampling_rate",
      "audio_channels",
      "normalize",
      "chunk_length_s",
      "overlap",
      "hidden_size",
      "num_filters",
      "num_residual_layers",
      "upsampling_ratios",
      "norm_type",
      "kernel_size",
      "last_kernel_size",
      "residual_kernel_size",
      "dilation_growth_rate",
      "use_causal_conv",
      "pad_mode",
      "compress",
      "num_lstm_layers",
      "trim_right_ratio",
      "codebook_size",
      "codebook_dim",
      "use_conv_shortcut"
    ],
    "chunk_length": [
      "self"
    ],
    "chunk_stride": [
      "self"
    ],
    "hop_length": [
      "self"
    ],
    "codebook_nbits": [
      "self"
    ],
    "frame_rate": [
      "self"
    ],
    "num_quantizers": [
      "self"
    ]
  },
  "LightGlueConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "keypoint_detector_config",
      "descriptor_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "depth_confidence",
      "width_confidence",
      "filter_threshold",
      "initializer_range",
      "hidden_act",
      "attention_dropout",
      "attention_bias",
      "trust_remote_code"
    ]
  },
  "LightGlueKeypointMatchingOutput": {},
  "LightGlueImageProcessorKwargs": {},
  "LightGlueImageProcessor": {
    "post_process_keypoint_matching": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ]
  },
  "LightGlueImageProcessorFast": {
    "post_process_keypoint_matching": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ]
  },
  "LightGluePositionalEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "keypoints",
      "output_hidden_states"
    ]
  },
  "LightGlueAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "LightGlueMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LightGlueTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "descriptors",
      "keypoints",
      "attention_mask",
      "output_hidden_states",
      "output_attentions"
    ]
  },
  "sigmoid_log_double_softmax": [
    "similarity",
    "matchability0",
    "matchability1"
  ],
  "LightGlueMatchAssignmentLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "descriptors",
      "mask"
    ],
    "get_matchability": [
      "self",
      "descriptors"
    ]
  },
  "LightGlueTokenConfidenceLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "descriptors"
    ]
  },
  "LightGluePreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": []
  },
  "get_matches_from_scores": [
    "scores",
    "threshold"
  ],
  "normalize_keypoints": [
    "keypoints",
    "height",
    "width"
  ],
  "LightGlueForKeypointMatching": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_confidence_threshold": [
      "self",
      "layer_index"
    ],
    "_keypoint_processing": [
      "self",
      "descriptors",
      "keypoints",
      "output_hidden_states"
    ],
    "_get_early_stopped_image_pairs": [
      "self",
      "keypoint_confidences",
      "layer_index",
      "mask",
      "num_points"
    ],
    "_get_keypoint_matching": [
      "self",
      "descriptors",
      "mask",
      "layer_index",
      "early_stops"
    ],
    "_get_pruning_mask": [
      "self",
      "confidences",
      "scores",
      "layer_index"
    ],
    "_do_layer_keypoint_pruning": [
      "self",
      "descriptors",
      "keypoints",
      "mask",
      "indices",
      "prune_output",
      "keypoint_confidences",
      "layer_index"
    ],
    "_concat_early_stopped_outputs": [
      "self",
      "early_stops_indices",
      "final_pruned_keypoints_indices",
      "final_pruned_keypoints_iterations",
      "matches",
      "matching_scores"
    ],
    "_do_final_keypoint_pruning": [
      "self",
      "indices",
      "matches",
      "matching_scores",
      "num_keypoints"
    ],
    "_match_image_pair": [
      "self",
      "keypoints",
      "descriptors",
      "height",
      "width",
      "mask",
      "output_attentions",
      "output_hidden_states"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "_is_valid_image": [
    "image"
  ],
  "flatten_pair_images": [
    "images"
  ],
  "is_grayscale": [
    "image"
  ],
  "convert_to_grayscale": [
    "image"
  ],
  "validate_and_format_image_pairs": [
    "images"
  ],
  "GraniteSpeechEncoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "input_dim",
      "num_layers",
      "hidden_dim",
      "feedforward_mult",
      "num_heads",
      "dim_head",
      "output_dim",
      "context_size",
      "max_pos_emb",
      "dropout",
      "conv_kernel_size",
      "conv_expansion_factor"
    ]
  },
  "GraniteSpeechConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "encoder_config",
      "projector_config",
      "audio_token_index",
      "initializer_range",
      "has_lora_adapter",
      "downsample_rate",
      "window_size"
    ]
  },
  "GraniteSpeechCausalLMOutputWithPast": {},
  "GraniteSpeechEncoderProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteSpeechConformerFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteSpeechConformerAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_dists"
    ]
  },
  "GraniteSpeechConformerDepthWiseConv1d": {
    "__init__": [
      "self",
      "chan_in",
      "chan_out",
      "kernel_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteSpeechConformerConvModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteSpeechConformerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_dists"
    ]
  },
  "GraniteSpeechPreTrainedModel": {
    "input_modalities": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GraniteSpeechCTCEncoder": {
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteSpeechForConditionalGeneration": {
    "__init__": [
      "self",
      "config"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_audio_features": [
      "self",
      "input_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "input_features_mask",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "input_features",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "get_merged_audio_embeddings": [
      "self",
      "input_ids",
      "audio_features",
      "input_features_mask"
    ],
    "generate": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ],
    "_get_adapter_name": [
      "self"
    ]
  },
  "small_init_method": [
    "dim"
  ],
  "wang_init_method": [
    "n_layers",
    "dim"
  ],
  "xLSTMPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_is_stateful": [],
    "_module_name_map": [
      "self",
      "module"
    ],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "xLSTMCache": {
    "__init__": [
      "self",
      "config",
      "max_batch_size",
      "dtype",
      "device"
    ],
    "reset": [
      "self"
    ]
  },
  "xLSTMOutput": {},
  "xLSTMModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embedding"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "use_cache",
      "output_hidden_states"
    ]
  },
  "xLSTMCausalLMOutput": {},
  "xLSTMForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "labels",
      "use_cache",
      "output_hidden_states"
    ]
  },
  "xLSTMConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "embedding_dim",
      "num_hidden_layers",
      "num_blocks",
      "num_heads",
      "use_bias",
      "norm_reduction_force_float32",
      "tie_word_embeddings",
      "add_out_norm",
      "norm_eps",
      "qk_dim_factor",
      "v_dim_factor",
      "chunkwise_kernel",
      "sequence_kernel",
      "step_kernel",
      "mode",
      "chunk_size",
      "return_last_states",
      "autocast_kernel_dtype",
      "eps",
      "inference_state_dtype",
      "ffn_proj_factor",
      "ffn_round_up_to_multiple_of",
      "gate_soft_cap",
      "output_logit_soft_cap",
      "weight_mode",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_inference_chunksize"
    ],
    "qk_dim": [
      "self"
    ],
    "v_dim": [
      "self"
    ],
    "qk_head_dim": [
      "self"
    ],
    "v_head_dim": [
      "self"
    ],
    "to_xlstm_block_config": [
      "self"
    ]
  },
  "EfficientLoFTRImageProcessorFast": {
    "post_process_keypoint_matching": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ]
  },
  "EfficientLoFTRKeypointMatchingOutput": {},
  "compute_embeddings": [
    "inv_freq",
    "embed_height",
    "embed_width",
    "hidden_size"
  ],
  "EfficientLoFTRRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "layer_type"
    ]
  },
  "EfficientLoFTRConvNormLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "EfficientLoFTRRepVGGBlock": {
    "__init__": [
      "self",
      "config",
      "stage_idx",
      "block_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientLoFTRRepVGGStage": {
    "__init__": [
      "self",
      "config",
      "stage_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientLoFTRepVGG": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientLoFTRAggregationLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states"
    ]
  },
  "EfficientLoFTRAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "position_embeddings"
    ]
  },
  "EfficientLoFTRMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientLoFTRAggregatedAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "position_embeddings"
    ]
  },
  "EfficientLoFTRLocalFeatureTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings"
    ]
  },
  "EfficientLoFTRLocalFeatureTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings"
    ]
  },
  "EfficientLoFTROutConvBlock": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual_states"
    ]
  },
  "EfficientLoFTRFineFusionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward_pyramid": [
      "self",
      "hidden_states",
      "residual_states"
    ],
    "forward": [
      "self",
      "coarse_features",
      "residual_features"
    ]
  },
  "EfficientLoFTRPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "extract_one_channel_pixel_values": [
      "self",
      "pixel_values"
    ]
  },
  "EfficientLoFTRModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "mask_border": [
    "tensor",
    "border_margin",
    "value"
  ],
  "create_meshgrid": [
    "height",
    "width",
    "normalized_coordinates",
    "device",
    "dtype"
  ],
  "spatial_expectation2d": [
    "input",
    "normalized_coordinates"
  ],
  "EfficientLoFTRForKeypointMatching": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_matches_from_scores": [
      "self",
      "scores"
    ],
    "_coarse_matching": [
      "self",
      "coarse_features",
      "coarse_scale"
    ],
    "_get_first_stage_fine_matching": [
      "self",
      "fine_confidence",
      "coarse_matched_keypoints",
      "fine_window_size",
      "fine_scale"
    ],
    "_get_second_stage_fine_matching": [
      "self",
      "indices",
      "fine_matches",
      "fine_confidence",
      "fine_window_size",
      "fine_scale"
    ],
    "_fine_matching": [
      "self",
      "fine_features_0",
      "fine_features_1",
      "coarse_matched_keypoints",
      "fine_scale"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "EfficientLoFTRImageProcessorKwargs": {},
  "EfficientLoFTRImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_grayscale"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_grayscale",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_keypoint_matching": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ],
    "visualize_keypoint_matching": [
      "self",
      "images",
      "keypoint_matching_output"
    ],
    "_get_color": [
      "self",
      "score"
    ]
  },
  "EfficientLoFTRConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "stage_num_blocks",
      "out_features",
      "stage_stride",
      "hidden_size",
      "activation_function",
      "q_aggregation_kernel_size",
      "kv_aggregation_kernel_size",
      "q_aggregation_stride",
      "kv_aggregation_stride",
      "num_attention_layers",
      "num_attention_heads",
      "attention_dropout",
      "attention_bias",
      "mlp_activation_function",
      "coarse_matching_skip_softmax",
      "coarse_matching_threshold",
      "coarse_matching_temperature",
      "coarse_matching_border_removal",
      "fine_kernel_size",
      "batch_norm_eps",
      "rope_parameters",
      "fine_matching_slice_dim",
      "fine_matching_regress_temperature",
      "initializer_range"
    ]
  },
  "round_by_factor": [
    "number",
    "factor"
  ],
  "find_closest_aspect_ratio": [
    "aspect_ratio",
    "target_ratios",
    "width",
    "height",
    "image_size"
  ],
  "get_image_size_for_max_num_patches": [
    "image_height",
    "image_width",
    "patch_size",
    "max_num_patches",
    "eps"
  ],
  "convert_image_to_patches": [
    "images",
    "patch_size"
  ],
  "pad_along_first_dim": [
    "images",
    "target_length",
    "pad_value"
  ],
  "Lfm2VlImageProcessorKwargs": {},
  "Lfm2VlImageProcessorFast": {
    "downsample_factor": [],
    "do_image_splitting": [],
    "min_tiles": [],
    "max_tiles": [],
    "use_thumbnail": [],
    "min_image_tokens": [],
    "max_image_tokens": [],
    "encoder_patch_size": [],
    "tile_size": [],
    "max_pixels_tolerance": [],
    "do_resize": [],
    "size": [],
    "resample": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "do_pad": [],
    "return_row_col_info": [],
    "image_mean": [],
    "image_std": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_target_ratios": [
      "self",
      "min_tiles",
      "max_tiles"
    ],
    "_get_grid_layout": [
      "self",
      "height",
      "width",
      "min_tiles",
      "max_tiles",
      "tile_size"
    ],
    "crop_image_to_patches": [
      "self",
      "image",
      "min_tiles",
      "max_tiles",
      "tile_size",
      "use_thumbnail",
      "thumbnail_size",
      "interpolation",
      "antialias"
    ],
    "smart_resize": [
      "self",
      "height",
      "width",
      "downsample_factor",
      "min_image_tokens",
      "max_image_tokens",
      "encoder_patch_size"
    ],
    "_is_image_too_large": [
      "self",
      "height",
      "width",
      "max_image_tokens",
      "encoder_patch_size",
      "downsample_factor",
      "max_pixels_tolerance"
    ],
    "resize_and_split": [
      "self",
      "images",
      "downsample_factor",
      "min_tiles",
      "max_tiles",
      "use_thumbnail",
      "min_image_tokens",
      "max_image_tokens",
      "encoder_patch_size",
      "tile_size",
      "max_pixels_tolerance",
      "interpolation"
    ],
    "_preprocess": [
      "self",
      "images",
      "size",
      "interpolation",
      "do_resize",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "downsample_factor",
      "do_image_splitting",
      "min_tiles",
      "max_tiles",
      "use_thumbnail",
      "min_image_tokens",
      "max_image_tokens",
      "encoder_patch_size",
      "tile_size",
      "max_pixels_tolerance",
      "return_tensors",
      "disable_grouping",
      "do_pad",
      "return_row_col_info"
    ]
  },
  "Lfm2VlTextKwargs": {},
  "Lfm2VlProcessorKwargs": {
    "_defaults": []
  },
  "Lfm2VlProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "expand_text_with_placeholders": [
      "self",
      "text",
      "images",
      "image_rows",
      "image_cols",
      "image_sizes",
      "use_image_special_tokens"
    ],
    "_build_image_tokens": [
      "self",
      "rows",
      "cols",
      "tokens_per_tile",
      "tokens_for_image",
      "use_thumbnail",
      "use_image_special_tokens"
    ],
    "_compute_tokens_per_tile": [
      "self",
      "tile_size",
      "encoder_patch_size",
      "downsample_factor"
    ],
    "_compute_tokens_for_image": [
      "self",
      "image_size",
      "encoder_patch_size",
      "downsample_factor"
    ],
    "_get_image_num_tokens": [
      "self",
      "image_size"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Lfm2VlConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_id",
      "projector_hidden_act",
      "projector_hidden_size",
      "projector_bias",
      "projector_use_layernorm",
      "downsample_factor",
      "tie_word_embeddings"
    ]
  },
  "Lfm2VlMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ],
    "pixel_unshuffle": [
      "self",
      "hidden_states"
    ]
  },
  "Lfm2VlPreTrainedModel": {
    "_can_compile_fullgraph": [],
    "base_model_prefix": []
  },
  "Lfm2VlCausalLMOutputWithPast": {},
  "Lfm2VlModelOutputWithPast": {},
  "Lfm2VlModel": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "spatial_shapes",
      "pixel_attention_mask"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "pixel_values",
      "spatial_shapes",
      "pixel_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Lfm2VlForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "get_image_features": [
      "self",
      "pixel_values",
      "spatial_shapes",
      "pixel_attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "spatial_shapes",
      "pixel_attention_mask",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "DonutSwinEncoderOutput": {},
  "DonutSwinModelOutput": {},
  "DonutSwinImageClassifierOutput": {},
  "DonutSwinEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "DonutSwinPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "maybe_pad": [
      "self",
      "pixel_values",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DonutSwinPatchMerging": {
    "__init__": [
      "self",
      "input_resolution",
      "dim",
      "norm_layer"
    ],
    "maybe_pad": [
      "self",
      "input_feature",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "input_feature",
      "input_dimensions"
    ]
  },
  "DonutSwinDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "DonutSwinSelfAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "create_relative_position_index": [
      "self"
    ]
  },
  "DonutSwinSelfOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "DonutSwinAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "DonutSwinIntermediate": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DonutSwinOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DonutSwinLayer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "num_heads",
      "drop_path_rate",
      "shift_size"
    ],
    "set_shift_and_window_size": [
      "self",
      "input_resolution"
    ],
    "get_attn_mask": [
      "self",
      "height",
      "width",
      "dtype",
      "device"
    ],
    "maybe_pad": [
      "self",
      "hidden_states",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "always_partition"
    ]
  },
  "DonutSwinStage": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "depth",
      "num_heads",
      "drop_path",
      "downsample"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "always_partition"
    ]
  },
  "DonutSwinEncoder": {
    "__init__": [
      "self",
      "config",
      "grid_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "output_hidden_states",
      "output_hidden_states_before_downsampling",
      "always_partition",
      "return_dict"
    ]
  },
  "DonutSwinPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DonutSwinModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "use_mask_token"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "DonutSwinForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "DonutImageProcessorKwargs": {},
  "DonutImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_thumbnail",
      "do_align_long_axis",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "align_long_axis": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "pad_image": [
      "self",
      "image",
      "size",
      "random_padding",
      "data_format",
      "input_data_format"
    ],
    "thumbnail": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_thumbnail",
      "do_align_long_axis",
      "do_pad",
      "random_padding",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "DonutProcessorKwargs": {
    "_defaults": []
  },
  "DonutProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "model_input_names": [
      "self"
    ],
    "token2json": [
      "self",
      "tokens",
      "is_inner_value",
      "added_vocab"
    ]
  },
  "DonutImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_thumbnail": [],
    "do_align_long_axis": [],
    "do_pad": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "align_long_axis": [
      "self",
      "image",
      "size"
    ],
    "pad_image": [
      "self",
      "image",
      "size",
      "random_padding"
    ],
    "thumbnail": [
      "self",
      "image",
      "size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "do_thumbnail",
      "do_align_long_axis",
      "do_pad",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "DonutSwinConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim",
      "depths",
      "num_heads",
      "window_size",
      "mlp_ratio",
      "qkv_bias",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "drop_path_rate",
      "hidden_act",
      "use_absolute_embeddings",
      "initializer_range",
      "layer_norm_eps"
    ]
  },
  "markdown_compatible": [
    "text"
  ],
  "normalize_list_like_lines": [
    "generation"
  ],
  "find_next_punctuation": [
    "text",
    "start_idx"
  ],
  "truncate_repetitions": [
    "text",
    "min_len"
  ],
  "remove_numbers": [
    "lines"
  ],
  "get_slices": [
    "lines",
    "clean_lines"
  ],
  "remove_slice_from_lines": [
    "lines",
    "clean_text",
    "slice"
  ],
  "NougatTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "errors",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "vocab",
      "merges"
    ],
    "remove_hallucinated_references": [
      "self",
      "text"
    ],
    "correct_tables": [
      "self",
      "generation"
    ],
    "post_process_single": [
      "self",
      "generation",
      "fix_markdown"
    ],
    "post_process_generation": [
      "self",
      "generation",
      "fix_markdown",
      "num_workers"
    ]
  },
  "NougatImageProcessorKwargs": {},
  "NougatImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_crop_margin",
      "do_resize",
      "size",
      "resample",
      "do_thumbnail",
      "do_align_long_axis",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "python_find_non_zero": [
      "self",
      "image"
    ],
    "python_bounding_rect": [
      "self",
      "coordinates"
    ],
    "crop_margin": [
      "self",
      "image",
      "gray_threshold",
      "data_format",
      "input_data_format"
    ],
    "align_long_axis": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "pad_image": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "thumbnail": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_crop_margin",
      "do_resize",
      "size",
      "resample",
      "do_thumbnail",
      "do_align_long_axis",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "NougatImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_rescale": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "python_find_non_zero": [
      "self",
      "image"
    ],
    "python_bounding_rect": [
      "self",
      "coordinates"
    ],
    "crop_margin": [
      "self",
      "image",
      "gray_threshold"
    ],
    "align_long_axis": [
      "self",
      "image",
      "size"
    ],
    "thumbnail": [
      "self",
      "image",
      "size"
    ],
    "pad_images": [
      "self",
      "image",
      "size"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "do_align_long_axis",
      "do_thumbnail",
      "do_pad",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_crop_margin",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "NougatProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "do_crop_margin",
      "do_resize",
      "size",
      "resample",
      "do_thumbnail",
      "do_align_long_axis",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format",
      "text_pair",
      "text_target",
      "text_pair_target",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "post_process_generation": [
      "self"
    ]
  },
  "get_factors": [
    "dividend"
  ],
  "get_max_res_without_distortion": [
    "image_size",
    "target_size"
  ],
  "find_supported_resolutions": [
    "max_num_chunks",
    "patch_size"
  ],
  "pad_to_best_fit": [
    "images",
    "target_size",
    "background_color"
  ],
  "get_best_fit": [
    "image_size",
    "possible_resolutions",
    "resize_to_max_canvas"
  ],
  "Llama4ImageProcessorKwargs": {},
  "Llama4ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "max_patches": [],
    "resize_to_max_canvas": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "rescale_and_normalize": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "size",
      "max_patches",
      "resize_to_max_canvas",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "Llama4ProcessorKwargs": {
    "_defaults": []
  },
  "chat_template": [],
  "Llama4Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "patch_size",
      "pixel_shuffle_ratio",
      "fake_image_token",
      "image_token",
      "start_of_image_token",
      "end_of_image_token",
      "patch_token",
      "tile_x_separator_token",
      "tile_y_separator_token",
      "chat_template"
    ],
    "_prompt_split_image": [
      "self",
      "aspect_ratio",
      "num_patches_per_chunk"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ]
  },
  "Llama4VisionConfig": {
    "base_model_tp_plan": [],
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "hidden_act",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "intermediate_size",
      "vision_output_dim",
      "image_size",
      "patch_size",
      "norm_eps",
      "vision_feature_select_strategy",
      "initializer_range",
      "pixel_shuffle_ratio",
      "projector_input_dim",
      "projector_output_dim",
      "multi_modal_projector_bias",
      "projector_dropout",
      "attention_dropout",
      "rope_parameters"
    ]
  },
  "Llama4TextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_ep_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "intermediate_size_mlp",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "attention_dropout",
      "num_experts_per_tok",
      "num_local_experts",
      "moe_layers",
      "interleave_moe_layer_step",
      "use_qk_norm",
      "output_router_logits",
      "router_aux_loss_coef",
      "router_jitter_noise",
      "rope_parameters",
      "no_rope_layers",
      "no_rope_layer_interval",
      "attention_chunk_size",
      "layer_types",
      "attn_temperature_tuning",
      "floor_scale",
      "attn_scale"
    ]
  },
  "Llama4Config": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "boi_token_index",
      "eoi_token_index",
      "image_token_index",
      "tie_word_embeddings"
    ]
  },
  "Llama4TextExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4TextMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Llama4TextL2Norm": {
    "__init__": [
      "self",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Llama4TextRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Llama4Router": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4TextMoe": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4TextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "vision_eager_attention_forward": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "scaling",
    "dropout"
  ],
  "Llama4TextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Llama4TextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Llama4PreTrainedModel": {
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Llama4TextModel": {
    "_no_split_modules": [],
    "base_model_prefix": [],
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Llama4ForCausalLM": {
    "_no_split_modules": [],
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Llama4CausalLMOutputWithPast": {},
  "Llama4VisionMLP2": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "pixel_shuffle": [
    "input_tensor",
    "shuffle_ratio"
  ],
  "Llama4VisionPixelShuffleMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoded_patches"
    ]
  },
  "reshape_for_broadcast": [
    "freqs_ci",
    "query"
  ],
  "vision_apply_rotary_emb": [
    "query",
    "key",
    "freqs_ci"
  ],
  "Llama4VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "freqs_ci",
      "attention_mask",
      "past_key_values"
    ]
  },
  "Llama4VisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4VisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "freqs_ci",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Llama4VisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "freqs_ci",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Llama4UnfoldConvolution": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4VisionRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4VisionModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Llama4ForConditionalGeneration": {
    "_no_split_modules": [],
    "_tp_plan": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_select_strategy"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_select_strategy",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "CsmOutputWithPast": {},
  "CsmRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "CsmRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "CsmMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CsmAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "CsmDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "CsmPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CsmDepthDecoderModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "backbone_last_hidden_state",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "CsmCodebooksHead": {
    "__init__": [
      "self",
      "hidden_size",
      "num_codebooks",
      "vocab_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_position"
    ]
  },
  "CsmDepthDecoderForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "backbone_last_hidden_state",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position"
    ]
  },
  "CsmBackboneModelEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "CsmBackboneModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "CsmForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "from_pretrained": [
      "cls"
    ],
    "save_pretrained": [
      "self"
    ],
    "_merge_input_ids_with_input_values": [
      "self",
      "input_ids",
      "input_values",
      "input_values_cutoffs",
      "labels"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_values",
      "attention_mask",
      "input_values_cutoffs",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "CsmGenerateOutput": {},
  "CsmGenerationMixin": {
    "_get_stopping_criteria": [
      "self"
    ],
    "_prepare_generation_config": [
      "self",
      "generation_config"
    ],
    "_sample": [
      "self",
      "input_ids",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus",
      "streamer"
    ],
    "generate": [
      "self",
      "input_ids",
      "input_values",
      "input_values_cutoffs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "synced_gpus",
      "streamer",
      "output_audio"
    ]
  },
  "CsmAudioKwargs": {},
  "CsmProcessorKwargs": {
    "_defaults": []
  },
  "CsmProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer",
      "chat_template"
    ],
    "_get_encoded_length": [
      "audio_length",
      "kernel_sizes",
      "strides",
      "dilations",
      "use_causal_conv"
    ],
    "save_audio": [
      "self",
      "audio",
      "saving_path"
    ],
    "__call__": [
      "self",
      "text",
      "audio",
      "output_labels",
      "depth_decoder_labels_ratio"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "CsmDepthDecoderConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "num_codebooks",
      "backbone_hidden_size",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "head_dim"
    ]
  },
  "CsmConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "num_codebooks",
      "vocab_size",
      "text_vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "codebook_pad_token_id",
      "codebook_eos_token_id",
      "bos_token_id",
      "eos_token_id",
      "audio_token_id",
      "audio_eos_token_id",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "head_dim",
      "tie_codebooks_embeddings",
      "depth_decoder_config",
      "codec_config"
    ]
  },
  "ModernBertDecoderConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "initializer_cutoff_factor",
      "norm_eps",
      "norm_bias",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "cls_token_id",
      "sep_token_id",
      "attention_bias",
      "attention_dropout",
      "embedding_dropout",
      "mlp_bias",
      "mlp_dropout",
      "decoder_bias",
      "classifier_dropout",
      "classifier_bias",
      "classifier_activation",
      "use_cache",
      "local_attention",
      "global_attn_every_n_layers",
      "layer_types",
      "tie_word_embeddings",
      "rope_parameters"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "ModernBertDecoderEmbeddings": {},
  "ModernBertDecoderMLP": {},
  "ModernBertDecoderRotaryEmbedding": {},
  "ModernBertDecoderAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ModernBertDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ModernBertDecoderPredictionHead": {},
  "ModernBertDecoderPreTrainedModel": {
    "_skip_keys_device_placement": [],
    "_no_split_modules": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_check_and_adjust_attn_implementation": [
      "self",
      "attn_implementation",
      "is_init_check"
    ]
  },
  "ModernBertDecoderModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "ModernBertDecoderForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "ModernBertDecoderForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache"
    ]
  },
  "BitNetRMSNorm": {},
  "BitNetMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BitNetAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BitNetDecoderLayer": {},
  "BitNetModel": {},
  "BitNetForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "forward": [
      "self"
    ]
  },
  "BitNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "attention_bias",
      "attention_dropout",
      "rope_parameters"
    ]
  },
  "BitNetRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "BitNetPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "MvpConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "classifier_dropout",
      "scale_embedding",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "is_encoder_decoder",
      "decoder_start_token_id",
      "use_prompt",
      "prompt_length",
      "prompt_mid_dim",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "MvpLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "MvpAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "attn_prompt",
      "output_attentions",
      "cache_position"
    ]
  },
  "MvpEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "self_attn_prompt",
      "output_attentions"
    ]
  },
  "MvpDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "self_attn_prompt",
      "cross_attn_prompt",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MvpClassificationHead": {
    "__init__": [
      "self",
      "input_dim",
      "inner_dim",
      "num_classes",
      "pooler_dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MvpPrompt": {
    "__init__": [
      "self",
      "config",
      "num_layers",
      "num_heads"
    ],
    "forward": [
      "self",
      "prompt_ids"
    ]
  },
  "MvpPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "MvpEncoder": {
    "__init__": [
      "self",
      "config",
      "embed_tokens",
      "use_prompt"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MvpDecoder": {
    "__init__": [
      "self",
      "config",
      "use_prompt"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MvpModel": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "set_lightweight_tuning": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MvpForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "set_lightweight_tuning": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "MvpForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "set_lightweight_tuning": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MvpForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "set_lightweight_tuning": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MvpDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "MvpForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "set_lightweight_tuning": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BridgeTowerImageProcessorKwargs": {},
  "BridgeTowerImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_center_crop",
      "crop_size",
      "do_pad"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "size_divisor",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "center_crop": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "images",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_center_crop",
      "crop_size",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "BridgeTowerProcessorKwargs": {
    "_defaults": []
  },
  "BridgeTowerProcessor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "_TOKENIZER_FOR_DOC": [],
  "BridgeTowerModelOutput": {},
  "BridgeTowerContrastiveOutput": {},
  "BridgeTowerResidualAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "attention": [
      "self",
      "hidden_state",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "BridgeTowerTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "BridgeTowerVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "BridgeTowerVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "attention_mask",
      "interpolate_pos_encoding"
    ],
    "forward_pre": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward_post": [
      "self",
      "hidden_state"
    ]
  },
  "BridgeTowerLinkTower": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cross_modal_hidden_states",
      "attention_mask"
    ]
  },
  "BridgeTowerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BridgeTowerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BridgeTowerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BridgeTowerPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BridgeTowerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BridgeTowerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "BridgeTowerAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BridgeTowerBertCrossLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "encoder_attention_mask",
      "past_key_values"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "BridgeTowerTextLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "BridgeTowerTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "BridgeTowerTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "BridgeTowerPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BridgeTowerVisionModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "dtype": [
      "self"
    ],
    "forward": [
      "self",
      "image",
      "image_mask",
      "interpolate_pos_encoding"
    ]
  },
  "BridgeTowerTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "BridgeTowerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "image_token_type_idx",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels",
      "interpolate_pos_encoding"
    ],
    "get_cls_features": [
      "self",
      "text_features",
      "image_features"
    ]
  },
  "BridgeTowerPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BridgeTowerMLMHead": {
    "__init__": [
      "self",
      "config",
      "weight"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BridgeTowerITMHead": {
    "__init__": [
      "self",
      "hidden_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BridgeTowerForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "BridgeTowerForImageAndTextRetrieval": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "BridgeTowerContrastiveHead": {
    "__init__": [
      "self",
      "hidden_size",
      "embed_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BridgeTowerForContrastiveLearning": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "return_loss"
    ]
  },
  "BridgeTowerVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_channels",
      "patch_size",
      "image_size",
      "initializer_factor",
      "layer_norm_eps",
      "stop_gradient",
      "share_layernorm",
      "remove_last_layer"
    ]
  },
  "BridgeTowerTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "initializer_factor",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "BridgeTowerConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "share_cross_modal_transformer_layers",
      "hidden_act",
      "hidden_size",
      "initializer_factor",
      "layer_norm_eps",
      "share_link_tower_layers",
      "link_tower_type",
      "num_attention_heads",
      "num_hidden_layers",
      "tie_word_embeddings",
      "init_layernorm_from_vision_encoder",
      "text_config",
      "vision_config"
    ]
  },
  "BridgeTowerImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "size_divisor": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "size_divisor",
      "interpolation",
      "antialias"
    ],
    "center_crop": [
      "self",
      "image",
      "size"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "size_divisor",
      "interpolation",
      "do_pad",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "to_dict": [
      "self"
    ]
  },
  "JanusPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "JanusVQVAEOutput": {},
  "JanusBaseModelOutputWithPast": {},
  "JanusCausalLMOutputWithPast": {},
  "JanusVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "JanusVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "JanusVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "JanusVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "JanusVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_input_embeddings": [
      "self"
    ]
  },
  "JanusVisionAlignerMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVQVAEVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ],
    "get_codebook_entry": [
      "self",
      "image_tokens"
    ]
  },
  "JanusVQVAEResnetBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "conv_shortcut"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVQVAEAttnBlock": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVQVAEConvDownsample": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVQVAEConvUpsample": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVQVAEMidBlock": {
    "__init__": [
      "self",
      "config",
      "channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVQVAEEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "JanusVQVAEDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "JanusVQVAEModelOutput": {},
  "JanusVQVAE": {
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "pixel_values"
    ],
    "decode": [
      "self",
      "image_tokens"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "JanusVQVAEAlignerMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusVQVAEHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "JanusModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "inputs_embeds",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "JanusForConditionalGeneration": {
    "_tied_weights_keys": [],
    "output_modalities": [],
    "_can_compile_fullgraph": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "prepare_embeddings_for_image_generation": [
      "self",
      "inputs"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "pixel_values",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "decode_image_tokens": [
      "self",
      "image_tokens"
    ],
    "generate": [
      "self",
      "inputs",
      "attention_mask",
      "logits_processor"
    ]
  },
  "JanusImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "min_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "min_size",
      "interpolation",
      "antialias"
    ],
    "pad_to_square": [
      "self",
      "images",
      "background_color"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "min_size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors",
      "do_pad"
    ],
    "postprocess": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors"
    ]
  },
  "JanusImageProcessorKwargs": {},
  "JanusImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "min_size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "do_pad"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "do_convert_rgb",
      "background_color",
      "do_pad",
      "data_format",
      "input_data_format"
    ],
    "pad_to_square": [
      "self",
      "image",
      "background_color",
      "data_format",
      "input_data_format"
    ],
    "postprocess": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "input_data_format",
      "return_tensors"
    ],
    "unnormalize": [
      "self",
      "image",
      "image_mean",
      "image_std",
      "input_data_format"
    ]
  },
  "JanusTextKwargs": {},
  "JanusProcessorKwargs": {
    "_defaults": []
  },
  "JanusProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "use_default_system_prompt"
    ],
    "__call__": [
      "self",
      "text",
      "images"
    ],
    "postprocess": [
      "self",
      "images"
    ],
    "post_process_multimodal_output": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "generation_mode"
    ]
  },
  "JanusVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "patch_size",
      "image_size",
      "attention_dropout",
      "layer_norm_eps",
      "hidden_act",
      "mlp_ratio",
      "attention_bias",
      "hidden_dropout_rate",
      "projection_dim",
      "projection_dropout",
      "use_qk_norm",
      "initializer_range",
      "depth",
      "num_image_tokens"
    ]
  },
  "JanusVQVAEConfig": {
    "__init__": [
      "self",
      "embed_dim",
      "num_embeddings",
      "double_latent",
      "latent_channels",
      "num_patches",
      "in_channels",
      "out_channels",
      "base_channels",
      "channel_multiplier",
      "num_res_blocks",
      "dropout",
      "initializer_range",
      "projection_dim",
      "num_hidden_layers",
      "hidden_act",
      "image_token_embed_dim"
    ]
  },
  "JanusConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "vq_config",
      "image_token_id"
    ]
  },
  "HieraConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "embed_dim",
      "image_size",
      "patch_size",
      "patch_stride",
      "patch_padding",
      "mlp_ratio",
      "depths",
      "num_heads",
      "embed_dim_multiplier",
      "num_query_pool",
      "query_stride",
      "masked_unit_size",
      "masked_unit_attention",
      "drop_path_rate",
      "num_channels",
      "hidden_act",
      "initializer_range",
      "layer_norm_init",
      "layer_norm_eps",
      "decoder_hidden_size",
      "decoder_depth",
      "decoder_num_heads",
      "normalize_pixel_loss",
      "mask_ratio",
      "out_features",
      "out_indices"
    ]
  },
  "HieraEncoderOutput": {},
  "HieraModelOutput": {},
  "HieraForImageClassificationOutput": {},
  "HieraForPreTrainingOutput": {},
  "HieraPatchEmbeddings": {
    "__init__": [
      "self",
      "config",
      "is_mae"
    ],
    "masked_conv": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ],
    "random_masking": [
      "self",
      "pixel_values",
      "noise"
    ],
    "forward": [
      "self",
      "pixel_values",
      "noise"
    ]
  },
  "HieraEmbeddings": {
    "__init__": [
      "self",
      "config",
      "is_mae"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "pos_embeds",
      "height",
      "width"
    ],
    "get_position_embedding": [
      "self",
      "embeddings",
      "height",
      "width",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "pixel_values",
      "noise",
      "interpolate_pos_encoding"
    ]
  },
  "HieraMaskUnitAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "hidden_size_output",
      "num_heads",
      "query_stride",
      "window_size",
      "use_mask_unit_attn"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "HieraDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "HieraMlp": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HieraLayer": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "hidden_size_output",
      "num_heads",
      "drop_path",
      "query_stride",
      "window_size",
      "use_mask_unit_attn"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "HieraStage": {
    "__init__": [
      "self",
      "config",
      "depth",
      "hidden_size",
      "hidden_size_output",
      "num_heads",
      "drop_path",
      "query_stride",
      "window_size",
      "use_mask_unit_attn",
      "stage_num"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "undo_windowing": [
    "hidden_states",
    "shape",
    "mask_unit_shape"
  ],
  "HieraEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "reroll": [
      "self",
      "hidden_states",
      "stage_idx",
      "bool_masked_pos"
    ],
    "forward": [
      "self",
      "hidden_states",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "unroll": [
    "hidden_states",
    "image_shape",
    "patch_stride",
    "schedule"
  ],
  "HieraPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "HieraPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HieraModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "is_mae"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "noise",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "HieraDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoder_hidden_states",
      "bool_masked_pos",
      "output_attentions"
    ]
  },
  "HieraMultiScaleHead": {
    "__init__": [
      "self",
      "config"
    ],
    "apply_fusion_head": [
      "self",
      "head",
      "hidden_states"
    ],
    "forward": [
      "self",
      "feature_maps"
    ]
  },
  "HieraForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "get_pixel_label_2d": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ],
    "forward_loss": [
      "self",
      "pixel_values",
      "logits",
      "bool_masked_pos"
    ],
    "forward": [
      "self",
      "pixel_values",
      "noise",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "HieraForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "HieraBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "SEWConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "squeeze_factor",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "SEWNoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWGroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWSamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWUpsampling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "SEWAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SEWFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SEWEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SEWPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask"
    ]
  },
  "SEWModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SEWForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "tie_weights": [
      "self"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "SEWForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "ZambaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "tie_word_embeddings",
      "hidden_size",
      "attention_hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "attention_head_dim",
      "num_key_value_heads",
      "n_mamba_heads",
      "hidden_act",
      "hidden_mamba_act",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "num_logits_to_keep",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_position_embeddings",
      "attention_dropout",
      "attn_layer_period",
      "attn_layer_offset",
      "use_mamba_kernels",
      "mamba_d_state",
      "mamba_d_conv",
      "mamba_expand",
      "mamba_dt_rank",
      "time_step_min",
      "time_step_max",
      "time_step_floor",
      "mamba_conv_bias",
      "mamba_proj_bias"
    ],
    "_layers_block_type": [
      "self",
      "num_hidden_layers",
      "attn_layer_period",
      "attn_layer_offset"
    ]
  },
  "ZambaRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ZambaHybridDynamicCache": {
    "is_compileable": [],
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device"
    ],
    "__len__": [
      "self"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ]
  },
  "ZambaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_idx",
      "attention_mask",
      "past_key_values"
    ]
  },
  "ZambaMambaMixer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "attention_mask"
    ],
    "slow_forward": [
      "self",
      "input_states",
      "cache_params",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "attention_mask"
    ]
  },
  "ZambaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ZambaAttentionDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "original_hidden_states",
      "layer_idx",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "ZambaMambaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "original_hidden_states",
      "layer_idx",
      "attention_mask",
      "causal_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_ids",
      "transformer_hidden_states"
    ]
  },
  "ZambaHybridLayer": {
    "__init__": [
      "self",
      "shared_transf",
      "linear",
      "mamba"
    ],
    "forward": [
      "self",
      "hidden_states",
      "original_hidden_states",
      "layer_idx",
      "attention_mask",
      "causal_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "ZambaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ZambaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position"
    ]
  },
  "ZambaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "is_first_iteration"
    ]
  },
  "ZambaForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "HunYuanMoEV1RMSNorm": {},
  "HunYuanMoEV1MLP": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "HunYuanMoEV1Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "HunYuanMoEV1Gate": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HunYuanMoEV1Experts": {},
  "HunYuanMoEV1Moe": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "route_tokens_to_experts": [
      "self",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HunYuanMoEV1DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "HunYuanMoEV1PreTrainedModel": {
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "HunYuanMoEV1RotaryEmbedding": {},
  "HunYuanMoEV1Model": {},
  "HunYuanMoEV1ForCausalLM": {},
  "HunYuanMoEV1ForSequenceClassification": {},
  "HunYuanMoEV1Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "eod_token_id",
      "sep_token_id",
      "pretraining_tp",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "num_experts",
      "moe_topk",
      "head_dim"
    ],
    "_rope_parameters_validation": [
      "self"
    ]
  },
  "SplinterTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "question_token",
      "tokenize_chinese_chars",
      "strip_accents"
    ],
    "question_token_id": [
      "self"
    ],
    "update_post_processor": [
      "self"
    ]
  },
  "SplinterConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "question_token_id"
    ]
  },
  "SplinterEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "SplinterSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SplinterSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "SplinterAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SplinterIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SplinterOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "SplinterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "SplinterEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SplinterPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SplinterModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SplinterFullyConnectedLayer": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "hidden_act"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "QuestionAwareSpanSelectionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "positions"
    ]
  },
  "SplinterForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "question_positions"
    ]
  },
  "SplinterForPreTrainingOutput": {},
  "SplinterForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "question_positions"
    ],
    "_prepare_question_positions": [
      "self",
      "input_ids"
    ]
  },
  "INF": [],
  "FunnelEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds"
    ]
  },
  "FunnelAttentionStructure": {
    "__init__": [
      "self",
      "config"
    ],
    "init_attention_inputs": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "token_type_ids"
    ],
    "token_type_ids_to_mat": [
      "self",
      "token_type_ids"
    ],
    "get_position_embeds": [
      "self",
      "seq_len",
      "dtype",
      "device"
    ],
    "stride_pool_pos": [
      "self",
      "pos_id",
      "block_index"
    ],
    "relative_pos": [
      "self",
      "pos",
      "stride",
      "pooled_pos",
      "shift"
    ],
    "stride_pool": [
      "self",
      "tensor",
      "axis"
    ],
    "pool_tensor": [
      "self",
      "tensor",
      "mode",
      "stride"
    ],
    "pre_attention_pooling": [
      "self",
      "output",
      "attention_inputs"
    ],
    "post_attention_pooling": [
      "self",
      "attention_inputs"
    ]
  },
  "_relative_shift_gather": [
    "positional_attn",
    "context_len",
    "shift"
  ],
  "FunnelRelMultiheadAttention": {
    "__init__": [
      "self",
      "config",
      "block_index"
    ],
    "relative_positional_attention": [
      "self",
      "position_embeds",
      "q_head",
      "context_len",
      "cls_mask"
    ],
    "relative_token_type_attention": [
      "self",
      "token_type_mat",
      "q_head",
      "cls_mask"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_inputs",
      "output_attentions"
    ]
  },
  "FunnelPositionwiseFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden"
    ]
  },
  "FunnelLayer": {
    "__init__": [
      "self",
      "config",
      "block_index"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_inputs",
      "output_attentions"
    ]
  },
  "FunnelEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "token_type_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "upsample": [
    "x",
    "stride",
    "target_len",
    "separate_cls",
    "truncate_seq"
  ],
  "FunnelDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "final_hidden",
      "first_block_hidden",
      "attention_mask",
      "token_type_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelDiscriminatorPredictions": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "discriminator_hidden_states"
    ]
  },
  "FunnelPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "FunnelClassificationHead": {
    "__init__": [
      "self",
      "config",
      "n_labels"
    ],
    "forward": [
      "self",
      "hidden"
    ]
  },
  "FunnelForPreTrainingOutput": {},
  "FunnelBaseModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FunnelForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "_model_names": [],
  "FunnelTokenizer": {
    "vocab_files_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "bos_token",
      "eos_token",
      "clean_text",
      "tokenize_chinese_chars",
      "strip_accents",
      "wordpieces_prefix"
    ]
  },
  "FunnelConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "block_sizes",
      "block_repeats",
      "num_decoder_layers",
      "d_model",
      "n_head",
      "d_head",
      "d_inner",
      "hidden_act",
      "hidden_dropout",
      "attention_dropout",
      "activation_dropout",
      "initializer_range",
      "initializer_std",
      "layer_norm_eps",
      "pooling_type",
      "attention_type",
      "separate_cls",
      "truncate_seq",
      "pool_q_only",
      "pad_token_id",
      "tie_word_embeddings"
    ],
    "num_hidden_layers": [
      "self",
      "value"
    ],
    "num_blocks": [
      "self",
      "value"
    ]
  },
  "Mistral3Config": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "is_composition": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "projector_hidden_act",
      "vision_feature_layer",
      "multimodal_projector_bias",
      "spatial_merge_size",
      "tie_word_embeddings"
    ]
  },
  "Mistral3RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Mistral3PatchMerger": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features",
      "image_sizes"
    ]
  },
  "Mistral3MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features",
      "image_sizes"
    ]
  },
  "Mistral3CausalLMOutputWithPast": {},
  "Mistral3ModelOutputWithPast": {},
  "Mistral3PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": []
  },
  "Mistral3Model": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer",
      "output_hidden_states"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "image_sizes"
    ]
  },
  "Mistral3ForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep",
      "image_sizes"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "MoonshineEncoderMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_act"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MoonshineDecoderMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_act"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MoonshineRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MoonshineAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_causal",
      "num_attention_heads",
      "num_key_value_heads"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position",
      "key_value_states"
    ]
  },
  "MoonshineEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "MoonshineDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_ids",
      "encoder_position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings",
      "encoder_position_embeddings"
    ]
  },
  "MoonshinePreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ]
  },
  "MoonshineEncoder": {
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask"
    ]
  },
  "MoonshineDecoder": {
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "MoonshineModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "freeze_encoder": [
      "self"
    ],
    "_mask_input_features": [
      "self",
      "input_features",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "decoder_position_ids",
      "use_cache",
      "cache_position"
    ]
  },
  "MoonshineForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "decoder_position_ids",
      "use_cache",
      "cache_position",
      "labels"
    ]
  },
  "MoonshineConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "encoder_num_hidden_layers",
      "decoder_num_hidden_layers",
      "encoder_num_attention_heads",
      "decoder_num_attention_heads",
      "encoder_num_key_value_heads",
      "decoder_num_key_value_heads",
      "pad_head_dim_to_multiple_of",
      "encoder_hidden_act",
      "decoder_hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "decoder_start_token_id",
      "use_cache",
      "rope_parameters",
      "is_encoder_decoder",
      "attention_bias",
      "attention_dropout",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings"
    ]
  },
  "XLMRobertaXLEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "XLMRobertaXLSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "XLMRobertaXLCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "XLMRobertaXLSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "XLMRobertaXLAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "XLMRobertaXLOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "XLMRobertaXLIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XLMRobertaXLLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "XLMRobertaXLEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "XLMRobertaXLPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "XLMRobertaXLPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XLMRobertaXLModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "XLMRobertaXLLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "XLMRobertaXLClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "XLMRobertaXLForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "XLMRobertaXLForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "XLMRobertaXLForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "XLMRobertaXLForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "XLMRobertaXLForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "XLMRobertaXLForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "XLMRobertaXLConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "BloomConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "n_layer",
      "n_head",
      "layer_norm_epsilon",
      "initializer_range",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "apply_residual_connection_post_layernorm",
      "hidden_dropout",
      "attention_dropout",
      "pretraining_tp",
      "slow_but_exact",
      "tie_word_embeddings"
    ]
  },
  "build_alibi_tensor": [
    "attention_mask",
    "num_heads",
    "dtype"
  ],
  "dropout_add": [
    "x",
    "residual",
    "prob",
    "training"
  ],
  "bloom_gelu_forward": [
    "x"
  ],
  "bloom_gelu_back": [
    "g",
    "x"
  ],
  "GeLUFunction": {
    "forward": [
      "ctx",
      "input"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "BloomGelu": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BloomAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_reshape": [
      "self",
      "fused_qkv"
    ],
    "_merge_heads": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual",
      "alibi",
      "attention_mask",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "BloomMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual"
    ]
  },
  "BloomBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "alibi",
      "attention_mask",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "BloomPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_can_compile_fullgraph": []
  },
  "BloomModel": {
    "__init__": [
      "self",
      "config"
    ],
    "build_alibi_tensor": [
      "self",
      "attention_mask",
      "num_heads",
      "dtype"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "BloomForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "use_cache",
      "is_first_iteration"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BloomForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BloomForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BloomForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VoxtralAttention": {},
  "VoxtralEncoderLayer": {},
  "VoxtralPreTrainedModel": {
    "_supports_flex_attn": [],
    "_supports_cache_class": [],
    "_supports_attention_backend": [],
    "_can_compile_fullgraph": [],
    "_no_split_modules": []
  },
  "VoxtralEncoder": {
    "_can_record_outputs": [],
    "forward": [
      "self",
      "input_features",
      "attention_mask"
    ]
  },
  "VoxtralMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_features"
    ]
  },
  "VoxtralForConditionalGeneration": {
    "_keep_in_fp32_modules_strict": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "get_audio_features": [
      "self",
      "input_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ]
  },
  "VoxtralAudioKwargs": {},
  "VoxtralProcessorKwargs": {
    "_defaults": []
  },
  "VoxtralProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "_retrieve_input_features": [
      "self",
      "audio",
      "max_source_positions"
    ],
    "apply_chat_template": [
      "self",
      "conversation"
    ],
    "__call__": [
      "self",
      "text"
    ],
    "apply_transcription_request": [
      "self",
      "audio",
      "model_id",
      "language",
      "sampling_rate",
      "format"
    ]
  },
  "VoxtralEncoderConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "scale_embedding",
      "activation_function",
      "num_mel_bins",
      "max_source_positions",
      "initializer_range",
      "attention_dropout"
    ]
  },
  "VoxtralConfig": {
    "model_type": [],
    "sub_configs": [],
    "_default_text_config_kwargs": [],
    "__init__": [
      "self",
      "audio_config",
      "text_config",
      "audio_token_id",
      "projector_hidden_act"
    ]
  },
  "ErnieEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "ErnieSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ErnieCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "ErnieSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ErnieAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ErnieIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ErnieOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ErnieLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "ErniePooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ErniePredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ErnieLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ErnieEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "ErniePreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ErnieModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "ErnieForPreTrainingOutput": {},
  "ErniePreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "ErnieForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "next_sentence_label"
    ]
  },
  "ErnieOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "ErnieForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "ErnieForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "ErnieOnlyNSPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "ErnieForNextSentencePrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ErnieForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ErnieForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ErnieForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "ErnieForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "task_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "ErnieConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "task_type_vocab_size",
      "use_task_id",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "Qwen2VLVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "embed_dim",
      "hidden_size",
      "hidden_act",
      "mlp_ratio",
      "num_heads",
      "in_channels",
      "patch_size",
      "spatial_merge_size",
      "temporal_patch_size",
      "initializer_range"
    ]
  },
  "Qwen2VLTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "use_sliding_window",
      "sliding_window",
      "max_window_layers",
      "layer_types",
      "attention_dropout",
      "rope_parameters",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "Qwen2VLConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "vision_start_token_id",
      "vision_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "Qwen2VLImageProcessorKwargs": {},
  "Qwen2VLImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "min_pixels",
      "max_pixels",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "min_pixels",
      "max_pixels",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "Qwen2VLModelOutputWithPast": {},
  "Qwen2VLCausalLMOutputWithPast": {},
  "Qwen2VLRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "VisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "PatchEmbed": {
    "__init__": [
      "self",
      "patch_size",
      "temporal_patch_size",
      "in_channels",
      "embed_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "spatial_merge_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VisionMlp": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim",
      "hidden_act"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen2VLVisionBlock": {
    "__init__": [
      "self",
      "config",
      "attn_implementation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen2VLAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen2VLDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen2VLPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen2VisionTransformerPretrainedModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "_input_embed_layer": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_dtype": [
      "self"
    ],
    "get_device": [
      "self"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Qwen2VLTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Qwen2VLModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position"
    ]
  },
  "Qwen2VLForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "Qwen2VLProcessorKwargs": {
    "_defaults": []
  },
  "Qwen2VLProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes",
      "video_sizes"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ]
  },
  "Qwen2VLVideoProcessorInitKwargs": {},
  "Qwen2VLVideoProcessor": {
    "resample": [],
    "size": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "min_frames": [],
    "max_frames": [],
    "do_sample_frames": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "min_pixels",
      "max_pixels"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "temporal_patch_size",
      "min_frames",
      "max_frames",
      "num_frames",
      "fps"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "return_tensors"
    ],
    "get_num_of_video_patches": [
      "self",
      "num_frames",
      "height",
      "width",
      "videos_kwargs"
    ]
  },
  "Qwen2VLImageProcessorFast": {
    "do_resize": [],
    "resample": [],
    "size": [],
    "do_rescale": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "min_pixels",
      "max_pixels"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "disable_grouping",
      "return_tensors"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "concat_pairs": [
    "tensor_tuple0",
    "tensor_tuple1"
  ],
  "log_sinkhorn_iterations": [
    "log_cost_matrix",
    "log_source_distribution",
    "log_target_distribution",
    "num_iterations"
  ],
  "log_optimal_transport": [
    "scores",
    "reg_param",
    "iterations"
  ],
  "arange_like": [
    "x",
    "dim"
  ],
  "SuperGlueKeypointMatchingOutput": {},
  "SuperGlueMultiLayerPerceptron": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "SuperGlueKeypointEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "keypoints",
      "scores",
      "output_hidden_states"
    ]
  },
  "SuperGlueSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "SuperGlueSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SUPERGLUE_SELF_ATTENTION_CLASSES": [],
  "SuperGlueAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "SuperGlueAttentionalPropagation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "descriptors",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "SuperGlueAttentionalGNN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "descriptors",
      "mask",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "SuperGlueFinalProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "descriptors"
    ]
  },
  "SuperGluePreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SuperGlueForKeypointMatching": {
    "__init__": [
      "self",
      "config"
    ],
    "_match_image_pair": [
      "self",
      "keypoints",
      "descriptors",
      "scores",
      "height",
      "width",
      "mask",
      "output_attentions",
      "output_hidden_states"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SuperGlueImageProcessorKwargs": {},
  "SuperGlueImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_grayscale"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_grayscale",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_keypoint_matching": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ],
    "visualize_keypoint_matching": [
      "self",
      "images",
      "keypoint_matching_output"
    ],
    "_get_color": [
      "self",
      "score"
    ]
  },
  "SuperGlueConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "keypoint_detector_config",
      "hidden_size",
      "keypoint_encoder_sizes",
      "gnn_layers_types",
      "num_attention_heads",
      "sinkhorn_iterations",
      "matching_threshold",
      "initializer_range",
      "is_decoder"
    ]
  },
  "SuperGlueImageProcessorFast": {
    "resample": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_prepare_images_structure": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "size",
      "rescale_factor",
      "do_rescale",
      "do_resize",
      "interpolation",
      "do_grayscale",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_keypoint_matching": [
      "self",
      "outputs",
      "target_sizes",
      "threshold"
    ],
    "visualize_keypoint_matching": [
      "self",
      "images",
      "keypoint_matching_output"
    ],
    "_get_color": [
      "self",
      "score"
    ]
  },
  "NystromformerEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "NystromformerSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "iterative_inv": [
      "self",
      "mat",
      "n_iter"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "NystromformerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "NystromformerAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "NystromformerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "NystromformerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "NystromformerLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "NystromformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NystromformerPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "NystromformerLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "NystromformerOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "NystromformerPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "NystromformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NystromformerForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NystromformerClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "NystromformerForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NystromformerForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NystromformerForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NystromformerForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NystromformerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "segment_means_seq_len",
      "num_landmarks",
      "conv_kernel_size",
      "inv_coeff_init_option",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "PoolFormerImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "crop_pct": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "crop_pct",
      "interpolation",
      "antialias"
    ],
    "center_crop": [
      "self",
      "image",
      "size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_pct",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "PoolFormerImageProcessorKwargs": {},
  "PoolFormerImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "crop_pct",
      "resample",
      "do_center_crop",
      "crop_size",
      "rescale_factor",
      "do_rescale",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "crop_pct",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_pct",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "PoolFormerDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PoolFormerEmbeddings": {
    "__init__": [
      "self",
      "hidden_size",
      "num_channels",
      "patch_size",
      "stride",
      "padding",
      "norm_layer"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "PoolFormerGroupNorm": {
    "__init__": [
      "self",
      "num_channels"
    ]
  },
  "PoolFormerPooling": {
    "__init__": [
      "self",
      "pool_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PoolFormerOutput": {
    "__init__": [
      "self",
      "config",
      "dropout_prob",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PoolFormerLayer": {
    "__init__": [
      "self",
      "config",
      "num_channels",
      "pool_size",
      "hidden_size",
      "intermediate_size",
      "drop_path"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PoolFormerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PoolFormerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PoolFormerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PoolFormerFinalPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PoolFormerForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PoolFormerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "patch_size",
      "stride",
      "pool_size",
      "mlp_ratio",
      "depths",
      "hidden_sizes",
      "patch_sizes",
      "strides",
      "padding",
      "num_encoder_blocks",
      "drop_path_rate",
      "hidden_act",
      "use_layer_scale",
      "layer_scale_init_value",
      "initializer_range"
    ]
  },
  "ProphetNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "activation_dropout",
      "activation_function",
      "vocab_size",
      "hidden_size",
      "encoder_ffn_dim",
      "num_encoder_layers",
      "num_encoder_attention_heads",
      "decoder_ffn_dim",
      "num_decoder_layers",
      "num_decoder_attention_heads",
      "attention_dropout",
      "dropout",
      "max_position_embeddings",
      "init_std",
      "is_encoder_decoder",
      "add_cross_attention",
      "decoder_start_token_id",
      "ngram",
      "num_buckets",
      "relative_max_distance",
      "disable_ngram_loss",
      "eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "is_decoder",
      "tie_word_embeddings"
    ],
    "num_hidden_layers": [
      "self",
      "value"
    ]
  },
  "ngram_attention_bias": [
    "sequence_length",
    "ngram",
    "device",
    "dtype"
  ],
  "compute_relative_buckets": [
    "num_buckets",
    "max_distance",
    "relative_positions",
    "is_bidirectional"
  ],
  "compute_all_stream_relative_buckets": [
    "num_buckets",
    "max_distance",
    "position_ids"
  ],
  "ProphetNetSeq2SeqLMOutput": {},
  "ProphetNetSeq2SeqModelOutput": {},
  "ProphetNetDecoderModelOutput": {},
  "ProphetNetDecoderLMOutput": {},
  "ProphetNetPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "ProphetNetPositionalEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_shape",
      "device",
      "attention_mask",
      "past_key_values",
      "position_ids"
    ],
    "_forward": [
      "self",
      "position_ids"
    ]
  },
  "ProphetNetAttention": {
    "__init__": [
      "self",
      "config",
      "num_attn_heads",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "ProphetNetFeedForward": {
    "__init__": [
      "self",
      "config",
      "ffn_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ProphetNetNgramSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "batch_size"
    ],
    "prepare_for_onnx_export_": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "attention_mask",
      "extended_predict_attention_mask",
      "main_relative_position_buckets",
      "predict_relative_position_buckets",
      "position_ids",
      "cache_position"
    ],
    "get_main_relative_pos_embeddings": [
      "self",
      "hidden_states",
      "attn_weights",
      "position_ids",
      "main_relative_position_buckets"
    ],
    "get_predict_relative_pos_embeddings": [
      "self",
      "hidden_states",
      "attn_weights",
      "position_ids",
      "predict_relative_position_buckets"
    ]
  },
  "ProphetNetEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ProphetNetDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attn_mask",
      "extended_predict_attention_mask",
      "main_relative_position_buckets",
      "predict_relative_position_buckets",
      "position_ids",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "ProphetNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ProphetNetDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "compute_buffered_relative_buckets": [
      "self",
      "position_ids"
    ],
    "prepare_attention_mask": [
      "self",
      "hidden_states",
      "attention_mask"
    ],
    "prepare_predict_attention_mask": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "ProphetNetModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ProphetNetForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_compute_loss": [
      "self",
      "logits",
      "labels",
      "ignore_index"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ],
    "get_encoder": [
      "self",
      "modality"
    ]
  },
  "ProphetNetForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "_compute_loss": [
      "self",
      "logits",
      "labels",
      "ignore_index"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "use_cache",
      "is_first_iteration"
    ]
  },
  "ProphetNetDecoderWrapper": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "ProphetNetTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "do_lower_case",
      "do_basic_tokenize",
      "never_split",
      "unk_token",
      "sep_token",
      "x_sep_token",
      "pad_token",
      "mask_token",
      "tokenize_chinese_chars",
      "strip_accents",
      "clean_up_tokenization_spaces"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "Owlv2TextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Owlv2VisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "Owlv2Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value",
      "return_dict"
    ]
  },
  "Owlv2ImagesKwargs": {},
  "Owlv2ProcessorKwargs": {
    "_defaults": []
  },
  "Owlv2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "post_process_grounded_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "text_labels"
    ],
    "post_process_image_guided_detection": [
      "self",
      "outputs",
      "threshold",
      "nms_threshold",
      "target_sizes"
    ]
  },
  "_preprocess_resize_output_shape": [
    "image",
    "output_shape"
  ],
  "_clip_warp_output": [
    "input_image",
    "output_image"
  ],
  "Owlv2ImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "do_resize",
      "size",
      "resample",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "pad": [
      "self",
      "image",
      "data_format",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "anti_aliasing",
      "anti_aliasing_sigma",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_pad",
      "do_resize",
      "size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_image_guided_detection": [
      "self",
      "outputs",
      "threshold",
      "nms_threshold",
      "target_sizes"
    ]
  },
  "owlv2_loss": [
    "similarity"
  ],
  "Owlv2Output": {
    "to_tuple": [
      "self"
    ]
  },
  "Owlv2ObjectDetectionOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Owlv2ImageGuidedObjectDetectionOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Owlv2VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "Owlv2TextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "Owlv2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "Owlv2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Owlv2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "Owlv2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Owlv2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Owlv2TextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Owlv2TextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Owlv2VisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Owlv2VisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Owlv2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_base_image_embeds",
      "return_dict"
    ]
  },
  "Owlv2BoxPredictionHead": {
    "__init__": [
      "self",
      "config",
      "out_dim"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "Owlv2ClassPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeds",
      "query_embeds",
      "query_mask"
    ]
  },
  "Owlv2ForObjectDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "normalize_grid_corner_coordinates": [
      "num_patches_height",
      "num_patches_width"
    ],
    "objectness_predictor": [
      "self",
      "image_features"
    ],
    "compute_box_bias": [
      "self",
      "num_patches_height",
      "num_patches_width"
    ],
    "box_predictor": [
      "self",
      "image_feats",
      "feature_map",
      "interpolate_pos_encoding"
    ],
    "class_predictor": [
      "self",
      "image_feats",
      "query_embeds",
      "query_mask"
    ],
    "image_text_embedder": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding"
    ],
    "image_embedder": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding"
    ],
    "embed_image_query": [
      "self",
      "query_image_features",
      "query_feature_map",
      "interpolate_pos_encoding"
    ],
    "image_guided_detection": [
      "self",
      "pixel_values",
      "query_pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Owlv2ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "model_input_names": [],
    "rescale_factor": [],
    "do_pad": [],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_image_guided_detection": [
      "self",
      "outputs",
      "threshold",
      "nms_threshold",
      "target_sizes"
    ],
    "_pad_images": [
      "self",
      "images",
      "constant_value"
    ],
    "pad": [
      "self",
      "images",
      "disable_grouping",
      "constant_value"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "anti_aliasing",
      "anti_aliasing_sigma"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "RoCBertTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "word_shape_file",
      "word_pronunciation_file",
      "do_lower_case",
      "do_basic_tokenize",
      "never_split",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "tokenize_chinese_chars",
      "strip_accents"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "text_target",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "max_target_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "encode_plus": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "do_lower_case": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text",
      "split_special_tokens"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "prepare_for_model": [
      "self",
      "ids",
      "shape_ids",
      "pronunciation_ids",
      "pair_ids",
      "pair_shape_ids",
      "pair_pronunciation_ids",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "prepend_batch_axis"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_prepare_for_model": [
      "self",
      "batch_ids_pairs",
      "batch_shape_ids_pairs",
      "batch_pronunciation_ids_pairs",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_token_to_shape_id": [
      "self",
      "token"
    ],
    "convert_tokens_to_shape_ids": [
      "self",
      "tokens"
    ],
    "_convert_token_to_pronunciation_id": [
      "self",
      "token"
    ],
    "convert_tokens_to_pronunciation_ids": [
      "self",
      "tokens"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "cls_token_id",
      "sep_token_id"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "RoCBertBasicTokenizer": {
    "__init__": [
      "self",
      "do_lower_case",
      "never_split",
      "tokenize_chinese_chars",
      "strip_accents",
      "do_split_on_punc"
    ],
    "tokenize": [
      "self",
      "text",
      "never_split"
    ],
    "_run_strip_accents": [
      "self",
      "text"
    ],
    "_run_split_on_punc": [
      "self",
      "text",
      "never_split"
    ],
    "_tokenize_chinese_chars": [
      "self",
      "text"
    ],
    "_is_chinese_char": [
      "self",
      "cp"
    ],
    "_clean_text": [
      "self",
      "text"
    ]
  },
  "RoCBertWordpieceTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "unk_token",
      "max_input_chars_per_word"
    ],
    "tokenize": [
      "self",
      "text"
    ]
  },
  "RoCBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "pad_token_id",
      "classifier_dropout",
      "enable_pronunciation",
      "enable_shape",
      "pronunciation_embed_dim",
      "pronunciation_vocab_size",
      "shape_embed_dim",
      "shape_vocab_size",
      "concat_input",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "RoCBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "RoCBertSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "RoCBertCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "RoCBertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RoCBertAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "RoCBertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RoCBertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RoCBertLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "RoCBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "RoCBertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RoCBertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RoCBertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RoCBertOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "RoCBertPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RoCBertModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_pronunciation_embeddings": [
      "self"
    ],
    "set_pronunciation_embeddings": [
      "self",
      "value"
    ],
    "get_shape_embeddings": [
      "self"
    ],
    "set_shape_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "RoCBertForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "attack_input_ids",
      "attack_input_shape_ids",
      "attack_input_pronunciation_ids",
      "attack_attention_mask",
      "attack_token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels_input_ids",
      "labels_input_shape_ids",
      "labels_input_pronunciation_ids",
      "labels_attention_mask",
      "labels_token_type_ids"
    ]
  },
  "RoCBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "RoCBertForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "past_key_values",
      "attention_mask"
    ]
  },
  "RoCBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "RoCBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "RoCBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "RoCBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_shape_ids",
      "input_pronunciation_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "Qwen3Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "use_sliding_window",
      "sliding_window",
      "max_window_layers",
      "layer_types",
      "attention_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Qwen3RMSNorm": {},
  "Qwen3MLP": {},
  "Qwen3RotaryEmbedding": {},
  "Qwen3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Qwen3ForCausalLM": {
    "forward": [
      "self"
    ]
  },
  "Qwen3ForSequenceClassification": {},
  "Qwen3ForTokenClassification": {},
  "Qwen3ForQuestionAnswering": {},
  "Qwen3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen3PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Qwen3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Glm4MoeLiteConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "moe_intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "n_shared_experts",
      "n_routed_experts",
      "routed_scaling_factor",
      "kv_lora_rank",
      "q_lora_rank",
      "qk_rope_head_dim",
      "v_head_dim",
      "qk_nope_head_dim",
      "n_group",
      "topk_group",
      "num_experts_per_tok",
      "norm_topk_prob",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "pretraining_tp",
      "tie_word_embeddings",
      "rope_parameters",
      "rope_interleave",
      "mlp_layer_types",
      "attention_bias",
      "attention_dropout"
    ]
  },
  "Glm4MoeLiteRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Glm4MoeLiteAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Glm4MoeLiteMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Glm4MoeLiteTopkRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4MoeLiteRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Glm4MoeLiteNaiveMoe": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Glm4MoeLiteMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4MoeLiteDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Glm4MoeLitePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_keep_in_fp32_modules_strict": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Glm4MoeLiteModel": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "Glm4MoeLiteForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Qwen2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Qwen2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Qwen2RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Qwen2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Qwen2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Qwen2ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Qwen2ForSequenceClassification": {},
  "Qwen2ForTokenClassification": {},
  "Qwen2ForQuestionAnswering": {
    "base_model_prefix": []
  },
  "MAX_MODEL_INPUT_SIZES": [],
  "PRETOKENIZE_REGEX": [],
  "Qwen2Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "add_prefix_space"
    ]
  },
  "Qwen2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "use_sliding_window",
      "sliding_window",
      "max_window_layers",
      "layer_types",
      "attention_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "BboxInput": [],
  "Kosmos2ImagesKwargs": {},
  "Kosmos2TextKwargs": {},
  "Kosmos2ProcessorKwargs": {
    "_defaults": []
  },
  "Kosmos2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "num_patch_index_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_check_bboxes_for_single_text": [
      "self",
      "bboxes"
    ],
    "_preprocess_single_example": [
      "self",
      "text",
      "image",
      "bboxes",
      "img_info_tokens"
    ],
    "preprocess_examples": [
      "self",
      "texts",
      "images",
      "bboxes",
      "num_image_tokens"
    ],
    "post_process_generation": [
      "self",
      "text",
      "cleanup_and_extract"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens"
    ],
    "model_input_names": [
      "self"
    ],
    "_insert_patch_index_tokens": [
      "self",
      "text",
      "bboxes"
    ],
    "_convert_bbox_to_patch_index_tokens": [
      "self",
      "bbox"
    ]
  },
  "coordinate_to_patch_index": [
    "bbox",
    "num_patches_per_side"
  ],
  "patch_index_to_coordinate": [
    "ul_idx",
    "lr_idx",
    "num_patches_per_side"
  ],
  "extract_entities_with_patch_indices": [
    "text"
  ],
  "adjust_entity_positions": [
    "entity",
    "text"
  ],
  "_cleanup_spaces": [
    "text",
    "entities"
  ],
  "clean_text_and_extract_entities_with_bboxes": [
    "text",
    "num_patches_per_side"
  ],
  "Kosmos2TextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "embed_dim",
      "layers",
      "ffn_dim",
      "attention_heads",
      "activation_function",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "layerdrop",
      "layer_norm_eps",
      "init_std",
      "scale_embedding",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_cross_attention"
    ]
  },
  "Kosmos2VisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "Kosmos2Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "latent_query_num",
      "tie_word_embeddings"
    ]
  },
  "_expand_mask": [
    "mask",
    "dtype",
    "tgt_len"
  ],
  "_make_causal_mask": [
    "input_ids_shape",
    "dtype",
    "device",
    "past_key_values_length"
  ],
  "BaseModelOutputWithProjectionAttentions": {},
  "Kosmos2ModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Kosmos2ForConditionalGenerationModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Kosmos2VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "Kosmos2VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "Kosmos2VisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Kosmos2VisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "Kosmos2VisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Kosmos2VisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Kosmos2TextSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "past_key_values_length",
      "position_ids"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "past_key_values_length",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "KosmosTextAttention": {
    "__init__": [
      "self",
      "config",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "add_inner_attn_layernorm",
      "bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "Kosmos2TextFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Kosmos2TextBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "Kosmos2TextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "_prepare_decoder_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "forward_embedding": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_embeds",
      "img_input_mask",
      "past_key_values_length",
      "position_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "image_embeds",
      "image_embeds_position_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Kosmos2PreTrainedModel": {
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Kosmos2VisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Kosmos2TextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "image_embeds",
      "image_embeds_position_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Kosmos2TextForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "image_embeds",
      "image_embeds_position_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "position_ids",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "image_embeds",
      "image_embeds_position_mask",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "is_first_iteration"
    ]
  },
  "Kosmos2ImageToTextProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "Kosmos2Model": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "image_embeds_position_mask",
      "attention_mask",
      "past_key_values",
      "image_embeds",
      "inputs_embeds",
      "position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Kosmos2ForConditionalGeneration": {
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "image_embeds_position_mask",
      "attention_mask",
      "past_key_values",
      "image_embeds",
      "inputs_embeds",
      "position_ids",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "logits_to_keep"
    ],
    "generate": [
      "self",
      "pixel_values",
      "image_embeds_position_mask",
      "input_ids",
      "attention_mask",
      "image_embeds",
      "inputs_embeds"
    ]
  },
  "TextNetImageProcessorKwargs": {},
  "TextNetImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "TextNetImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "size_divisor": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias",
      "size_divisor"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "size_divisor",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "TextNetConvLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TextNetRepConvLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TextNetStage": {
    "__init__": [
      "self",
      "config",
      "depth"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "TextNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TextNetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": []
  },
  "TextNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TextNetForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TextNetBackbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TextNetConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "stem_kernel_size",
      "stem_stride",
      "stem_num_channels",
      "stem_out_channels",
      "stem_act_func",
      "image_size",
      "conv_layer_kernel_sizes",
      "conv_layer_strides",
      "hidden_sizes",
      "batch_norm_eps",
      "initializer_range",
      "out_features",
      "out_indices"
    ]
  },
  "Qwen2MoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "use_sliding_window",
      "sliding_window",
      "max_window_layers",
      "attention_dropout",
      "decoder_sparse_step",
      "moe_intermediate_size",
      "shared_expert_intermediate_size",
      "num_experts_per_tok",
      "num_experts",
      "norm_topk_prob",
      "output_router_logits",
      "router_aux_loss_coef",
      "mlp_only_layers",
      "qkv_bias",
      "layer_types",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Qwen2MoeRMSNorm": {},
  "Qwen2MoeRotaryEmbedding": {},
  "Qwen2MoeMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ]
  },
  "Qwen2MoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Qwen2MoeExperts": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Qwen2MoeTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen2MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen2MoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Qwen2MoePreTrainedModel": {
    "_can_record_outputs": []
  },
  "Qwen2MoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Qwen2MoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ]
  },
  "Qwen2MoeForSequenceClassification": {},
  "Qwen2MoeForTokenClassification": {},
  "Qwen2MoeForQuestionAnswering": {},
  "RecurrentGemmaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_hidden_layers",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_attention_heads",
      "lru_width",
      "attention_window_size",
      "conv1d_width",
      "logits_soft_cap",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "hidden_activation",
      "rope_parameters",
      "block_types",
      "attention_dropout",
      "num_key_value_heads",
      "attention_bias",
      "w_init_variance_scale",
      "tie_word_embeddings"
    ],
    "layers_block_type": [
      "self"
    ]
  },
  "_MAX_SQRT_GRADIENT": [],
  "RecurrentGemmaRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "RecurrentGemmaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "RecurrentGemmaSdpaAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_ids",
      "attention_mask",
      "cache_position",
      "use_cache"
    ],
    "_setup_cache": [
      "self",
      "batch_size",
      "device",
      "dtype"
    ],
    "_update_cache": [
      "self",
      "key_states",
      "value_states"
    ]
  },
  "SqrtBoundDerivative": {
    "forward": [
      "ctx",
      "x"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "RecurrentGemmaRglru": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "activations",
      "position_ids"
    ],
    "_rnn_scan": [
      "self",
      "hidden_states",
      "recurrent_gate",
      "reset",
      "recurrent_states",
      "acc_dtype"
    ]
  },
  "RecurrentGemmaRecurrentBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_states",
      "position_ids",
      "attention_mask",
      "cache_position",
      "use_cache"
    ],
    "_setup_cache": [
      "self",
      "batch",
      "device",
      "dtype"
    ]
  },
  "TEMPORAL_BLOCK_CLASSES": [],
  "RecurrentGemmaMlp": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RecurrentGemmaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "activations",
      "position_ids",
      "attention_mask",
      "cache_position",
      "use_cache"
    ]
  },
  "RecurrentGemmaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_setup_cache": [
      "self",
      "config",
      "batch",
      "device",
      "dtype"
    ],
    "reset_cache": [
      "self",
      "batch",
      "device",
      "dtype"
    ]
  },
  "RecurrentGemmaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "cache_position",
      "inputs_embeds",
      "use_cache",
      "output_hidden_states",
      "return_dict"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position"
    ]
  },
  "RecurrentGemmaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "cache_position",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "UNICODE_VOCAB_SIZE": [],
  "PAD": [],
  "CLS": [],
  "SEP": [],
  "BOS": [],
  "MASK": [],
  "RESERVED": [],
  "CanineTokenizer": {
    "model_input_names": [],
    "__init__": [
      "self",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "pad_token",
      "mask_token",
      "add_prefix_space",
      "model_max_length"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ]
  },
  "CanineConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "downsampling_rate",
      "upsampling_kernel_size",
      "num_hash_functions",
      "num_hash_buckets",
      "local_transformer_stride"
    ]
  },
  "_PRIMES": [],
  "CanineModelOutputWithPooling": {},
  "CanineEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "_hash_bucket_tensors": [
      "self",
      "input_ids",
      "num_hashes",
      "num_buckets"
    ],
    "_embed_hash_buckets": [
      "self",
      "input_ids",
      "embedding_size",
      "num_hashes",
      "num_buckets"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "CharactersToMolecules": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "char_encoding"
    ]
  },
  "ConvProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "final_seq_char_positions"
    ]
  },
  "CanineSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "from_tensor",
      "to_tensor",
      "attention_mask",
      "output_attentions"
    ]
  },
  "CanineSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "CanineAttention": {
    "__init__": [
      "self",
      "config",
      "local",
      "always_attend_to_first_position",
      "first_position_attends_to_all",
      "attend_from_chunk_width",
      "attend_from_chunk_stride",
      "attend_to_chunk_width",
      "attend_to_chunk_stride"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "CanineIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CanineOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "CanineLayer": {
    "__init__": [
      "self",
      "config",
      "local",
      "always_attend_to_first_position",
      "first_position_attends_to_all",
      "attend_from_chunk_width",
      "attend_from_chunk_stride",
      "attend_to_chunk_width",
      "attend_to_chunk_stride"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "CanineEncoder": {
    "__init__": [
      "self",
      "config",
      "local",
      "always_attend_to_first_position",
      "first_position_attends_to_all",
      "attend_from_chunk_width",
      "attend_from_chunk_stride",
      "attend_to_chunk_width",
      "attend_to_chunk_stride"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CaninePooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CaninePredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CanineLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "CanineOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "CaninePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CanineModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "_create_3d_attention_mask_from_input_mask": [
      "self",
      "from_tensor",
      "to_mask"
    ],
    "_downsample_attention_mask": [
      "self",
      "char_attention_mask",
      "downsampling_rate"
    ],
    "_repeat_molecules": [
      "self",
      "molecules",
      "char_seq_length"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CanineForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CanineForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CanineForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CanineForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTNeoXTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "errors",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "add_prefix_space",
      "trim_offsets"
    ]
  },
  "GPTNeoXConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "attention_dropout",
      "hidden_dropout",
      "classifier_dropout",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings",
      "use_parallel_residual",
      "rope_parameters",
      "attention_bias",
      "is_decoder"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "GPTNeoXMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GPTNeoXRotaryEmbedding": {
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ]
  },
  "GPTNeoXAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "layer_past",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GPTNeoXLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "use_cache",
      "layer_past",
      "output_attentions",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GPTNeoXPreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_keys_to_ignore_on_load_unexpected": []
  },
  "GPT_NEOX_START_DOCSTRING": [],
  "GPT_NEOX_INPUTS_DOCSTRING": [],
  "GPTNeoXModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "GPTNeoXForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GPTNeoXForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "GPTNeoXForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "GPTNeoXForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "GPTNeoXRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GPTNeoXDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "BrosConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "dim_bbox",
      "bbox_scale",
      "n_relations",
      "classifier_dropout_prob",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "BrosSpadeOutput": {},
  "BrosPositionalEmbedding1D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pos_seq"
    ]
  },
  "BrosPositionalEmbedding2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "bbox"
    ]
  },
  "BrosBboxEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "bbox"
    ]
  },
  "BrosTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "BrosSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "bbox_pos_emb",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "BrosSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BrosAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "bbox_pos_emb",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ]
  },
  "BrosIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BrosOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BrosLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "bbox_pos_emb",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "BrosEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "bbox_pos_emb",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BrosPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BrosRelationExtractor": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query_layer",
      "key_layer"
    ]
  },
  "BrosPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BrosModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BrosForTokenClassification": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "bbox_first_token_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BrosSpadeEEForTokenClassification": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "bbox_first_token_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "initial_token_labels",
      "subsequent_token_labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BrosSpadeELForTokenClassification": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "bbox_first_token_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BrosProcessorKwargs": {
    "_defaults": []
  },
  "BrosProcessor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "tokenizer"
    ]
  },
  "Jais2Config": {
    "model_type": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "head_dim",
      "rope_parameters"
    ]
  },
  "Jais2MLP": {},
  "Jais2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Jais2PreTrainedModel": {},
  "Jais2Model": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Jais2ForCausalLM": {
    "forward": [
      "self"
    ]
  },
  "Jais2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Jais2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MBartLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "MBartScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "MBartAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "MBartEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "MBartDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MBartClassificationHead": {
    "__init__": [
      "self",
      "input_dim",
      "inner_dim",
      "num_classes",
      "pooler_dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MBartPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "MBartEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_backward_compatibility_gradient_checkpointing": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MBartDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MBartModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MBartForConditionalGeneration": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "MBartForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MBartForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MBartDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "MBartForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MBartTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "src_lang",
      "tgt_lang",
      "additional_special_tokens"
    ],
    "src_lang": [
      "self",
      "new_src_lang"
    ],
    "_build_translation_inputs": [
      "self",
      "raw_inputs",
      "return_tensors",
      "src_lang",
      "tgt_lang"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ],
    "set_src_lang_special_tokens": [
      "self",
      "src_lang"
    ],
    "set_tgt_lang_special_tokens": [
      "self",
      "lang"
    ]
  },
  "MBartConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "classifier_dropout",
      "scale_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "decoder_start_token_id",
      "forced_eos_token_id",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "FastVlmConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_id",
      "projector_hidden_act",
      "vision_feature_select_strategy",
      "vision_feature_layer",
      "multimodal_projector_bias",
      "tie_word_embeddings"
    ]
  },
  "FastVlmMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "FastVlmPreTrainedModel": {},
  "FastVlmModelOutputWithPast": {},
  "FastVlmModel": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "cache_position"
    ]
  },
  "FastVlmCausalLMOutputWithPast": {},
  "FastVlmForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "labels",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "OlmoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "OlmoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "OlmoeMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OlmoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "OlmoeExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "OlmoeTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "OlmoeSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "OlmoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "OlmoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_record_outputs": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "OlmoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "OlmoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "OlmoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "clip_qkv",
      "num_experts_per_tok",
      "num_experts",
      "output_router_logits",
      "router_aux_loss_coef",
      "norm_topk_prob"
    ]
  },
  "LongformerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "attention_window",
      "sep_token_id",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "onnx_export",
      "tie_word_embeddings"
    ]
  },
  "LongformerBaseModelOutput": {},
  "LongformerBaseModelOutputWithPooling": {},
  "LongformerMaskedLMOutput": {},
  "LongformerQuestionAnsweringModelOutput": {},
  "LongformerSequenceClassifierOutput": {},
  "LongformerMultipleChoiceModelOutput": {},
  "LongformerTokenClassifierOutput": {},
  "_get_question_end_index": [
    "input_ids",
    "sep_token_id"
  ],
  "_compute_global_attention_mask": [
    "input_ids",
    "sep_token_id",
    "before_sep_token"
  ],
  "LongformerEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ]
  },
  "LongformerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "is_index_masked",
      "is_index_global_attn",
      "is_global_attn",
      "output_attentions"
    ],
    "_pad_and_transpose_last_two_dims": [
      "hidden_states_padded",
      "padding"
    ],
    "_pad_and_diagonalize": [
      "chunked_hidden_states"
    ],
    "_chunk": [
      "hidden_states",
      "window_overlap",
      "onnx_export"
    ],
    "_mask_invalid_locations": [
      "input_tensor",
      "affected_seq_len"
    ],
    "_sliding_chunks_query_key_matmul": [
      "self",
      "query",
      "key",
      "window_overlap"
    ],
    "_sliding_chunks_matmul_attn_probs_value": [
      "self",
      "attn_probs",
      "value",
      "window_overlap"
    ],
    "_get_global_attn_indices": [
      "is_index_global_attn"
    ],
    "_concat_with_global_key_attn_probs": [
      "self",
      "key_vectors",
      "query_vectors",
      "max_num_global_attn_indices",
      "is_index_global_attn_nonzero",
      "is_local_index_global_attn_nonzero",
      "is_local_index_no_global_attn_nonzero"
    ],
    "_compute_attn_output_with_global_indices": [
      "self",
      "value_vectors",
      "attn_probs",
      "max_num_global_attn_indices",
      "is_index_global_attn_nonzero",
      "is_local_index_global_attn_nonzero"
    ],
    "_compute_global_attn_output_from_hidden": [
      "self",
      "hidden_states",
      "max_num_global_attn_indices",
      "is_local_index_global_attn_nonzero",
      "is_index_global_attn_nonzero",
      "is_local_index_no_global_attn_nonzero",
      "is_index_masked"
    ]
  },
  "LongformerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LongformerAttention": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "is_index_masked",
      "is_index_global_attn",
      "is_global_attn",
      "output_attentions"
    ]
  },
  "LongformerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongformerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LongformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "is_index_masked",
      "is_index_global_attn",
      "is_global_attn",
      "output_attentions"
    ],
    "ff_chunk": [
      "self",
      "attn_output"
    ]
  },
  "LongformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "padding_len",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LongformerPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongformerLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "LongformerPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": []
  },
  "LongformerModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "_pad_to_window_size": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "pad_token_id"
    ],
    "_merge_to_attention_mask": [
      "self",
      "attention_mask",
      "global_attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "global_attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LongformerForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "global_attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LongformerForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "global_attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LongformerClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongformerForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "global_attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LongformerForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "global_attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LongformerForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "global_attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Ernie4_5_MoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Ernie4_5_MoeMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Ernie4_5_MoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Ernie4_5_MoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Ernie4_5_MoeStatics": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_MoeExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Ernie4_5_MoeTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_MoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Ernie4_5_MoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_keep_in_fp32_modules_strict": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Ernie4_5_MoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Ernie4_5_MoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Ernie4_5_MoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "use_bias",
      "moe_intermediate_size",
      "moe_k",
      "moe_num_experts",
      "moe_num_shared_experts",
      "moe_layer_start_index",
      "moe_layer_end_index",
      "moe_layer_interval",
      "moe_norm_min",
      "output_router_logits",
      "router_aux_loss_coef"
    ]
  },
  "NllbTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "src_lang",
      "tgt_lang",
      "additional_special_tokens",
      "extra_special_tokens",
      "legacy_behaviour"
    ],
    "src_lang": [
      "self",
      "new_src_lang"
    ],
    "_build_translation_inputs": [
      "self",
      "raw_inputs",
      "return_tensors",
      "src_lang",
      "tgt_lang"
    ],
    "prepare_seq2seq_batch": [
      "self",
      "src_texts",
      "src_lang",
      "tgt_texts",
      "tgt_lang",
      "max_length",
      "max_target_length",
      "padding",
      "return_tensors",
      "truncation"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ],
    "set_src_lang_special_tokens": [
      "self",
      "src_lang"
    ],
    "set_tgt_lang_special_tokens": [
      "self",
      "lang"
    ]
  },
  "x_clip_loss": [
    "similarity"
  ],
  "XCLIPOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "XCLIPVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "XCLIPTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "XCLIPAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "XCLIPMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "XCLIPEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "XCLIPDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "XCLIPVisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "XCLIPPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "XCLIPEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XCLIPTextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XCLIPTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XCLIPVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XCLIPVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "XCLIPVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XCLIPMultiframeIntegrationTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XCLIPCrossAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "batch_size"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "values"
    ]
  },
  "PromptGeneratorLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "visual"
    ]
  },
  "XCLIPPromptGenerator": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "text",
      "visual"
    ]
  },
  "XCLIPModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_video_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "XCLIPProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "XCLIPTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "XCLIPVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mit_hidden_size",
      "mit_intermediate_size",
      "mit_num_hidden_layers",
      "mit_num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "num_frames",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "drop_path_rate"
    ]
  },
  "XCLIPConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "prompt_layers",
      "prompt_alpha",
      "prompt_hidden_act",
      "prompt_num_attention_heads",
      "prompt_attention_dropout",
      "prompt_projection_dropout",
      "logit_scale_init_value"
    ]
  },
  "ParakeetEncoderConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "attention_bias",
      "convolution_bias",
      "conv_kernel_size",
      "subsampling_factor",
      "subsampling_conv_channels",
      "num_mel_bins",
      "subsampling_conv_kernel_size",
      "subsampling_conv_stride",
      "dropout",
      "dropout_positions",
      "layerdrop",
      "activation_dropout",
      "attention_dropout",
      "max_position_embeddings",
      "scale_input",
      "initializer_range"
    ]
  },
  "ParakeetCTCConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vocab_size",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "encoder_config",
      "pad_token_id"
    ],
    "from_encoder_config": [
      "cls",
      "encoder_config"
    ]
  },
  "ParakeetTokenizer": {
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "group_tokens"
    ]
  },
  "ParakeetEncoderModelOutput": {},
  "ParakeetEncoderRelPositionalEncoding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ParakeetEncoderFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ParakeetEncoderConvolutionModule": {
    "__init__": [
      "self",
      "config",
      "module_config"
    ]
  },
  "ParakeetEncoderAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ],
    "_rel_shift": [
      "self",
      "attention_scores"
    ]
  },
  "ParakeetEncoderSubsamplingConv2D": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_output_length": [
      "self",
      "input_lengths",
      "conv_layer"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask"
    ]
  },
  "ParakeetEncoderBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "ParakeetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flat_attention_mask": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_flash_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_subsampling_output_length": [
      "self",
      "input_lengths"
    ],
    "_get_output_attention_mask": [
      "self",
      "attention_mask",
      "target_length"
    ]
  },
  "ParakeetEncoder": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attention_mask"
    ]
  },
  "ParakeetGenerateOutput": {},
  "ParakeetForCTC": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "labels"
    ],
    "generate": [
      "self",
      "input_features",
      "attention_mask",
      "return_dict_in_generate"
    ]
  },
  "EPSILON": [],
  "LOG_ZERO_GUARD_VALUE": [],
  "ParakeetFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "hop_length",
      "n_fft",
      "win_length",
      "preemphasis",
      "padding_value"
    ],
    "_torch_extract_fbank_features": [
      "self",
      "waveform",
      "device"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "truncation",
      "pad_to_multiple_of",
      "return_tensors",
      "return_attention_mask",
      "padding",
      "max_length",
      "sampling_rate",
      "do_normalize",
      "device",
      "return_token_timestamps"
    ]
  },
  "ParakeetProcessorKwargs": {
    "_defaults": []
  },
  "ParakeetProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "audio",
      "text",
      "sampling_rate"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "DeiTEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "DeiTPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DeiTSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeiTSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "DeiTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeiTIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeiTOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "DeiTLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeiTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeiTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DeiTModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "use_mask_token"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "DeiTPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeiTForMaskedImageModeling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "DeiTForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "interpolate_pos_encoding"
    ]
  },
  "DeiTForImageClassificationWithTeacherOutput": {},
  "DeiTForImageClassificationWithTeacher": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "DeiTImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "rescale_factor",
      "do_rescale",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "DeiTImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": []
  },
  "DeiTConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "encoder_stride",
      "pooler_output_size",
      "pooler_act"
    ]
  },
  "DebertaV2Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "split_by_punct",
      "bos_token",
      "eos_token",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "add_prefix_space",
      "unk_id"
    ]
  },
  "DebertaV2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "relative_attention",
      "max_relative_positions",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "position_biased_input",
      "pos_att_type",
      "pooler_dropout",
      "pooler_hidden_act",
      "legacy",
      "tie_word_embeddings"
    ]
  },
  "DebertaV2SelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "make_log_bucket_position": [
    "relative_pos",
    "bucket_size",
    "max_position"
  ],
  "DebertaV2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "query_states",
      "relative_pos",
      "rel_embeddings"
    ]
  },
  "DebertaV2Intermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DebertaV2Output": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "DebertaV2Layer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "query_states",
      "relative_pos",
      "rel_embeddings",
      "output_attentions"
    ]
  },
  "ConvLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual_states",
      "input_mask"
    ]
  },
  "DebertaV2Embeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "mask",
      "inputs_embeds"
    ]
  },
  "DebertaV2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_rel_embedding": [
      "self"
    ],
    "get_attention_mask": [
      "self",
      "attention_mask"
    ],
    "get_rel_pos": [
      "self",
      "hidden_states",
      "query_states",
      "relative_pos"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_hidden_states",
      "output_attentions",
      "query_states",
      "relative_pos",
      "return_dict"
    ]
  },
  "DebertaV2PreTrainedModel": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DebertaV2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LegacyDebertaV2PredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LegacyDebertaV2LMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LegacyDebertaV2OnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "DebertaV2LMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "word_embeddings"
    ]
  },
  "DebertaV2OnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "word_embeddings"
    ]
  },
  "DebertaV2ForMaskedLM": {
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DebertaV2ForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DebertaV2ForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DebertaV2ForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DebertaV2ForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "embedding_size",
      "trigram_input",
      "use_bottleneck",
      "intra_bottleneck_size",
      "use_bottleneck_attention",
      "key_query_shared_bottleneck",
      "num_feedforward_networks",
      "normalization_type",
      "classifier_activation",
      "classifier_dropout",
      "tie_word_embeddings"
    ]
  },
  "NoNorm": {
    "__init__": [
      "self",
      "feat_size",
      "eps"
    ],
    "forward": [
      "self",
      "input_tensor"
    ]
  },
  "MobileBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "MobileBertSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query_tensor",
      "key_tensor",
      "value_tensor",
      "attention_mask"
    ]
  },
  "MobileBertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual_tensor"
    ]
  },
  "MobileBertAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query_tensor",
      "key_tensor",
      "value_tensor",
      "layer_input",
      "attention_mask"
    ]
  },
  "MobileBertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "OutputBottleneck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual_tensor"
    ]
  },
  "MobileBertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "intermediate_states",
      "residual_tensor_1",
      "residual_tensor_2"
    ]
  },
  "BottleneckLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Bottleneck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FFNOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual_tensor"
    ]
  },
  "FFNLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileBertLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "MobileBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "MobileBertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileBertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileBertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileBertOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "MobileBertPreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "MobileBertPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MobileBertForPreTrainingOutput": {},
  "MobileBertModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "MobileBertForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "next_sentence_label"
    ]
  },
  "MobileBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "MobileBertOnlyNSPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "MobileBertForNextSentencePrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "MobileBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "MobileBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "MobileBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "MobileBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "MobileBertTokenizer": [],
  "MobileBertTokenizerFast": [],
  "Swinv2Config": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim",
      "depths",
      "num_heads",
      "window_size",
      "pretrained_window_sizes",
      "mlp_ratio",
      "qkv_bias",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "drop_path_rate",
      "hidden_act",
      "use_absolute_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "encoder_stride",
      "out_features",
      "out_indices"
    ]
  },
  "Swinv2EncoderOutput": {},
  "Swinv2ModelOutput": {},
  "Swinv2MaskedImageModelingOutput": {},
  "Swinv2ImageClassifierOutput": {},
  "Swinv2DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Swinv2Embeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "Swinv2PatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "maybe_pad": [
      "self",
      "pixel_values",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Swinv2PatchMerging": {
    "__init__": [
      "self",
      "input_resolution",
      "dim",
      "norm_layer"
    ],
    "maybe_pad": [
      "self",
      "input_feature",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "input_feature",
      "input_dimensions"
    ]
  },
  "Swinv2SelfAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size",
      "pretrained_window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "create_coords_table_and_index": [
      "self"
    ]
  },
  "Swinv2SelfOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Swinv2Attention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size",
      "pretrained_window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Swinv2Intermediate": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Swinv2Output": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Swinv2Layer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "num_heads",
      "drop_path_rate",
      "shift_size",
      "pretrained_window_size"
    ],
    "_compute_window_shift": [
      "self",
      "target_window_size",
      "target_shift_size"
    ],
    "get_attn_mask": [
      "self",
      "height",
      "width",
      "dtype"
    ],
    "maybe_pad": [
      "self",
      "hidden_states",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions"
    ]
  },
  "Swinv2Stage": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "depth",
      "num_heads",
      "drop_path",
      "downsample",
      "pretrained_window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions"
    ]
  },
  "Swinv2Encoder": {
    "__init__": [
      "self",
      "config",
      "grid_size",
      "pretrained_window_sizes"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "output_hidden_states",
      "output_hidden_states_before_downsampling",
      "return_dict"
    ]
  },
  "Swinv2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Swinv2Model": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "use_mask_token"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Swinv2ForMaskedImageModeling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Swinv2ForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "Swinv2Backbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CwmConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings",
      "attention_dropout",
      "pretraining_tp",
      "mlp_bias",
      "rope_parameters",
      "sliding_window",
      "layer_types"
    ]
  },
  "CwmRotaryEmbedding": {},
  "CwmAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "CwmDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "CwmPreTrainedModel": {},
  "CwmModelOutputWithPast": {},
  "CwmModel": {
    "config_class": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "CwmForCausalLM": {},
  "CwmRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "CwmMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Sam2VideoInferenceCache": {
    "__init__": [
      "self",
      "inference_device",
      "inference_state_device",
      "max_vision_features_cache_size"
    ],
    "cache_vision_features": [
      "self",
      "frame_idx",
      "features"
    ],
    "get_vision_features": [
      "self",
      "frame_idx"
    ],
    "clear_all": [
      "self"
    ]
  },
  "Sam2VideoInferenceSession": {
    "__init__": [
      "self",
      "video",
      "video_height",
      "video_width",
      "inference_device",
      "inference_state_device",
      "video_storage_device",
      "dtype",
      "max_vision_features_cache_size"
    ],
    "num_frames": [
      "self"
    ],
    "obj_id_to_idx": [
      "self",
      "obj_id"
    ],
    "obj_idx_to_id": [
      "self",
      "obj_idx"
    ],
    "get_obj_num": [
      "self"
    ],
    "add_point_inputs": [
      "self",
      "obj_idx",
      "frame_idx",
      "inputs"
    ],
    "remove_point_inputs": [
      "self",
      "obj_idx",
      "frame_idx"
    ],
    "add_mask_inputs": [
      "self",
      "obj_idx",
      "frame_idx",
      "inputs"
    ],
    "remove_mask_inputs": [
      "self",
      "obj_idx",
      "frame_idx"
    ],
    "store_output": [
      "self",
      "obj_idx",
      "frame_idx",
      "output_key",
      "output_value",
      "is_conditioning_frame"
    ],
    "get_output": [
      "self",
      "obj_idx",
      "frame_idx",
      "output_key",
      "is_conditioning_frame"
    ],
    "add_new_frame": [
      "self",
      "pixel_values",
      "frame_idx"
    ],
    "get_frame": [
      "self",
      "frame_idx"
    ],
    "reset_tracking_data": [
      "self"
    ],
    "reset_inference_session": [
      "self"
    ]
  },
  "Sam2VideoLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "Sam2VideoPositionEmbeddingSine": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "Sam2VideoAttention": {
    "__init__": [
      "self",
      "config",
      "downsample_rate"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_similarity"
    ]
  },
  "Sam2VideoTwoWayAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "skip_first_layer_pe"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "query_point_embedding",
      "key_point_embedding",
      "attention_similarity"
    ]
  },
  "Sam2VideoFeedForward": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "activation",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam2VideoImageSegmentationOutput": {},
  "Sam2VideoSegmentationOutput": {},
  "Sam2VideoPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Sam2VideoVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ],
    "create_inv_freq": [
      "self"
    ]
  },
  "Sam2VideoRoPEAttention": {
    "__init__": [
      "self",
      "config",
      "kv_in_dim",
      "rope_k_repeat"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "position_embeddings",
      "num_k_exclude_rope"
    ]
  },
  "Sam2VideoMemoryAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "key_point_embedding",
      "rope_position_embeddings",
      "num_k_exclude_rope"
    ]
  },
  "Sam2VideoMemoryAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "current_vision_features",
      "memory",
      "current_vision_position_embeddings",
      "memory_posision_embeddings",
      "num_object_pointer_tokens"
    ]
  },
  "Sam2VideoMemoryFuserCXBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam2VideoMemoryFuser": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Sam2VideoMaskDownSamplerLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Sam2VideoMaskDownSampler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Sam2VideoMemoryEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_features",
      "masks"
    ]
  },
  "Sam2VideoPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_coords",
      "input_shape"
    ]
  },
  "Sam2VideoVisionEncoderOutput": {},
  "Sam2VideoMaskEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "masks"
    ]
  },
  "Sam2VideoPromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ],
    "forward": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ]
  },
  "Sam2VideoTwoWayTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "point_embeddings",
      "image_embeddings",
      "image_positional_embeddings",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "Sam2VideoMaskDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "high_resolution_features",
      "attention_similarity",
      "target_embedding"
    ],
    "_get_stability_scores": [
      "self",
      "mask_logits"
    ],
    "_dynamic_multimask_via_stability": [
      "self",
      "all_mask_logits",
      "all_iou_scores"
    ]
  },
  "NO_OBJ_SCORE": [],
  "get_1d_sine_pe": [
    "pos_inds",
    "dim",
    "temperature"
  ],
  "Sam2VideoModel": {
    "input_modalities": [],
    "_can_record_outputs": [],
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_prompt_embeddings": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ],
    "forward": [
      "self",
      "inference_session",
      "frame_idx",
      "frame",
      "reverse",
      "run_mem_encoder"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "_prepare_vision_features": [
      "self",
      "inference_session",
      "frame_idx",
      "batch_size"
    ],
    "_single_frame_forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "image_embeddings",
      "multimask_output",
      "attention_similarity",
      "target_embedding"
    ],
    "_use_mask_as_output": [
      "self",
      "backbone_features",
      "high_res_features",
      "mask_inputs"
    ],
    "_select_closest_cond_frames": [
      "self",
      "frame_idx",
      "cond_frame_outputs",
      "max_cond_frame_num"
    ],
    "_gather_memory_frame_outputs": [
      "self",
      "inference_session",
      "obj_idx",
      "frame_idx",
      "track_in_reverse_time"
    ],
    "_build_memory_attention_inputs": [
      "self",
      "temporal_positions_and_previous_outputs",
      "device"
    ],
    "_get_object_pointers": [
      "self",
      "inference_session",
      "obj_idx",
      "frame_idx",
      "num_total_frames",
      "device",
      "track_in_reverse_time",
      "streaming"
    ],
    "_process_object_pointers": [
      "self",
      "temporal_offsets",
      "pointer_tokens",
      "max_object_pointers_to_use",
      "batch_size",
      "num_channels",
      "device"
    ],
    "_prepare_memory_conditioned_features": [
      "self",
      "inference_session",
      "frame_idx",
      "obj_idx",
      "is_initial_conditioning_frame",
      "current_vision_features",
      "current_vision_positional_embeddings",
      "num_total_frames",
      "track_in_reverse_time",
      "streaming"
    ],
    "_use_multimask": [
      "self",
      "is_init_cond_frame",
      "point_inputs"
    ],
    "_run_single_frame_inference": [
      "self",
      "inference_session",
      "frame_idx",
      "obj_idx",
      "batch_size",
      "is_init_cond_frame",
      "point_inputs",
      "mask_inputs",
      "reverse",
      "prev_sam_mask_logits",
      "streaming"
    ],
    "_encode_new_memory": [
      "self",
      "current_vision_feats",
      "pred_masks_high_res",
      "object_score_logits",
      "is_mask_from_pts"
    ],
    "_batch_encode_memories": [
      "self",
      "inference_session",
      "frame_idx",
      "objects_needing_memory_encoding",
      "high_res_masks_for_memory",
      "object_score_logits_for_memory",
      "is_mask_from_pts_per_obj"
    ],
    "propagate_in_video_iterator": [
      "self",
      "inference_session",
      "start_frame_idx",
      "max_frame_num_to_track",
      "reverse",
      "show_progress_bar"
    ]
  },
  "Sam2VideoVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "model_input_names": [],
    "_preprocess": [
      "self",
      "videos",
      "size",
      "return_tensors"
    ],
    "post_process_masks": [
      "self",
      "masks",
      "original_sizes",
      "reshaped_input_sizes",
      "mask_threshold",
      "binarize",
      "pad_size"
    ]
  },
  "Sam2VideoPromptEncoderConfig": {},
  "Sam2VideoMaskDecoderConfig": {},
  "Sam2VideoConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "prompt_encoder_config",
      "mask_decoder_config",
      "initializer_range",
      "num_maskmem",
      "image_size",
      "sigmoid_scale_for_mem_enc",
      "sigmoid_bias_for_mem_enc",
      "enable_occlusion_spatial_embedding",
      "multimask_output_in_sam",
      "multimask_min_pt_num",
      "multimask_max_pt_num",
      "multimask_output_for_tracking",
      "max_object_pointers_in_encoder",
      "max_cond_frame_num",
      "enable_temporal_pos_encoding_for_object_pointers",
      "memory_attention_hidden_size",
      "memory_attention_num_layers",
      "memory_attention_num_attention_heads",
      "memory_attention_downsample_rate",
      "memory_attention_feed_forward_hidden_size",
      "memory_attention_feed_forward_hidden_act",
      "memory_attention_dropout",
      "memory_attention_rope_theta",
      "memory_attention_rope_feat_sizes",
      "memory_attention_rope_dropout",
      "memory_encoder_hidden_size",
      "memory_encoder_output_channels",
      "mask_downsampler_embed_dim",
      "mask_downsampler_kernel_size",
      "mask_downsampler_stride",
      "mask_downsampler_padding",
      "mask_downsampler_total_stride",
      "mask_downsampler_hidden_act",
      "memory_fuser_num_layers",
      "memory_fuser_embed_dim",
      "memory_fuser_intermediate_dim",
      "memory_fuser_kernel_size",
      "memory_fuser_padding",
      "memory_fuser_layer_scale_init_value",
      "memory_fuser_hidden_act"
    ]
  },
  "Sam2VideoProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "video_processor",
      "target_size",
      "point_pad_value"
    ],
    "init_video_session": [
      "self",
      "video",
      "inference_device",
      "inference_state_device",
      "processing_device",
      "video_storage_device",
      "max_vision_features_cache_size",
      "dtype"
    ],
    "add_inputs_to_inference_session": [
      "self",
      "inference_session",
      "frame_idx",
      "obj_ids",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "original_size",
      "clear_old_inputs"
    ],
    "process_new_points_or_boxes_for_video_frame": [
      "self",
      "inference_session",
      "frame_idx",
      "obj_ids",
      "input_points",
      "input_labels",
      "input_boxes",
      "original_size",
      "clear_old_inputs"
    ],
    "process_new_mask_for_video_frame": [
      "self",
      "inference_session",
      "frame_idx",
      "obj_ids",
      "input_masks"
    ]
  },
  "Glm4Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "attention_dropout",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "attention_bias"
    ]
  },
  "Glm4MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Glm4Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Glm4RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Glm4RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Glm4PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Glm4Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "Glm4ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Glm4ForSequenceClassification": {},
  "Glm4ForTokenClassification": {},
  "ExaoneMoeConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_dropout",
      "sliding_window",
      "sliding_window_pattern",
      "layer_types",
      "mlp_layer_types",
      "first_k_dense_replace",
      "moe_intermediate_size",
      "num_experts",
      "num_experts_per_tok",
      "num_shared_experts",
      "norm_topk_prob",
      "routed_scaling_factor",
      "n_group",
      "topk_group"
    ]
  },
  "ExaoneMoeAttention": {},
  "ExaoneMoeMLP": {},
  "ExaoneMoeTopkRouter": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "ExaoneMoeExperts": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "ExaoneMoeSparseMoEBlock": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "ExaoneMoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "ExaoneMoePreTrainedModel": {
    "_can_record_outputs": [],
    "_can_compile_fullgraph": [],
    "_keep_in_fp32_modules_strict": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ExaoneMoeModel": {},
  "ExaoneMoeForCausalLM": {
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "ExaoneMoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ExaoneMoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "BartphoTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "is_fast": [],
    "__init__": [
      "self",
      "vocab_file",
      "monolingual_vocab_file",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "sp_model_kwargs"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_token_to_id_with_added_voc": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "_align_added_tokens_with_fairseq_vocab": [
      "self"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "BertJapaneseTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "spm_file",
      "do_lower_case",
      "do_word_tokenize",
      "do_subword_tokenize",
      "word_tokenizer_type",
      "subword_tokenizer_type",
      "never_split",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "mecab_kwargs",
      "sudachi_kwargs",
      "jumanpp_kwargs"
    ],
    "do_lower_case": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "MecabTokenizer": {
    "__init__": [
      "self",
      "do_lower_case",
      "never_split",
      "normalize_text",
      "mecab_dic",
      "mecab_option"
    ],
    "tokenize": [
      "self",
      "text",
      "never_split"
    ]
  },
  "SudachiTokenizer": {
    "__init__": [
      "self",
      "do_lower_case",
      "never_split",
      "normalize_text",
      "trim_whitespace",
      "sudachi_split_mode",
      "sudachi_config_path",
      "sudachi_resource_dir",
      "sudachi_dict_type",
      "sudachi_projection"
    ],
    "tokenize": [
      "self",
      "text",
      "never_split"
    ]
  },
  "JumanppTokenizer": {
    "__init__": [
      "self",
      "do_lower_case",
      "never_split",
      "normalize_text",
      "trim_whitespace"
    ],
    "tokenize": [
      "self",
      "text",
      "never_split"
    ]
  },
  "CharacterTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "unk_token",
      "normalize_text"
    ],
    "tokenize": [
      "self",
      "text"
    ]
  },
  "SentencepieceTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "unk_token",
      "do_lower_case",
      "remove_space",
      "keep_accents",
      "sp_model_kwargs"
    ],
    "preprocess_text": [
      "self",
      "inputs"
    ],
    "tokenize": [
      "self",
      "text"
    ]
  },
  "MinistralMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MinistralAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "MinistralRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MinistralDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "MinistralPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "MinistralRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MinistralModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "MinistralForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MinistralForSequenceClassification": {},
  "MinistralForTokenClassification": {},
  "MinistralForQuestionAnswering": {
    "base_model_prefix": []
  },
  "MinistralConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "sliding_window",
      "attention_dropout",
      "layer_types"
    ]
  },
  "NemotronConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "head_dim",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias"
    ]
  },
  "_cast_if_autocast_enabled": [
    "device_type"
  ],
  "NemotronLayerNorm1P": {
    "__init__": [
      "self",
      "normalized_shape",
      "eps",
      "elementwise_affine",
      "bias",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "NemotronRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "NemotronMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "NemotronAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "NemotronFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "NemotronSdpaAttention": {
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "NEMOTRON_ATTENTION_CLASSES": [],
  "NemotronDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "NemotronPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "NemotronModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "NemotronForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "NemotronForSequenceClassification": {},
  "NemotronForQuestionAnswering": {
    "base_model_prefix": []
  },
  "NemotronForTokenClassification": {},
  "ASTConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "patch_size",
      "qkv_bias",
      "frequency_stride",
      "time_stride",
      "max_length",
      "num_mel_bins"
    ],
    "_get_non_default_generation_parameters": [
      "self"
    ]
  },
  "ASTEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "get_shape": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "ASTPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "ASTSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ASTSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ASTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ASTIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ASTOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ASTLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ASTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ASTPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ASTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "ASTMLPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ASTForAudioClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "labels"
    ]
  },
  "MistralConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "sliding_window",
      "attention_dropout"
    ]
  },
  "MistralMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MistralAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "MistralRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MistralDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "MistralPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "MistralRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MistralModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "MistralForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MistralForTokenClassification": {},
  "MistralForSequenceClassification": {},
  "MistralForQuestionAnswering": {},
  "Ernie4_5_VL_MoeVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "intermediate_size",
      "num_heads",
      "in_channels",
      "patch_size",
      "spatial_merge_size",
      "temporal_merge_size",
      "rms_norm_eps",
      "initializer_range"
    ]
  },
  "Ernie4_5_VL_MoeTextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "use_bias",
      "rope_parameters",
      "mlp_layer_types",
      "moe_intermediate_size",
      "moe_k",
      "moe_num_experts",
      "moe_num_shared_experts",
      "moe_norm_min",
      "output_router_logits",
      "router_aux_loss_coef",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id"
    ]
  },
  "Ernie4_5_VL_MoeConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_start_token_id",
      "image_end_token_id",
      "image_token_id",
      "video_start_token_id",
      "video_end_token_id",
      "video_token_id",
      "tie_word_embeddings"
    ]
  },
  "Ernie4_5_VL_MoeVideoProcessorInitKwargs": {},
  "Ernie4_5_VL_MoeVideoProcessor": {
    "resample": [],
    "size": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "min_frames": [],
    "max_frames": [],
    "do_sample_frames": [],
    "draw_on_frames": [],
    "font": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "get_video_processor_dict": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "to_dict": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "_further_process_kwargs": [
      "self",
      "size"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "min_frames",
      "max_frames",
      "num_frames",
      "fps"
    ],
    "_convert_timestamp": [
      "self",
      "time_stamp_in_seconds"
    ],
    "_render_image_with_timestamp": [
      "self",
      "image",
      "timestamp",
      "size_factor"
    ],
    "_prepare_input_videos": [
      "self",
      "videos",
      "input_data_format",
      "device",
      "video_metadata",
      "draw_on_frames"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "merge_size",
      "return_tensors"
    ],
    "preprocess": [
      "self",
      "videos"
    ]
  },
  "Ernie4_5_VL_MoeImageProcessorFast": {
    "do_resize": [],
    "resample": [],
    "size": [],
    "do_rescale": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "merge_size",
      "disable_grouping",
      "return_tensors"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "Ernie4_5_VL_MoeProcessorKwargs": {
    "_defaults": []
  },
  "Ernie4_5_VL_MoeProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "chat_template"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "model_input_names": [
      "self"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes",
      "video_sizes"
    ]
  },
  "Ernie4_5_VL_MoeTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ],
    "recomposition_to_3d": [
      "self",
      "freq"
    ]
  },
  "rotate_half_text": [
    "x"
  ],
  "Ernie4_5_VL_MoeTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Ernie4_5_VL_MoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Ernie4_5_VL_MoeMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Ernie4_5_VL_MoeMoeStatics": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_VL_MoeMoeTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_VL_MoeMoeExperts": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Ernie4_5_VL_MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_VL_MoeMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "moe_mm_token_type_ids"
    ]
  },
  "Ernie4_5_VL_MoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "moe_mm_token_type_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "Ernie4_5_VL_MoeVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Ernie4_5_VL_MoeVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Ernie4_5_VL_MoePreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_keep_in_fp32_modules_strict": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Ernie4_5_VL_MoeTextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "moe_mm_token_type_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Ernie4_5VLVisionMLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim",
      "hidden_act"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Ernie4_5_VL_MoePatchEmbed": {
    "__init__": [
      "self",
      "patch_size",
      "in_channels",
      "embed_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_VL_MoeVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Ernie4_5_VL_MoeVisionTransformerPretrainedModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "_input_embed_layer": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Ernie4_5_VL_MoeVisionMLP": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "out_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Ernie4_5_VL_MoeVariableResolutionResamplerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_temporal_slicing": [
      "self",
      "hidden_states",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Ernie4_5_VL_MoeModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask",
      "mm_token_type_ids"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "mm_token_type_ids",
      "moe_mm_token_type_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position"
    ],
    "get_position_ids": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position",
      "mm_token_type_ids"
    ]
  },
  "Ernie4_5_VL_MoeForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "mm_token_type_ids",
      "moe_mm_token_type_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "inputs_embeds",
      "attention_mask",
      "cache_position",
      "past_key_values",
      "image_grid_thw",
      "video_grid_thw",
      "use_cache",
      "is_first_iteration",
      "position_ids"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "Ernie4_5_VL_MoeImageProcessorKwargs": {},
  "Ernie4_5_VL_MoeImageProcessor": {
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "BARK_SUBMODELCONFIG_START_DOCSTRING": [],
  "BarkSubModelConfig": {
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "block_size",
      "input_vocab_size",
      "output_vocab_size",
      "num_layers",
      "num_heads",
      "hidden_size",
      "dropout",
      "bias",
      "initializer_range",
      "use_cache"
    ]
  },
  "BarkSemanticConfig": {
    "model_type": [],
    "base_config_key": []
  },
  "BarkCoarseConfig": {
    "model_type": [],
    "base_config_key": []
  },
  "BarkFineConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "tie_word_embeddings",
      "n_codes_total",
      "n_codes_given"
    ]
  },
  "BarkConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "semantic_config",
      "coarse_acoustics_config",
      "fine_acoustics_config",
      "codec_config",
      "initializer_range"
    ]
  },
  "BarkSemanticGenerationConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "eos_token_id",
      "renormalize_logits",
      "max_new_tokens",
      "output_scores",
      "return_dict_in_generate",
      "output_hidden_states",
      "output_attentions",
      "temperature",
      "do_sample",
      "text_encoding_offset",
      "text_pad_token",
      "semantic_infer_token",
      "semantic_vocab_size",
      "max_input_semantic_length",
      "semantic_rate_hz",
      "min_eos_p"
    ]
  },
  "BarkCoarseGenerationConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "renormalize_logits",
      "output_scores",
      "return_dict_in_generate",
      "output_hidden_states",
      "output_attentions",
      "temperature",
      "do_sample",
      "coarse_semantic_pad_token",
      "coarse_rate_hz",
      "n_coarse_codebooks",
      "coarse_infer_token",
      "max_coarse_input_length",
      "max_coarse_history",
      "sliding_window_len"
    ]
  },
  "BarkFineGenerationConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "temperature",
      "max_fine_history_length",
      "max_fine_input_length",
      "n_fine_codebooks"
    ],
    "validate": [
      "self"
    ]
  },
  "BarkGenerationConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "semantic_config",
      "coarse_acoustics_config",
      "fine_acoustics_config",
      "sample_rate",
      "codebook_size"
    ],
    "from_sub_model_configs": [
      "cls",
      "semantic_config",
      "coarse_acoustics_config",
      "fine_acoustics_config"
    ],
    "to_dict": [
      "self"
    ]
  },
  "BarkSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "_split_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "_merge_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "BarkSelfFlashAttention2": {
    "__init__": [
      "self"
    ],
    "_split_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "_merge_heads": [
      "self",
      "tensor",
      "num_heads",
      "attn_head_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "BARK_ATTENTION_CLASSES": [],
  "BarkMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BarkBlock": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "attention_mask",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "BarkPreTrainedModel": {
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "device": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BarkCausalModel": {
    "output_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "attention_mask",
      "input_embeds",
      "past_key_values",
      "position_ids",
      "use_cache",
      "cache_position"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "position_ids",
      "labels",
      "input_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BarkSemanticModel": {
    "base_model_prefix": [],
    "generate": [
      "self",
      "input_ids",
      "semantic_generation_config",
      "history_prompt",
      "attention_mask"
    ]
  },
  "BarkCoarseModel": {
    "base_model_prefix": [],
    "preprocess_histories": [
      "self",
      "max_coarse_history",
      "semantic_to_coarse_ratio",
      "batch_size",
      "semantic_generation_config",
      "codebook_size",
      "history_prompt"
    ],
    "generate": [
      "self",
      "semantic_output",
      "semantic_generation_config",
      "coarse_generation_config",
      "codebook_size",
      "history_prompt",
      "return_output_lengths"
    ]
  },
  "BarkFineModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_output_embeddings"
    ],
    "_resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "forward": [
      "self",
      "codebook_idx",
      "input_ids",
      "attention_mask",
      "position_ids",
      "labels",
      "input_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "coarse_output",
      "semantic_generation_config",
      "coarse_generation_config",
      "fine_generation_config",
      "codebook_size",
      "history_prompt"
    ]
  },
  "BarkModel": {
    "__init__": [
      "self",
      "config"
    ],
    "can_generate": [
      "cls"
    ],
    "device": [
      "self"
    ],
    "enable_cpu_offload": [
      "self",
      "accelerator_id"
    ],
    "codec_decode": [
      "self",
      "fine_output",
      "output_lengths"
    ],
    "generate": [
      "self",
      "input_ids",
      "history_prompt",
      "return_output_lengths"
    ]
  },
  "BarkProcessor": {
    "preset_shape": [],
    "__init__": [
      "self",
      "tokenizer",
      "speaker_embeddings"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_processor_name_or_path",
      "speaker_embeddings_dict_path"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "speaker_embeddings_dict_path",
      "speaker_embeddings_directory",
      "push_to_hub"
    ],
    "_load_voice_preset": [
      "self",
      "voice_preset"
    ],
    "_validate_voice_preset_dict": [
      "self",
      "voice_preset"
    ],
    "available_voice_presets": [
      "self"
    ],
    "_verify_speaker_embeddings": [
      "self",
      "remove_unavailable"
    ],
    "__call__": [
      "self",
      "text",
      "voice_preset",
      "return_tensors",
      "max_length",
      "add_special_tokens",
      "return_attention_mask",
      "return_token_type_ids"
    ]
  },
  "Glm4vImageProcessorKwargs": {},
  "Glm4vImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "Glm4vImageProcessorFast": {
    "do_resize": [],
    "resample": [],
    "size": [],
    "do_rescale": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "merge_size": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "disable_grouping",
      "return_tensors"
    ],
    "preprocess": [
      "self",
      "images"
    ]
  },
  "Glm4vProcessorKwargs": {
    "_defaults": []
  },
  "Glm4vProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes",
      "video_sizes"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "replace_frame_token_id": [
      "self",
      "timestamp_sec"
    ]
  },
  "Glm4vVideoProcessorInitKwargs": {},
  "Glm4vVideoProcessor": {
    "resample": [],
    "size": [],
    "max_image_size": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "max_duration": [],
    "merge_size": [],
    "valid_kwargs": [],
    "num_frames": [],
    "fps": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "fps"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "return_tensors"
    ]
  },
  "Glm4vRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Glm4VisionMlp": {
    "__init__": [
      "self",
      "config",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Glm4vVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4vVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Glm4vVisionPatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "hidden_act",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Glm4vVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "embeddings",
      "lengths",
      "image_shapes",
      "h_coords",
      "w_coords"
    ]
  },
  "Glm4vVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Glm4vVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Glm4vTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ],
    "apply_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ]
  },
  "rotate_half_llm": [
    "x"
  ],
  "Glm4vTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Glm4vTextMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4vTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "Glm4vModelOutputWithPast": {},
  "Glm4vPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Glm4vVisionModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Glm4vTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Glm4vModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position"
    ]
  },
  "Glm4vCausalLMOutputWithPast": {},
  "Glm4vForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "Glm4vVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "attention_bias",
      "attention_dropout",
      "num_heads",
      "in_channels",
      "image_size",
      "patch_size",
      "rms_norm_eps",
      "spatial_merge_size",
      "temporal_patch_size",
      "out_hidden_size",
      "intermediate_size",
      "initializer_range"
    ]
  },
  "Glm4vTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "attention_dropout",
      "rope_parameters",
      "pad_token_id"
    ]
  },
  "Glm4vConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "image_start_token_id",
      "image_end_token_id",
      "video_start_token_id",
      "video_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "ClapTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_factor",
      "layer_norm_eps",
      "projection_dim",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "projection_hidden_act"
    ]
  },
  "ClapAudioConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "window_size",
      "num_mel_bins",
      "spec_size",
      "hidden_act",
      "patch_size",
      "patch_stride",
      "num_classes",
      "hidden_size",
      "projection_dim",
      "depths",
      "num_attention_heads",
      "enable_fusion",
      "hidden_dropout_prob",
      "fusion_type",
      "patch_embed_input_channels",
      "flatten_patch_embeds",
      "patch_embeds_hidden_size",
      "enable_patch_layer_norm",
      "drop_path_rate",
      "attention_probs_dropout_prob",
      "qkv_bias",
      "mlp_ratio",
      "aff_block_r",
      "num_hidden_layers",
      "projection_hidden_act",
      "layer_norm_eps",
      "initializer_factor"
    ]
  },
  "ClapConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "audio_config",
      "logit_scale_init_value",
      "projection_dim",
      "projection_hidden_act",
      "initializer_factor"
    ]
  },
  "ClapProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ]
  },
  "interpolate": [
    "hidden_states",
    "ratio"
  ],
  "ClapTextModelOutput": {},
  "ClapAudioModelOutput": {},
  "ClapOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "ClapDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClapAudioAFFBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual"
    ]
  },
  "ClapAudioPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "is_longer_idx"
    ]
  },
  "ClapAudioSelfAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "create_relative_position_index": [
      "self"
    ]
  },
  "ClapAudioSelfOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ClapAudioAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ClapAudioIntermediate": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClapAudioOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClapAudioLayer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "num_heads",
      "drop_path_rate",
      "shift_size"
    ],
    "set_shift_and_window_size": [
      "self",
      "input_resolution"
    ],
    "get_attn_mask": [
      "self",
      "height",
      "width",
      "dtype",
      "device"
    ],
    "maybe_pad": [
      "self",
      "hidden_states",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "always_partition"
    ]
  },
  "ClapAudioStage": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "depth",
      "num_heads",
      "drop_path",
      "downsample"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "always_partition"
    ]
  },
  "ClapAudioPatchMerging": {
    "__init__": [
      "self",
      "input_resolution",
      "dim",
      "norm_layer"
    ],
    "maybe_pad": [
      "self",
      "input_feature",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "input_feature",
      "input_dimensions"
    ]
  },
  "ClapAudioEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "reshape_mel2img": [
      "self",
      "normalized_input_features"
    ],
    "forward": [
      "self",
      "input_features",
      "is_longer",
      "output_attentions",
      "output_hidden_states",
      "output_hidden_states_before_downsampling",
      "always_partition",
      "return_dict"
    ]
  },
  "ClapProjectionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClapTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "ClapTextSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ClapTextSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ClapTextAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ClapTextIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClapTextOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ClapTextLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "ClapTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ClapTextPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClapPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ClapAudioModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "is_longer",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ClapTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ClapModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_audio_features": [
      "self",
      "input_features",
      "is_longer",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "is_longer",
      "attention_mask",
      "position_ids",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ClapTextModelWithProjection": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ClapAudioModelWithProjection": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "is_longer",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ClapFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "hop_length",
      "max_length_s",
      "fft_window_size",
      "padding_value",
      "return_attention_mask",
      "frequency_min",
      "frequency_max",
      "top_db",
      "truncation",
      "padding"
    ],
    "to_dict": [
      "self"
    ],
    "_np_extract_fbank_features": [
      "self",
      "waveform",
      "mel_filters"
    ],
    "_random_mel_fusion": [
      "self",
      "mel",
      "total_frames",
      "chunk_frames"
    ],
    "_get_input_mel": [
      "self",
      "waveform",
      "max_length",
      "truncation",
      "padding"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "truncation",
      "padding",
      "max_length",
      "sampling_rate",
      "return_tensors"
    ]
  },
  "CpmTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "do_lower_case",
      "remove_space",
      "keep_accents",
      "bos_token",
      "eos_token",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "additional_special_tokens",
      "sp_model_kwargs"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ],
    "preprocess_text": [
      "self",
      "inputs"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "_decode": [
      "self"
    ]
  },
  "CpmTokenizerFast": {
    "__init__": [
      "self",
      "vocab_file",
      "tokenizer_file",
      "do_lower_case",
      "remove_space",
      "keep_accents",
      "bos_token",
      "eos_token",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "additional_special_tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs"
    ],
    "_decode": [
      "self"
    ]
  },
  "sequential_experts_gemm": [
    "token_states",
    "expert_weights",
    "tokens_per_expert"
  ],
  "AriaTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "intermediate_size",
      "moe_num_experts",
      "moe_topk",
      "moe_num_shared_experts",
      "pad_token_id"
    ]
  },
  "AriaConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "vision_feature_layer",
      "text_config",
      "projector_patch_to_query_dict",
      "image_token_index",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "AriaTextRMSNorm": {},
  "AriaProjectorMLP": {
    "__init__": [
      "self",
      "in_features",
      "hidden_features",
      "output_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AriaCrossAttention": {
    "__init__": [
      "self",
      "config",
      "dropout_rate"
    ],
    "forward": [
      "self",
      "key_value_states",
      "hidden_states",
      "attn_mask"
    ]
  },
  "AriaProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "key_value_states",
      "attn_mask"
    ]
  },
  "AriaImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "image_mean",
      "image_std",
      "max_image_size",
      "min_image_size",
      "split_resolutions",
      "split_image",
      "do_convert_rgb",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "resample"
    ],
    "preprocess": [
      "self",
      "images",
      "image_mean",
      "image_std",
      "max_image_size",
      "min_image_size",
      "split_image",
      "do_convert_rgb",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "resample",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "_resize_for_patching": [
      "self",
      "image",
      "target_resolution",
      "resample",
      "input_data_format"
    ],
    "_get_padding_size": [
      "self",
      "original_resolution",
      "target_resolution"
    ],
    "_pad_for_patching": [
      "self",
      "image",
      "target_resolution",
      "input_data_format"
    ],
    "pad": [
      "self",
      "image",
      "padding",
      "mode",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "get_image_patches": [
      "self",
      "image",
      "grid_pinpoints",
      "patch_size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "AriaImagesKwargs": {},
  "AriaProcessorKwargs": {
    "_defaults": []
  },
  "AriaProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "size_conversion"
    ],
    "__call__": [
      "self",
      "text",
      "images"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "AriaSharedExpertsMLP": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "AriaGroupedExpertsGemm": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "groups"
    ],
    "forward": [
      "self",
      "input",
      "tokens_per_expert"
    ]
  },
  "AriaExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states",
      "router_logits"
    ]
  },
  "AriaTextMoELayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AriaTextAttention": {},
  "AriaTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "AriaTextPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "AriaPreTrainedModel": {
    "base_model_prefix": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "AriaTextModel": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "AriaTextForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "AriaCausalLMOutputWithPast": {},
  "AriaModelOutputWithPast": {},
  "AriaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_create_patch_attention_mask": [
      "self",
      "pixel_mask"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_mask",
      "vision_feature_layer",
      "output_hidden_states"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_mask",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "AriaForConditionalGeneration": {
    "_tied_weights_keys": [],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_mask",
      "vision_feature_layer"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_mask",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "logits_to_keep",
      "cache_position"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_mask",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "AriaTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "ByteRewriter": {
    "LEAF": [],
    "__init__": [
      "self",
      "rewriting_rules"
    ],
    "add_leaf": [
      "self",
      "hash_tree",
      "byte_in_sequence",
      "byte_out_sequence"
    ],
    "construct_hash_tree": [
      "self",
      "rewriting_rules"
    ],
    "search_hash_tree": [
      "self",
      "byte_sequence"
    ],
    "rewrite_bytes": [
      "self",
      "in_bytes",
      "reverse"
    ]
  },
  "MyT5Tokenizer": {
    "model_input_names": [],
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "eos_token",
      "unk_token",
      "pad_token",
      "extra_ids",
      "additional_special_tokens"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "_add_eos_if_not_present": [
      "self",
      "token_ids"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "morphological_encode": [
      "self",
      "indices"
    ],
    "morphological_decode": [
      "self",
      "indices"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "Lfm2MoeRMSNorm": {},
  "Lfm2MoeRotaryEmbedding": {},
  "Lfm2MoeMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ]
  },
  "Lfm2MoeExperts": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Lfm2MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Lfm2MoeHybridConvCache": {},
  "Lfm2MoeAttention": {},
  "Lfm2MoeShortConv": {},
  "Lfm2MoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Lfm2MoePreTrainedModel": {
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Lfm2MoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Lfm2MoeForCausalLM": {},
  "Lfm2MoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "moe_intermediate_size",
      "num_hidden_layers",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "max_position_embeddings",
      "initializer_range",
      "use_cache",
      "norm_eps",
      "num_attention_heads",
      "num_key_value_heads",
      "conv_bias",
      "conv_L_cache",
      "num_dense_layers",
      "num_experts_per_tok",
      "num_experts",
      "use_expert_bias",
      "routed_scaling_factor",
      "norm_topk_prob",
      "layer_types"
    ]
  },
  "UniSpeechForPreTrainingOutput": {},
  "UniSpeechSamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechNoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechGroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "UniSpeechFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "UniSpeechFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "UniSpeechEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechAttnAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechEncoderLayerStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "UniSpeechEncoderStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechGumbelVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_perplexity": [
      "probs"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UniSpeechPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask"
    ]
  },
  "UniSpeechBaseModelOutput": [],
  "UniSpeechModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "set_gumbel_temperature": [
      "self",
      "temperature"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "compute_contrastive_logits": [
      "target_features",
      "negative_features",
      "predicted_features",
      "temperature"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UniSpeechForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "tie_weights": [
      "self"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "UniSpeechForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "UniSpeechConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "feat_quantizer_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "do_stable_layer_norm",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "num_codevectors_per_group",
      "num_codevector_groups",
      "contrastive_logits_temperature",
      "num_negatives",
      "codevector_dim",
      "proj_codevector_dim",
      "diversity_loss_weight",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "num_ctc_classes",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "replace_prob"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "lsh_cumulation": [],
  "load_cuda_kernels": [],
  "to_contiguous": [
    "input_tensors"
  ],
  "hashing": [
    "query",
    "key",
    "num_hash",
    "hash_len"
  ],
  "YosoCumulation": {
    "forward": [
      "ctx",
      "query_mask",
      "key_mask",
      "query",
      "key",
      "value",
      "config"
    ],
    "backward": [
      "ctx",
      "grad"
    ]
  },
  "YosoLSHCumulation": {
    "forward": [
      "ctx",
      "query_mask",
      "key_mask",
      "query",
      "key",
      "value",
      "config"
    ],
    "backward": [
      "ctx",
      "grad"
    ]
  },
  "YosoEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "YosoSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "YosoSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "YosoAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "YosoIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YosoOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "YosoLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "YosoEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YosoPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YosoLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "YosoOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "YosoPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "YosoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YosoForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YosoClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "YosoForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YosoForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YosoForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YosoForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YosoConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "use_expectation",
      "hash_code_len",
      "num_hash",
      "conv_window",
      "use_fast_hash",
      "lsh_backward",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "T5Gemma2TextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "attention_bias",
      "attention_dropout",
      "query_pre_attn_scalar",
      "sliding_window",
      "layer_types",
      "final_logit_softcapping",
      "attn_logit_softcapping",
      "rope_parameters"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "T5Gemma2EncoderConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "mm_tokens_per_image",
      "boi_token_index",
      "eoi_token_index",
      "image_token_index",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "T5Gemma2DecoderConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "attention_bias",
      "attention_dropout",
      "query_pre_attn_scalar",
      "sliding_window",
      "layer_types",
      "final_logit_softcapping",
      "attn_logit_softcapping",
      "rope_parameters"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "T5Gemma2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "encoder",
      "decoder",
      "is_encoder_decoder",
      "dropout_rate",
      "attention_dropout",
      "classifier_dropout_rate",
      "initializer_range",
      "image_token_index",
      "tie_word_embeddings"
    ]
  },
  "T5Gemma2RMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "T5Gemma2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "T5Gemma2RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len",
      "layer_type"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "layer_type"
    ]
  },
  "T5Gemma2SelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "T5Gemma2MergedAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "merged_attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "cache_position"
    ]
  },
  "T5Gemma2EncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids"
    ]
  },
  "T5Gemma2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "merged_attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "encoder_hidden_states"
    ]
  },
  "T5Gemma2LMHead": {
    "__init__": [
      "self",
      "hidden_size",
      "vocab_size",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5Gemma2ClassificationHead": {
    "__init__": [
      "self",
      "hidden_size",
      "num_labels",
      "classifier_dropout_rate"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5Gemma2MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_outputs"
    ]
  },
  "T5Gemma2TextScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale",
      "eoi_token_index"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "T5Gemma2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "input_ids"
    ]
  },
  "sliding_window_mask_function": [
    "sliding_window",
    "is_causal"
  ],
  "T5Gemma2TextEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config",
      "eoi_token_index"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "token_type_ids"
    ]
  },
  "T5Gemma2Encoder": {
    "__init__": [
      "self",
      "config",
      "eoi_token_index"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_image_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "pixel_values",
      "token_type_ids"
    ]
  },
  "T5Gemma2Decoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config",
      "eoi_token_index"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "T5Gemma2Model": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "T5Gemma2ForConditionalGeneration": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "vision_tower": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "_prepare_cache_for_generation": [
      "self",
      "generation_config",
      "model_kwargs",
      "generation_mode",
      "batch_size",
      "max_cache_length"
    ]
  },
  "T5Gemma2ForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "T5Gemma2ForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "NON_SPEECH_TOKENS": [],
  "NON_SPEECH_TOKENS_MULTI": [],
  "WhisperConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "num_mel_bins",
      "encoder_layers",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_attention_heads",
      "decoder_ffn_dim",
      "encoder_ffn_dim",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "decoder_start_token_id",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "scale_embedding",
      "max_source_positions",
      "max_target_positions",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "suppress_tokens",
      "begin_suppress_tokens",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "median_filter_width",
      "tie_word_embeddings"
    ]
  },
  "LANGUAGES": [],
  "TO_LANGUAGE_CODE": [],
  "TASK_IDS": [],
  "WhisperTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "normalizer_file",
      "unk_token",
      "bos_token",
      "eos_token",
      "add_prefix_space",
      "language",
      "task",
      "predict_timestamps"
    ],
    "_decode_with_timestamps": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "time_precision",
      "segment_size"
    ],
    "_compute_offsets": [
      "self",
      "token_ids",
      "time_precision",
      "segment_size"
    ],
    "timestamp_ids": [
      "self",
      "time_precision"
    ],
    "_preprocess_token_ids": [
      "self",
      "token_ids",
      "skip_special_tokens"
    ],
    "_filter_timestamp_ids": [
      "self",
      "text"
    ],
    "decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "output_offsets",
      "time_precision",
      "decode_with_timestamps",
      "normalize",
      "basic_normalize",
      "remove_diacritics"
    ],
    "_decode": [
      "self"
    ],
    "normalize": [
      "self",
      "text"
    ],
    "basic_normalize": [
      "text",
      "remove_diacritics"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "set_prefix_tokens": [
      "self",
      "language",
      "task",
      "predict_timestamps"
    ],
    "prefix_tokens": [
      "self"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "get_decoder_prompt_ids": [
      "self",
      "task",
      "language",
      "no_timestamps"
    ],
    "_decode_asr": [
      "self",
      "model_outputs"
    ],
    "get_prompt_ids": [
      "self",
      "text",
      "return_tensors"
    ],
    "_strip_prompt": [
      "self",
      "token_ids",
      "prompt_token_id",
      "decoder_start_token_id"
    ],
    "_convert_to_list": [
      "token_ids"
    ]
  },
  "_combine_tokens_into_words": [
    "tokenizer",
    "tokens",
    "language",
    "prepend_punctuations",
    "append_punctuations"
  ],
  "_decode_asr": [
    "tokenizer",
    "model_outputs"
  ],
  "_collate_word_timestamps": [
    "tokenizer",
    "tokens",
    "token_timestamps",
    "language",
    "return_language"
  ],
  "_split_tokens_on_unicode": [
    "tokenizer",
    "tokens"
  ],
  "_split_tokens_on_spaces": [
    "tokenizer",
    "tokens"
  ],
  "_merge_punctuations": [
    "words",
    "tokens",
    "indices",
    "prepended",
    "appended"
  ],
  "WhisperProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "get_decoder_prompt_ids": [
      "self",
      "task",
      "language",
      "no_timestamps"
    ],
    "__call__": [
      "self"
    ],
    "get_prompt_ids": [
      "self",
      "text",
      "return_tensors"
    ]
  },
  "WhisperFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "hop_length",
      "chunk_length",
      "n_fft",
      "padding_value",
      "dither",
      "return_attention_mask"
    ],
    "_np_extract_fbank_features": [
      "self",
      "waveform_batch",
      "device"
    ],
    "_torch_extract_fbank_features": [
      "self",
      "waveform",
      "device"
    ],
    "zero_mean_unit_var_norm": [
      "input_values",
      "attention_mask",
      "padding_value"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "truncation",
      "pad_to_multiple_of",
      "return_tensors",
      "return_attention_mask",
      "padding",
      "max_length",
      "sampling_rate",
      "do_normalize",
      "device"
    ]
  },
  "_median_filter": [
    "inputs",
    "filter_width"
  ],
  "_dynamic_time_warping": [
    "matrix"
  ],
  "_get_attr_from_logit_processors": [
    "logits_processor",
    "logit_processor_class",
    "attribute_name"
  ],
  "_pad_to_max_length": [
    "current_segments",
    "pad_token_id",
    "device",
    "padding_side",
    "padding",
    "bos_token_tensor",
    "cut_off_length",
    "return_token_timestamps",
    "force_unique_generate_call",
    "skip_ending_double_timestamps",
    "timestamp_begin"
  ],
  "WhisperGenerationMixin": {
    "_extract_token_timestamps": [
      "self",
      "generate_outputs",
      "alignment_heads",
      "time_precision",
      "num_frames",
      "num_input_ids"
    ],
    "generate": [
      "self",
      "input_features",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus",
      "return_timestamps",
      "task",
      "language",
      "is_multilingual",
      "prompt_ids",
      "prompt_condition_type",
      "condition_on_prev_tokens",
      "temperature",
      "compression_ratio_threshold",
      "logprob_threshold",
      "no_speech_threshold",
      "num_segment_frames",
      "attention_mask",
      "time_precision",
      "time_precision_features",
      "return_token_timestamps",
      "return_segments",
      "return_dict_in_generate",
      "force_unique_generate_call",
      "monitor_progress"
    ],
    "generate_with_fallback": [
      "self",
      "segment_input",
      "decoder_input_ids",
      "cur_bsz",
      "seek",
      "batch_idx_map",
      "temperatures",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus",
      "return_token_timestamps",
      "do_condition_on_prev_tokens",
      "is_shortform",
      "batch_size",
      "attention_mask",
      "kwargs"
    ],
    "_prepare_segments": [
      "prompt_ids",
      "batch_size",
      "generation_config"
    ],
    "_postprocess_outputs": [
      "self",
      "seek_outputs",
      "decoder_input_ids",
      "return_token_timestamps",
      "generation_config",
      "is_shortform",
      "seek",
      "batch_idx_map"
    ],
    "_stack_split_outputs": [
      "self",
      "seek_outputs",
      "model_output_type",
      "device",
      "kwargs"
    ],
    "_need_fallback": [
      "self",
      "seek_sequence",
      "seek_outputs",
      "index",
      "logits_processor",
      "generation_config",
      "vocab_size",
      "temperature"
    ],
    "_expand_variables_for_generation": [
      "self",
      "input_features",
      "seek",
      "max_frames",
      "init_tokens",
      "batch_size",
      "condition_on_prev_tokens",
      "generation_config"
    ],
    "_setup_no_speech_detection": [
      "logits_processor",
      "segment_input",
      "decoder_input_ids",
      "kwargs"
    ],
    "_retrieve_total_input_frames": [
      "input_features",
      "input_stride",
      "kwargs"
    ],
    "_maybe_warn_unused_inputs": [
      "condition_on_prev_tokens",
      "temperature",
      "compression_ratio_threshold",
      "logprob_threshold",
      "no_speech_threshold",
      "total_input_frames"
    ],
    "_set_return_outputs": [
      "return_dict_in_generate",
      "return_token_timestamps",
      "logprob_threshold",
      "generation_config"
    ],
    "_set_return_timestamps": [
      "self",
      "return_timestamps",
      "is_shortform",
      "generation_config"
    ],
    "_set_language_and_task": [
      "language",
      "task",
      "is_multilingual",
      "generation_config"
    ],
    "_retrieve_init_tokens": [
      "self",
      "input_features",
      "batch_size",
      "generation_config",
      "config",
      "num_segment_frames",
      "kwargs"
    ],
    "detect_language": [
      "self",
      "input_features",
      "encoder_outputs",
      "generation_config",
      "num_segment_frames"
    ],
    "_check_decoder_input_ids": [
      "kwargs"
    ],
    "_set_num_frames": [
      "return_token_timestamps",
      "generation_config",
      "attention_mask",
      "kwargs"
    ],
    "_set_thresholds_and_condition": [
      "generation_config",
      "logprob_threshold",
      "compression_ratio_threshold",
      "no_speech_threshold",
      "condition_on_prev_tokens"
    ],
    "_set_prompt_condition_type": [
      "generation_config",
      "prompt_condition_type"
    ],
    "_set_condition_on_prev_tokens": [
      "condition_on_prev_tokens",
      "generation_config"
    ],
    "_retrieve_max_frames_and_seek": [
      "batch_size",
      "attention_mask",
      "total_input_frames",
      "is_shortform"
    ],
    "_retrieve_logit_processors": [
      "self",
      "generation_config",
      "logits_processor",
      "begin_index",
      "num_beams",
      "device"
    ],
    "_maybe_reduce_batch": [
      "input_features",
      "seek",
      "max_frames",
      "cur_bsz",
      "batch_idx_map"
    ],
    "_get_input_segment": [
      "input_features",
      "seek",
      "seek_num_frames",
      "num_segment_frames",
      "cur_bsz",
      "batch_idx_map"
    ],
    "_prepare_decoder_input_ids": [
      "cur_bsz",
      "init_tokens",
      "current_segments",
      "batch_idx_map",
      "do_condition_on_prev_tokens",
      "prompt_ids",
      "generation_config",
      "config",
      "device",
      "suppress_tokens",
      "timestamp_begin",
      "kwargs"
    ],
    "_set_max_new_tokens_and_length": [
      "self",
      "config",
      "decoder_input_ids",
      "generation_config"
    ],
    "_retrieve_compression_ratio": [
      "tokens",
      "vocab_size"
    ],
    "_retrieve_avg_logprobs": [
      "scores",
      "tokens",
      "temperature"
    ],
    "_retrieve_segment": [
      "seek_sequence",
      "seek_outputs",
      "time_offset",
      "timestamp_begin",
      "seek_num_frames",
      "time_precision",
      "time_precision_features",
      "input_stride",
      "prev_idx",
      "idx",
      "return_token_timestamps",
      "decoder_input_ids"
    ]
  },
  "sinusoids": [
    "length",
    "channels",
    "max_timescale"
  ],
  "WhisperPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "WhisperAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "layer_idx",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "WhisperEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "WhisperDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "WhisperPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ]
  },
  "WhisperEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "WhisperDecoder": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "inputs_embeds",
      "position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "WhisperModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "freeze_encoder": [
      "self"
    ],
    "_mask_input_features": [
      "self",
      "input_features",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "decoder_position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "WhisperForConditionalGeneration": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "freeze_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "decoder_position_ids",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "WhisperDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self"
    ]
  },
  "WhisperForCausalLM": {
    "_tied_weights_keys": [],
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "WhisperForAudioClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_encoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "encoder_outputs",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ADDITIONAL_DIACRITICS": [],
  "remove_symbols_and_diacritics": [
    "s",
    "keep"
  ],
  "remove_symbols": [
    "s"
  ],
  "BasicTextNormalizer": {
    "__init__": [
      "self",
      "remove_diacritics",
      "split_letters"
    ],
    "__call__": [
      "self",
      "s"
    ]
  },
  "EnglishSpellingNormalizer": {
    "__init__": [
      "self",
      "english_spelling_mapping"
    ],
    "__call__": [
      "self",
      "s"
    ]
  },
  "EnglishTextNormalizer": {
    "__init__": [
      "self",
      "english_spelling_mapping"
    ],
    "__call__": [
      "self",
      "s"
    ]
  },
  "LayoutXLMConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "max_2d_position_embeddings",
      "max_rel_pos",
      "rel_pos_bins",
      "fast_qkv",
      "max_rel_2d_pos",
      "rel_2d_pos_bins",
      "convert_sync_batchnorm",
      "image_feature_pool_shape",
      "coordinate_size",
      "shape_size",
      "has_relative_attention_bias",
      "has_spatial_attention_bias",
      "has_visual_segment_embedding",
      "detectron2_config_args"
    ],
    "get_default_detectron2_config": [
      "cls"
    ],
    "get_detectron2_config": [
      "self"
    ]
  },
  "LAYOUTXLM_ENCODE_KWARGS_DOCSTRING": [],
  "LayoutXLMTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "cls_token_box",
      "sep_token_box",
      "pad_token_box",
      "pad_token_label",
      "only_label_first_subword",
      "add_prefix_space"
    ],
    "_get_token_id": [
      "self",
      "token"
    ],
    "encode_plus": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "LayoutXLMProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "return_tensors"
    ],
    "get_overflowing_images": [
      "self",
      "images",
      "overflow_to_sample_mapping"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "PLBartScaledWordEmbedding": {},
  "PLBartPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PLBartEncoder": {},
  "PLBartDecoder": {},
  "PLBartModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "PLBartForConditionalGeneration": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "PLBartClassificationHead": {},
  "PLBartForSequenceClassification": {
    "forward": []
  },
  "PLBartForCausalLM": {
    "forward": []
  },
  "FAIRSEQ_LANGUAGE_CODES_MAP": [],
  "PLBartTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "language_codes",
      "src_lang",
      "tgt_lang",
      "sp_model_kwargs",
      "additional_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "src_lang": [
      "self",
      "new_src_lang"
    ],
    "_build_translation_inputs": [
      "self",
      "raw_inputs",
      "return_tensors",
      "src_lang",
      "tgt_lang"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "prepare_seq2seq_batch": [
      "self",
      "src_texts",
      "src_lang",
      "tgt_texts",
      "tgt_lang"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ],
    "set_src_lang_special_tokens": [
      "self",
      "src_lang"
    ],
    "set_tgt_lang_special_tokens": [
      "self",
      "lang"
    ],
    "_convert_lang_code_special_format": [
      "self",
      "lang"
    ],
    "clean_up_tokenization": [
      "self",
      "out_string"
    ],
    "decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ]
  },
  "PLBartConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "classifier_dropout",
      "scale_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "forced_eos_token_id",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "PLBartLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "PLBartAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "PLBartEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "PLBartDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "PLBartDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "Ernie4_5RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Ernie4_5MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Ernie4_5Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Ernie4_5RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Ernie4_5DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Ernie4_5PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Ernie4_5Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "Ernie4_5ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Ernie4_5Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "use_bias",
      "head_dim"
    ]
  },
  "Swin2SRImageProcessorKwargs": {},
  "Swin2SRImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "size_divisor"
    ],
    "pad": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "size_divisor",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "Swin2SRConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "num_channels_out",
      "embed_dim",
      "depths",
      "num_heads",
      "window_size",
      "mlp_ratio",
      "qkv_bias",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "drop_path_rate",
      "hidden_act",
      "use_absolute_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "upscale",
      "img_range",
      "resi_connection",
      "upsampler"
    ]
  },
  "Swin2SREncoderOutput": {},
  "Swin2SRDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Swin2SREmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Swin2SRPatchEmbeddings": {
    "__init__": [
      "self",
      "config",
      "normalize_patches"
    ],
    "forward": [
      "self",
      "embeddings"
    ]
  },
  "Swin2SRPatchUnEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "embeddings",
      "x_size"
    ]
  },
  "Swin2SRPatchMerging": {
    "__init__": [
      "self",
      "input_resolution",
      "dim",
      "norm_layer"
    ],
    "maybe_pad": [
      "self",
      "input_feature",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "input_feature",
      "input_dimensions"
    ]
  },
  "Swin2SRSelfAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size",
      "pretrained_window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "create_coords_table_and_index": [
      "self"
    ]
  },
  "Swin2SRSelfOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Swin2SRAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size",
      "pretrained_window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Swin2SRIntermediate": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Swin2SROutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Swin2SRLayer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "num_heads",
      "drop_path_rate",
      "shift_size",
      "pretrained_window_size"
    ],
    "_compute_window_shift": [
      "self",
      "target_window_size",
      "target_shift_size"
    ],
    "get_attn_mask": [
      "self",
      "height",
      "width",
      "dtype"
    ],
    "maybe_pad": [
      "self",
      "hidden_states",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions"
    ]
  },
  "Swin2SRStage": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "depth",
      "num_heads",
      "drop_path",
      "pretrained_window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions"
    ]
  },
  "Swin2SREncoder": {
    "__init__": [
      "self",
      "config",
      "grid_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Swin2SRPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Swin2SRModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "pad_and_normalize": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Upsample": {
    "__init__": [
      "self",
      "scale",
      "num_features"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "UpsampleOneStep": {
    "__init__": [
      "self",
      "scale",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PixelShuffleUpsampler": {
    "__init__": [
      "self",
      "config",
      "num_features"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "NearestConvUpsampler": {
    "__init__": [
      "self",
      "config",
      "num_features"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "PixelShuffleAuxUpsampler": {
    "__init__": [
      "self",
      "config",
      "num_features"
    ],
    "forward": [
      "self",
      "sequence_output",
      "bicubic",
      "height",
      "width"
    ]
  },
  "Swin2SRForImageSuperResolution": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Swin2SRImageProcessorFast": {
    "do_rescale": [],
    "rescale_factor": [],
    "do_pad": [],
    "size_divisor": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "pad": [
      "self",
      "images",
      "size_divisor"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "size_divisor",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "VipLlavaConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "projector_hidden_act",
      "projector_layernorm_eps",
      "vision_feature_layers",
      "image_seq_length",
      "tie_word_embeddings"
    ]
  },
  "VipLlavaModelOutputWithPast": {},
  "VipLlavaCausalLMOutputWithPast": {},
  "VipLlavaMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VipLlavaPreTrainedModel": {},
  "VipLlavaModel": {
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_layers",
      "output_hidden_states"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layers",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "VipLlavaForConditionalGeneration": {
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_layers"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layers",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "DeepseekVLImageProcessorKwargs": {},
  "DeepseekVLImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "min_size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "do_pad"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "do_convert_rgb",
      "background_color",
      "do_pad",
      "data_format",
      "input_data_format"
    ],
    "pad_to_square": [
      "self",
      "image",
      "background_color",
      "data_format",
      "input_data_format"
    ]
  },
  "DeepseekVLConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "tie_word_embeddings"
    ]
  },
  "DeepseekVLBaseModelOutputWithPast": {},
  "DeepseekVLCausalLMOutputWithPast": {},
  "DeepseekVLAligner": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_encodings"
    ]
  },
  "DeepseekVLPreTrainedModel": {
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DeepseekVLModel": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DeepseekVLForConditionalGeneration": {
    "output_modalities": [],
    "prepare_embeddings_for_image_generation": [
      "self"
    ],
    "decode_image_tokens": [
      "self"
    ],
    "generate": [
      "self"
    ]
  },
  "DeepseekVLImageProcessorFast": {
    "__init__": [
      "self"
    ],
    "postprocess": [
      "self"
    ]
  },
  "DeepseekVLProcessorKwargs": {
    "_defaults": []
  },
  "DeepseekVLProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "num_image_tokens"
    ],
    "__call__": [
      "self",
      "text",
      "images"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "LlavaNextVideoVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": []
  },
  "LlavaNextVideoConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "projector_hidden_act",
      "multimodal_projector_bias",
      "vision_feature_select_strategy",
      "vision_feature_layer",
      "image_grid_pinpoints",
      "video_token_index",
      "spatial_pool_mode",
      "spatial_pool_stride",
      "image_seq_length",
      "video_seq_length",
      "tie_word_embeddings"
    ]
  },
  "LlavaNextVideoModelOutputWithPast": {},
  "LlavaNextVideoCausalLMOutputWithPast": {},
  "LlavaNextVideoPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "LlavaNextVideoMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "LlavaNextVideoPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LlavaNextVideoModel": {
    "_checkpoint_conversion_mapping": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "pack_image_features": [
      "self",
      "image_features",
      "image_sizes",
      "vision_feature_select_strategy",
      "image_newline"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "output_hidden_states"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_values_videos",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "get_video_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "output_hidden_states"
    ]
  },
  "LlavaNextVideoForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "pack_image_features": [
      "self",
      "image_features",
      "image_sizes",
      "vision_feature_select_strategy",
      "image_newline"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer",
      "vision_feature_select_strategy"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_values_videos",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_values_videos",
      "image_sizes",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ],
    "get_video_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy"
    ]
  },
  "LlavaNextVideoProcessorKwargs": {
    "_defaults": []
  },
  "LlavaNextVideoProcessor": {
    "__init__": [
      "self",
      "video_processor",
      "image_processor",
      "tokenizer",
      "chat_template",
      "patch_size",
      "vision_feature_select_strategy",
      "video_token",
      "image_token",
      "num_additional_image_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_get_number_of_features": [
      "self",
      "orig_height",
      "orig_width",
      "height",
      "width"
    ],
    "_get_unpadded_features": [
      "self",
      "height",
      "width",
      "patches_height",
      "patches_width",
      "scale_height",
      "scale_width"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "LlavaOnevisionImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "crop_size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_pad": [],
    "image_grid_pinpoints": [],
    "model_input_names": [],
    "pad_to_square": [
      "self",
      "images",
      "background_color"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "batch_num_images",
      "do_resize",
      "size",
      "image_grid_pinpoints",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "LlavaOnevisionModelOutputWithPast": {},
  "LlavaOnevisionCausalLMOutputWithPast": {},
  "LlavaOnevisionPreTrainedModel": {},
  "LlavaOnevisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "pack_image_features": [
      "self",
      "image_features",
      "image_sizes",
      "image_newline",
      "vision_aspect_ratio"
    ],
    "apply_pooling": [
      "self",
      "image_features"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "vision_aspect_ratio",
      "batch_num_images",
      "output_hidden_states"
    ],
    "get_video_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "output_hidden_states"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "image_sizes",
      "pixel_values_videos",
      "image_sizes_videos",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "vision_aspect_ratio",
      "batch_num_images",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "LlavaOnevisionForConditionalGeneration": {
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "image_sizes",
      "pixel_values_videos",
      "image_sizes_videos",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "vision_aspect_ratio",
      "batch_num_images",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "image_sizes",
      "pixel_values_videos",
      "image_sizes_videos",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "vision_aspect_ratio",
      "batch_num_images"
    ]
  },
  "LlavaOnevisionVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "rescale_factor": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": []
  },
  "LlavaOnevisionConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "video_token_index",
      "projector_hidden_act",
      "vision_feature_select_strategy",
      "vision_feature_layer",
      "vision_aspect_ratio",
      "image_grid_pinpoints",
      "multimodal_projector_bias",
      "tie_word_embeddings"
    ]
  },
  "LlavaOnevisionImageProcessorKwargs": {},
  "LlavaOnevisionImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "image_grid_pinpoints",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_convert_rgb"
    ],
    "pad": [
      "self",
      "image",
      "padding",
      "mode",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "_resize_for_patching": [
      "self",
      "image",
      "target_resolution",
      "resample",
      "input_data_format"
    ],
    "_get_padding_size": [
      "self",
      "original_resolution",
      "target_resolution"
    ],
    "_pad_for_patching": [
      "self",
      "image",
      "target_resolution",
      "input_data_format"
    ],
    "get_image_patches": [
      "self",
      "image",
      "grid_pinpoints",
      "size",
      "patch_size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_pad_for_batching": [
      "self",
      "pixel_values",
      "data_format",
      "input_data_format"
    ],
    "pad_to_square": [
      "self",
      "image",
      "background_color",
      "data_format",
      "input_data_format"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "image_grid_pinpoints",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "LlavaOnevisionMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "LlavaOnevisionProcessorKwargs": {
    "_defaults": []
  },
  "LlavaOnevisionProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "num_image_tokens",
      "vision_feature_select_strategy",
      "chat_template",
      "image_token",
      "video_token",
      "vision_aspect_ratio"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_expand_image_tokens": [
      "self",
      "text",
      "image_sizes",
      "height",
      "width",
      "special_token",
      "batch_num_images"
    ],
    "_get_number_of_features": [
      "self",
      "orig_height",
      "orig_width",
      "height",
      "width"
    ],
    "_get_unpadded_features": [
      "self",
      "height",
      "width",
      "patches_height",
      "patches_width",
      "scale_height",
      "scale_width"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes",
      "video_sizes"
    ]
  },
  "DinatEncoderOutput": {},
  "DinatModelOutput": {},
  "DinatImageClassifierOutput": {},
  "DinatEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DinatPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DinatDownsampler": {
    "__init__": [
      "self",
      "dim",
      "norm_layer"
    ],
    "forward": [
      "self",
      "input_feature"
    ]
  },
  "DinatDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "NeighborhoodAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "NeighborhoodAttentionOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "NeighborhoodAttentionModule": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "DinatIntermediate": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DinatOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DinatLayer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "dilation",
      "drop_path_rate"
    ],
    "maybe_pad": [
      "self",
      "hidden_states",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "DinatStage": {
    "__init__": [
      "self",
      "config",
      "dim",
      "depth",
      "num_heads",
      "dilations",
      "drop_path_rate",
      "downsample"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "DinatEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "output_hidden_states",
      "output_hidden_states_before_downsampling",
      "return_dict"
    ]
  },
  "DinatPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": []
  },
  "DinatModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DinatForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DinatBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "DinatConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "patch_size",
      "num_channels",
      "embed_dim",
      "depths",
      "num_heads",
      "kernel_size",
      "dilations",
      "mlp_ratio",
      "qkv_bias",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "drop_path_rate",
      "hidden_act",
      "initializer_range",
      "layer_norm_eps",
      "layer_scale_init_value",
      "out_features",
      "out_indices"
    ]
  },
  "get_embed_positions": [
    "embed_positions",
    "position_ids"
  ],
  "GPTJAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_split_heads": [
      "self",
      "tensor",
      "num_attention_heads",
      "attn_head_size",
      "rotary"
    ],
    "_merge_heads": [
      "self",
      "tensor",
      "num_attention_heads",
      "attn_head_size"
    ],
    "_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "_get_embed_positions": [
      "self",
      "position_ids"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "position_ids",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTJFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "position_ids",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTJ_ATTENTION_CLASSES": [],
  "GPTJMLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GPTJBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "position_ids",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "GPTJPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GPTJModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "GPTJForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GPTJForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTJForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GPTJConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "n_positions",
      "n_embd",
      "n_layer",
      "n_head",
      "rotary_dim",
      "n_inner",
      "activation_function",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "tie_word_embeddings"
    ]
  },
  "IBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "quant_mode",
      "force_dequant"
    ]
  },
  "QuantEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "max_norm",
      "norm_type",
      "scale_grad_by_freq",
      "sparse",
      "_weight",
      "weight_bit",
      "momentum",
      "quant_mode"
    ],
    "forward": [
      "self",
      "x",
      "positions",
      "incremental_state"
    ]
  },
  "QuantAct": {
    "__init__": [
      "self",
      "activation_bit",
      "act_range_momentum",
      "per_channel",
      "channel_len",
      "quant_mode"
    ],
    "__repr__": [
      "self"
    ],
    "forward": [
      "self",
      "x",
      "pre_act_scaling_factor",
      "identity",
      "identity_scaling_factor",
      "specified_min",
      "specified_max"
    ]
  },
  "QuantLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "weight_bit",
      "bias_bit",
      "per_channel",
      "quant_mode"
    ],
    "__repr__": [
      "self"
    ],
    "forward": [
      "self",
      "x",
      "prev_act_scaling_factor"
    ]
  },
  "IntGELU": {
    "__init__": [
      "self",
      "quant_mode",
      "force_dequant"
    ],
    "int_erf": [
      "self",
      "x_int",
      "scaling_factor"
    ],
    "forward": [
      "self",
      "x",
      "scaling_factor"
    ]
  },
  "IntSoftmax": {
    "__init__": [
      "self",
      "output_bit",
      "quant_mode",
      "force_dequant"
    ],
    "int_polynomial": [
      "self",
      "x_int",
      "scaling_factor"
    ],
    "int_exp": [
      "self",
      "x_int",
      "scaling_factor"
    ],
    "forward": [
      "self",
      "x",
      "scaling_factor"
    ]
  },
  "IntLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape",
      "eps",
      "output_bit",
      "quant_mode",
      "force_dequant"
    ],
    "set_shift": [
      "self",
      "y_int"
    ],
    "overflow_fallback": [
      "self",
      "y_int"
    ],
    "forward": [
      "self",
      "x",
      "scaling_factor"
    ]
  },
  "get_percentile_min_max": [
    "input",
    "lower_percentile",
    "upper_percentile",
    "output_tensor"
  ],
  "linear_quantize": [
    "input",
    "scale",
    "zero_point",
    "inplace"
  ],
  "symmetric_linear_quantization_params": [
    "num_bits",
    "saturation_min",
    "saturation_max",
    "per_channel"
  ],
  "SymmetricQuantFunction": {
    "forward": [
      "ctx",
      "x",
      "k",
      "percentile_mode",
      "scale"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "floor_ste": {
    "forward": [
      "ctx",
      "x"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "round_ste": {
    "forward": [
      "ctx",
      "x"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "batch_frexp": [
    "inputs",
    "max_bit"
  ],
  "FixedPointMul": {
    "forward": [
      "ctx",
      "pre_act",
      "pre_act_scaling_factor",
      "bit_num",
      "z_scaling_factor",
      "identity",
      "identity_scaling_factor"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "IBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ]
  },
  "IBertSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "hidden_states_scaling_factor",
      "attention_mask",
      "output_attentions"
    ]
  },
  "IBertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "hidden_states_scaling_factor",
      "input_tensor",
      "input_tensor_scaling_factor"
    ]
  },
  "IBertAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "hidden_states_scaling_factor",
      "attention_mask",
      "output_attentions"
    ]
  },
  "IBertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "hidden_states_scaling_factor"
    ]
  },
  "IBertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "hidden_states_scaling_factor",
      "input_tensor",
      "input_tensor_scaling_factor"
    ]
  },
  "IBertLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "hidden_states_scaling_factor",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output",
      "attention_output_scaling_factor"
    ]
  },
  "IBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "hidden_states_scaling_factor",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "IBertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IBertPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens"
    ]
  },
  "IBertModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "IBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "IBertLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "IBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "IBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "IBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "IBertClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "IBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OwlViTImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "center_crop": [
      "self",
      "image",
      "crop_size",
      "data_format",
      "input_data_format"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_image_guided_detection": [
      "self",
      "outputs",
      "threshold",
      "nms_threshold",
      "target_sizes"
    ]
  },
  "owlvit_loss": [
    "similarity"
  ],
  "OwlViTOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "OwlViTObjectDetectionOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "OwlViTImageGuidedObjectDetectionOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "OwlViTVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "OwlViTTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "OwlViTAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "OwlViTMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "OwlViTEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "OwlViTPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "OwlViTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OwlViTTextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OwlViTTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OwlViTVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "OwlViTVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "OwlViTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "return_loss",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_base_image_embeds",
      "return_dict"
    ]
  },
  "OwlViTBoxPredictionHead": {
    "__init__": [
      "self",
      "config",
      "out_dim"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "OwlViTClassPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeds",
      "query_embeds",
      "query_mask"
    ]
  },
  "OwlViTForObjectDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "normalize_grid_corner_coordinates": [
      "num_patches_height",
      "num_patches_width"
    ],
    "compute_box_bias": [
      "self",
      "num_patches_height",
      "num_patches_width"
    ],
    "box_predictor": [
      "self",
      "image_feats",
      "feature_map",
      "interpolate_pos_encoding"
    ],
    "class_predictor": [
      "self",
      "image_feats",
      "query_embeds",
      "query_mask"
    ],
    "image_text_embedder": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding"
    ],
    "image_embedder": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding"
    ],
    "embed_image_query": [
      "self",
      "query_image_features",
      "query_feature_map",
      "interpolate_pos_encoding"
    ],
    "image_guided_detection": [
      "self",
      "pixel_values",
      "query_pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "OwlViTImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "model_input_names": [],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_image_guided_detection": [
      "self",
      "outputs",
      "threshold",
      "nms_threshold",
      "target_sizes"
    ]
  },
  "OwlViTTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "OwlViTVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "OwlViTConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value",
      "return_dict"
    ]
  },
  "OwlViTImagesKwargs": {},
  "OwlViTProcessorKwargs": {
    "_defaults": []
  },
  "OwlViTProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "post_process": [
      "self"
    ],
    "post_process_grounded_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "text_labels"
    ],
    "post_process_image_guided_detection": [
      "self",
      "outputs",
      "threshold",
      "nms_threshold",
      "target_sizes"
    ]
  },
  "RTDetrImageProcessorKwargs": {},
  "RTDetrImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "format",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_annotations",
      "do_pad",
      "pad_size"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "size",
      "resample"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "annotation",
      "constant_values",
      "data_format",
      "input_data_format",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "images",
      "annotations",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "update_bboxes",
      "pad_size"
    ],
    "preprocess": [
      "self",
      "images",
      "annotations",
      "return_segmentation_masks",
      "masks_path",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "format",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "use_focal_loss"
    ]
  },
  "RTDetrImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "format": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "size": [],
    "default_to_square": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "do_convert_annotations": [],
    "__init__": [
      "self"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "target_size",
      "threshold",
      "interpolation"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "annotation",
      "update_bboxes",
      "fill"
    ],
    "_preprocess": [
      "self",
      "images",
      "annotations",
      "masks_path",
      "return_segmentation_masks",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "format",
      "return_tensors"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes",
      "use_focal_loss"
    ]
  },
  "RTDetrDecoderOutput": {},
  "RTDetrModelOutput": {},
  "RTDetrObjectDetectionOutput": {},
  "RTDetrMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size",
      "activation_function"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RTDetrFrozenBatchNorm2d": {},
  "RTDetrSelfAttention": {},
  "RTDetrConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "RTDetrConvNormLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RTDetrEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings"
    ]
  },
  "RTDetrRepVggBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "RTDetrCSPRepLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RTDetrMultiscaleDeformableAttention": {},
  "RTDetrDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "object_queries_position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "RTDetrSinePositionEmbedding": {
    "__init__": [
      "self",
      "embed_dim",
      "temperature"
    ],
    "forward": [
      "self",
      "width",
      "height",
      "device",
      "dtype"
    ]
  },
  "RTDetrAIFILayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RTDetrMLPPredictionHead": {},
  "RTDetrPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RTDetrHybridEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds"
    ]
  },
  "RTDetrDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index"
    ]
  },
  "RTDetrModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "generate_anchors": [
      "self",
      "spatial_shapes",
      "grid_size",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "encoder_outputs",
      "inputs_embeds",
      "labels"
    ]
  },
  "RTDetrForObjectDetection": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "_set_aux_loss": [
      "self",
      "outputs_class",
      "outputs_coord"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "encoder_outputs",
      "inputs_embeds",
      "labels"
    ]
  },
  "RTDetrResNetConvLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "activation"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "RTDetrResNetEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "RTDetrResNetShortCut": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "RTDetrResNetBasicLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "should_apply_shortcut"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RTDetrResNetBottleNeckLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RTDetrResNetStage": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "depth"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "RTDetrResNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RTDetrResNetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RTDetrResNetBackbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RTDetrConfig": {
    "model_type": [],
    "sub_configs": [],
    "layer_types": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "initializer_range",
      "initializer_bias_prior_prob",
      "layer_norm_eps",
      "batch_norm_eps",
      "backbone_config",
      "freeze_backbone_batch_norms",
      "encoder_hidden_dim",
      "encoder_in_channels",
      "feat_strides",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "dropout",
      "activation_dropout",
      "encode_proj_layers",
      "positional_encoding_temperature",
      "encoder_activation_function",
      "activation_function",
      "eval_size",
      "normalize_before",
      "hidden_expansion",
      "d_model",
      "num_queries",
      "decoder_in_channels",
      "decoder_ffn_dim",
      "num_feature_levels",
      "decoder_n_points",
      "decoder_layers",
      "decoder_attention_heads",
      "decoder_activation_function",
      "attention_dropout",
      "num_denoising",
      "label_noise_ratio",
      "box_noise_scale",
      "learn_initial_query",
      "anchor_image_size",
      "disable_custom_kernels",
      "with_box_refine",
      "is_encoder_decoder",
      "matcher_alpha",
      "matcher_gamma",
      "matcher_class_cost",
      "matcher_bbox_cost",
      "matcher_giou_cost",
      "use_focal_loss",
      "auxiliary_loss",
      "focal_loss_alpha",
      "focal_loss_gamma",
      "weight_loss_vfl",
      "weight_loss_bbox",
      "weight_loss_giou",
      "eos_coefficient"
    ]
  },
  "RTDetrResNetConfig": {
    "model_type": [],
    "layer_types": [],
    "__init__": [
      "self",
      "num_channels",
      "embedding_size",
      "hidden_sizes",
      "depths",
      "layer_type",
      "hidden_act",
      "downsample_in_first_stage",
      "downsample_in_bottleneck",
      "out_features",
      "out_indices"
    ]
  },
  "RemBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "RemBertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RemBertSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "RemBertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RemBertAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "RemBertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RemBertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RemBertLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "RemBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "RemBertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RemBertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RemBertOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "RemBertPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RemBertModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "RemBertForMaskedLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RemBertForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "RemBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RemBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RemBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RemBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RemBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "input_embedding_size",
      "output_embedding_size",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "classifier_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "RemBertTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "keep_accents",
      "bos_token",
      "eos_token",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "add_prefix_space",
      "remove_space"
    ]
  },
  "TimeSeriesFeatureEmbedder": {
    "__init__": [
      "self",
      "cardinalities",
      "embedding_dims"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "TimeSeriesStdScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "TimeSeriesMeanScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "TimeSeriesNOPScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "TimeSeriesSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "create_weight": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "TimeSeriesValueEmbedding": {
    "__init__": [
      "self",
      "feature_size",
      "d_model"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "TimeSeriesTransformerAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "TimeSeriesTransformerEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "TimeSeriesTransformerDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "TimeSeriesTransformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "TimeSeriesTransformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TimeSeriesTransformerDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "TimeSeriesTransformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_past_length": [
      "self"
    ],
    "get_lagged_subsequences": [
      "self",
      "sequence",
      "subsequences_length",
      "shift"
    ],
    "create_network_inputs": [
      "self",
      "past_values",
      "past_time_features",
      "static_categorical_features",
      "static_real_features",
      "past_observed_mask",
      "future_values",
      "future_time_features"
    ],
    "forward": [
      "self",
      "past_values",
      "past_time_features",
      "past_observed_mask",
      "static_categorical_features",
      "static_real_features",
      "future_values",
      "future_time_features",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "output_hidden_states",
      "output_attentions",
      "use_cache",
      "return_dict",
      "cache_position"
    ]
  },
  "TimeSeriesTransformerForPrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "output_params": [
      "self",
      "dec_output"
    ],
    "output_distribution": [
      "self",
      "params",
      "loc",
      "scale",
      "trailing_n"
    ],
    "forward": [
      "self",
      "past_values",
      "past_time_features",
      "past_observed_mask",
      "static_categorical_features",
      "static_real_features",
      "future_values",
      "future_time_features",
      "future_observed_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "output_hidden_states",
      "output_attentions",
      "use_cache",
      "return_dict",
      "cache_position"
    ],
    "generate": [
      "self",
      "past_values",
      "past_time_features",
      "future_time_features",
      "past_observed_mask",
      "static_categorical_features",
      "static_real_features",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "TimeSeriesTransformerConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "prediction_length",
      "context_length",
      "distribution_output",
      "loss",
      "input_size",
      "lags_sequence",
      "scaling",
      "num_dynamic_real_features",
      "num_static_categorical_features",
      "num_static_real_features",
      "num_time_features",
      "cardinality",
      "embedding_dimension",
      "encoder_ffn_dim",
      "decoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_attention_heads",
      "encoder_layers",
      "decoder_layers",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "attention_dropout",
      "activation_dropout",
      "num_parallel_samples",
      "init_std",
      "use_cache"
    ],
    "_number_of_features": [
      "self"
    ]
  },
  "BartLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "BartScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "BartAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "BartEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "BartDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "BartClassificationHead": {
    "__init__": [
      "self",
      "input_dim",
      "inner_dim",
      "num_classes",
      "pooler_dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BartPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "PretrainedBartModel": {
    "__init_subclass__": [
      "self"
    ]
  },
  "BartPretrainedModel": {
    "__init_subclass__": [
      "self"
    ]
  },
  "BartEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BartDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BartModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BartForConditionalGeneration": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_missing": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "BartForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BartForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BartDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "BartForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BartConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "classifier_dropout",
      "scale_embedding",
      "use_cache",
      "num_labels",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "is_encoder_decoder",
      "decoder_start_token_id",
      "forced_eos_token_id",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "BartTokenizer": [],
  "BartTokenizerFast": [],
  "VisionEncoderDecoderConfig": {
    "model_type": [],
    "sub_configs": [],
    "has_no_defaults_at_init": [],
    "__init__": [
      "self"
    ],
    "from_encoder_decoder_configs": [
      "cls",
      "encoder_config",
      "decoder_config"
    ]
  },
  "VisionEncoderDecoderModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "__init__": [
      "self",
      "config",
      "encoder",
      "decoder"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "from_encoder_decoder_pretrained": [
      "cls",
      "encoder_pretrained_model_name_or_path",
      "decoder_pretrained_model_name_or_path"
    ],
    "forward": [
      "self",
      "pixel_values",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "Kosmos2_5ImageProcessorFast": {
    "do_normalize": [],
    "do_convert_rgb": [],
    "patch_size": [],
    "max_patches": [],
    "rescale_factor": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "normalize": [
      "self",
      "image"
    ],
    "extract_flattened_patches": [
      "self",
      "image",
      "max_patches",
      "patch_size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_normalize",
      "max_patches",
      "patch_size",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "Kosmos2_5ImageProcessorKwargs": {},
  "Kosmos2_5ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_convert_rgb",
      "do_normalize",
      "patch_size",
      "max_patches"
    ],
    "extract_flattened_patches": [
      "self",
      "image",
      "max_patches",
      "patch_size",
      "input_data_format"
    ],
    "normalize": [
      "self",
      "image",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_convert_rgb",
      "do_normalize",
      "max_patches",
      "patch_size",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "Kosmos2_5ProcessorKwargs": {
    "_defaults": []
  },
  "Kosmos2_5Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "num_image_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Kosmos2_5TextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "embed_dim",
      "layers",
      "ffn_dim",
      "attention_heads",
      "activation_function",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "layerdrop",
      "layer_norm_eps",
      "init_std",
      "scale_embedding",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Kosmos2_5VisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "patch_embed_hidden_size",
      "intermediate_size",
      "head_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "dense_act_fn",
      "layer_norm_eps",
      "dropout_rate",
      "attention_dropout",
      "max_num_patches",
      "initializer_factor",
      "initializer_range"
    ]
  },
  "Kosmos2_5Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "latent_query_num",
      "tie_word_embeddings"
    ]
  },
  "KOSMOS2_5_START_DOCSTRING": [],
  "KOSMOS2_5_VISION_INPUTS_DOCSTRING": [],
  "KOSMOS2_5_TEXT_INPUTS_DOCSTRING": [],
  "KOSMOS2_5_INPUTS_DOCSTRING": [],
  "Kosmos2_5ModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Kosmos2_5ForConditionalGenerationModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Kosmos2_5LayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Kosmos2_5VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "flattened_patches"
    ]
  },
  "Kosmos2_5VisionMlp": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Kosmos2_5VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Kosmos2_5VisionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Kosmos2_5VisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_prepare_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "Kosmos2_5TextSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "past_key_values_length",
      "position_ids"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "past_key_values_length",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "Kosmos2_5TextFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Kosmos2_5TextAttention": {
    "__init__": [
      "self",
      "config",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Kosmos2_5TextBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "Kosmos2_5TextTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "image_embeds",
      "image_embeds_position_mask",
      "past_key_values",
      "inputs_embeds",
      "position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "Kosmos2_5ImageToTextProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "Kosmos2_5PreTrainedModel": {
    "config_class": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_cache_class": [],
    "_supports_sdpa": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Kosmos2_5VisionModel": {
    "config_class": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "flattened_patches",
      "attention_mask",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "Kosmos2_5TextModel": {
    "config_class": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "image_embeds",
      "image_embeds_position_mask",
      "past_key_values",
      "inputs_embeds",
      "position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "Kosmos2_5Model": {
    "config_class": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "flattened_patches",
      "width",
      "height",
      "image_embeds_position_mask",
      "attention_mask",
      "past_key_values",
      "image_embeds",
      "inputs_embeds",
      "position_ids",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "Kosmos2_5TextForCausalLM": {
    "config_class": [],
    "input_modalities": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "image_embeds",
      "image_embeds_position_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_embeds",
      "image_embeds_position_mask",
      "past_key_values",
      "attention_mask",
      "use_cache",
      "cache_position",
      "position_ids",
      "is_first_iteration"
    ]
  },
  "Kosmos2_5ForConditionalGeneration": {
    "config_class": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "flattened_patches",
      "width",
      "height",
      "image_embeds_position_mask",
      "attention_mask",
      "past_key_values",
      "image_embeds",
      "inputs_embeds",
      "position_ids",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "flattened_patches",
      "image_embeds",
      "image_embeds_position_mask",
      "past_key_values",
      "attention_mask",
      "use_cache",
      "cache_position",
      "position_ids",
      "is_first_iteration"
    ]
  },
  "InformerConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "prediction_length",
      "context_length",
      "distribution_output",
      "loss",
      "input_size",
      "lags_sequence",
      "scaling",
      "num_dynamic_real_features",
      "num_static_real_features",
      "num_static_categorical_features",
      "num_time_features",
      "cardinality",
      "embedding_dimension",
      "d_model",
      "encoder_ffn_dim",
      "decoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_attention_heads",
      "encoder_layers",
      "decoder_layers",
      "is_encoder_decoder",
      "activation_function",
      "dropout",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "attention_dropout",
      "activation_dropout",
      "num_parallel_samples",
      "init_std",
      "use_cache",
      "attention_type",
      "sampling_factor",
      "distil"
    ],
    "_number_of_features": [
      "self"
    ]
  },
  "InformerFeatureEmbedder": {},
  "InformerStdScaler": {},
  "InformerMeanScaler": {},
  "InformerNOPScaler": {},
  "InformerSinusoidalPositionalEmbedding": {},
  "InformerValueEmbedding": {},
  "InformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "InformerAttention": {},
  "InformerProbSparseAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "sampling_factor",
      "bias",
      "layer_idx"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "InformerConvLayer": {
    "__init__": [
      "self",
      "c_in"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "InformerEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "InformerDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "InformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "InformerDecoder": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "InformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "InformerForPrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "SEWDConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "squeeze_factor",
      "max_position_embeddings",
      "position_buckets",
      "share_att_key",
      "relative_attention",
      "pos_att_type",
      "norm_rel_ebd",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "final_dropout",
      "initializer_range",
      "layer_norm_eps",
      "feature_layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ],
    "to_dict": [
      "self"
    ]
  },
  "get_mask": [
    "input",
    "local_context"
  ],
  "SEWDNoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWDLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWDGroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWDPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWDSamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWDUpsampling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWDFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "XSoftmax": {
    "forward": [
      "ctx",
      "input",
      "mask",
      "dim"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ],
    "symbolic": [
      "g",
      "self",
      "mask",
      "dim"
    ]
  },
  "DropoutContext": {
    "__init__": [
      "self"
    ]
  },
  "XDropout": {
    "forward": [
      "ctx",
      "input",
      "local_ctx"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ],
    "symbolic": [
      "g",
      "input",
      "local_ctx"
    ]
  },
  "StableDropout": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "x"
    ],
    "clear_context": [
      "self"
    ],
    "init_context": [
      "self",
      "reuse_mask",
      "scale"
    ],
    "get_context": [
      "self"
    ]
  },
  "SEWDSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "SEWDAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "query_states",
      "relative_pos",
      "rel_embeddings"
    ]
  },
  "SEWDIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SEWDOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "SEWDLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "query_states",
      "relative_pos",
      "rel_embeddings",
      "output_attentions"
    ]
  },
  "SEWDTransformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_rel_embedding": [
      "self"
    ],
    "get_attention_mask": [
      "self",
      "attention_mask"
    ],
    "get_rel_pos": [
      "self",
      "hidden_states",
      "query_states",
      "relative_pos"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_hidden_states",
      "output_attentions",
      "query_states",
      "relative_pos",
      "return_dict"
    ]
  },
  "SEWDEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SEWDPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask"
    ]
  },
  "SEWDModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SEWDForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "tie_weights": [
      "self"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "SEWDForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "XcodecOutput": {},
  "XcodecEncoderOutput": {},
  "XcodecDecoderOutput": {},
  "ResidualUnit": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "SemanticEncoderBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "SemanticEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "SemanticDecoderBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "SemanticDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "XcodecEuclideanCodebook": {
    "__init__": [
      "self",
      "config"
    ],
    "quantize": [
      "self",
      "hidden_states"
    ],
    "encode": [
      "self",
      "hidden_states"
    ],
    "decode": [
      "self",
      "embed_ind"
    ]
  },
  "XcodecVectorQuantization": {
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "hidden_states"
    ],
    "decode": [
      "self",
      "embed_ind"
    ]
  },
  "XcodecResidualVectorQuantization": {
    "__init__": [
      "self",
      "config"
    ],
    "get_bandwidth_per_quantizer": [
      "self"
    ],
    "get_num_quantizers_for_bandwidth": [
      "self",
      "bandwidth"
    ],
    "encode": [
      "self",
      "embeddings",
      "bandwidth"
    ],
    "decode": [
      "self",
      "codes"
    ]
  },
  "XcodecPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "_get_conv1d_layers": [
      "self",
      "module"
    ],
    "_get_conv1d_output_lengths": [
      "self",
      "input_length",
      "module"
    ]
  },
  "XcodecModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_adjust_dac_decoder": [
      "decoder"
    ],
    "_extract_semantic_features": [
      "self",
      "input_values"
    ],
    "encode": [
      "self",
      "input_values",
      "bandwidth",
      "return_dict"
    ],
    "decode": [
      "self",
      "audio_codes",
      "return_dict"
    ],
    "forward": [
      "self",
      "input_values",
      "audio_codes",
      "bandwidth",
      "return_dict"
    ]
  },
  "XcodecConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "target_bandwidths",
      "sample_rate",
      "kernel_size",
      "channel_ratios",
      "strides",
      "block_dilations",
      "unit_kernel_size",
      "codebook_size",
      "codebook_dim",
      "initializer_range",
      "acoustic_model_config",
      "semantic_model_config"
    ],
    "frame_rate": [
      "self"
    ],
    "semantic_hidden_size": [
      "self"
    ],
    "hop_length": [
      "self"
    ],
    "codebook_nbits": [
      "self"
    ],
    "hidden_size": [
      "self"
    ],
    "num_quantizers": [
      "self"
    ]
  },
  "ClvpFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "default_audio_length",
      "hop_length",
      "chunk_length",
      "n_fft",
      "padding_value",
      "mel_norms",
      "return_attention_mask"
    ],
    "_np_extract_fbank_features": [
      "self",
      "waveform"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "sampling_rate",
      "truncation",
      "pad_to_multiple_of",
      "return_tensors",
      "return_attention_mask",
      "padding",
      "max_length"
    ]
  },
  "ClvpProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self"
    ]
  },
  "EnglishNormalizer": {
    "__init__": [
      "self"
    ],
    "number_to_words": [
      "self",
      "num"
    ],
    "convert_to_ascii": [
      "self",
      "text"
    ],
    "_expand_dollars": [
      "self",
      "m"
    ],
    "_remove_commas": [
      "self",
      "m"
    ],
    "_expand_decimal_point": [
      "self",
      "m"
    ],
    "_expand_ordinal": [
      "self",
      "num"
    ],
    "_expand_number": [
      "self",
      "m"
    ],
    "normalize_numbers": [
      "self",
      "text"
    ],
    "expand_abbreviations": [
      "self",
      "text"
    ],
    "collapse_whitespace": [
      "self",
      "text"
    ],
    "__call__": [
      "self",
      "text"
    ]
  },
  "ClvpEncoderConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "projection_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "dropout",
      "use_rotary_embedding",
      "use_attention_bias",
      "summary_type",
      "initializer_factor",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "config_type"
    ]
  },
  "ClvpDecoderConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "max_text_tokens",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "n_inner",
      "num_mel_attn_blocks",
      "activation_function",
      "resid_pdrop",
      "embd_pdrop",
      "attention_dropout",
      "layer_norm_epsilon",
      "initializer_range",
      "summary_type",
      "summary_use_proj",
      "summary_activation",
      "summary_proj_to_labels",
      "summary_first_dropout",
      "use_cache",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "feature_size",
      "use_attention_bias",
      "initializer_factor",
      "decoder_fixing_codes",
      "add_cross_attention"
    ]
  },
  "ClvpConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "speech_config",
      "decoder_config",
      "projection_dim",
      "logit_scale_init_value",
      "initializer_factor"
    ]
  },
  "clvp_loss": [
    "similarity"
  ],
  "_pad_extra_bos_eos_tokens": [
    "input_ids",
    "attention_mask",
    "pad_token_id",
    "bos_token_id",
    "eos_token_id",
    "add_bos_token",
    "add_eos_token"
  ],
  "ClvpEncoderOutput": {},
  "ClvpOutput": {},
  "ClvpRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ClvpRotaryPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClvpSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "rotary_pos_emb",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "ClvpGatedLinearUnit": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClvpEncoderMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClvpEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "rotary_pos_emb",
      "attention_mask",
      "position_ids",
      "output_attentions"
    ]
  },
  "ClvpSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "ClvpDecoderMLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ClvpDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "attention_mask",
      "position_ids",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "ClvpConditioningEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "compute_groupnorm_groups": [
      "self",
      "channels",
      "groups"
    ],
    "forward": [
      "self",
      "input_features",
      "input_ids",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "ClvpPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ClvpEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ClvpDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ClvpModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ClvpForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "_prepare_model_inputs": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "conditioning_embeds",
      "cache_position",
      "is_first_iteration"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ClvpModelForConditionalGeneration": {
    "__init__": [
      "self",
      "config"
    ],
    "fix_speech_decoder_output": [
      "self",
      "speech_ids"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "text_encoder_inputs_embeds",
      "attention_mask"
    ],
    "get_speech_features": [
      "self",
      "speech_ids",
      "input_ids",
      "input_features",
      "conditioning_encoder_inputs_embeds",
      "attention_mask",
      "generation_config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "conditioning_encoder_inputs_embeds",
      "text_encoder_inputs_embeds",
      "attention_mask",
      "return_loss",
      "output_hidden_states",
      "output_attentions",
      "return_dict",
      "cache_position"
    ],
    "generate": [
      "self",
      "input_ids",
      "input_features",
      "attention_mask",
      "generation_config",
      "pad_to_max_mel_tokens",
      "output_hidden_states"
    ]
  },
  "ClvpTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "errors",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "add_prefix_space"
    ],
    "vocab_size": [
      "self"
    ],
    "normalizer": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "clean_up_tokenization": [
      "self",
      "text"
    ]
  },
  "AyaVisionMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ],
    "pixel_shuffle": [
      "self",
      "image_features"
    ]
  },
  "AyaVisionPreTrainedModel": {
    "_can_compile_fullgraph": [],
    "_can_record_outputs": []
  },
  "AyaVisionCausalLMOutputWithPast": {},
  "AyaVisionModelOutputWithPast": {},
  "AyaVisionModel": {
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "output_hidden_states"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "use_cache",
      "cache_position"
    ]
  },
  "AyaVisionForConditionalGeneration": {
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "labels",
      "cache_position",
      "logits_to_keep",
      "image_sizes"
    ]
  },
  "AyaVisionConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "vision_feature_select_strategy",
      "vision_feature_layer",
      "downsample_factor",
      "adapter_layer_norm_eps",
      "image_token_index",
      "tie_word_embeddings"
    ]
  },
  "AyaVisionProcessorKwargs": {
    "_defaults": []
  },
  "AyaVisionProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "patch_size",
      "img_size",
      "image_token",
      "downsample_factor",
      "start_of_img_token",
      "end_of_img_token",
      "img_patch_token",
      "img_line_break_token",
      "tile_token",
      "tile_global_token",
      "chat_template"
    ],
    "_prompt_split_image": [
      "self",
      "num_patches"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "FuyuPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": []
  },
  "FuyuModel": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "gather_continuous_embeddings": [
      "self",
      "word_embeddings",
      "continuous_embeddings",
      "image_patch_input_indices"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "image_patches",
      "image_patches_indices",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FuyuForCausalLM": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "image_patches",
      "image_patches_indices",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "image_patches",
      "image_patches_indices",
      "cache_position",
      "is_first_iteration"
    ]
  },
  "FuyuImageProcessorFast": {
    "do_resize": [],
    "size": [],
    "patch_size": [],
    "resample": [],
    "do_pad": [],
    "padding_value": [],
    "padding_mode": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_rescale": [],
    "rescale_factor": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "_prepare_images_structure": [
      "self",
      "images",
      "expected_ndims"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "padding_value",
      "padding_mode",
      "disable_grouping",
      "return_tensors"
    ],
    "get_num_patches": [
      "self",
      "image_height",
      "image_width",
      "patch_size"
    ],
    "patchify_image": [
      "self",
      "image",
      "patch_size"
    ],
    "preprocess_with_tokenizer_info": [
      "self",
      "image_input",
      "image_present",
      "image_unpadded_h",
      "image_unpadded_w",
      "image_placeholder_id",
      "image_newline_id",
      "variable_sized",
      "patch_size"
    ],
    "_further_process_kwargs": [
      "self",
      "patch_size"
    ]
  },
  "make_list_of_list_of_images": [
    "images"
  ],
  "FuyuImagesKwargs": {},
  "FuyuBatchFeature": {
    "convert_to_tensors": [
      "self",
      "tensor_type"
    ],
    "to": [
      "self"
    ]
  },
  "FuyuImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_pad",
      "padding_value",
      "padding_mode",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_rescale",
      "rescale_factor",
      "patch_size"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "pad_image": [
      "self",
      "image",
      "size",
      "mode",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_pad",
      "padding_value",
      "padding_mode",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_rescale",
      "rescale_factor",
      "patch_size",
      "data_format",
      "input_data_format",
      "return_tensors"
    ],
    "get_num_patches": [
      "self",
      "image_height",
      "image_width",
      "patch_size"
    ],
    "patchify_image": [
      "self",
      "image",
      "patch_size"
    ],
    "preprocess_with_tokenizer_info": [
      "self",
      "image_input",
      "image_present",
      "image_unpadded_h",
      "image_unpadded_w",
      "image_placeholder_id",
      "image_newline_id",
      "variable_sized",
      "patch_size"
    ]
  },
  "FuyuConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_act",
      "max_position_embeddings",
      "image_size",
      "patch_size",
      "num_channels",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "qk_layernorm",
      "hidden_dropout",
      "attention_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "image_token_id",
      "text_config"
    ]
  },
  "TEXT_REPR_BBOX_OPEN": [],
  "TEXT_REPR_BBOX_CLOSE": [],
  "TEXT_REPR_POINT_OPEN": [],
  "TEXT_REPR_POINT_CLOSE": [],
  "TOKEN_BBOX_OPEN_STRING": [],
  "TOKEN_BBOX_CLOSE_STRING": [],
  "TOKEN_POINT_OPEN_STRING": [],
  "TOKEN_POINT_CLOSE_STRING": [],
  "BEGINNING_OF_ANSWER_STRING": [],
  "FuyuProcessorKwargs": {
    "_defaults": []
  },
  "full_unpacked_stream_to_tensor": [
    "all_bi_tokens_to_place",
    "full_unpacked_stream",
    "fill_value",
    "batch_size",
    "new_seq_len",
    "offset"
  ],
  "construct_full_unpacked_stream": [
    "num_real_text_tokens",
    "input_stream",
    "image_tokens",
    "batch_size",
    "num_sub_sequences"
  ],
  "_replace_string_repr_with_token_tags": [
    "prompt"
  ],
  "_segment_prompt_into_text_token_conversions": [
    "prompt"
  ],
  "_transform_coordinates_and_tokenize": [
    "prompt",
    "scale_factor",
    "tokenizer"
  ],
  "_transform_within_tags": [
    "text",
    "scale_factor",
    "tokenizer"
  ],
  "_tokenize_prompts_with_image_and_batch": [
    "tokenizer",
    "prompts",
    "scale_factors",
    "max_tokens_to_generate",
    "max_position_embeddings",
    "add_BOS",
    "add_beginning_of_answer_token"
  ],
  "original_to_transformed_h_coords": [
    "original_coords",
    "scale_h"
  ],
  "original_to_transformed_w_coords": [
    "original_coords",
    "scale_w"
  ],
  "scale_point_to_transformed_image": [
    "x",
    "y",
    "scale_factor"
  ],
  "scale_bbox_to_transformed_image": [
    "top",
    "left",
    "bottom",
    "right",
    "scale_factor"
  ],
  "FuyuProcessor": {
    "_load_tokenizer_from_pretrained": [
      "cls",
      "sub_processor_type",
      "pretrained_model_name_or_path",
      "subfolder"
    ],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "_left_pad_inputs_with_attention_mask": [
      "self",
      "model_inputs",
      "return_attention_mask"
    ],
    "get_sample_encoding": [
      "self",
      "prompts",
      "scale_factors",
      "image_unpadded_heights",
      "image_unpadded_widths",
      "image_placeholder_id",
      "image_newline_id",
      "tensor_batch_images"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "post_process_box_coordinates": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "DetrImageProcessorKwargs": {},
  "DetrImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "format",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_annotations",
      "do_pad",
      "pad_size"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "size",
      "resample"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "annotation",
      "constant_values",
      "data_format",
      "input_data_format",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "images",
      "annotations",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "update_bboxes",
      "pad_size"
    ],
    "preprocess": [
      "self",
      "images",
      "annotations",
      "return_segmentation_masks",
      "masks_path",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "format",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "DetrDecoderOutput": {},
  "DetrModelOutput": {},
  "DetrObjectDetectionOutput": {},
  "DetrSegmentationOutput": {},
  "DetrFrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DetrConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "DetrSinePositionEmbedding": {
    "__init__": [
      "self",
      "num_position_features",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "DetrLearnedPositionEmbedding": {
    "__init__": [
      "self",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "DetrSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "DetrCrossAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_embeddings",
      "encoder_position_embeddings"
    ]
  },
  "DetrMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DetrEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings"
    ]
  },
  "DetrDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings",
      "object_queries_position_embeddings",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "DetrConvBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "activation"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DetrFPNFusionStage": {
    "__init__": [
      "self",
      "fpn_channels",
      "current_channels",
      "output_channels",
      "activation"
    ],
    "forward": [
      "self",
      "features",
      "fpn_features"
    ]
  },
  "DetrMaskHeadSmallConv": {
    "__init__": [
      "self",
      "input_channels",
      "fpn_channels",
      "hidden_size",
      "activation_function"
    ],
    "forward": [
      "self",
      "features",
      "attention_masks",
      "fpn_features"
    ]
  },
  "DetrMHAttentionMap": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "query_states",
      "key_states",
      "attention_mask"
    ]
  },
  "DetrPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_supports_flex_attn": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DetrEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "spatial_position_embeddings"
    ]
  },
  "DetrDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "spatial_position_embeddings",
      "object_queries_position_embeddings"
    ]
  },
  "DetrModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds"
    ]
  },
  "DetrMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DetrForObjectDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "DetrForSegmentation": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "DetrConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "num_channels",
      "num_queries",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "init_xavier_std",
      "auxiliary_loss",
      "position_embedding_type",
      "dilation",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "mask_loss_coefficient",
      "dice_loss_coefficient",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "eos_coefficient"
    ]
  },
  "DetrImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "format": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "size": [],
    "default_to_square": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "target_size",
      "threshold",
      "interpolation"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "annotation",
      "update_bboxes",
      "fill"
    ],
    "_preprocess": [
      "self",
      "images",
      "annotations",
      "masks_path",
      "return_segmentation_masks",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "format",
      "return_tensors"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "DepthProImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "DepthProConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "fusion_hidden_size",
      "patch_size",
      "initializer_range",
      "intermediate_hook_ids",
      "intermediate_feature_dims",
      "scaled_images_ratios",
      "scaled_images_overlap_ratios",
      "scaled_images_feature_dims",
      "merge_padding_value",
      "use_batch_norm_in_fusion_residual",
      "use_bias_in_fusion_residual",
      "use_fov_model",
      "num_fov_head_layers",
      "image_model_config",
      "patch_model_config",
      "fov_model_config"
    ]
  },
  "DepthProOutput": {},
  "DepthProDepthEstimatorOutput": {},
  "split_to_patches": [
    "pixel_values",
    "patch_size",
    "overlap_ratio"
  ],
  "reshape_features": [
    "hidden_states"
  ],
  "merge_patches": [
    "patches",
    "batch_size",
    "padding"
  ],
  "reconstruct_feature_maps": [
    "hidden_state",
    "batch_size",
    "padding",
    "output_size"
  ],
  "DepthProPatchEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DepthProImageEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DepthProEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DepthProFeatureUpsampleBlock": {
    "__init__": [
      "self",
      "config",
      "input_dims",
      "intermediate_dims",
      "output_dims",
      "n_upsample_layers",
      "use_proj",
      "bias"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DepthProFeatureUpsample": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DepthProFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DepthProNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DepthProPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_no_split_modules": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DepthProModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DepthProPreActResidualLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DepthProFeatureFusionLayer": {
    "__init__": [
      "self",
      "config",
      "use_deconv"
    ],
    "forward": [
      "self",
      "hidden_state",
      "residual"
    ]
  },
  "DepthProFeatureFusionStage": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DepthProFovEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "DepthProFovHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DepthProFovModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "global_features"
    ]
  },
  "DepthProDepthEstimationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DepthProForDepthEstimation": {
    "__init__": [
      "self",
      "config",
      "use_fov_model"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DepthProImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_validate_input_arguments": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "WavLMSamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "num_buckets",
      "max_distance",
      "has_relative_position_bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions",
      "index"
    ],
    "torch_multi_head_self_attention": [
      "self",
      "hidden_states",
      "attention_mask",
      "gated_position_bias",
      "output_attentions"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length"
    ],
    "_relative_positions_bucket": [
      "self",
      "relative_positions"
    ]
  },
  "WavLMFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "has_relative_position_bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions",
      "index"
    ]
  },
  "WavLMEncoderLayerStableLayerNorm": {
    "__init__": [
      "self",
      "config",
      "has_relative_position_bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "WavLMEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "WavLMEncoderStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "WavLMGumbelVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_perplexity": [
      "probs"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths",
      "add_adapter"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask",
      "add_adapter"
    ]
  },
  "WavLMNoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMGroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "WavLMAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMAdapter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "WavLMBaseModelOutput": [],
  "WavLMModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "WavLMForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "tie_weights": [
      "self"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "WavLMForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "WavLMForAudioFrameClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "WavLMForXVector": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "_get_tdnn_output_lengths": [
      "self",
      "input_lengths"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "WavLMConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "num_buckets",
      "max_bucket_distance",
      "do_stable_layer_norm",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "num_codevectors_per_group",
      "num_codevector_groups",
      "contrastive_logits_temperature",
      "num_negatives",
      "codevector_dim",
      "proj_codevector_dim",
      "diversity_loss_weight",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "tdnn_dim",
      "tdnn_kernel",
      "tdnn_dilation",
      "xvector_output_dim",
      "num_ctc_classes",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_adapter",
      "adapter_kernel_size",
      "adapter_stride",
      "num_adapter_layers",
      "output_hidden_size"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "IdeficsPerceiverResampler": {
    "__init__": [
      "self",
      "config",
      "embed_dim",
      "depth",
      "n_heads",
      "head_dim",
      "n_latents"
    ],
    "forward": [
      "self",
      "context"
    ]
  },
  "IdeficsPerceiverAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "n_heads",
      "head_dim",
      "qk_layer_norms"
    ],
    "forward": [
      "self",
      "context",
      "latents"
    ]
  },
  "IdeficsMLP": {
    "__init__": [
      "self",
      "intermediate_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IdeficsTextKwargs": {},
  "IdeficsProcessorKwargs": {
    "_defaults": []
  },
  "incremental_to_binary_attention_mask": [
    "incremental_mask",
    "return_tensors",
    "num_classes"
  ],
  "image_attention_mask_for_packed_input_ids": [
    "input_ids",
    "tokenizer",
    "return_tensors"
  ],
  "image_attention_mask_for_packed_input_ids_pt": [
    "input_ids",
    "tokenizer"
  ],
  "IdeficsProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "image_size",
      "add_end_of_utterance_token"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "IDEFICS_STANDARD_MEAN": [],
  "IDEFICS_STANDARD_STD": [],
  "IdeficsImageProcessorKwargs": {},
  "IdeficsImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "image_size",
      "image_mean",
      "image_std",
      "image_num_channels",
      "do_rescale",
      "rescale_factor"
    ],
    "preprocess": [
      "self",
      "images",
      "image_num_channels",
      "image_size",
      "image_mean",
      "image_std",
      "transform",
      "do_rescale",
      "rescale_factor",
      "return_tensors"
    ]
  },
  "IdeficsVisionModelOutput": {},
  "IdeficsVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "IdeficsVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "IdeficsVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IdeficsVisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "IdeficsVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "IdeficsVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "IdeficsVisionConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "embed_dim",
      "image_size",
      "intermediate_size",
      "patch_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "initializer_factor"
    ]
  },
  "IdeficsPerceiverConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "use_resampler",
      "resampler_n_latents",
      "resampler_depth",
      "resampler_n_heads",
      "resampler_head_dim",
      "qk_layer_norms_perceiver"
    ]
  },
  "IdeficsConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vocab_size",
      "additional_vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "dropout",
      "hidden_act",
      "initializer_range",
      "alpha_initializer",
      "alphas_initializer_range",
      "alpha_type",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "cross_layer_interval",
      "qk_layer_norms",
      "freeze_text_layers",
      "freeze_text_module_exceptions",
      "freeze_lm_head",
      "freeze_vision_layers",
      "freeze_vision_module_exceptions",
      "use_resampler",
      "vision_config",
      "perceiver_config"
    ]
  },
  "IdeficsBaseModelOutputWithPast": {},
  "IdeficsCausalLMOutputWithPast": {},
  "expand_inputs_for_generation": [
    "input_ids",
    "expand_size",
    "is_encoder_decoder",
    "attention_mask",
    "encoder_outputs"
  ],
  "freeze_model": [
    "model",
    "module_exceptions"
  ],
  "IdeficsDecoupledEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "num_additional_embeddings",
      "embedding_dim",
      "partially_freeze",
      "device",
      "dtype",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "IdeficsDecoupledLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "out_additional_features",
      "bias",
      "partially_freeze",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "input"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "IdeficsRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "IdeficsEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "device"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x",
      "seq_len"
    ]
  },
  "IdeficsAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "dropout",
      "is_cross_attention",
      "config",
      "qk_layer_norms",
      "layer_idx"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "IdeficsDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "IdeficsGatedCrossAttentionLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "image_hidden_states",
      "image_attention_mask",
      "cross_attention_gate",
      "past_key_values"
    ]
  },
  "IdeficsPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "IdeficsModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_relevant_params": [
      "self",
      "config"
    ],
    "freeze_text_layers": [
      "self",
      "module_exceptions"
    ],
    "freeze_vision_layers": [
      "self",
      "module_exceptions"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "image_encoder_embeddings",
      "perceiver_embeddings",
      "image_attention_mask",
      "use_cache",
      "interpolate_pos_encoding",
      "cache_position"
    ]
  },
  "IdeficsForVisionText2Text": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config",
      "vision_model"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "image_encoder_embeddings",
      "perceiver_embeddings",
      "image_attention_mask",
      "labels",
      "use_cache",
      "interpolate_pos_encoding",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "past_key_values",
      "cache_position",
      "pixel_values",
      "image_hidden_states",
      "image_attention_mask",
      "use_cache"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder"
    ]
  },
  "CodeLlamaTokenizer": {
    "vocab_files_names": [],
    "padding_side": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "clean_up_tokenization_spaces",
      "unk_token",
      "bos_token",
      "eos_token",
      "prefix_token",
      "middle_token",
      "suffix_token",
      "eot_token",
      "fill_token",
      "additional_special_tokens",
      "use_default_system_prompt",
      "add_prefix_space",
      "add_bos_token"
    ],
    "prefix_token": [
      "self"
    ],
    "prefix_id": [
      "self"
    ],
    "middle_token": [
      "self"
    ],
    "middle_id": [
      "self"
    ],
    "suffix_token": [
      "self"
    ],
    "suffix_id": [
      "self"
    ],
    "eot_id": [
      "self"
    ],
    "eot_token": [
      "self"
    ],
    "set_infilling_processor": [
      "self",
      "reset",
      "suffix_first",
      "add_special_tokens"
    ],
    "tokenize": [
      "self",
      "text",
      "suffix",
      "suffix_first"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "suffix",
      "suffix_first",
      "add_special_tokens"
    ]
  },
  "CodeLlamaTokenizerFast": [],
  "PhiRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PhiAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "PhiMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PhiDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PhiPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "PhiModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "PhiForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "PhiForSequenceClassification": {},
  "PhiForTokenClassification": {},
  "PhiConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "resid_pdrop",
      "embd_pdrop",
      "attention_dropout",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "qk_layernorm",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ]
  },
  "IJepaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "pooler_output_size",
      "pooler_act"
    ]
  },
  "IJepaEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "interpolate_pos_encoding"
    ]
  },
  "IJepaPreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "IJepaModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "use_mask_token"
    ]
  },
  "IJepaForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "interpolate_pos_encoding"
    ]
  },
  "IJepaPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "IJepaSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IJepaSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "IJepaAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IJepaIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IJepaOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "IJepaLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IJepaEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "IJepaPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwitchTransformersTop1Router": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwitchTransformersLayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwitchTransformersDenseActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwitchTransformersExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "selected_experts",
      "routing_weights"
    ]
  },
  "SwitchTransformersSparseMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwitchTransformersLayerFF": {
    "__init__": [
      "self",
      "config",
      "is_sparse"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SwitchTransformersAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_values",
      "query_length",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "SwitchTransformersLayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "SwitchTransformersLayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "query_length",
      "output_attentions",
      "cache_position"
    ]
  },
  "SwitchTransformersBlock": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "is_sparse",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "SwitchTransformersPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_can_compile_fullgraph": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "SwitchTransformersStack": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "SwitchTransformersModel": {
    "_tied_weights_keys": [],
    "_input_embed_layer": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "cache_position"
    ]
  },
  "router_z_loss_func": [
    "router_logits"
  ],
  "SwitchTransformersForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "output_router_logits",
      "cache_position"
    ],
    "_unpack_router_logits": [
      "self",
      "router_outputs"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "SwitchTransformersEncoderModel": {
    "_tied_weights_keys": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "use_cache"
    ]
  },
  "SwitchTransformersConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "d_kv",
      "d_ff",
      "expert_capacity",
      "num_layers",
      "num_sparse_encoder_layers",
      "num_decoder_layers",
      "num_sparse_decoder_layers",
      "num_heads",
      "num_experts",
      "router_bias",
      "router_jitter_noise",
      "router_dtype",
      "router_ignore_padding_tokens",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "dropout_rate",
      "layer_norm_epsilon",
      "router_z_loss_coef",
      "router_aux_loss_coef",
      "initializer_factor",
      "dense_act_fn",
      "is_encoder_decoder",
      "add_router_probs",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "CONTROL_CODES": [],
  "CTRLTokenizer": {
    "vocab_files_names": [],
    "control_codes": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "unk_token"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ]
  },
  "CTRLConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "n_positions",
      "n_embd",
      "dff",
      "n_layer",
      "n_head",
      "resid_pdrop",
      "embd_pdrop",
      "layer_norm_epsilon",
      "initializer_range",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "angle_defn": [
    "pos",
    "i",
    "d_model_size"
  ],
  "positional_encoding": [
    "position",
    "d_model_size",
    "dtype"
  ],
  "scaled_dot_product_attention": [
    "q",
    "k",
    "v",
    "mask",
    "attention_mask"
  ],
  "point_wise_feed_forward_network": [
    "d_model_size",
    "dff"
  ],
  "CTRLPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CTRLModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "CTRLLMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "use_cache",
      "is_first_iteration"
    ]
  },
  "CTRLForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MLukeTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "task",
      "max_entity_length",
      "max_mention_length",
      "entity_token_1",
      "entity_token_2",
      "entity_unk_token",
      "entity_pad_token",
      "entity_mask_token",
      "entity_mask2_token",
      "vocab",
      "entity_vocab"
    ],
    "_post_init": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "num_special_tokens_to_add": [
      "self",
      "pair"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "entity_spans",
      "entity_spans_pair",
      "entities",
      "entities_pair",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "max_entity_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "entity_spans",
      "entity_spans_pair",
      "entities",
      "entities_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "max_entity_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "batch_entity_spans_or_entity_spans_pairs",
      "batch_entities_or_entities_pairs",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "max_entity_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_check_entity_input_format": [
      "self",
      "entities",
      "entity_spans"
    ],
    "_create_input_sequence": [
      "self",
      "text",
      "text_pair",
      "entities",
      "entities_pair",
      "entity_spans",
      "entity_spans_pair"
    ],
    "_batch_prepare_for_model": [
      "self",
      "batch_ids_pairs",
      "batch_entity_ids_pairs",
      "batch_entity_token_spans_pairs",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "max_entity_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_length",
      "verbose"
    ],
    "prepare_for_model": [
      "self",
      "ids",
      "pair_ids",
      "entity_ids",
      "pair_entity_ids",
      "entity_token_spans",
      "pair_entity_token_spans",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "max_entity_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "prepend_batch_axis"
    ],
    "pad": [
      "self",
      "encoded_inputs",
      "padding",
      "max_length",
      "max_entity_length",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask",
      "return_tensors",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "max_entity_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "symmetrize": [
    "x"
  ],
  "average_product_correct": [
    "x"
  ],
  "RotaryEmbedding": {
    "__init__": [
      "self",
      "dim"
    ],
    "_update_cos_sin_tables": [
      "self",
      "x",
      "seq_dimension"
    ],
    "forward": [
      "self",
      "q",
      "k"
    ]
  },
  "EsmContactPredictionHead": {
    "__init__": [
      "self",
      "in_features",
      "bias",
      "eos_idx"
    ],
    "forward": [
      "self",
      "tokens",
      "attentions"
    ]
  },
  "EsmEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ]
  },
  "EsmSelfAttention": {
    "__init__": [
      "self",
      "config",
      "position_embedding_type",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "EsmSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "EsmAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "EsmIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EsmOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "EsmLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "EsmEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "EsmPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EsmPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "accepts_loss_kwargs": [],
    "_no_split_modules": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "get_output_embeddings": [
      "self"
    ]
  },
  "EsmModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ],
    "predict_contacts": [
      "self",
      "tokens",
      "attention_mask"
    ]
  },
  "EsmForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ],
    "predict_contacts": [
      "self",
      "tokens",
      "attention_mask"
    ]
  },
  "EsmLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "EsmForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "EsmForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "EsmClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "StructureModuleConfig": {
    "to_dict": [
      "self"
    ]
  },
  "TrunkConfig": {
    "__post_init__": [
      "self"
    ],
    "to_dict": [
      "self"
    ]
  },
  "EsmFoldConfig": {
    "__post_init__": [
      "self"
    ],
    "to_dict": [
      "self"
    ]
  },
  "EsmConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vocab_size",
      "mask_token_id",
      "pad_token_id",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "position_embedding_type",
      "use_cache",
      "emb_layer_norm_before",
      "token_dropout",
      "is_folding_model",
      "esmfold_config",
      "vocab_list",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ],
    "to_dict": [
      "self"
    ]
  },
  "get_default_vocab_list": [],
  "load_vocab_file": [
    "vocab_file"
  ],
  "EsmTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "unk_token",
      "cls_token",
      "pad_token",
      "mask_token",
      "eos_token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "get_vocab": [
      "self"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "id_to_token": [
      "self",
      "index"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "vocab_size": [
      "self"
    ]
  },
  "EsmForProteinFoldingOutput": {},
  "is_fp16_enabled": [
    "device_type"
  ],
  "is_deepspeed_initialized": [],
  "collate_dense_tensors": [
    "samples",
    "pad_v"
  ],
  "flatten_final_dims": [
    "t",
    "no_dims"
  ],
  "permute_final_dims": [
    "tensor",
    "inds"
  ],
  "dict_multimap": [
    "fn",
    "dicts"
  ],
  "EsmFoldLinear": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim",
      "bias",
      "init",
      "init_fn"
    ]
  },
  "EsmFoldLayerNorm": {
    "__init__": [
      "self",
      "c_in",
      "eps"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "softmax_no_cast": [
    "t",
    "dim"
  ],
  "EsmFoldAttention": {
    "__init__": [
      "self",
      "c_q",
      "c_k",
      "c_v",
      "c_hidden",
      "no_heads",
      "gating"
    ],
    "_prep_qkv": [
      "self",
      "q_x",
      "kv_x"
    ],
    "_wrap_up": [
      "self",
      "o",
      "q_x"
    ],
    "forward": [
      "self",
      "q_x",
      "kv_x",
      "biases",
      "use_memory_efficient_kernel",
      "use_lma",
      "lma_q_chunk_size",
      "lma_kv_chunk_size",
      "use_flash",
      "flash_mask"
    ]
  },
  "EsmFoldTriangleAttention": {
    "__init__": [
      "self",
      "c_in",
      "c_hidden",
      "no_heads",
      "starting",
      "inf"
    ],
    "_chunk": [
      "self",
      "x",
      "biases",
      "chunk_size",
      "use_memory_efficient_kernel",
      "use_lma",
      "inplace_safe"
    ],
    "forward": [
      "self",
      "x",
      "mask",
      "chunk_size",
      "use_memory_efficient_kernel",
      "use_lma",
      "inplace_safe"
    ]
  },
  "EsmFoldTriangleMultiplicativeUpdate": {
    "__init__": [
      "self",
      "config",
      "_outgoing"
    ],
    "_combine_projections": [
      "self",
      "a",
      "b",
      "_inplace_chunk_size"
    ],
    "_inference_forward": [
      "self",
      "z",
      "mask",
      "inplace_chunk_size",
      "with_add"
    ],
    "forward": [
      "self",
      "z",
      "mask",
      "inplace_safe",
      "_add_with_inplace",
      "_inplace_chunk_size"
    ]
  },
  "EsmFoldPreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EsmFoldSelfAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "head_width",
      "gated"
    ],
    "forward": [
      "self",
      "x",
      "mask",
      "bias",
      "indices"
    ]
  },
  "EsmFoldDropout": {
    "__init__": [
      "self",
      "r",
      "batch_dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EsmFoldSequenceToPair": {
    "__init__": [
      "self",
      "sequence_state_dim",
      "inner_dim",
      "pairwise_state_dim"
    ],
    "forward": [
      "self",
      "sequence_state"
    ]
  },
  "EsmFoldPairToSequence": {
    "__init__": [
      "self",
      "pairwise_state_dim",
      "num_heads"
    ],
    "forward": [
      "self",
      "pairwise_state"
    ]
  },
  "EsmFoldResidueMLP": {
    "__init__": [
      "self",
      "embed_dim",
      "inner_dim",
      "dropout"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EsmFoldTriangularSelfAttentionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_state",
      "pairwise_state",
      "mask",
      "chunk_size"
    ]
  },
  "EsmCategoricalMixture": {
    "__init__": [
      "self",
      "param",
      "bins",
      "start",
      "end"
    ],
    "log_prob": [
      "self",
      "true"
    ],
    "mean": [
      "self"
    ]
  },
  "categorical_lddt": [
    "logits",
    "bins"
  ],
  "get_axial_mask": [
    "mask"
  ],
  "EsmFoldRelativePosition": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "residue_index",
      "mask"
    ]
  },
  "EsmFoldAngleResnetBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "a"
    ]
  },
  "EsmFoldAngleResnet": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "s",
      "s_initial"
    ]
  },
  "EsmFoldInvariantPointAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "s",
      "z",
      "r",
      "mask",
      "_offload_inference",
      "_z_reference_list"
    ]
  },
  "EsmFoldBackboneUpdate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "s"
    ]
  },
  "EsmFoldStructureModuleTransitionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "s"
    ]
  },
  "EsmFoldStructureModuleTransition": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "s"
    ]
  },
  "EsmFoldStructureModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "evoformer_output_dict",
      "aatype",
      "mask",
      "_offload_inference"
    ],
    "_init_residue_constants": [
      "self",
      "float_dtype",
      "device"
    ],
    "torsion_angles_to_frames": [
      "self",
      "r",
      "alpha",
      "f"
    ],
    "frames_and_literature_positions_to_atom14_pos": [
      "self",
      "r",
      "f"
    ]
  },
  "EsmFoldingTrunk": {
    "__init__": [
      "self",
      "config"
    ],
    "set_chunk_size": [
      "self",
      "chunk_size"
    ],
    "forward": [
      "self",
      "seq_feats",
      "pair_feats",
      "true_aa",
      "residx",
      "mask",
      "no_recycles"
    ],
    "distogram": [
      "coords",
      "min_bin",
      "max_bin",
      "num_bins"
    ]
  },
  "EsmForProteinFolding": {
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "__init__": [
      "self",
      "config"
    ],
    "_af2_to_esm_from_vocab_list": [
      "vocab_list"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "masking_pattern",
      "num_recycles",
      "output_hidden_states"
    ],
    "af2_idx_to_esm_idx": [
      "self",
      "aa",
      "mask"
    ],
    "compute_language_model_representations": [
      "self",
      "esmaa"
    ],
    "bert_mask": [
      "self",
      "aa",
      "esmaa",
      "mask",
      "pattern"
    ],
    "infer": [
      "self",
      "seqs",
      "position_ids"
    ],
    "output_to_pdb": [
      "output"
    ],
    "infer_pdb": [
      "self",
      "seqs"
    ],
    "infer_pdbs": [
      "self",
      "seqs"
    ]
  },
  "_calculate_bin_centers": [
    "boundaries"
  ],
  "_calculate_expected_aligned_error": [
    "alignment_confidence_breaks",
    "aligned_distance_error_probs"
  ],
  "compute_predicted_aligned_error": [
    "logits",
    "max_bin",
    "no_bins"
  ],
  "compute_tm": [
    "logits",
    "residue_weights",
    "max_bin",
    "no_bins",
    "eps"
  ],
  "make_atom14_masks": [
    "protein"
  ],
  "make_atom14_masks_np": [
    "batch"
  ],
  "pseudo_beta_fn": [
    "aatype",
    "all_atom_positions",
    "all_atom_masks"
  ],
  "atom14_to_atom37": [
    "atom14",
    "batch"
  ],
  "build_template_angle_feat": [
    "template_feats"
  ],
  "build_template_pair_feat": [
    "batch",
    "min_bin",
    "max_bin",
    "no_bins",
    "use_unit_vector",
    "eps",
    "inf"
  ],
  "build_extra_msa_feat": [
    "batch"
  ],
  "torsion_angles_to_frames": [
    "r",
    "alpha",
    "aatype",
    "rrgdf"
  ],
  "frames_and_literature_positions_to_atom14_pos": [
    "r",
    "aatype",
    "default_frames",
    "group_idx",
    "atom_mask",
    "lit_positions"
  ],
  "_fetch_dims": [
    "tree"
  ],
  "_flat_idx_to_idx": [
    "flat_idx",
    "dims"
  ],
  "_get_minimal_slice_set": [
    "start",
    "end",
    "dims",
    "start_edges",
    "end_edges"
  ],
  "_chunk_slice": [
    "t",
    "flat_start",
    "flat_end",
    "no_batch_dims"
  ],
  "chunk_layer": [
    "layer",
    "inputs",
    "chunk_size",
    "no_batch_dims",
    "low_mem",
    "_out",
    "_add_into_out"
  ],
  "ChunkSizeTuner": {
    "__init__": [
      "self",
      "max_chunk_size"
    ],
    "_determine_favorable_chunk_size": [
      "self",
      "fn",
      "args",
      "min_chunk_size"
    ],
    "_compare_arg_caches": [
      "self",
      "ac1",
      "ac2"
    ],
    "tune_chunk_size": [
      "self",
      "representative_fn",
      "args",
      "min_chunk_size"
    ]
  },
  "FeatureDict": [],
  "PICO_TO_ANGSTROM": [],
  "Protein": {},
  "from_proteinnet_string": [
    "proteinnet_str"
  ],
  "get_pdb_headers": [
    "prot",
    "chain_id"
  ],
  "add_pdb_headers": [
    "prot",
    "pdb_str"
  ],
  "to_pdb": [
    "prot"
  ],
  "ideal_atom_mask": [
    "prot"
  ],
  "from_prediction": [
    "features",
    "result",
    "b_factors",
    "chain_index",
    "remark",
    "parents",
    "parents_chain_index"
  ],
  "ca_ca": [],
  "Bond": [],
  "BondAngle": [],
  "map_structure_with_atom_order": [
    "in_list",
    "first_call"
  ],
  "load_stereo_chemical_props": [],
  "atom_type_num": [],
  "restype_num": [],
  "unk_restype_index": [],
  "sequence_to_onehot": [
    "sequence",
    "mapping",
    "map_unknown_to_x"
  ],
  "unk_restype": [],
  "_make_standard_atom_mask": [],
  "STANDARD_ATOM_MASK": [],
  "chi_angle_atom": [
    "atom_index"
  ],
  "chi_atom_1_one_hot": [],
  "chi_atom_2_one_hot": [],
  "chi_angles_atom_indices": [],
  "chi_groups_for_atom": [],
  "_make_rigid_transformation_4x4": [
    "ex",
    "ey",
    "translation"
  ],
  "restype_atom37_to_rigid_group": [],
  "restype_atom37_mask": [],
  "restype_atom37_rigid_group_positions": [],
  "restype_atom14_to_rigid_group": [],
  "restype_atom14_mask": [],
  "restype_atom14_rigid_group_positions": [],
  "restype_rigid_group_default_frame": [],
  "_make_rigid_group_constants": [],
  "make_atom14_dists_bounds": [
    "overlap_tolerance",
    "bond_length_tolerance_factor"
  ],
  "restype_atom14_ambiguous_atoms": [],
  "_make_atom14_ambiguity_feats": [],
  "aatype_to_str_sequence": [
    "aatype"
  ],
  "rot_matmul": [
    "a",
    "b"
  ],
  "rot_vec_mul": [
    "r",
    "t"
  ],
  "identity_rot_mats": [
    "batch_dims",
    "dtype",
    "device",
    "requires_grad"
  ],
  "identity_trans": [
    "batch_dims",
    "dtype",
    "device",
    "requires_grad"
  ],
  "identity_quats": [
    "batch_dims",
    "dtype",
    "device",
    "requires_grad"
  ],
  "_to_mat": [
    "pairs"
  ],
  "_QTR_MAT": [],
  "quat_to_rot": [
    "quat"
  ],
  "rot_to_quat": [
    "rot"
  ],
  "_QUAT_MULTIPLY": [],
  "_QUAT_MULTIPLY_BY_VEC": [],
  "_get_quat": [
    "quat_key",
    "dtype",
    "device"
  ],
  "quat_multiply": [
    "quat1",
    "quat2"
  ],
  "quat_multiply_by_vec": [
    "quat",
    "vec"
  ],
  "invert_rot_mat": [
    "rot_mat"
  ],
  "invert_quat": [
    "quat"
  ],
  "Rotation": {
    "__init__": [
      "self",
      "rot_mats",
      "quats",
      "normalize_quats"
    ],
    "identity": [
      "shape",
      "dtype",
      "device",
      "requires_grad",
      "fmt"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "__mul__": [
      "self",
      "right"
    ],
    "__rmul__": [
      "self",
      "left"
    ],
    "shape": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "device": [
      "self"
    ],
    "requires_grad": [
      "self"
    ],
    "get_rot_mats": [
      "self"
    ],
    "get_quats": [
      "self"
    ],
    "get_cur_rot": [
      "self"
    ],
    "compose_q_update_vec": [
      "self",
      "q_update_vec",
      "normalize_quats"
    ],
    "compose_r": [
      "self",
      "r"
    ],
    "compose_q": [
      "self",
      "r",
      "normalize_quats"
    ],
    "apply": [
      "self",
      "pts"
    ],
    "invert_apply": [
      "self",
      "pts"
    ],
    "invert": [
      "self"
    ],
    "unsqueeze": [
      "self",
      "dim"
    ],
    "cat": [
      "rs",
      "dim"
    ],
    "map_tensor_fn": [
      "self",
      "fn"
    ],
    "cuda": [
      "self"
    ],
    "to": [
      "self",
      "device",
      "dtype"
    ],
    "detach": [
      "self"
    ]
  },
  "Rigid": {
    "__init__": [
      "self",
      "rots",
      "trans"
    ],
    "identity": [
      "shape",
      "dtype",
      "device",
      "requires_grad",
      "fmt"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "__mul__": [
      "self",
      "right"
    ],
    "__rmul__": [
      "self",
      "left"
    ],
    "shape": [
      "self"
    ],
    "device": [
      "self"
    ],
    "get_rots": [
      "self"
    ],
    "get_trans": [
      "self"
    ],
    "compose_q_update_vec": [
      "self",
      "q_update_vec"
    ],
    "compose": [
      "self",
      "r"
    ],
    "apply": [
      "self",
      "pts"
    ],
    "invert_apply": [
      "self",
      "pts"
    ],
    "invert": [
      "self"
    ],
    "map_tensor_fn": [
      "self",
      "fn"
    ],
    "to_tensor_4x4": [
      "self"
    ],
    "from_tensor_4x4": [
      "t"
    ],
    "to_tensor_7": [
      "self"
    ],
    "from_tensor_7": [
      "t",
      "normalize_quats"
    ],
    "from_3_points": [
      "p_neg_x_axis",
      "origin",
      "p_xy_plane",
      "eps"
    ],
    "unsqueeze": [
      "self",
      "dim"
    ],
    "cat": [
      "ts",
      "dim"
    ],
    "apply_rot_fn": [
      "self",
      "fn"
    ],
    "apply_trans_fn": [
      "self",
      "fn"
    ],
    "scale_translation": [
      "self",
      "trans_scale_factor"
    ],
    "stop_rot_gradient": [
      "self"
    ],
    "make_transform_from_reference": [
      "n_xyz",
      "ca_xyz",
      "c_xyz",
      "eps"
    ],
    "cuda": [
      "self"
    ]
  },
  "add": [
    "m1",
    "m2",
    "inplace"
  ],
  "masked_mean": [
    "mask",
    "value",
    "dim",
    "eps"
  ],
  "pts_to_distogram": [
    "pts",
    "min_bin",
    "max_bin",
    "no_bins"
  ],
  "one_hot": [
    "x",
    "v_bins"
  ],
  "batched_gather": [
    "data",
    "inds",
    "dim",
    "no_batch_dims"
  ],
  "dict_map": [
    "fn",
    "dic",
    "leaf_type"
  ],
  "tree_map": [
    "fn",
    "tree",
    "leaf_type"
  ],
  "tensor_tree_map": [],
  "LEGACY_INDEX_PATH": [],
  "Index": {
    "get_doc_dicts": [
      "self",
      "doc_ids"
    ],
    "get_top_docs": [
      "self",
      "question_hidden_states",
      "n_docs"
    ],
    "is_initialized": [
      "self"
    ],
    "init_index": [
      "self"
    ]
  },
  "LegacyIndex": {
    "INDEX_FILENAME": [],
    "PASSAGE_FILENAME": [],
    "__init__": [
      "self",
      "vector_size",
      "index_path"
    ],
    "_resolve_path": [
      "self",
      "index_path",
      "filename"
    ],
    "_load_passages": [
      "self"
    ],
    "_deserialize_index": [
      "self"
    ],
    "is_initialized": [
      "self"
    ],
    "init_index": [
      "self"
    ],
    "get_doc_dicts": [
      "self",
      "doc_ids"
    ],
    "get_top_docs": [
      "self",
      "question_hidden_states",
      "n_docs"
    ]
  },
  "HFIndexBase": {
    "__init__": [
      "self",
      "vector_size",
      "dataset",
      "index_initialized"
    ],
    "_check_dataset_format": [
      "self",
      "with_index"
    ],
    "init_index": [
      "self"
    ],
    "is_initialized": [
      "self"
    ],
    "get_doc_dicts": [
      "self",
      "doc_ids"
    ],
    "get_top_docs": [
      "self",
      "question_hidden_states",
      "n_docs"
    ]
  },
  "CanonicalHFIndex": {
    "__init__": [
      "self",
      "vector_size",
      "dataset_name",
      "dataset_split",
      "index_name",
      "index_path",
      "use_dummy_dataset",
      "dataset_revision"
    ],
    "init_index": [
      "self"
    ]
  },
  "CustomHFIndex": {
    "__init__": [
      "self",
      "vector_size",
      "dataset",
      "index_path"
    ],
    "load_from_disk": [
      "cls",
      "vector_size",
      "dataset_path",
      "index_path"
    ],
    "init_index": [
      "self"
    ]
  },
  "RagRetriever": {
    "__init__": [
      "self",
      "config",
      "question_encoder_tokenizer",
      "generator_tokenizer",
      "index",
      "init_retrieval"
    ],
    "_build_index": [
      "config"
    ],
    "from_pretrained": [
      "cls",
      "retriever_name_or_path",
      "indexed_dataset"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ],
    "init_retrieval": [
      "self"
    ],
    "postprocess_docs": [
      "self",
      "docs",
      "input_strings",
      "prefix",
      "n_docs",
      "return_tensors"
    ],
    "_chunk_tensor": [
      "self",
      "t",
      "chunk_size"
    ],
    "_main_retrieve": [
      "self",
      "question_hidden_states",
      "n_docs"
    ],
    "retrieve": [
      "self",
      "question_hidden_states",
      "n_docs"
    ],
    "set_ctx_encoder_tokenizer": [
      "self",
      "ctx_encoder_tokenizer"
    ],
    "__call__": [
      "self",
      "question_input_ids",
      "question_hidden_states",
      "prefix",
      "n_docs",
      "return_tensors"
    ]
  },
  "RagTokenizer": {
    "__init__": [
      "self",
      "question_encoder",
      "generator"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "__call__": [
      "self"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ]
  },
  "RAG_CONFIG_DOC": [],
  "RagConfig": {
    "model_type": [],
    "has_no_defaults_at_init": [],
    "__init__": [
      "self",
      "vocab_size",
      "is_encoder_decoder",
      "prefix",
      "bos_token_id",
      "pad_token_id",
      "eos_token_id",
      "decoder_start_token_id",
      "title_sep",
      "doc_sep",
      "n_docs",
      "max_combined_length",
      "retrieval_vector_size",
      "retrieval_batch_size",
      "dataset",
      "dataset_split",
      "index_name",
      "index_path",
      "passages_path",
      "use_dummy_dataset",
      "reduce_loss",
      "label_smoothing",
      "do_deduplication",
      "exclude_bos_score",
      "do_marginalize",
      "output_retrieved",
      "use_cache",
      "dataset_revision"
    ],
    "from_question_encoder_generator_configs": [
      "cls",
      "question_encoder_config",
      "generator_config"
    ]
  },
  "RetrievAugLMMarginOutput": {},
  "RetrievAugLMOutput": {},
  "RagPreTrainedModel": {
    "base_model_prefix": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "from_pretrained_question_encoder_generator": [
      "cls",
      "question_encoder_pretrained_model_name_or_path",
      "generator_pretrained_model_name_or_path",
      "retriever"
    ]
  },
  "RagModel": {
    "__init__": [
      "self",
      "config",
      "question_encoder",
      "generator",
      "retriever"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_outputs",
      "decoder_input_ids",
      "decoder_attention_mask",
      "past_key_values",
      "doc_scores",
      "context_input_ids",
      "context_attention_mask",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "output_retrieved",
      "n_docs"
    ]
  },
  "RagSequenceForGeneration": {
    "__init__": [
      "self",
      "config",
      "question_encoder",
      "generator",
      "retriever"
    ],
    "set_retriever": [
      "self",
      "retriever"
    ],
    "set_context_encoder_for_training": [
      "self",
      "ctx_encoder"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_outputs",
      "decoder_input_ids",
      "decoder_attention_mask",
      "past_key_values",
      "context_input_ids",
      "context_attention_mask",
      "doc_scores",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "output_retrieved",
      "exclude_bos_score",
      "reduce_loss",
      "labels",
      "n_docs"
    ],
    "retriever": [
      "self"
    ],
    "generator": [
      "self"
    ],
    "question_encoder": [
      "self"
    ],
    "generate": [
      "self",
      "input_ids",
      "attention_mask",
      "context_input_ids",
      "context_attention_mask",
      "doc_scores",
      "do_deduplication",
      "num_return_sequences",
      "num_beams",
      "n_docs"
    ],
    "get_nll": [
      "self",
      "seq_logits",
      "doc_scores",
      "target",
      "reduce_loss",
      "epsilon",
      "exclude_bos_score",
      "n_docs"
    ],
    "_cat_and_pad": [
      "tensors",
      "pad_token_id"
    ]
  },
  "RagTokenForGeneration": {
    "__init__": [
      "self",
      "config",
      "question_encoder",
      "generator",
      "retriever"
    ],
    "set_retriever": [
      "self",
      "retriever"
    ],
    "set_context_encoder_for_training": [
      "self",
      "ctx_encoder"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "decoder_input_ids",
      "past_key_values",
      "attention_mask",
      "use_cache",
      "encoder_outputs",
      "doc_scores",
      "n_docs"
    ],
    "retriever": [
      "self"
    ],
    "generator": [
      "self"
    ],
    "question_encoder": [
      "self"
    ],
    "_reorder_cache": [
      "past_key_values",
      "beam_idx"
    ],
    "marginalize": [
      "self",
      "seq_logits",
      "doc_scores",
      "n_docs"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_outputs",
      "decoder_input_ids",
      "decoder_attention_mask",
      "past_key_values",
      "context_input_ids",
      "context_attention_mask",
      "doc_scores",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "output_retrieved",
      "do_marginalize",
      "reduce_loss",
      "labels",
      "n_docs"
    ],
    "generate": [
      "self",
      "input_ids",
      "attention_mask",
      "context_input_ids",
      "context_attention_mask",
      "doc_scores",
      "n_docs",
      "generation_config",
      "prefix_allowed_tokens_fn",
      "logits_processor",
      "stopping_criteria"
    ],
    "_temporary_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "shift_tokens_right": [
      "self",
      "input_ids",
      "start_token_id"
    ],
    "get_nll": [
      "self",
      "seq_logits",
      "doc_scores",
      "target",
      "reduce_loss",
      "epsilon",
      "n_docs"
    ]
  },
  "GlmAsrRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "GlmAsrAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings"
    ]
  },
  "GlmAsrMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GlmAsrEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings"
    ]
  },
  "GlmAsrPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": []
  },
  "GlmAsrEncoder": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features"
    ]
  },
  "GlmAsrMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_features"
    ]
  },
  "GlmAsrForConditionalGeneration": {
    "_keep_in_fp32_modules_strict": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "get_audio_features": [
      "self",
      "input_features",
      "input_features_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "input_features_mask",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ]
  },
  "GlmAsrEncoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rope_parameters",
      "attention_dropout",
      "num_mel_bins"
    ]
  },
  "GlmAsrConfig": {
    "model_type": [],
    "sub_configs": [],
    "_default_text_config_kwargs": [],
    "__init__": [
      "self",
      "audio_config",
      "text_config",
      "audio_token_id",
      "projector_hidden_act"
    ]
  },
  "GlmAsrProcessorKwargs": {
    "_defaults": []
  },
  "GlmAsrProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer",
      "chat_template",
      "audio_token",
      "default_transcription_prompt",
      "max_audio_len"
    ],
    "_get_audio_token_length": [
      "self",
      "audio_lengths"
    ],
    "__call__": [
      "self",
      "text",
      "audio",
      "output_labels"
    ],
    "model_input_names": [
      "self"
    ],
    "apply_transcription_request": [
      "self",
      "audio",
      "prompt"
    ],
    "batch_decode": [
      "self"
    ],
    "_strip_assistant_prefix_and_quotes": [
      "self",
      "text"
    ]
  },
  "GitVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range"
    ]
  },
  "GitConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "use_cache",
      "tie_word_embeddings",
      "bos_token_id",
      "eos_token_id",
      "num_image_with_embedding"
    ]
  },
  "GitVisionModelOutput": {},
  "GitEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "GitSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "GitSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "GIT_SELF_ATTENTION_CLASSES": [],
  "GitAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position",
      "output_attentions"
    ]
  },
  "GitIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GitOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "GitLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "GitEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "GitPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GitVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "GitVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GitVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "GitVisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions"
    ]
  },
  "GitVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "causal_attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GitVisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "GitVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "GitProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "embeddings"
    ]
  },
  "GitModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "pixel_values",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict",
      "cache_position"
    ]
  },
  "GitForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "pixel_values",
      "inputs_embeds",
      "labels",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict",
      "logits_to_keep",
      "cache_position"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "pixel_values",
      "attention_mask",
      "use_cache",
      "cache_position",
      "is_first_iteration"
    ]
  },
  "GitProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "InstructBlipProcessorKwargs": {
    "_defaults": []
  },
  "InstructBlipProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "qformer_tokenizer",
      "num_query_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "InstructBlipForConditionalGenerationModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "InstructBlipVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "InstructBlipAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "InstructBlipEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds"
    ]
  },
  "InstructBlipVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_input_embeddings": [
      "self"
    ]
  },
  "InstructBlipQFormerMultiHeadAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention"
    ],
    "save_attn_gradients": [
      "self",
      "attn_gradients"
    ],
    "get_attn_gradients": [
      "self"
    ],
    "save_attention_map": [
      "self",
      "attention_map"
    ],
    "get_attention_map": [
      "self"
    ],
    "transpose_for_scores": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "InstructBlipQFormerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "InstructBlipQFormerAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "InstructBlipQFormerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "InstructBlipQFormerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "InstructBlipQFormerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "query_length"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ],
    "feed_forward_chunk_query": [
      "self",
      "attention_output"
    ]
  },
  "InstructBlipQFormerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "query_length"
    ]
  },
  "InstructBlipQFormerEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "query_embeds",
      "past_key_values_length"
    ]
  },
  "InstructBlipQFormerModel": {
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_extended_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "device",
      "has_query"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "query_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "InstructBlipModel": {
    "main_input_name": [],
    "_keep_in_fp32_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "_preprocess_accelerate": [
      "self"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "inputs_embeds",
      "interpolate_pos_encoding"
    ]
  },
  "InstructBlipForConditionalGeneration": {
    "main_input_name": [],
    "_can_compile_fullgraph": [],
    "_keep_in_fp32_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_encoder": [
      "self",
      "modality"
    ],
    "get_decoder": [
      "self"
    ],
    "_preprocess_accelerate": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "interpolate_pos_encoding"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "inputs_embeds",
      "labels",
      "interpolate_pos_encoding"
    ],
    "generate": [
      "self",
      "pixel_values",
      "qformer_input_ids",
      "qformer_attention_mask",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "interpolate_pos_encoding"
    ]
  },
  "InstructBlipVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "qkv_bias"
    ]
  },
  "InstructBlipQFormerConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "cross_attention_frequency",
      "encoder_hidden_size"
    ]
  },
  "InstructBlipConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "qformer_config",
      "text_config",
      "num_query_tokens",
      "image_token_index"
    ]
  },
  "ModalitySizeType": [],
  "PreprocessorOutputType": [],
  "PreprocessorType": [],
  "PostprocessorType": [],
  "PerceiverModelOutput": {},
  "PerceiverDecoderOutput": {},
  "PerceiverMaskedLMOutput": {},
  "PerceiverClassifierOutput": {},
  "PerceiverEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "batch_size"
    ]
  },
  "PerceiverSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "qk_channels",
      "v_channels",
      "num_heads",
      "q_dim",
      "kv_dim"
    ],
    "transpose_for_scores": [
      "self",
      "x",
      "channels_per_head"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "inputs",
      "inputs_mask",
      "output_attentions"
    ]
  },
  "PerceiverSelfOutput": {
    "__init__": [
      "self",
      "config",
      "input_channels",
      "output_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PerceiverAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "qk_channels",
      "v_channels",
      "num_heads",
      "q_dim",
      "kv_dim",
      "use_query_residual"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "inputs",
      "inputs_mask",
      "output_attentions"
    ]
  },
  "PerceiverMLP": {
    "__init__": [
      "self",
      "config",
      "input_size",
      "widening_factor"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PerceiverLayer": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention",
      "qk_channels",
      "v_channels",
      "num_heads",
      "q_dim",
      "kv_dim",
      "widening_factor",
      "use_query_residual"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "inputs",
      "inputs_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "PerceiverEncoder": {
    "__init__": [
      "self",
      "config",
      "kv_dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "inputs",
      "inputs_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PerceiverPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PerceiverModel": {
    "__init__": [
      "self",
      "config",
      "decoder",
      "input_preprocessor",
      "output_postprocessor"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "subsampled_output_points",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "PerceiverForMaskedLM": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict",
      "input_ids"
    ]
  },
  "PerceiverForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict",
      "input_ids"
    ]
  },
  "PerceiverForImageClassificationLearned": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "interpolate_pos_encoding",
      "return_dict",
      "pixel_values"
    ]
  },
  "PerceiverForImageClassificationFourier": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict",
      "pixel_values"
    ]
  },
  "PerceiverForImageClassificationConvProcessing": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict",
      "pixel_values"
    ]
  },
  "PerceiverForOpticalFlow": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "PerceiverForMultimodalAutoencoding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "attention_mask",
      "subsampled_output_points",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "PerceiverAbstractDecoder": {
    "decoder_query": [
      "self",
      "inputs",
      "modality_sizes",
      "inputs_without_pos",
      "subsampled_points"
    ],
    "num_query_channels": [
      "self"
    ],
    "forward": [
      "self",
      "query",
      "z",
      "query_mask"
    ]
  },
  "PerceiverProjectionDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "decoder_query": [
      "self",
      "inputs",
      "modality_sizes",
      "inputs_without_pos",
      "subsampled_points"
    ],
    "forward": [
      "self",
      "query",
      "z",
      "query_mask"
    ]
  },
  "PerceiverBasicDecoder": {
    "__init__": [
      "self",
      "config",
      "output_num_channels",
      "position_encoding_type",
      "output_index_dims",
      "num_channels",
      "subsampled_index_dims",
      "qk_channels",
      "v_channels",
      "num_heads",
      "widening_factor",
      "use_query_residual",
      "concat_preprocessed_input",
      "final_project",
      "position_encoding_only"
    ],
    "num_query_channels": [
      "self"
    ],
    "decoder_query": [
      "self",
      "inputs",
      "modality_sizes",
      "inputs_without_pos",
      "subsampled_points"
    ],
    "forward": [
      "self",
      "query",
      "z",
      "query_mask",
      "output_attentions"
    ]
  },
  "PerceiverClassificationDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "num_query_channels": [
      "self"
    ],
    "decoder_query": [
      "self",
      "inputs",
      "modality_sizes",
      "inputs_without_pos",
      "subsampled_points"
    ],
    "forward": [
      "self",
      "query",
      "z",
      "query_mask",
      "output_attentions"
    ]
  },
  "PerceiverOpticalFlowDecoder": {
    "__init__": [
      "self",
      "config",
      "output_image_shape",
      "output_num_channels",
      "rescale_factor"
    ],
    "num_query_channels": [
      "self"
    ],
    "decoder_query": [
      "self",
      "inputs",
      "modality_sizes",
      "inputs_without_pos",
      "subsampled_points"
    ],
    "forward": [
      "self",
      "query",
      "z",
      "query_mask",
      "output_attentions"
    ]
  },
  "PerceiverBasicVideoAutoencodingDecoder": {
    "__init__": [
      "self",
      "config",
      "output_shape",
      "position_encoding_type"
    ],
    "num_query_channels": [
      "self"
    ],
    "decoder_query": [
      "self",
      "inputs",
      "modality_sizes",
      "inputs_without_pos",
      "subsampled_points"
    ],
    "forward": [
      "self",
      "query",
      "z",
      "query_mask"
    ]
  },
  "restructure": [
    "modality_sizes",
    "inputs"
  ],
  "PerceiverMultimodalDecoder": {
    "__init__": [
      "self",
      "config",
      "modalities",
      "num_outputs",
      "output_num_channels",
      "min_padding_size",
      "subsampled_index_dims"
    ],
    "num_query_channels": [
      "self"
    ],
    "decoder_query": [
      "self",
      "inputs",
      "modality_sizes",
      "inputs_without_pos",
      "subsampled_points"
    ],
    "forward": [
      "self",
      "query",
      "z",
      "query_mask",
      "output_attentions"
    ]
  },
  "space_to_depth": [
    "frames",
    "temporal_block_size",
    "spatial_block_size"
  ],
  "Conv2dSamePadding": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "Conv2DDownsample": {
    "__init__": [
      "self",
      "num_layers",
      "in_channels",
      "out_channels",
      "use_batchnorm"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "generate_fourier_features": [
    "pos",
    "num_bands",
    "max_resolution",
    "concat_pos",
    "sine_only"
  ],
  "build_linear_positions": [
    "index_dims",
    "output_range"
  ],
  "PerceiverAbstractPositionEncoding": {
    "num_dimensions": [
      "self"
    ],
    "output_size": [
      "self"
    ],
    "forward": [
      "self",
      "batch_size",
      "pos"
    ]
  },
  "PerceiverTrainablePositionEncoding": {
    "__init__": [
      "self",
      "index_dims",
      "num_channels"
    ],
    "num_dimensions": [
      "self"
    ],
    "output_size": [
      "self"
    ],
    "interpolate_pos_encoding": [
      "self",
      "position_embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "batch_size",
      "interpolate_pos_encoding",
      "input_size"
    ]
  },
  "_check_or_build_spatial_positions": [
    "pos",
    "index_dims",
    "batch_size"
  ],
  "PerceiverFourierPositionEncoding": {
    "__init__": [
      "self",
      "num_bands",
      "max_resolution",
      "concat_pos",
      "sine_only"
    ],
    "num_dimensions": [
      "self"
    ],
    "output_size": [
      "self"
    ],
    "forward": [
      "self",
      "index_dims",
      "batch_size",
      "device",
      "dtype",
      "pos"
    ]
  },
  "AbstractPreprocessor": {
    "num_channels": [
      "self"
    ]
  },
  "PerceiverTextPreprocessor": {
    "__init__": [
      "self",
      "config"
    ],
    "num_channels": [
      "self"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "network_input_is_1d",
      "interpolate_pos_encoding"
    ]
  },
  "PerceiverEmbeddingDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "embedding_layer"
    ]
  },
  "PerceiverMultimodalPostprocessor": {
    "__init__": [
      "self",
      "modalities",
      "input_is_dict"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "modality_sizes"
    ]
  },
  "PerceiverClassificationPostprocessor": {
    "__init__": [
      "self",
      "config",
      "in_channels"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "modality_sizes"
    ]
  },
  "PerceiverAudioPostprocessor": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "postproc_type"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "modality_sizes"
    ]
  },
  "PerceiverProjectionPostprocessor": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "modality_sizes"
    ]
  },
  "PerceiverImagePreprocessor": {
    "__init__": [
      "self",
      "config",
      "prep_type",
      "spatial_downsample",
      "temporal_downsample",
      "position_encoding_type",
      "in_channels",
      "out_channels",
      "conv_after_patching",
      "conv_after_patching_in_channels",
      "conv2d_use_batchnorm",
      "concat_or_add_pos",
      "project_pos_dim"
    ],
    "num_channels": [
      "self"
    ],
    "_build_network_inputs": [
      "self",
      "inputs",
      "network_input_is_1d",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "network_input_is_1d",
      "interpolate_pos_encoding"
    ]
  },
  "PerceiverOneHotPreprocessor": {
    "__init__": [
      "self",
      "config"
    ],
    "num_channels": [
      "self"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "network_input_is_1d"
    ]
  },
  "PerceiverAudioPreprocessor": {
    "__init__": [
      "self",
      "config",
      "prep_type",
      "samples_per_patch",
      "position_encoding_type",
      "concat_or_add_pos",
      "out_channels",
      "project_pos_dim"
    ],
    "num_channels": [
      "self"
    ],
    "_build_network_inputs": [
      "self",
      "inputs"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "network_input_is_1d",
      "interpolate_pos_encoding"
    ]
  },
  "PerceiverMultimodalPreprocessor": {
    "__init__": [
      "self",
      "modalities",
      "mask_probs",
      "min_padding_size"
    ],
    "num_channels": [
      "self"
    ],
    "forward": [
      "self",
      "inputs",
      "pos",
      "network_input_is_1d",
      "interpolate_pos_encoding"
    ]
  },
  "PerceiverImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_center_crop",
      "crop_size",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "center_crop": [
      "self",
      "image",
      "crop_size",
      "size",
      "data_format",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_center_crop",
      "crop_size",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "PerceiverConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_latents",
      "d_latents",
      "d_model",
      "num_blocks",
      "num_self_attends_per_block",
      "num_self_attention_heads",
      "num_cross_attention_heads",
      "qk_channels",
      "v_channels",
      "cross_attention_shape_for_attention",
      "self_attention_widening_factor",
      "cross_attention_widening_factor",
      "hidden_act",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "use_query_residual",
      "vocab_size",
      "max_position_embeddings",
      "image_size",
      "train_size",
      "num_frames",
      "audio_samples_per_frame",
      "samples_per_patch",
      "output_shape",
      "output_num_channels",
      "_label_trainable_num_channels"
    ]
  },
  "PerceiverTokenizer": {
    "model_input_names": [],
    "__init__": [
      "self",
      "pad_token",
      "bos_token",
      "eos_token",
      "mask_token",
      "cls_token",
      "sep_token",
      "model_max_length"
    ],
    "get_vocab": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "PerceiverImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "center_crop": [
      "self",
      "image",
      "crop_size",
      "size"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "Qwen3NextRMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "Qwen3NextDynamicCache": {
    "is_compileable": [],
    "__init__": [
      "self",
      "config"
    ],
    "__len__": [
      "self"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position",
      "layer_idx"
    ],
    "has_previous_state": [
      "self"
    ]
  },
  "Qwen3NextRotaryEmbedding": {
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ]
  },
  "Qwen3NextRMSNorm": {},
  "Qwen3NextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "torch_causal_conv1d_update": [
    "hidden_states",
    "conv_state",
    "weight",
    "bias",
    "activation"
  ],
  "l2norm": [
    "x",
    "dim",
    "eps"
  ],
  "torch_chunk_gated_delta_rule": [
    "query",
    "key",
    "value",
    "g",
    "beta",
    "chunk_size",
    "initial_state",
    "output_final_state",
    "use_qk_l2norm_in_kernel"
  ],
  "torch_recurrent_gated_delta_rule": [
    "query",
    "key",
    "value",
    "g",
    "beta",
    "initial_state",
    "output_final_state",
    "use_qk_l2norm_in_kernel"
  ],
  "Qwen3NextGatedDeltaNet": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "fix_query_key_value_ordering": [
      "self",
      "mixed_qkvz",
      "mixed_ba"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "Qwen3NextMLP": {},
  "Qwen3NextExperts": {},
  "Qwen3NextSparseMoeBlock": {},
  "Qwen3NextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "Qwen3NextPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_can_record_outputs": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen3NextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ],
    "_update_linear_attn_mask": [
      "self",
      "attention_mask",
      "cache_position"
    ]
  },
  "Qwen3NextForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Qwen3NextForSequenceClassification": {},
  "Qwen3NextForTokenClassification": {},
  "Qwen3NextForQuestionAnswering": {},
  "Qwen3NextTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3NextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "head_dim",
      "linear_conv_kernel_dim",
      "linear_key_head_dim",
      "linear_value_head_dim",
      "linear_num_key_heads",
      "linear_num_value_heads",
      "decoder_sparse_step",
      "moe_intermediate_size",
      "shared_expert_intermediate_size",
      "num_experts_per_tok",
      "num_experts",
      "norm_topk_prob",
      "output_router_logits",
      "router_aux_loss_coef",
      "mlp_only_layers",
      "layer_types",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "mra_cuda_kernel": [],
  "sparse_max": [
    "sparse_qk_prod",
    "indices",
    "query_num_block",
    "key_num_block"
  ],
  "sparse_mask": [
    "mask",
    "indices",
    "block_size"
  ],
  "mm_to_sparse": [
    "dense_query",
    "dense_key",
    "indices",
    "block_size"
  ],
  "sparse_dense_mm": [
    "sparse_query",
    "indices",
    "dense_key",
    "query_num_block",
    "block_size"
  ],
  "transpose_indices": [
    "indices",
    "dim_1_block",
    "dim_2_block"
  ],
  "MraSampledDenseMatMul": {
    "forward": [
      "ctx",
      "dense_query",
      "dense_key",
      "indices",
      "block_size"
    ],
    "backward": [
      "ctx",
      "grad"
    ],
    "operator_call": [
      "dense_query",
      "dense_key",
      "indices",
      "block_size"
    ]
  },
  "MraSparseDenseMatMul": {
    "forward": [
      "ctx",
      "sparse_query",
      "indices",
      "dense_key",
      "query_num_block"
    ],
    "backward": [
      "ctx",
      "grad"
    ],
    "operator_call": [
      "sparse_query",
      "indices",
      "dense_key",
      "query_num_block"
    ]
  },
  "MraReduceSum": {
    "operator_call": [
      "sparse_query",
      "indices",
      "query_num_block",
      "key_num_block"
    ]
  },
  "get_low_resolution_logit": [
    "query",
    "key",
    "block_size",
    "mask",
    "value"
  ],
  "get_block_idxes": [
    "low_resolution_logit",
    "num_blocks",
    "approx_mode",
    "initial_prior_first_n_blocks",
    "initial_prior_diagonal_n_blocks"
  ],
  "mra2_attention": [
    "query",
    "key",
    "value",
    "mask",
    "num_blocks",
    "approx_mode",
    "block_size",
    "initial_prior_first_n_blocks",
    "initial_prior_diagonal_n_blocks"
  ],
  "MraEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "MraSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "MraSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MraAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "MraIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MraOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MraLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "MraEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MraPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MraLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MraOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "MraPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MraModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MraForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MraClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MraForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MraForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MraForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MraForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MraConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "block_per_row",
      "approx_mode",
      "initial_prior_first_n_blocks",
      "initial_prior_diagonal_n_blocks",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "VivitConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "num_frames",
      "tubelet_size",
      "num_channels",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "qkv_bias"
    ]
  },
  "VivitImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "offset",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "rescale": [
      "self",
      "image",
      "scale",
      "offset",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "offset",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "videos",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "offset",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "VivitTubeletEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "VivitEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "VivitSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VivitSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "VivitAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VivitIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VivitOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "VivitLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VivitEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VivitPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VivitPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VivitModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "VivitForVideoClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "interpolate_pos_encoding"
    ]
  },
  "MobileViTV2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "image_size",
      "patch_size",
      "expand_ratio",
      "hidden_act",
      "conv_kernel_size",
      "output_stride",
      "classifier_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "aspp_out_channels",
      "atrous_rates",
      "aspp_dropout_prob",
      "semantic_loss_ignore_index",
      "n_attn_blocks",
      "base_attn_unit_dims",
      "width_multiplier",
      "ffn_multiplier",
      "attn_dropout",
      "ffn_dropout"
    ]
  },
  "make_divisible": [
    "value",
    "divisor",
    "min_value"
  ],
  "clip": [
    "value",
    "min_val",
    "max_val"
  ],
  "MobileViTV2ConvLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "bias",
      "dilation",
      "use_normalization",
      "use_activation"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTV2InvertedResidual": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "dilation"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTV2MobileNetLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "num_stages"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTV2LinearSelfAttention": {
    "__init__": [
      "self",
      "config",
      "embed_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTV2FFN": {
    "__init__": [
      "self",
      "config",
      "embed_dim",
      "ffn_latent_dim",
      "ffn_dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTV2TransformerLayer": {
    "__init__": [
      "self",
      "config",
      "embed_dim",
      "ffn_latent_dim",
      "dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTV2Transformer": {
    "__init__": [
      "self",
      "config",
      "n_layers",
      "d_model"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTV2Layer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "attn_unit_dim",
      "n_attn_blocks",
      "dilation",
      "stride"
    ],
    "unfolding": [
      "self",
      "feature_map"
    ],
    "folding": [
      "self",
      "patches",
      "output_size"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTV2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileViTV2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MobileViTV2Model": {
    "__init__": [
      "self",
      "config",
      "expand_output"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileViTV2ForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "MobileViTV2ASPPPooling": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTV2ASPP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTV2DeepLabV3": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTV2ForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MusicgenProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "get_decoder_prompt_ids": [
      "self",
      "task",
      "language",
      "no_timestamps"
    ],
    "__call__": [
      "self"
    ],
    "batch_decode": [
      "self"
    ],
    "_decode_audio": [
      "self",
      "audio_values",
      "padding_mask"
    ]
  },
  "MusicgenDecoderConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "num_hidden_layers",
      "ffn_dim",
      "num_attention_heads",
      "layerdrop",
      "use_cache",
      "activation_function",
      "hidden_size",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "initializer_factor",
      "scale_embedding",
      "num_codebooks",
      "audio_channels",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "is_decoder",
      "add_cross_attention",
      "cross_attention_hidden_size"
    ]
  },
  "MusicgenConfig": {
    "model_type": [],
    "sub_configs": [],
    "has_no_defaults_at_init": [],
    "__init__": [
      "self",
      "text_encoder",
      "audio_encoder",
      "decoder"
    ],
    "sampling_rate": [
      "self"
    ]
  },
  "MusicgenUnconditionalInput": {},
  "MusicgenSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length"
    ]
  },
  "MusicgenAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "MusicgenDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MusicgenPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MusicgenDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "_update_cross_attn_mask": [
      "self",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "input_shape",
      "inputs_embeds"
    ]
  },
  "MusicgenModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MusicgenForCausalLM": {
    "output_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "delay_pattern_mask",
      "guidance_scale"
    ],
    "build_delay_pattern_mask": [
      "self",
      "input_ids",
      "pad_token_id",
      "max_length"
    ],
    "apply_delay_pattern_mask": [
      "input_ids",
      "decoder_pad_token_mask"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "synced_gpus",
      "streamer"
    ]
  },
  "MusicgenForConditionalGeneration": {
    "output_modalities": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "__init__": [
      "self",
      "config",
      "text_encoder",
      "audio_encoder",
      "decoder"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "from_sub_models_pretrained": [
      "cls",
      "text_encoder_pretrained_model_name_or_path",
      "audio_encoder_pretrained_model_name_or_path",
      "decoder_pretrained_model_name_or_path"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "input_values",
      "padding_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "decoder_input_ids",
      "past_key_values",
      "attention_mask",
      "decoder_attention_mask",
      "use_cache",
      "encoder_outputs",
      "decoder_delay_pattern_mask",
      "guidance_scale",
      "cache_position"
    ],
    "_prepare_decoder_input_ids_for_generation": [
      "self",
      "batch_size",
      "model_input_name",
      "model_kwargs",
      "decoder_start_token_id",
      "bos_token_id",
      "device"
    ],
    "_prepare_text_encoder_kwargs_for_generation": [
      "self",
      "inputs_tensor",
      "model_kwargs",
      "model_input_name",
      "generation_config"
    ],
    "_prepare_audio_encoder_kwargs_for_generation": [
      "self",
      "input_values",
      "model_kwargs",
      "model_input_name"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ],
    "resize_token_embeddings": [
      "self"
    ],
    "freeze_audio_encoder": [
      "self"
    ],
    "freeze_text_encoder": [
      "self"
    ],
    "_maybe_initialize_input_ids_for_generation": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "_get_decoder_start_token_id": [
      "self",
      "decoder_start_token_id",
      "bos_token_id"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "synced_gpus",
      "streamer"
    ],
    "get_unconditional_inputs": [
      "self",
      "num_samples"
    ]
  },
  "LevitConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "num_channels",
      "kernel_size",
      "stride",
      "padding",
      "patch_size",
      "hidden_sizes",
      "num_attention_heads",
      "depths",
      "key_dim",
      "drop_path_rate",
      "mlp_ratio",
      "attention_ratio",
      "initializer_range"
    ]
  },
  "LevitForImageClassificationWithTeacherOutput": {},
  "LevitConvEmbeddings": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "groups",
      "bn_weight_init"
    ],
    "forward": [
      "self",
      "embeddings"
    ]
  },
  "LevitPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "MLPLayerWithBN": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "bn_weight_init"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitSubsample": {
    "__init__": [
      "self",
      "stride",
      "resolution"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitAttention": {
    "__init__": [
      "self",
      "hidden_sizes",
      "key_dim",
      "num_attention_heads",
      "attention_ratio",
      "resolution"
    ],
    "train": [
      "self",
      "mode"
    ],
    "get_attention_biases": [
      "self",
      "device"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitAttentionSubsample": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "key_dim",
      "num_attention_heads",
      "attention_ratio",
      "stride",
      "resolution_in",
      "resolution_out"
    ],
    "train": [
      "self",
      "mode"
    ],
    "get_attention_biases": [
      "self",
      "device"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitMLPLayer": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitResidualLayer": {
    "__init__": [
      "self",
      "module",
      "drop_rate"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitStage": {
    "__init__": [
      "self",
      "config",
      "idx",
      "hidden_sizes",
      "key_dim",
      "depths",
      "num_attention_heads",
      "attention_ratio",
      "mlp_ratio",
      "down_ops",
      "resolution_in"
    ],
    "get_resolution": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LevitClassificationLayer": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "LevitPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LevitModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LevitForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LevitForImageClassificationWithTeacher": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LevitImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "LevitImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ]
  },
  "SeamlessM4Tv2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "t2u_vocab_size",
      "char_vocab_size",
      "hidden_size",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "max_position_embeddings",
      "is_encoder_decoder",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "activation_function",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "scale_embedding",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "decoder_start_token_id",
      "max_new_tokens",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "speech_encoder_layers",
      "speech_encoder_attention_heads",
      "speech_encoder_intermediate_size",
      "speech_encoder_hidden_act",
      "speech_encoder_dropout",
      "add_adapter",
      "speech_encoder_layerdrop",
      "feature_projection_input_dim",
      "adaptor_kernel_size",
      "adaptor_stride",
      "adaptor_dropout",
      "num_adapter_layers",
      "position_embeddings_type",
      "conv_depthwise_kernel_size",
      "left_max_position_embeddings",
      "right_max_position_embeddings",
      "speech_encoder_chunk_size",
      "speech_encoder_left_chunk_num",
      "t2u_bos_token_id",
      "t2u_pad_token_id",
      "t2u_eos_token_id",
      "t2u_encoder_layers",
      "t2u_encoder_ffn_dim",
      "t2u_encoder_attention_heads",
      "t2u_decoder_layers",
      "t2u_decoder_ffn_dim",
      "t2u_decoder_attention_heads",
      "t2u_max_position_embeddings",
      "t2u_variance_predictor_embed_dim",
      "t2u_variance_predictor_hidden_dim",
      "t2u_variance_predictor_kernel_size",
      "t2u_variance_pred_dropout",
      "sampling_rate",
      "upsample_initial_channel",
      "upsample_rates",
      "upsample_kernel_sizes",
      "resblock_kernel_sizes",
      "resblock_dilation_sizes",
      "leaky_relu_slope",
      "unit_hifi_gan_vocab_size",
      "unit_embed_dim",
      "lang_embed_dim",
      "spkr_embed_dim",
      "vocoder_num_langs",
      "vocoder_num_spkrs",
      "variance_predictor_kernel_size",
      "var_pred_dropout",
      "vocoder_offset",
      "tie_word_embeddings"
    ]
  },
  "SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS": [],
  "SeamlessM4Tv2GenerationOutput": {},
  "SeamlessM4Tv2TextToUnitDecoderOutput": {},
  "SeamlessM4Tv2TextToUnitOutput": {},
  "SeamlessM4Tv2ConformerFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4Tv2ConformerFeedForward": {
    "__init__": [
      "self",
      "config",
      "act_fn",
      "dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4Tv2ConformerConvolutionModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SeamlessM4Tv2ConformerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "use_position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SeamlessM4Tv2ConformerEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "conv_attention_mask"
    ]
  },
  "SeamlessM4Tv2ConformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_apply_chunk_attention": [
      "self",
      "attention_mask",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4Tv2ConformerAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_sub_sample_lengths_from_attention_mask": [
      "self",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SeamlessM4Tv2ConformerAdapter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SeamlessM4Tv2ScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "SeamlessM4Tv2SinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "past_key_values_length",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "SeamlessM4Tv2Attention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "SeamlessM4Tv2FeedForwardNetwork": {
    "__init__": [
      "self",
      "config",
      "ffn_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SeamlessM4Tv2EncoderLayer": {
    "__init__": [
      "self",
      "config",
      "encoder_ffn_dim",
      "encoder_attention_heads"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "SeamlessM4Tv2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "SeamlessM4Tv2TextToUnitDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "decoder_ffn_dim",
      "decoder_attention_heads"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "padding_mask",
      "output_attentions"
    ]
  },
  "SeamlessM4Tv2PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_compute_sub_sample_lengths_from_attention_mask": [
      "self",
      "attention_mask"
    ],
    "_indices_to_subwords": [
      "self",
      "input_ids"
    ],
    "_count_character_length_in_subword": [
      "self",
      "input_ids",
      "subwords_batch",
      "merge_space_with_prev_subword",
      "pad_token_id",
      "unk_token_id",
      "space"
    ],
    "_get_char_input_ids": [
      "self",
      "input_ids",
      "subwords_batch",
      "char_count_per_id",
      "pad_token_id",
      "unk_token_id"
    ],
    "_hard_upsample": [
      "self",
      "hidden_states",
      "durations"
    ]
  },
  "SeamlessM4Tv2SpeechEncoder": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4Tv2Encoder": {
    "__init__": [
      "self",
      "config",
      "embed_tokens",
      "is_t2u_encoder"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4Tv2Decoder": {
    "__init__": [
      "self",
      "config",
      "embed_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SeamlessM4Tv2TextToUnitDecoder": {
    "__init__": [
      "self",
      "config",
      "embed_tokens"
    ],
    "forward": [
      "self",
      "char_input_ids",
      "char_count_per_id",
      "encoder_hidden_states",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4Tv2TextToUnitModel": {
    "__init__": [
      "self",
      "config",
      "embed_tokens_decoder"
    ],
    "forward": [
      "self",
      "input_ids",
      "char_input_ids",
      "char_count_per_id",
      "attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4Tv2TextToUnitForConditionalGeneration": {
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config",
      "embed_tokens_decoder"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "char_input_ids",
      "char_count_per_id",
      "attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SeamlessM4Tv2VariancePredictor": {
    "__init__": [
      "self",
      "embed_dim",
      "hidden_dim",
      "kernel_size",
      "var_pred_dropout"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask"
    ]
  },
  "SeamlessM4Tv2HifiGan": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_embeds"
    ]
  },
  "SeamlessM4Tv2CodeHifiGan": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "_get_dur_output_lengths": [
      "self",
      "input_ids",
      "dur_out"
    ],
    "_get_output_hifigan_lengths": [
      "self",
      "input_lengths"
    ],
    "forward": [
      "self",
      "input_ids",
      "speaker_id",
      "lang_id"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "SeamlessM4Tv2ForTextToText": {
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_ids",
      "tgt_lang",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus"
    ]
  },
  "SeamlessM4Tv2ForSpeechToText": {
    "input_modalities": [],
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_features",
      "tgt_lang",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus"
    ]
  },
  "SeamlessM4Tv2ForTextToSpeech": {
    "output_modalities": [],
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "generate": [
      "self",
      "input_ids",
      "return_intermediate_token_ids",
      "tgt_lang",
      "speaker_id"
    ]
  },
  "SeamlessM4Tv2ForSpeechToSpeech": {
    "input_modalities": [],
    "output_modalities": [],
    "_keys_to_ignore_on_load_missing": [],
    "main_input_name": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_encoder": [
      "self"
    ],
    "get_decoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_features",
      "return_intermediate_token_ids",
      "tgt_lang",
      "speaker_id"
    ]
  },
  "SeamlessM4Tv2Model": {
    "input_modalities": [],
    "output_modalities": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config",
      "current_modality"
    ],
    "set_modality": [
      "self",
      "modality"
    ],
    "get_encoder": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "generate": [
      "self",
      "input_ids",
      "input_features",
      "return_intermediate_token_ids",
      "tgt_lang",
      "speaker_id",
      "generate_speech"
    ]
  },
  "MarianSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "create_weight": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "MarianAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "MarianEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "MarianDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MarianPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "MarianEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MarianDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MarianModel": {
    "_keys_to_ignore_on_load_missing": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_decoder_input_embeddings": [
      "self"
    ],
    "set_decoder_input_embeddings": [
      "self",
      "value"
    ],
    "resize_decoder_token_embeddings": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MarianMTModel": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_missing": [],
    "_keys_to_ignore_on_save": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of"
    ],
    "resize_decoder_token_embeddings": [
      "self",
      "new_num_tokens"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "MarianDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "MarianForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MarianTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "source_spm",
      "target_spm",
      "vocab",
      "target_vocab_file",
      "source_lang",
      "target_lang",
      "unk_token",
      "eos_token",
      "pad_token",
      "model_max_length",
      "sp_model_kwargs",
      "separate_vocabs"
    ],
    "_setup_normalizer": [
      "self"
    ],
    "normalize": [
      "self",
      "x"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "remove_language_code": [
      "self",
      "text"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "batch_decode": [
      "self",
      "sequences"
    ],
    "decode": [
      "self",
      "token_ids"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "get_vocab": [
      "self"
    ],
    "get_src_vocab": [
      "self"
    ],
    "get_tgt_vocab": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ],
    "num_special_tokens_to_add": [
      "self"
    ],
    "_special_token_mask": [
      "self",
      "seq"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ]
  },
  "load_spm": [
    "path",
    "sp_model_kwargs"
  ],
  "save_json": [
    "data",
    "path"
  ],
  "load_json": [
    "path"
  ],
  "MarianConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "decoder_vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "forced_eos_token_id",
      "share_encoder_decoder_embeddings",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "EdgeTamVideoLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "EdgeTamVideoMemoryFuserCXBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EdgeTamVideoVisionEncoderOutput": {},
  "EdgeTamVideoVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "end_x",
      "end_y"
    ],
    "forward": [
      "self"
    ],
    "create_inv_freq": [
      "self"
    ]
  },
  "EdgeTamVideoAttention": {
    "__init__": [
      "self",
      "config",
      "downsample_rate"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_similarity"
    ]
  },
  "apply_rotary_pos_emb_2d_self_attn": [
    "q",
    "k",
    "cos",
    "sin"
  ],
  "EdgeTamVideoRoPESelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "position_embeddings"
    ]
  },
  "apply_rotary_pos_emb_2d_cross_attn": [
    "q",
    "k",
    "cos",
    "sin",
    "cos_k",
    "sin_k",
    "num_k_exclude_rope",
    "repeat_freqs_k"
  ],
  "EdgeTamVideoRoPECrossAttention": {
    "__init__": [
      "self",
      "config",
      "kv_in_dim"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "position_embeddings",
      "position_embeddings_k",
      "num_k_exclude_rope",
      "rope_k_repeat"
    ]
  },
  "EdgeTamVideoTwoWayAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "skip_first_layer_pe"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "query_point_embedding",
      "key_point_embedding",
      "attention_similarity"
    ]
  },
  "EdgeTamVideoPositionEmbeddingSine": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "EdgeTamVideoMemoryFuser": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EdgeTamVideoMaskDownSamplerLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EdgeTamVideoMaskDownSampler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EdgeTamVideoMemoryEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_features",
      "masks"
    ]
  },
  "EdgeTamVideoFeedForward": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "activation",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EdgeTamVideoPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_coords",
      "input_shape"
    ]
  },
  "EdgeTamVideoPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EdgeTamVideoInferenceCache": {
    "__init__": [
      "self",
      "inference_device",
      "inference_state_device",
      "max_vision_features_cache_size"
    ],
    "cache_vision_features": [
      "self",
      "frame_idx",
      "features"
    ],
    "get_vision_features": [
      "self",
      "frame_idx"
    ],
    "clear_all": [
      "self"
    ]
  },
  "EdgeTamVideoInferenceSession": {
    "__init__": [
      "self",
      "video",
      "video_height",
      "video_width",
      "inference_device",
      "inference_state_device",
      "video_storage_device",
      "dtype",
      "max_vision_features_cache_size"
    ],
    "num_frames": [
      "self"
    ],
    "obj_id_to_idx": [
      "self",
      "obj_id"
    ],
    "obj_idx_to_id": [
      "self",
      "obj_idx"
    ],
    "get_obj_num": [
      "self"
    ],
    "add_point_inputs": [
      "self",
      "obj_idx",
      "frame_idx",
      "inputs"
    ],
    "remove_point_inputs": [
      "self",
      "obj_idx",
      "frame_idx"
    ],
    "add_mask_inputs": [
      "self",
      "obj_idx",
      "frame_idx",
      "inputs"
    ],
    "remove_mask_inputs": [
      "self",
      "obj_idx",
      "frame_idx"
    ],
    "store_output": [
      "self",
      "obj_idx",
      "frame_idx",
      "output_key",
      "output_value",
      "is_conditioning_frame"
    ],
    "get_output": [
      "self",
      "obj_idx",
      "frame_idx",
      "output_key",
      "is_conditioning_frame"
    ],
    "add_new_frame": [
      "self",
      "pixel_values",
      "frame_idx"
    ],
    "get_frame": [
      "self",
      "frame_idx"
    ],
    "reset_tracking_data": [
      "self"
    ],
    "reset_inference_session": [
      "self"
    ]
  },
  "EdgeTamVideoMemoryAttentionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "EdgeTamVideoMemoryAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "key_point_embedding",
      "rope_position_embeddings",
      "rope_position_embeddings_k",
      "num_k_exclude_rope",
      "rope_k_repeat"
    ]
  },
  "EdgeTamVideoMemoryAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "current_vision_features",
      "memory",
      "current_vision_position_embeddings",
      "memory_posision_embeddings",
      "num_object_pointer_tokens",
      "num_spatial_memory_tokens"
    ]
  },
  "EdgeTamVideoPerceiverMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EdgeTamVideoPerceiverAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "positional_encoding"
    ]
  },
  "EdgeTamVideoPerceiverEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "latents",
      "input_features",
      "positional_encoding"
    ]
  },
  "EdgeTamVideoPerceiverResampler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "positional_encoding"
    ],
    "_forward_1d": [
      "self",
      "hidden_states",
      "positional_encoding"
    ],
    "_forward_2d": [
      "self",
      "hidden_states"
    ]
  },
  "EdgeTamVideoImageSegmentationOutput": {},
  "EdgeTamVideoSegmentationOutput": {},
  "EdgeTamVideoMaskEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "masks"
    ]
  },
  "EdgeTamVideoPromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ],
    "forward": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ]
  },
  "EdgeTamVideoTwoWayTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "point_embeddings",
      "image_embeddings",
      "image_positional_embeddings",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "EdgeTamVideoMaskDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "high_resolution_features",
      "attention_similarity",
      "target_embedding"
    ],
    "_get_stability_scores": [
      "self",
      "mask_logits"
    ],
    "_dynamic_multimask_via_stability": [
      "self",
      "all_mask_logits",
      "all_iou_scores"
    ]
  },
  "EdgeTamVideoModel": {
    "input_modalities": [],
    "_can_record_outputs": [],
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_prompt_embeddings": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ],
    "forward": [
      "self",
      "inference_session",
      "frame_idx",
      "frame",
      "reverse"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "_prepare_vision_features": [
      "self",
      "inference_session",
      "frame_idx",
      "batch_size"
    ],
    "_single_frame_forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "image_embeddings",
      "multimask_output",
      "attention_similarity",
      "target_embedding"
    ],
    "_use_mask_as_output": [
      "self",
      "backbone_features",
      "high_res_features",
      "mask_inputs"
    ],
    "_select_closest_cond_frames": [
      "self",
      "frame_idx",
      "cond_frame_outputs",
      "max_cond_frame_num"
    ],
    "_gather_memory_frame_outputs": [
      "self",
      "inference_session",
      "obj_idx",
      "frame_idx",
      "track_in_reverse_time"
    ],
    "_build_memory_attention_inputs": [
      "self",
      "temporal_positions_and_previous_outputs",
      "device"
    ],
    "_get_object_pointers": [
      "self",
      "inference_session",
      "obj_idx",
      "frame_idx",
      "num_total_frames",
      "device",
      "track_in_reverse_time",
      "streaming"
    ],
    "_process_object_pointers": [
      "self",
      "temporal_offsets",
      "pointer_tokens",
      "max_object_pointers_to_use",
      "batch_size",
      "num_channels",
      "device"
    ],
    "_prepare_memory_conditioned_features": [
      "self",
      "inference_session",
      "frame_idx",
      "obj_idx",
      "is_initial_conditioning_frame",
      "current_vision_features",
      "current_vision_positional_embeddings",
      "num_total_frames",
      "track_in_reverse_time",
      "streaming"
    ],
    "_use_multimask": [
      "self",
      "is_init_cond_frame",
      "point_inputs"
    ],
    "_run_single_frame_inference": [
      "self",
      "inference_session",
      "frame_idx",
      "obj_idx",
      "batch_size",
      "is_init_cond_frame",
      "point_inputs",
      "mask_inputs",
      "reverse",
      "run_mem_encoder",
      "prev_sam_mask_logits",
      "streaming"
    ],
    "_encode_new_memory": [
      "self",
      "current_vision_feats",
      "pred_masks_high_res",
      "object_score_logits",
      "is_mask_from_pts"
    ],
    "propagate_in_video_iterator": [
      "self",
      "inference_session",
      "start_frame_idx",
      "max_frame_num_to_track",
      "reverse",
      "show_progress_bar"
    ]
  },
  "EdgeTamVideoPromptEncoderConfig": {},
  "EdgeTamVideoMaskDecoderConfig": {},
  "EdgeTamVideoConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "prompt_encoder_config",
      "mask_decoder_config",
      "initializer_range",
      "num_maskmem",
      "image_size",
      "sigmoid_scale_for_mem_enc",
      "sigmoid_bias_for_mem_enc",
      "enable_occlusion_spatial_embedding",
      "multimask_output_in_sam",
      "multimask_min_pt_num",
      "multimask_max_pt_num",
      "multimask_output_for_tracking",
      "max_object_pointers_in_encoder",
      "max_cond_frame_num",
      "enable_temporal_pos_encoding_for_object_pointers",
      "memory_attention_hidden_size",
      "memory_attention_num_layers",
      "memory_attention_num_attention_heads",
      "memory_attention_downsample_rate",
      "memory_attention_mlp_hidden_size",
      "memory_attention_mlp_hidden_act",
      "memory_attention_dropout",
      "memory_attention_rope_theta",
      "memory_attention_rope_feat_sizes",
      "memory_attention_rope_k_sizes",
      "memory_attention_rope_dropout",
      "perceiver_resampler_num_latents",
      "perceiver_resampler_num_latents_2d",
      "perceiver_resampler_hidden_size",
      "perceiver_resampler_mlp_intermediate_size",
      "perceiver_resampler_num_attention_heads",
      "perceiver_resampler_attention_head_dim",
      "perceiver_resampler_num_layers",
      "perceiver_resampler_hidden_dropout",
      "perceiver_resampler_attention_dropout",
      "memory_encoder_hidden_size",
      "memory_encoder_output_channels",
      "mask_downsampler_embed_dim",
      "memory_fuser_intermediate_dim",
      "mask_downsampler_kernel_size",
      "mask_downsampler_stride",
      "mask_downsampler_padding",
      "mask_downsampler_total_stride",
      "mask_downsampler_hidden_act",
      "memory_fuser_num_layers",
      "memory_fuser_embed_dim",
      "memory_fuser_kernel_size",
      "memory_fuser_padding",
      "memory_fuser_layer_scale_init_value",
      "memory_fuser_hidden_act"
    ]
  },
  "TOKENIZER_MAPPING_NAMES": [],
  "TOKENIZER_MAPPING": [],
  "CONFIG_TO_TYPE": [],
  "load_merges": [
    "merges_file"
  ],
  "tokenizer_class_from_name": [
    "class_name"
  ],
  "get_tokenizer_config": [
    "pretrained_model_name_or_path",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only",
    "subfolder"
  ],
  "AutoTokenizer": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "register": [
      "config_class",
      "tokenizer_class",
      "slow_tokenizer_class",
      "fast_tokenizer_class",
      "exist_ok"
    ]
  },
  "MODEL_MAPPING_NAMES": [],
  "MODEL_FOR_PRETRAINING_MAPPING_NAMES": [],
  "MODEL_FOR_CAUSAL_LM_MAPPING_NAMES": [],
  "MODEL_FOR_IMAGE_MAPPING_NAMES": [],
  "MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING_NAMES": [],
  "MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING_NAMES": [],
  "MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES": [],
  "MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES": [],
  "MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING_NAMES": [],
  "MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING_NAMES": [],
  "MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_RETRIEVAL_MAPPING_NAMES": [],
  "MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES": [],
  "MODEL_FOR_MULTIMODAL_LM_MAPPING_NAMES": [],
  "MODEL_FOR_MASKED_LM_MAPPING_NAMES": [],
  "MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES": [],
  "MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES": [],
  "MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES": [],
  "MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES": [],
  "MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES": [],
  "MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES": [],
  "MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING_NAMES": [],
  "MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING_NAMES": [],
  "MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES": [],
  "MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES": [],
  "MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES": [],
  "MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_CTC_MAPPING_NAMES": [],
  "MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES": [],
  "MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES": [],
  "MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES": [],
  "MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_BACKBONE_MAPPING_NAMES": [],
  "MODEL_FOR_MASK_GENERATION_MAPPING_NAMES": [],
  "MODEL_FOR_KEYPOINT_DETECTION_MAPPING_NAMES": [],
  "MODEL_FOR_KEYPOINT_MATCHING_MAPPING_NAMES": [],
  "MODEL_FOR_TEXT_ENCODING_MAPPING_NAMES": [],
  "MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING_NAMES": [],
  "MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING_NAMES": [],
  "MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING_NAMES": [],
  "MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES": [],
  "MODEL_FOR_AUDIO_TOKENIZATION_NAMES": [],
  "MODEL_MAPPING": [],
  "MODEL_FOR_PRETRAINING_MAPPING": [],
  "MODEL_FOR_CAUSAL_LM_MAPPING": [],
  "MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING": [],
  "MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_IMAGE_SEGMENTATION_MAPPING": [],
  "MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING": [],
  "MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING": [],
  "MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING": [],
  "MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING": [],
  "MODEL_FOR_MULTIMODAL_LM_MAPPING": [],
  "MODEL_FOR_RETRIEVAL_MAPPING": [],
  "MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING": [],
  "MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING": [],
  "MODEL_FOR_MASKED_LM_MAPPING": [],
  "MODEL_FOR_IMAGE_MAPPING": [],
  "MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING": [],
  "MODEL_FOR_OBJECT_DETECTION_MAPPING": [],
  "MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING": [],
  "MODEL_FOR_DEPTH_ESTIMATION_MAPPING": [],
  "MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING": [],
  "MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_QUESTION_ANSWERING_MAPPING": [],
  "MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING": [],
  "MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_MULTIPLE_CHOICE_MAPPING": [],
  "MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING": [],
  "MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_CTC_MAPPING": [],
  "MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING": [],
  "MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_AUDIO_XVECTOR_MAPPING": [],
  "MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING": [],
  "MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING": [],
  "MODEL_FOR_BACKBONE_MAPPING": [],
  "MODEL_FOR_MASK_GENERATION_MAPPING": [],
  "MODEL_FOR_KEYPOINT_DETECTION_MAPPING": [],
  "MODEL_FOR_KEYPOINT_MATCHING_MAPPING": [],
  "MODEL_FOR_TEXT_ENCODING_MAPPING": [],
  "MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING": [],
  "MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING": [],
  "MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING": [],
  "MODEL_FOR_IMAGE_TO_IMAGE_MAPPING": [],
  "MODEL_FOR_AUDIO_TOKENIZATION_MAPPING": [],
  "AutoModelForMaskGeneration": {
    "_model_mapping": []
  },
  "AutoModelForKeypointDetection": {
    "_model_mapping": []
  },
  "AutoModelForKeypointMatching": {
    "_model_mapping": []
  },
  "AutoModelForTextEncoding": {
    "_model_mapping": []
  },
  "AutoModelForImageToImage": {
    "_model_mapping": []
  },
  "AutoModel": [],
  "AutoModelForPreTraining": [],
  "AutoModelForCausalLM": [],
  "AutoModelForMaskedLM": [],
  "AutoModelForSeq2SeqLM": [],
  "AutoModelForSequenceClassification": [],
  "AutoModelForQuestionAnswering": [],
  "AutoModelForTableQuestionAnswering": [],
  "AutoModelForVisualQuestionAnswering": [],
  "AutoModelForDocumentQuestionAnswering": [],
  "AutoModelForTokenClassification": [],
  "AutoModelForMultipleChoice": [],
  "AutoModelForNextSentencePrediction": [],
  "AutoModelForImageClassification": [],
  "AutoModelForZeroShotImageClassification": [],
  "AutoModelForImageSegmentation": [],
  "AutoModelForSemanticSegmentation": [],
  "AutoModelForTimeSeriesPrediction": [],
  "AutoModelForUniversalSegmentation": [],
  "AutoModelForInstanceSegmentation": [],
  "AutoModelForObjectDetection": [],
  "AutoModelForZeroShotObjectDetection": [],
  "AutoModelForDepthEstimation": [],
  "AutoModelForVideoClassification": [],
  "AutoModelForImageTextToText": [],
  "AutoModelForMultimodalLM": [],
  "AutoModelForAudioClassification": [],
  "AutoModelForCTC": [],
  "AutoModelForSpeechSeq2Seq": [],
  "AutoModelForAudioFrameClassification": [],
  "AutoModelForAudioXVector": [],
  "AutoModelForTextToSpectrogram": {
    "_model_mapping": []
  },
  "AutoModelForTextToWaveform": {
    "_model_mapping": []
  },
  "AutoBackbone": {
    "_model_mapping": []
  },
  "AutoModelForMaskedImageModeling": [],
  "AutoModelForAudioTokenization": [],
  "PROCESSOR_MAPPING_NAMES": [],
  "PROCESSOR_MAPPING": [],
  "processor_class_from_name": [
    "class_name"
  ],
  "AutoProcessor": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "register": [
      "config_class",
      "processor_class",
      "exist_ok"
    ]
  },
  "FEATURE_EXTRACTOR_MAPPING_NAMES": [],
  "FEATURE_EXTRACTOR_MAPPING": [],
  "feature_extractor_class_from_name": [
    "class_name"
  ],
  "get_feature_extractor_config": [
    "pretrained_model_name_or_path",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only"
  ],
  "AutoFeatureExtractor": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "register": [
      "config_class",
      "feature_extractor_class",
      "exist_ok"
    ]
  },
  "_CallableT": [],
  "CONFIG_MAPPING_NAMES": [],
  "MODEL_NAMES_MAPPING": [],
  "DEPRECATED_MODELS": [],
  "SPECIAL_MODEL_TYPE_TO_MODULE_NAME": [],
  "model_type_to_module_name": [
    "key"
  ],
  "config_class_to_model_type": [
    "config"
  ],
  "_LazyConfigMapping": {
    "__init__": [
      "self",
      "mapping"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "keys": [
      "self"
    ],
    "values": [
      "self"
    ],
    "items": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__contains__": [
      "self",
      "item"
    ],
    "register": [
      "self",
      "key",
      "value",
      "exist_ok"
    ]
  },
  "CONFIG_MAPPING": [],
  "_LazyLoadAllMappings": {
    "__init__": [
      "self",
      "mapping"
    ],
    "_initialize": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "keys": [
      "self"
    ],
    "values": [
      "self"
    ],
    "items": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__contains__": [
      "self",
      "item"
    ]
  },
  "_get_class_name": [
    "model_class"
  ],
  "_list_model_options": [
    "indent",
    "config_to_class",
    "use_model_types"
  ],
  "replace_list_option_in_docstrings": [
    "config_to_class",
    "use_model_types"
  ],
  "AutoConfig": {
    "__init__": [
      "self"
    ],
    "for_model": [
      "cls",
      "model_type"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "register": [
      "model_type",
      "config",
      "exist_ok"
    ]
  },
  "DEFAULT_TO_SLOW_IMAGE_PROCESSORS": [],
  "IMAGE_PROCESSOR_MAPPING": [],
  "get_image_processor_class_from_name": [
    "class_name"
  ],
  "get_image_processor_config": [
    "pretrained_model_name_or_path",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only"
  ],
  "_warning_fast_image_processor_available": [
    "fast_class"
  ],
  "AutoImageProcessor": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "register": [
      "config_class",
      "slow_image_processor_class",
      "fast_image_processor_class",
      "exist_ok"
    ]
  },
  "VIDEO_PROCESSOR_MAPPING": [],
  "video_processor_class_from_name": [
    "class_name"
  ],
  "get_video_processor_config": [
    "pretrained_model_name_or_path",
    "cache_dir",
    "force_download",
    "proxies",
    "token",
    "revision",
    "local_files_only"
  ],
  "AutoVideoProcessor": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "register": [
      "config_class",
      "video_processor_class",
      "exist_ok"
    ]
  },
  "_T": [],
  "_LazyAutoMappingValue": [],
  "CLASS_DOCSTRING": [],
  "FROM_CONFIG_DOCSTRING": [],
  "FROM_PRETRAINED_TORCH_DOCSTRING": [],
  "_get_model_class": [
    "config",
    "model_mapping"
  ],
  "_BaseAutoModelClass": {
    "_model_mapping": [],
    "__init__": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "_prepare_config_for_auto_class": [
      "cls",
      "config"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "register": [
      "cls",
      "config_class",
      "model_class",
      "exist_ok"
    ]
  },
  "_BaseAutoBackboneClass": {
    "_model_mapping": [],
    "_load_timm_backbone_from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "insert_head_doc": [
    "docstring",
    "head_doc"
  ],
  "auto_class_update": [
    "cls",
    "checkpoint_for_example",
    "head_doc"
  ],
  "get_values": [
    "model_mapping"
  ],
  "getattribute_from_module": [
    "module",
    "attr"
  ],
  "add_generation_mixin_to_remote_model": [
    "model_class"
  ],
  "_LazyAutoMapping": {
    "__init__": [
      "self",
      "config_mapping",
      "model_mapping"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "_load_attr_from_module": [
      "self",
      "model_type",
      "attr"
    ],
    "keys": [
      "self"
    ],
    "get": [
      "self",
      "key",
      "default"
    ],
    "__bool__": [
      "self"
    ],
    "values": [
      "self"
    ],
    "items": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__contains__": [
      "self",
      "item"
    ],
    "register": [
      "self",
      "key",
      "value",
      "exist_ok"
    ]
  },
  "DiffLlamaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DiffLlamaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "lambda_init_fn": [
    "layer_idx"
  ],
  "DiffLlamaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "DiffLlamaFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "DiffLlamaSdpaAttention": {
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "DiffLlamaRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "DIFFLLAMA_ATTENTION_CLASSES": [],
  "DiffLlamaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "DiffLlamaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DiffLlamaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "DiffLlamaForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "DiffLlamaForSequenceClassification": {},
  "DiffLlamaForQuestionAnswering": {
    "base_model_prefix": []
  },
  "DiffLlamaForTokenClassification": {},
  "DiffLlamaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "lambda_std_dev",
      "head_dim"
    ]
  },
  "UperNetConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "hidden_size",
      "initializer_range",
      "pool_scales",
      "use_auxiliary_head",
      "auxiliary_loss_weight",
      "auxiliary_in_channels",
      "auxiliary_channels",
      "auxiliary_num_convs",
      "auxiliary_concat_input",
      "loss_ignore_index"
    ]
  },
  "UperNetConvModule": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "padding",
      "bias",
      "dilation"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "UperNetPyramidPoolingBlock": {
    "__init__": [
      "self",
      "pool_scale",
      "in_channels",
      "channels"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "UperNetPyramidPoolingModule": {
    "__init__": [
      "self",
      "pool_scales",
      "in_channels",
      "channels",
      "align_corners"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "UperNetHead": {
    "__init__": [
      "self",
      "config",
      "in_channels"
    ],
    "psp_forward": [
      "self",
      "inputs"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "UperNetFCNHead": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "in_index",
      "kernel_size",
      "dilation"
    ],
    "forward": [
      "self",
      "encoder_hidden_states"
    ]
  },
  "UperNetPreTrainedModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": []
  },
  "UperNetForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "PPDocLayoutV3Config": {
    "model_type": [],
    "sub_configs": [],
    "layer_types": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "initializer_range",
      "initializer_bias_prior_prob",
      "layer_norm_eps",
      "batch_norm_eps",
      "tie_word_embeddings",
      "backbone_config",
      "freeze_backbone_batch_norms",
      "encoder_hidden_dim",
      "encoder_in_channels",
      "feat_strides",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "dropout",
      "activation_dropout",
      "encode_proj_layers",
      "positional_encoding_temperature",
      "encoder_activation_function",
      "activation_function",
      "eval_size",
      "normalize_before",
      "hidden_expansion",
      "mask_feature_channels",
      "x4_feat_dim",
      "d_model",
      "num_prototypes",
      "label_noise_ratio",
      "box_noise_scale",
      "mask_enhanced",
      "num_queries",
      "decoder_in_channels",
      "decoder_ffn_dim",
      "num_feature_levels",
      "decoder_n_points",
      "decoder_layers",
      "decoder_attention_heads",
      "decoder_activation_function",
      "attention_dropout",
      "num_denoising",
      "learn_initial_query",
      "anchor_image_size",
      "disable_custom_kernels",
      "is_encoder_decoder",
      "global_pointer_head_size",
      "gp_dropout_value"
    ]
  },
  "PPDocLayoutV3ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "__init__": [
      "self"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "disable_grouping",
      "return_tensors"
    ],
    "_get_order_seqs": [
      "self",
      "order_logits"
    ],
    "extract_custom_vertices": [
      "self",
      "polygon",
      "sharp_angle_thresh"
    ],
    "_mask2polygon": [
      "self",
      "mask",
      "epsilon_ratio"
    ],
    "_extract_polygon_points_by_masks": [
      "self",
      "boxes",
      "masks",
      "scale_ratio"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ]
  },
  "PPDocLayoutV3GlobalPointer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PPDocLayoutV3MultiscaleDeformableAttention": {},
  "PPDocLayoutV3PreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "mask_to_box_coordinate": [
    "mask",
    "dtype"
  ],
  "PPDocLayoutV3DecoderOutput": {},
  "PPDocLayoutV3ModelOutput": {},
  "PPDocLayoutV3MLPPredictionHead": {},
  "PPDocLayoutV3ConvLayer": {},
  "PPDocLayoutV3ScaleHead": {
    "__init__": [
      "self",
      "in_channels",
      "feature_channels",
      "fpn_stride",
      "base_stride",
      "align_corners"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PPDocLayoutV3MaskFeatFPN": {
    "__init__": [
      "self",
      "in_channels",
      "fpn_strides",
      "feature_channels",
      "dropout_ratio",
      "out_channels",
      "align_corners"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PPDocLayoutV3EncoderMaskOutput": {
    "__init__": [
      "self",
      "in_channels",
      "num_prototypes"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PPDocLayoutV3HybridEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "x4_feat"
    ]
  },
  "PPDocLayoutV3Decoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "order_head",
      "global_pointer",
      "mask_query_head",
      "norm",
      "mask_feat"
    ]
  },
  "PPDocLayoutV3Model": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "encoder_outputs",
      "labels"
    ]
  },
  "PPDocLayoutV3HybridEncoderOutput": {},
  "PPDocLayoutV3ForObjectDetectionOutput": {},
  "PPDocLayoutV3ForObjectDetection": {
    "_keys_to_ignore_on_load_missing": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "encoder_outputs",
      "labels"
    ]
  },
  "PPDocLayoutV3MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size",
      "activation_function"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PPDocLayoutV3SelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "PPDocLayoutV3ConvNormLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "PPDocLayoutV3EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "spatial_position_embeddings"
    ]
  },
  "PPDocLayoutV3RepVggBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PPDocLayoutV3CSPRepLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "PPDocLayoutV3SinePositionEmbedding": {
    "__init__": [
      "self",
      "embed_dim",
      "temperature"
    ],
    "forward": [
      "self",
      "width",
      "height",
      "device",
      "dtype"
    ]
  },
  "PPDocLayoutV3AIFILayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PPDocLayoutV3DecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "object_queries_position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "PPDocLayoutV3FrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PPDocLayoutV3ConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "LasrProcessorKwargs": {
    "_defaults": []
  },
  "LasrProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "audio",
      "text",
      "sampling_rate"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "LasrTokenizer": {
    "__init__": [
      "self",
      "eos_token",
      "unk_token",
      "pad_token",
      "extra_ids",
      "additional_special_tokens",
      "vocab",
      "vocab_file"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "group_tokens"
    ]
  },
  "LasrEncoderConfig": {
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "attention_bias",
      "convolution_bias",
      "conv_kernel_size",
      "subsampling_conv_channels",
      "subsampling_conv_kernel_size",
      "subsampling_conv_stride",
      "num_mel_bins",
      "dropout",
      "dropout_positions",
      "layerdrop",
      "activation_dropout",
      "attention_dropout",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "feed_forward_residual_weights",
      "conv_residual_weights",
      "batch_norm_momentum",
      "rope_parameters"
    ]
  },
  "LasrCTCConfig": {
    "__init__": [
      "self",
      "vocab_size",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "encoder_config",
      "pad_token_id"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "LasrEncoderSubsampling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features"
    ]
  },
  "LasrEncoderRotaryEmbedding": {},
  "LasrEncoderAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "LasrEncoderConvolutionModule": {
    "__init__": [
      "self",
      "config",
      "module_config"
    ]
  },
  "LasrEncoderBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "LasrPreTrainedModel": {
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_subsampling_output_length": [
      "self",
      "input_lengths"
    ]
  },
  "LasrEncoder": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask"
    ]
  },
  "LasrForCTC": {
    "generate": []
  },
  "LasrEncoderFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LasrGenerateOutput": {},
  "linear_to_mel_weight_matrix": [
    "num_mel_bins",
    "num_spectrogram_bins",
    "sample_rate",
    "lower_edge_hertz",
    "upper_edge_hertz",
    "dtype"
  ],
  "LasrFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "hop_length",
      "n_fft",
      "win_length",
      "padding_value"
    ],
    "_torch_extract_fbank_features": [
      "self",
      "waveform",
      "device"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "truncation",
      "pad_to_multiple_of",
      "return_tensors",
      "return_attention_mask",
      "padding",
      "max_length",
      "sampling_rate",
      "do_normalize",
      "device",
      "return_token_timestamps"
    ]
  },
  "XLNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "n_layer",
      "n_head",
      "d_inner",
      "ff_activation",
      "attn_type",
      "initializer_range",
      "layer_norm_eps",
      "dropout",
      "mem_len",
      "reuse_len",
      "use_mems_eval",
      "use_mems_train",
      "bi_data",
      "clamp_len",
      "same_length",
      "summary_type",
      "summary_use_proj",
      "summary_activation",
      "summary_last_dropout",
      "start_n_top",
      "end_n_top",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ],
    "max_position_embeddings": [
      "self",
      "value"
    ]
  },
  "SEG_ID_A": [],
  "SEG_ID_B": [],
  "SEG_ID_CLS": [],
  "SEG_ID_SEP": [],
  "SEG_ID_PAD": [],
  "XLNetTokenizer": {
    "vocab_files_names": [],
    "padding_side": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "unk_id",
      "do_lower_case",
      "remove_space",
      "keep_accents",
      "bos_token",
      "eos_token",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "additional_special_tokens"
    ]
  },
  "XLNetRelativeAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "rel_shift": [
      "x",
      "klen"
    ],
    "rel_shift_bnij": [
      "x",
      "klen"
    ],
    "rel_attn_core": [
      "self",
      "q_head",
      "k_head_h",
      "v_head_h",
      "k_head_r",
      "seg_mat",
      "attn_mask",
      "output_attentions"
    ],
    "post_attention": [
      "self",
      "h",
      "attn_vec",
      "residual"
    ],
    "forward": [
      "self",
      "h",
      "g",
      "attn_mask_h",
      "attn_mask_g",
      "r",
      "seg_mat",
      "mems",
      "target_mapping",
      "output_attentions"
    ]
  },
  "XLNetFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inp"
    ]
  },
  "XLNetLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "output_h",
      "output_g",
      "attn_mask_h",
      "attn_mask_g",
      "r",
      "seg_mat",
      "mems",
      "target_mapping",
      "output_attentions"
    ],
    "ff_chunk": [
      "self",
      "output_x"
    ]
  },
  "XLNetPoolerStartLogits": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "p_mask"
    ]
  },
  "XLNetPoolerEndLogits": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_states",
      "start_positions",
      "p_mask"
    ]
  },
  "XLNetPoolerAnswerClass": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_states",
      "start_positions",
      "cls_index"
    ]
  },
  "XLNetSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "XLNetPreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "XLNetModelOutput": {},
  "XLNetLMHeadModelOutput": {},
  "XLNetForSequenceClassificationOutput": {},
  "XLNetForTokenClassificationOutput": {},
  "XLNetForMultipleChoiceOutput": {},
  "XLNetForQuestionAnsweringSimpleOutput": {},
  "XLNetForQuestionAnsweringOutput": {},
  "XLNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "create_mask": [
      "self",
      "qlen",
      "mlen"
    ],
    "cache_mem": [
      "self",
      "curr_out",
      "prev_mem"
    ],
    "positional_embedding": [
      "pos_seq",
      "inv_freq",
      "bsz"
    ],
    "relative_positional_encoding": [
      "self",
      "qlen",
      "klen",
      "bsz"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "mems",
      "perm_mask",
      "target_mapping",
      "token_type_ids",
      "input_mask",
      "inputs_embeds",
      "use_mems",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLNetLMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "use_mems",
      "is_first_iteration"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "mems",
      "perm_mask",
      "target_mapping",
      "token_type_ids",
      "input_mask",
      "inputs_embeds",
      "labels",
      "use_mems",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ],
    "_reorder_cache": [
      "mems",
      "beam_idx"
    ]
  },
  "XLNetForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "mems",
      "perm_mask",
      "target_mapping",
      "token_type_ids",
      "input_mask",
      "inputs_embeds",
      "labels",
      "use_mems",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLNetForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "mems",
      "perm_mask",
      "target_mapping",
      "token_type_ids",
      "input_mask",
      "inputs_embeds",
      "labels",
      "use_mems",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLNetForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "input_mask",
      "attention_mask",
      "mems",
      "perm_mask",
      "target_mapping",
      "inputs_embeds",
      "labels",
      "use_mems",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLNetForQuestionAnsweringSimple": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "mems",
      "perm_mask",
      "target_mapping",
      "token_type_ids",
      "input_mask",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "use_mems",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLNetForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "mems",
      "perm_mask",
      "target_mapping",
      "token_type_ids",
      "input_mask",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "is_impossible",
      "cls_index",
      "p_mask",
      "use_mems",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Siglip2ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "patch_size": [],
    "max_num_patches": [],
    "valid_kwargs": [],
    "unused_kwargs": [],
    "__init__": [
      "self"
    ],
    "_validate_preprocess_kwargs": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "patch_size",
      "max_num_patches",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors"
    ]
  },
  "Siglip2VisionOutput": {},
  "Siglip2TextOutput": {},
  "Siglip2Output": {
    "to_tuple": [
      "self"
    ]
  },
  "Siglip2VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "resize_positional_embeddings": [
      "positional_embeddings",
      "spatial_shapes",
      "max_length"
    ],
    "forward": [
      "self",
      "pixel_values",
      "spatial_shapes"
    ]
  },
  "Siglip2TextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "Siglip2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Siglip2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Siglip2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Siglip2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Siglip2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "Siglip2VisionTransformer": {
    "_input_embed_layer": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "attention_mask",
      "spatial_shapes",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "Siglip2TextTransformer": {
    "_input_embed_layer": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "Siglip2TextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "Siglip2MultiheadAttentionPoolingHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "Siglip2VisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_attention_mask",
      "spatial_shapes"
    ]
  },
  "Siglip2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_attention_mask",
      "spatial_shapes"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_attention_mask",
      "spatial_shapes",
      "attention_mask",
      "position_ids",
      "return_loss",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "Siglip2ForImageClassification": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_attention_mask",
      "spatial_shapes",
      "labels",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "Siglip2ProcessorKwargs": {
    "_defaults": []
  },
  "Siglip2Processor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "Siglip2Tokenizer": {
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "mask_token"
    ],
    "_unk_id": [
      "self"
    ]
  },
  "Siglip2TextConfig": {},
  "Siglip2VisionConfig": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "num_patches",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout"
    ]
  },
  "Siglip2Config": {},
  "Siglip2ImageProcessorKwargs": {},
  "Siglip2ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "patch_size",
      "max_num_patches"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "input_data_format",
      "do_convert_rgb",
      "patch_size",
      "max_num_patches"
    ]
  },
  "DacFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "hop_length"
    ],
    "__call__": [
      "self",
      "raw_audio",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "sampling_rate"
    ]
  },
  "DacOutput": {},
  "DacEncoderOutput": {},
  "DacDecoderOutput": {},
  "DacVectorQuantize": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ],
    "decode_latents": [
      "self",
      "hidden_states"
    ]
  },
  "DacResidualUnit": {
    "__init__": [
      "self",
      "dimension",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DacEncoderBlock": {
    "__init__": [
      "self",
      "config",
      "stride",
      "stride_index"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DacDecoderBlock": {
    "__init__": [
      "self",
      "config",
      "stride",
      "stride_index"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DacResidualVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "n_quantizers"
    ],
    "from_codes": [
      "self",
      "audio_codes"
    ],
    "from_latents": [
      "self",
      "latents"
    ]
  },
  "DacDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DacEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DacPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "DacModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "input_values",
      "n_quantizers",
      "return_dict"
    ],
    "decode": [
      "self",
      "quantized_representation",
      "audio_codes",
      "return_dict"
    ],
    "forward": [
      "self",
      "input_values",
      "n_quantizers",
      "return_dict"
    ]
  },
  "DacConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "encoder_hidden_size",
      "downsampling_ratios",
      "decoder_hidden_size",
      "n_codebooks",
      "codebook_size",
      "codebook_dim",
      "quantizer_dropout",
      "commitment_loss_weight",
      "codebook_loss_weight",
      "sampling_rate"
    ],
    "frame_rate": [
      "self"
    ]
  },
  "Emu3VQVAEModelOutput": {},
  "Emu3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Emu3RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Emu3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Emu3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Emu3VQVAEVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Emu3VQVAEEncoderConvDownsample": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3VQVAEEncoderConvUpsample": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3VQVAEConv3d": {
    "__init__": [
      "self",
      "in_channel",
      "out_channel",
      "kernel_size",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3VQVAESpatialNorm": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "hidden_states",
      "quant_states"
    ]
  },
  "Emu3VQVAETemporalUpsample": {
    "__init__": [
      "self",
      "in_channel",
      "out_channel"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3VQVAETemporalDownsample": {
    "__init__": [
      "self",
      "in_channel",
      "out_channel"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3VQVAETemporalResnetBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3VQVAEResnetBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "quant_channels"
    ],
    "forward": [
      "self",
      "hidden_states",
      "quant_channels"
    ]
  },
  "Emu3VQVAEAttentionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Emu3VQVAEGroupNorm": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "input",
      "quant_states"
    ]
  },
  "Emu3VQVAEMiddleBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "quant_channels"
    ],
    "forward": [
      "self",
      "hidden_states",
      "quant_states"
    ]
  },
  "Emu3VQVAEDownBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3VQVAEUpBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "quant_states"
    ]
  },
  "Emu3VQVAEEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Emu3VQVAEDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "quant_states"
    ]
  },
  "Emu3VQVAE": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "pixel_values",
      "image_sizes"
    ],
    "decode": [
      "self",
      "hidden_states"
    ]
  },
  "Emu3ImageVocabularyMapping": {
    "__init__": [
      "self",
      "vocab_map"
    ],
    "image_tokens": [
      "self"
    ],
    "image_tokens_str": [
      "self"
    ],
    "img2bpe": [
      "self"
    ],
    "bpe2img": [
      "self"
    ],
    "bpe2img_mapping_tensor": [
      "self"
    ],
    "img2bpe_mapping_tensor": [
      "self"
    ],
    "convert_img2bpe": [
      "self",
      "img_batch"
    ],
    "convert_bpe2img": [
      "self",
      "img_batch"
    ]
  },
  "Emu3PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Emu3RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Emu3TextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "Emu3ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Emu3Model": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_tokens": [
      "self",
      "pixel_values",
      "image_sizes"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes"
    ],
    "decode_image_tokens": [
      "self",
      "image_tokens",
      "height",
      "width"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Emu3ForConditionalGeneration": {
    "output_modalities": [],
    "_tied_weights_keys": [],
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "decode_image_tokens": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "labels",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "is_first_iteration"
    ]
  },
  "Emu3VQVAEConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "codebook_size",
      "embed_dim",
      "latent_channels",
      "double_latent",
      "in_channels",
      "out_channels",
      "temporal_downsample_factor",
      "base_channels",
      "channel_multiplier",
      "num_res_blocks",
      "attn_resolutions",
      "hidden_size",
      "num_attention_heads",
      "attention_dropout"
    ]
  },
  "Emu3TextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "rope_parameters",
      "mlp_bias",
      "attention_bias",
      "attention_dropout",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "Emu3Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vq_config",
      "text_config",
      "vocabulary_map",
      "tie_word_embeddings"
    ]
  },
  "Emu3TextKwargs": {},
  "Emu3ProcessorKwargs": {
    "_defaults": []
  },
  "Emu3Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "calculate_generate_size": [
      "self",
      "ratio",
      "image_area",
      "spatial_factor"
    ],
    "postprocess": [
      "self",
      "images"
    ],
    "post_process_multimodal_output": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "generation_mode"
    ]
  },
  "Emu3ImageProcessorKwargs": {},
  "Emu3ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "do_pad",
      "min_pixels",
      "max_pixels",
      "spatial_factor"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ],
    "_pad_for_batching": [
      "self",
      "pixel_values",
      "image_sizes",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "do_pad",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "postprocess": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "input_data_format"
    ],
    "unnormalize": [
      "self",
      "image",
      "image_mean",
      "image_std",
      "input_data_format"
    ]
  },
  "FalconLinear": {
    "forward": [
      "self",
      "input"
    ]
  },
  "FalconRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "FalconAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "_split_heads": [
      "self",
      "fused_qkv"
    ],
    "_merge_heads": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "alibi",
      "attention_mask",
      "position_ids",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position",
      "position_embeddings"
    ]
  },
  "FalconFlashAttention2": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "alibi",
      "attention_mask",
      "position_ids",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position",
      "position_embeddings"
    ]
  },
  "FalconMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FALCON_ATTENTION_CLASSES": [],
  "FalconDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "alibi",
      "attention_mask",
      "position_ids",
      "layer_past",
      "use_cache",
      "output_attentions",
      "cache_position",
      "position_embeddings"
    ]
  },
  "FalconPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_check_and_enable_sdpa": [
      "cls",
      "config",
      "hard_check_only"
    ]
  },
  "FalconModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions",
      "alibi"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "FalconForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "FalconForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FalconForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FalconForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FalconConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_ln_in_parallel_attn",
      "layer_norm_epsilon",
      "initializer_range",
      "use_cache",
      "hidden_dropout",
      "attention_dropout",
      "num_kv_heads",
      "alibi",
      "new_decoder_architecture",
      "multi_query",
      "parallel_attn",
      "bias",
      "max_position_embeddings",
      "rope_parameters",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "ffn_hidden_size",
      "activation",
      "tie_word_embeddings"
    ],
    "head_dim": [
      "self"
    ],
    "rotary": [
      "self"
    ]
  },
  "SiglipImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format",
      "do_convert_rgb"
    ]
  },
  "SiglipTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "projection_size"
    ]
  },
  "SiglipVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout"
    ]
  },
  "SiglipConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config"
    ]
  },
  "SiglipVisionModelOutput": {},
  "SiglipTextModelOutput": {},
  "SiglipOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "SiglipVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "SiglipTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "SiglipAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SiglipMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SiglipEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SiglipPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SiglipEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "SiglipTextTransformer": {
    "_input_embed_layer": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "SiglipTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "SiglipVisionTransformer": {
    "_input_embed_layer": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "SiglipMultiheadAttentionPoolingHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "SiglipVisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "SiglipModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "interpolate_pos_encoding"
    ]
  },
  "SiglipForImageClassification": {
    "main_input_name": [],
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "interpolate_pos_encoding"
    ]
  },
  "SiglipProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "SiglipTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "eos_token",
      "unk_token",
      "pad_token",
      "additional_special_tokens",
      "sp_model_kwargs",
      "model_max_length",
      "do_lower_case"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "_add_eos_if_not_present": [
      "self",
      "token_ids"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ],
    "remove_punctuation": [
      "self",
      "text"
    ],
    "canonicalize_text": [
      "self",
      "text"
    ],
    "tokenize": [
      "self",
      "text",
      "add_special_tokens"
    ],
    "unk_token_length": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "SiglipImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": []
  },
  "DepthAnythingConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "patch_size",
      "initializer_range",
      "reassemble_hidden_size",
      "reassemble_factors",
      "neck_hidden_sizes",
      "fusion_hidden_size",
      "head_in_index",
      "head_hidden_size",
      "depth_estimation_type",
      "max_depth"
    ]
  },
  "DepthAnythingReassembleLayer": {
    "__init__": [
      "self",
      "config",
      "channels",
      "factor"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DepthAnythingReassembleStage": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "DepthAnythingPreActResidualLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DepthAnythingFeatureFusionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "residual",
      "size"
    ]
  },
  "DepthAnythingFeatureFusionStage": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "size"
    ]
  },
  "DepthAnythingPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": []
  },
  "DepthAnythingNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "DepthAnythingDepthEstimationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "DepthAnythingForDepthEstimation": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LiltTextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ],
    "create_position_ids_from_input_ids": [
      "self",
      "input_ids",
      "padding_idx"
    ],
    "create_position_ids_from_inputs_embeds": [
      "self",
      "inputs_embeds"
    ]
  },
  "LiltLayoutEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "bbox",
      "position_ids"
    ]
  },
  "LiltSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "transpose_for_scores": [
      "self",
      "x",
      "r"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layout_inputs",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LiltSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LiltAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layout_inputs",
      "attention_mask",
      "output_attentions"
    ]
  },
  "LiltIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LiltOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LiltLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layout_inputs",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ],
    "layout_feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "LiltEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layout_inputs",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LiltPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LiltPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LiltModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LiltForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LiltForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LiltClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "LiltForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LiltConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "classifier_dropout",
      "channel_shrink_ratio",
      "max_2d_position_embeddings"
    ]
  },
  "EncoderDecoderModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "__init__": [
      "self",
      "config",
      "encoder",
      "decoder"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "from_encoder_decoder_pretrained": [
      "cls",
      "encoder_pretrained_model_name_or_path",
      "decoder_pretrained_model_name_or_path"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ],
    "resize_token_embeddings": [
      "self"
    ]
  },
  "EncoderDecoderConfig": {
    "model_type": [],
    "sub_configs": [],
    "has_no_defaults_at_init": [],
    "__init__": [
      "self",
      "pad_token_id",
      "decoder_start_token_id"
    ],
    "from_encoder_decoder_configs": [
      "cls",
      "encoder_config",
      "decoder_config"
    ]
  },
  "MetaClip2TextConfig": {},
  "MetaClip2VisionConfig": {},
  "MetaClip2Config": {},
  "MetaClip2TextEmbeddings": {},
  "MetaClip2VisionEmbeddings": {},
  "MetaClip2Attention": {},
  "MetaClip2MLP": {},
  "MetaClip2PreTrainedModel": {
    "base_model_prefix": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MetaClip2TextTransformer": {
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "MetaClip2TextModel": {
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "MetaClip2TextModelWithProjection": {
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "MetaClip2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "return_loss",
      "interpolate_pos_encoding"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "MetaClip2VisionModel": {
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "MetaClip2VisionModelWithProjection": {
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "MetaClip2ForImageClassification": {},
  "MetaClip2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "MetaClip2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "MetaClip2TextModelOutput": {},
  "MetaClip2Output": {
    "to_tuple": [
      "self"
    ]
  },
  "metaclip_2_loss": [
    "similarity"
  ],
  "MetaClip2VisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "MetaClip2VisionModelOutput": {},
  "EXTRA_TOKENS": [],
  "PaliGemmaTextKwargs": {},
  "PaliGemmaProcessorKwargs": {
    "_defaults": []
  },
  "_is_str_or_image": [
    "elem"
  ],
  "PaliGemmaProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "PaligemmaModelOutputWithPast": {},
  "PaliGemmaCausalLMOutputWithPast": {},
  "PaliGemmaMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "PaliGemmaPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_can_compile_fullgraph": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": []
  },
  "PaliGemmaModel": {
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "token_type_ids",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PaliGemmaForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "token_type_ids",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "pixel_values",
      "attention_mask",
      "token_type_ids",
      "use_cache",
      "logits_to_keep",
      "labels",
      "is_first_iteration"
    ],
    "create_masks_for_generate": [
      "config",
      "input_embeds",
      "attention_mask",
      "cache_position",
      "past_key_values",
      "position_ids",
      "token_type_ids",
      "is_first_iteration"
    ]
  },
  "PaliGemmaConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "vocab_size",
      "projection_dim",
      "hidden_size",
      "tie_word_embeddings"
    ]
  },
  "DFineConfig": {
    "model_type": [],
    "sub_configs": [],
    "layer_types": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "initializer_range",
      "initializer_bias_prior_prob",
      "layer_norm_eps",
      "batch_norm_eps",
      "backbone_config",
      "freeze_backbone_batch_norms",
      "encoder_hidden_dim",
      "encoder_in_channels",
      "feat_strides",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "dropout",
      "activation_dropout",
      "encode_proj_layers",
      "positional_encoding_temperature",
      "encoder_activation_function",
      "activation_function",
      "eval_size",
      "normalize_before",
      "hidden_expansion",
      "d_model",
      "num_queries",
      "decoder_in_channels",
      "decoder_ffn_dim",
      "num_feature_levels",
      "decoder_n_points",
      "decoder_layers",
      "decoder_attention_heads",
      "decoder_activation_function",
      "attention_dropout",
      "num_denoising",
      "label_noise_ratio",
      "box_noise_scale",
      "learn_initial_query",
      "anchor_image_size",
      "with_box_refine",
      "is_encoder_decoder",
      "matcher_alpha",
      "matcher_gamma",
      "matcher_class_cost",
      "matcher_bbox_cost",
      "matcher_giou_cost",
      "use_focal_loss",
      "auxiliary_loss",
      "focal_loss_alpha",
      "focal_loss_gamma",
      "weight_loss_vfl",
      "weight_loss_bbox",
      "weight_loss_giou",
      "weight_loss_fgl",
      "weight_loss_ddf",
      "eos_coefficient",
      "eval_idx",
      "layer_scale",
      "max_num_bins",
      "reg_scale",
      "depth_mult",
      "top_prob_values",
      "lqe_hidden_dim",
      "lqe_layers",
      "decoder_offset_scale",
      "decoder_method",
      "up",
      "tie_word_embeddings"
    ]
  },
  "DFineDecoderOutput": {},
  "distance2bbox": [
    "points",
    "distance",
    "reg_scale"
  ],
  "DFineMLP": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "act"
    ],
    "forward": [
      "self",
      "stat_features"
    ]
  },
  "DFineGate": {
    "__init__": [
      "self",
      "d_model"
    ],
    "forward": [
      "self",
      "second_residual",
      "hidden_states"
    ]
  },
  "DFineFrozenBatchNorm2d": {},
  "DFineMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "reference_points",
      "encoder_hidden_states",
      "spatial_shapes",
      "spatial_shapes_list"
    ]
  },
  "DFineConvNormLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "padding",
      "activation"
    ]
  },
  "DFineRepVggBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels"
    ]
  },
  "DFineCSPRepLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "num_blocks",
      "expansion"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "DFineRepNCSPELAN4": {
    "__init__": [
      "self",
      "config",
      "act",
      "numb_blocks"
    ],
    "forward": [
      "self",
      "input_features"
    ]
  },
  "DFineSCDown": {
    "__init__": [
      "self",
      "config",
      "kernel_size",
      "stride"
    ],
    "forward": [
      "self",
      "input_features"
    ]
  },
  "DFineEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DFineAIFILayer": {},
  "DFineIntegral": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pred_corners",
      "project"
    ]
  },
  "DFineLQE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "scores",
      "pred_corners"
    ]
  },
  "DFineDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "DFineMLPPredictionHead": {},
  "DFinePreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DFineHybridEncoder": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DFineDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoder_hidden_states",
      "reference_points",
      "inputs_embeds",
      "spatial_shapes",
      "level_start_index",
      "spatial_shapes_list",
      "encoder_attention_mask",
      "memory_mask"
    ]
  },
  "DFineModel": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DFineForObjectDetection": {
    "_no_split_modules": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": []
  },
  "DFineSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "dropout",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings"
    ]
  },
  "DFineSinePositionEmbedding": {
    "__init__": [
      "self",
      "embed_dim",
      "temperature"
    ],
    "forward": [
      "self",
      "width",
      "height",
      "device",
      "dtype"
    ]
  },
  "DFineModelOutput": {},
  "DFineConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "DFineObjectDetectionOutput": {},
  "EfficientNetImageProcessorKwargs": {},
  "EfficientNetImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "rescale_factor",
      "rescale_offset",
      "do_rescale",
      "do_normalize",
      "image_mean",
      "image_std",
      "include_top"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "rescale": [
      "self",
      "image",
      "scale",
      "offset",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "rescale_offset",
      "do_normalize",
      "image_mean",
      "image_std",
      "include_top",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "EfficientNetImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "rescale_factor": [],
    "rescale_offset": [],
    "do_normalize": [],
    "include_top": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "rescale": [
      "self",
      "image",
      "scale",
      "offset"
    ],
    "_fuse_mean_std_and_rescale_factor": [
      "self",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_rescale",
      "rescale_factor",
      "device",
      "rescale_offset"
    ],
    "rescale_and_normalize": [
      "self",
      "images",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "rescale_offset"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "rescale_offset",
      "do_normalize",
      "include_top",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "preprocess": [
      "self",
      "images"
    ]
  },
  "EfficientNetEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "EfficientNetDepthwiseConv2d": {
    "__init__": [
      "self",
      "in_channels",
      "depth_multiplier",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "bias",
      "padding_mode"
    ]
  },
  "EfficientNetExpansionLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "out_dim",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientNetDepthwiseLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "stride",
      "kernel_size",
      "adjust_padding"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientNetSqueezeExciteLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "expand_dim",
      "expand"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientNetFinalBlockLayer": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "out_dim",
      "stride",
      "drop_rate",
      "id_skip"
    ],
    "forward": [
      "self",
      "embeddings",
      "hidden_states"
    ]
  },
  "EfficientNetBlock": {
    "__init__": [
      "self",
      "config",
      "in_dim",
      "out_dim",
      "stride",
      "expand_ratio",
      "kernel_size",
      "drop_rate",
      "id_skip",
      "adjust_padding"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EfficientNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "EfficientNetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EfficientNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "EfficientNetForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "EfficientNetConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "image_size",
      "width_coefficient",
      "depth_coefficient",
      "depth_divisor",
      "kernel_sizes",
      "in_channels",
      "out_channels",
      "depthwise_padding",
      "strides",
      "num_block_repeats",
      "expand_ratios",
      "squeeze_expansion_ratio",
      "hidden_act",
      "hidden_dim",
      "pooling_type",
      "initializer_range",
      "batch_norm_eps",
      "batch_norm_momentum",
      "dropout_rate",
      "drop_connect_rate"
    ]
  },
  "PeAudioVideoEncoderConfig": {
    "model_type": [],
    "base_config_key": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "audio_config",
      "video_config",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "rope_parameters",
      "attention_bias",
      "attention_dropout"
    ]
  },
  "PeAudioVideoConfig": {
    "model_type": [],
    "sub_configs": [],
    "_default_text_config_kwargs": [],
    "__init__": [
      "self",
      "text_config",
      "audio_video_config"
    ],
    "audio_config": [
      "self"
    ],
    "video_config": [
      "self"
    ]
  },
  "PeAudioVideoMaskedGroupNorm": {
    "forward": [
      "self",
      "x",
      "padding_mask"
    ]
  },
  "PeAudioVideoConvBlock1d": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "padding_mask"
    ]
  },
  "PeAudioVideoResnetBlock1d": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask"
    ]
  },
  "PeAudioVideoEncoderPatchEmbedder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "padding_mask"
    ]
  },
  "PeAudioVideoContrastiveHead": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PeAudioVideoEncoderEmbedder": {
    "__init__": [
      "self",
      "config"
    ],
    "_align_video_hidden_state": [
      "self",
      "video_hidden_state",
      "audio_hidden_state",
      "padding_mask_videos",
      "padding_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "pixel_values_videos",
      "padding_mask",
      "padding_mask_videos"
    ]
  },
  "PeAudioVideoEncoderAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "PeAudioVideoEncoderMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PeAudioVideoEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "PeAudioVideoEncoderRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PeAudioVideoEncoderRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PeAudioVideoPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PeAudioVideoEncoderOutput": {},
  "PeAudioVideoEncoder": {
    "main_input_name": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "pixel_values_videos",
      "padding_mask",
      "padding_mask_videos"
    ]
  },
  "PeAudioVideoOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "AudioVideoEmbeddings": {},
  "PeAudioVideoModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "_contrastive_loss": [
      "self",
      "logits"
    ],
    "get_text_audio_embeds": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_text_video_embeds": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_text_audio_video_embeds": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_audio_embeds": [
      "self",
      "input_values",
      "padding_mask"
    ],
    "get_video_embeds": [
      "self",
      "pixel_values_videos",
      "padding_mask_videos"
    ],
    "get_audio_video_embeds": [
      "self",
      "input_values",
      "pixel_values_videos",
      "padding_mask",
      "padding_mask_videos",
      "return_audio_embeds",
      "return_video_embeds"
    ],
    "get_audio_plus_text_embeds": [
      "self",
      "input_ids",
      "input_values",
      "attention_mask",
      "padding_mask"
    ],
    "get_video_plus_text_embeds": [
      "self",
      "input_ids",
      "pixel_values_videos",
      "attention_mask",
      "padding_mask_videos"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values_videos",
      "input_values",
      "attention_mask",
      "padding_mask_videos",
      "padding_mask",
      "return_loss"
    ]
  },
  "PeAudioVideoProcessor": {
    "attributes": [],
    "feature_extractor_class": [],
    "tokenizer_class": [],
    "video_processor_class": []
  },
  "BaseModelOutputWithCLSToken": {},
  "CvtDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "CvtEmbeddings": {
    "__init__": [
      "self",
      "patch_size",
      "num_channels",
      "embed_dim",
      "stride",
      "padding",
      "dropout_rate"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "CvtConvEmbeddings": {
    "__init__": [
      "self",
      "patch_size",
      "num_channels",
      "embed_dim",
      "stride",
      "padding"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "CvtSelfAttentionConvProjection": {
    "__init__": [
      "self",
      "embed_dim",
      "kernel_size",
      "padding",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "CvtSelfAttentionLinearProjection": {
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "CvtSelfAttentionProjection": {
    "__init__": [
      "self",
      "embed_dim",
      "kernel_size",
      "padding",
      "stride",
      "projection_method"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "CvtSelfAttention": {
    "__init__": [
      "self",
      "num_heads",
      "embed_dim",
      "kernel_size",
      "padding_q",
      "padding_kv",
      "stride_q",
      "stride_kv",
      "qkv_projection_method",
      "qkv_bias",
      "attention_drop_rate",
      "with_cls_token"
    ],
    "rearrange_for_multi_head_attention": [
      "self",
      "hidden_state"
    ],
    "forward": [
      "self",
      "hidden_state",
      "height",
      "width"
    ]
  },
  "CvtSelfOutput": {
    "__init__": [
      "self",
      "embed_dim",
      "drop_rate"
    ],
    "forward": [
      "self",
      "hidden_state",
      "input_tensor"
    ]
  },
  "CvtAttention": {
    "__init__": [
      "self",
      "num_heads",
      "embed_dim",
      "kernel_size",
      "padding_q",
      "padding_kv",
      "stride_q",
      "stride_kv",
      "qkv_projection_method",
      "qkv_bias",
      "attention_drop_rate",
      "drop_rate",
      "with_cls_token"
    ],
    "forward": [
      "self",
      "hidden_state",
      "height",
      "width"
    ]
  },
  "CvtIntermediate": {
    "__init__": [
      "self",
      "embed_dim",
      "mlp_ratio"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "CvtOutput": {
    "__init__": [
      "self",
      "embed_dim",
      "mlp_ratio",
      "drop_rate"
    ],
    "forward": [
      "self",
      "hidden_state",
      "input_tensor"
    ]
  },
  "CvtLayer": {
    "__init__": [
      "self",
      "num_heads",
      "embed_dim",
      "kernel_size",
      "padding_q",
      "padding_kv",
      "stride_q",
      "stride_kv",
      "qkv_projection_method",
      "qkv_bias",
      "attention_drop_rate",
      "drop_rate",
      "mlp_ratio",
      "drop_path_rate",
      "with_cls_token"
    ],
    "forward": [
      "self",
      "hidden_state",
      "height",
      "width"
    ]
  },
  "CvtStage": {
    "__init__": [
      "self",
      "config",
      "stage"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "CvtEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CvtPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "CvtModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CvtForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CvtConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "patch_sizes",
      "patch_stride",
      "patch_padding",
      "embed_dim",
      "num_heads",
      "depth",
      "mlp_ratio",
      "attention_drop_rate",
      "drop_rate",
      "drop_path_rate",
      "qkv_bias",
      "cls_token",
      "qkv_projection_method",
      "kernel_qkv",
      "padding_kv",
      "stride_kv",
      "padding_q",
      "stride_q",
      "initializer_range",
      "layer_norm_eps"
    ]
  },
  "SwiftFormerPatchEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiftFormerDropPath": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "SwiftFormerEmbeddings": {
    "__init__": [
      "self",
      "config",
      "index"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiftFormerConvEncoder": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiftFormerMlp": {
    "__init__": [
      "self",
      "config",
      "in_features"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiftFormerEfficientAdditiveAttention": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiftFormerLocalRepresentation": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiftFormerEncoderBlock": {
    "__init__": [
      "self",
      "config",
      "dim",
      "drop_path"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiftFormerStage": {
    "__init__": [
      "self",
      "config",
      "index"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "SwiftFormerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SwiftFormerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SwiftFormerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SwiftFormerForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SwiftFormerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "num_channels",
      "depths",
      "embed_dims",
      "mlp_ratio",
      "downsamples",
      "hidden_act",
      "down_patch_size",
      "down_stride",
      "down_pad",
      "drop_path_rate",
      "drop_mlp_rate",
      "drop_conv_encoder_rate",
      "use_layer_scale",
      "layer_scale_init_value",
      "batch_norm_eps"
    ]
  },
  "OPTLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "attention_mask",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "OPTAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "OPTDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "position_ids",
      "cache_position"
    ]
  },
  "OPTPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": []
  },
  "OPTDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "position_ids",
      "cache_position"
    ]
  },
  "OPTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "position_ids",
      "cache_position"
    ]
  },
  "OPTForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "position_ids",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "OPTForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "position_ids"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ]
  },
  "OPTForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "position_ids"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ]
  },
  "OPTConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "ffn_dim",
      "max_position_embeddings",
      "do_layer_norm_before",
      "_remove_final_layer_norm",
      "word_embed_proj_dim",
      "dropout",
      "attention_dropout",
      "num_attention_heads",
      "activation_function",
      "layerdrop",
      "init_std",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "enable_bias",
      "layer_norm_elementwise_affine",
      "tie_word_embeddings"
    ]
  },
  "Phi3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Phi3RotaryEmbedding": {},
  "Phi3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Phi3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Phi3PreTrainedModel": {
    "_version": []
  },
  "Phi3ForCausalLM": {
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "Phi3ForSequenceClassification": {},
  "Phi3ForTokenClassification": {},
  "Phi3RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Phi3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Phi3Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "resid_pdrop",
      "embd_pdrop",
      "attention_dropout",
      "hidden_act",
      "max_position_embeddings",
      "original_max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id",
      "sliding_window"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "default_theta",
      "ignore_keys"
    ],
    "validate_rope": [
      "self",
      "ignore_keys"
    ]
  },
  "ShieldGemma2Config": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "mm_tokens_per_image",
      "boi_token_index",
      "eoi_token_index",
      "image_token_index",
      "initializer_range"
    ]
  },
  "ShieldGemma2ProcessorKwargs": {
    "_defaults": []
  },
  "ShieldGemma2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "image_seq_length",
      "policy_definitions"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ]
  },
  "ShieldGemma2ImageClassifierOutputWithNoAttention": {},
  "ShieldGemma2ForImageClassification": {
    "input_modalities": [],
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "token_type_ids",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "AfmoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "AfmoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "AfmoeMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "AfmoeTokenChoiceRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "expert_bias"
    ]
  },
  "AfmoeExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "selected_experts",
      "routing_weights"
    ]
  },
  "AfmoeMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "AfmoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_value",
      "cache_position"
    ]
  },
  "AfmoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "AfmoePreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_can_record_outputs": [],
    "_keep_in_fp32_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "AfmoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "position_ids",
      "past_key_values",
      "cache_position",
      "use_cache"
    ]
  },
  "AfmoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "AfmoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "moe_intermediate_size",
      "num_hidden_layers",
      "num_dense_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_theta",
      "rope_parameters",
      "num_experts",
      "num_experts_per_tok",
      "num_shared_experts",
      "route_scale",
      "global_attn_every_n_layers",
      "sliding_window",
      "layer_types",
      "attention_dropout",
      "mup_enabled",
      "eos_token_id",
      "pad_token_id",
      "bos_token_id"
    ]
  },
  "ColPaliProcessorKwargs": {
    "_defaults": []
  },
  "ColPaliProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "visual_prompt_prefix",
      "query_prefix"
    ],
    "query_augmentation_token": [
      "self"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "process_images": [
      "self",
      "images"
    ],
    "process_queries": [
      "self",
      "text"
    ],
    "score_retrieval": [
      "self",
      "query_embeddings",
      "passage_embeddings",
      "batch_size",
      "output_dtype",
      "output_device"
    ]
  },
  "ColPaliPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ColPaliForRetrievalOutput": {},
  "ColPaliForRetrieval": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ]
  },
  "ColPaliConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vlm_config",
      "text_config",
      "embedding_dim"
    ]
  },
  "MAX_IMAGE_SIZE": [],
  "_resize_output_size_rescale_to_max_len": [
    "height",
    "width",
    "min_len",
    "max_len"
  ],
  "_resize_output_size_scale_below_upper_bound": [
    "height",
    "width",
    "max_len"
  ],
  "get_num_channels": [
    "images_list"
  ],
  "get_device_from_images": [
    "images_list"
  ],
  "SmolVLMImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "max_image_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_image_splitting": [],
    "do_pad": [],
    "return_row_col_info": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "_prepare_images_structure": [
      "self",
      "images",
      "expected_ndims"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias"
    ],
    "split_images": [
      "self",
      "images",
      "max_image_size",
      "interpolation"
    ],
    "resize_for_vision_encoder": [
      "self",
      "image",
      "vision_encoder_max_size",
      "interpolation"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "fill",
      "return_pixel_mask"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_image_splitting",
      "max_image_size",
      "return_row_col_info",
      "disable_grouping",
      "return_tensors"
    ],
    "to_dict": [
      "self"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "SmolVLMVisionConfig": {
    "model_type": []
  },
  "SmolVLMPreTrainedModel": {},
  "SmolVLMVisionTransformer": {},
  "SmolVLMConfig": {
    "model_type": []
  },
  "SmolVLMImageProcessor": {},
  "SmolVLMBaseModelOutputWithPast": {},
  "SmolVLMModel": {
    "inputs_merger": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_hidden_states"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_attention_mask",
      "image_hidden_states",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "SmolVLMForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "DEFAULT_CHAT_TEMPLATE": [],
  "_prompt_split_image": [
    "image_seq_len",
    "image_rows",
    "image_cols",
    "fake_token_around_image",
    "image_token",
    "global_image_token"
  ],
  "_prompt_single_image": [
    "image_seq_len",
    "fake_token_around_image",
    "image_token",
    "global_image_token"
  ],
  "get_image_prompt_string": [
    "image_rows",
    "image_cols",
    "image_seq_len",
    "fake_token_around_image",
    "image_token",
    "global_image_token"
  ],
  "SmolVLMProcessorKwargs": {
    "_defaults": []
  },
  "SmolVLMProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "image_seq_len",
      "chat_template"
    ],
    "expand_text_with_image_tokens": [
      "self",
      "text",
      "image_rows",
      "image_cols"
    ],
    "expand_text_with_video_tokens": [
      "self",
      "text",
      "video_inputs"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template"
    ]
  },
  "SmolVLMImageProcessorKwargs": {},
  "_crop": [
    "image",
    "w1",
    "h1",
    "w2",
    "h2",
    "data_format"
  ],
  "DEFAULT_SYSTEM_MESSAGE": [],
  "DEFAULT_VIDEO_INTRO": [],
  "DEFAULT_MEDIA_OUTTRO": [],
  "FRAME_TIMESTAMP_MESSAGE": [],
  "SmolVLMVideoProcessorInitKwargs": {},
  "SmolVLMVideoProcessor": {
    "resample": [],
    "size": [],
    "max_image_size": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_pad": [],
    "do_sample_frames": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "resize": [
      "self",
      "video",
      "size",
      "interpolation",
      "antialias"
    ],
    "pad": [
      "self",
      "video",
      "padded_size",
      "max_num_frames",
      "fill",
      "return_pixel_mask"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "num_frames",
      "fps",
      "skip_secs"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_pad",
      "image_mean",
      "image_std",
      "return_tensors"
    ]
  },
  "SmolVLMVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "patch_attention_mask"
    ]
  },
  "SmolVLMVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SmolVLMVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SmolVLMEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "SmolVLMEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "SmolVLMSimpleMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SmolVLMConnector": {
    "__init__": [
      "self",
      "config"
    ],
    "pixel_shuffle": [
      "self",
      "x",
      "scale_factor"
    ],
    "forward": [
      "self",
      "image_hidden_states"
    ]
  },
  "SmolVLMCausalLMOutputWithPast": {},
  "PvtImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "model_input_names": []
  },
  "PvtDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PvtPatchEmbeddings": {
    "__init__": [
      "self",
      "config",
      "image_size",
      "patch_size",
      "stride",
      "num_channels",
      "hidden_size",
      "cls_token"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "PvtSelfOutput": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PvtEfficientSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "sequences_reduction_ratio"
    ],
    "transpose_for_scores": [
      "self",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "PvtAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "sequences_reduction_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "PvtFFN": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "hidden_features",
      "out_features"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "PvtLayer": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_attention_heads",
      "drop_path",
      "sequences_reduction_ratio",
      "mlp_ratio"
    ],
    "forward": [
      "self",
      "hidden_states",
      "height",
      "width",
      "output_attentions"
    ]
  },
  "PvtEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PvtPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "PvtModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PvtForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PvtImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "PvtConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "num_channels",
      "num_encoder_blocks",
      "depths",
      "sequence_reduction_ratios",
      "hidden_sizes",
      "patch_sizes",
      "strides",
      "num_attention_heads",
      "mlp_ratios",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "drop_path_rate",
      "layer_norm_eps",
      "qkv_bias",
      "num_labels"
    ]
  },
  "BertGenerationSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BertGenerationSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BertGenerationCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "BertGenerationAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BertGenerationIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertGenerationOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BertGenerationLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "BertGenerationEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "BertGenerationPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BertGenerationEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "BertGenerationOnlyLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertGenerationDecoder": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BertGenerationTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "is_fast": [],
    "__init__": [
      "self",
      "vocab_file",
      "bos_token",
      "eos_token",
      "unk_token",
      "pad_token",
      "sep_token",
      "sp_model_kwargs"
    ]
  },
  "BertGenerationConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "BioGptConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "scale_embedding",
      "use_cache",
      "layerdrop",
      "activation_dropout",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "BioGptLearnedPositionalEmbedding": {
    "forward": [
      "self",
      "attention_mask",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "BioGptScaledWordEmbedding": {},
  "BioGptAttention": {},
  "BioGptDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "position_ids",
      "cache_position"
    ]
  },
  "BioGptPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": []
  },
  "BioGptModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BioGptForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BioGptForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BioGptForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ]
  },
  "BioGptTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "unk_token",
      "bos_token",
      "eos_token",
      "sep_token",
      "pad_token"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "moses_tokenize": [
      "self",
      "text",
      "lang"
    ],
    "moses_detokenize": [
      "self",
      "tokens",
      "lang"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text",
      "bypass_tokenizer"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ]
  },
  "TvpImageProcessorKwargs": {},
  "TvpImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "pad_size",
      "constant_values",
      "pad_mode",
      "do_normalize",
      "do_flip_channel_order",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "pad_image": [
      "self",
      "image",
      "pad_size",
      "constant_values",
      "pad_mode",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "pad_size",
      "constant_values",
      "pad_mode",
      "do_normalize",
      "do_flip_channel_order",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "videos",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "pad_size",
      "constant_values",
      "pad_mode",
      "do_normalize",
      "do_flip_channel_order",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "TvpVideoGroundingOutput": {},
  "TvpLoss": {
    "__init__": [
      "self",
      "losses"
    ],
    "loss_iou": [
      "self",
      "start_time",
      "end_time",
      "candidates_start_time",
      "candidates_end_time",
      "duration"
    ],
    "loss_distance": [
      "self",
      "start_time",
      "end_time",
      "candidates_start_time",
      "candidates_end_time",
      "duration"
    ],
    "loss_duration": [
      "self",
      "start_time",
      "end_time",
      "candidates_start_time",
      "candidates_end_time",
      "duration"
    ],
    "forward": [
      "self",
      "logits",
      "labels"
    ]
  },
  "TvpVisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "TvpVisualInputEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embedding",
      "height",
      "width"
    ],
    "add_2d_positional_embeddings": [
      "self",
      "grid",
      "interpolate_pos_encoding"
    ],
    "forward": [
      "self",
      "grid",
      "interpolate_pos_encoding"
    ]
  },
  "TvpTextInputEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "TvpAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_reshape": [
      "self",
      "tensor",
      "sequence_length",
      "batch_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "TvpIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TvpOutputLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "TvpEncodeLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "TvpEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TvpPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "TvpPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "TvpFrameDownPadPrompter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "TvpFramePadPrompter": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pad_encoding": [
      "self",
      "prompt",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pad_encoding"
    ]
  },
  "TVP_PROMPTER_CLASSES_MAPPING": [],
  "TvpModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "interpolate_pos_encoding"
    ]
  },
  "TvpVideoGroundingHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooler_output"
    ]
  },
  "TvpForVideoGrounding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "interpolate_pos_encoding"
    ]
  },
  "TvpProcessorKwargs": {
    "_defaults": []
  },
  "TvpProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "post_process_video_grounding": [
      "self",
      "logits",
      "video_durations"
    ]
  },
  "TvpImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_pad": [],
    "pad_size": [],
    "constant_values": [],
    "pad_mode": [],
    "do_normalize": [],
    "do_flip_channel_order": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "videos"
    ],
    "_prepare_images_structure": [
      "self",
      "images"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias"
    ],
    "_flip_channel_order": [
      "self",
      "frames"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_pad",
      "pad_size",
      "constant_values",
      "pad_mode",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_flip_channel_order",
      "return_tensors",
      "disable_grouping"
    ]
  },
  "TvpConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "distance_loss_weight",
      "duration_loss_weight",
      "visual_prompter_type",
      "visual_prompter_apply",
      "visual_prompt_size",
      "max_img_size",
      "num_frames",
      "vocab_size",
      "type_vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "max_position_embeddings",
      "max_grid_col_position_embeddings",
      "max_grid_row_position_embeddings",
      "hidden_dropout_prob",
      "hidden_act",
      "layer_norm_eps",
      "initializer_range",
      "attention_probs_dropout_prob",
      "pad_token_id"
    ]
  },
  "Blip2ProcessorKwargs": {
    "_defaults": []
  },
  "Blip2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "num_query_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ]
  },
  "Blip2VisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "qkv_bias"
    ]
  },
  "Blip2QFormerConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "cross_attention_frequency",
      "encoder_hidden_size",
      "use_qformer_text_input"
    ]
  },
  "Blip2Config": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "qformer_config",
      "text_config",
      "num_query_tokens",
      "image_text_hidden_size",
      "image_token_index"
    ]
  },
  "Blip2ForConditionalGenerationModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Blip2ImageTextMatchingModelOutput": {
    "to_tuple": [
      "self"
    ]
  },
  "Blip2TextModelOutput": {},
  "Blip2VisionModelOutput": {},
  "Blip2VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "Blip2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Blip2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Blip2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Blip2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Blip2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds"
    ]
  },
  "Blip2VisionModel": {
    "main_input_name": [],
    "input_modalities": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_input_embeddings": [
      "self"
    ]
  },
  "Blip2QFormerMultiHeadAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention"
    ],
    "save_attn_gradients": [
      "self",
      "attn_gradients"
    ],
    "get_attn_gradients": [
      "self"
    ],
    "save_attention_map": [
      "self",
      "attention_map"
    ],
    "get_attention_map": [
      "self"
    ],
    "transpose_for_scores": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "Blip2QFormerSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Blip2QFormerAttention": {
    "__init__": [
      "self",
      "config",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "Blip2QFormerIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Blip2QFormerOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Blip2QFormerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "query_length"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ],
    "feed_forward_chunk_query": [
      "self",
      "attention_output"
    ]
  },
  "Blip2QFormerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "query_length"
    ]
  },
  "Blip2TextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "query_embeds"
    ]
  },
  "Blip2QFormerModel": {
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_extended_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "device",
      "has_query"
    ],
    "forward": [
      "self",
      "query_embeds",
      "query_length",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "Blip2Model": {
    "main_input_name": [],
    "_keep_in_fp32_modules": [],
    "_supports_flash_attn": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_encoder": [
      "self",
      "modality"
    ],
    "get_text_features": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "labels"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_qformer_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "labels",
      "interpolate_pos_encoding"
    ]
  },
  "Blip2TextModelWithProjection": {
    "supports_gradient_checkpointing": [],
    "_keep_in_fp32_modules": [],
    "_supports_flash_attn": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "Blip2VisionModelWithProjection": {
    "main_input_name": [],
    "input_modalities": [],
    "_keep_in_fp32_modules": [],
    "_supports_flash_attn": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Blip2ForConditionalGeneration": {
    "main_input_name": [],
    "_can_compile_fullgraph": [],
    "_keep_in_fp32_modules": [],
    "_supports_flash_attn": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_encoder": [
      "self",
      "modality"
    ],
    "_preprocess_accelerate": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "inputs_embeds",
      "labels",
      "interpolate_pos_encoding"
    ],
    "generate": [
      "self",
      "pixel_values",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "interpolate_pos_encoding"
    ]
  },
  "Blip2ForImageTextRetrieval": {
    "main_input_name": [],
    "input_modalities": [],
    "_keep_in_fp32_modules": [],
    "_supports_flash_attn": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "attention_mask",
      "use_image_text_matching_head",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2FeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "return_attention_mask",
      "do_normalize"
    ],
    "zero_mean_unit_var_norm": [
      "input_values",
      "attention_mask",
      "padding_value"
    ],
    "__call__": [
      "self",
      "raw_speech",
      "padding",
      "max_length",
      "truncation",
      "pad_to_multiple_of",
      "return_attention_mask",
      "return_tensors",
      "sampling_rate"
    ]
  },
  "Wav2Vec2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "feat_quantizer_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "do_stable_layer_norm",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "num_codevectors_per_group",
      "num_codevector_groups",
      "contrastive_logits_temperature",
      "num_negatives",
      "codevector_dim",
      "proj_codevector_dim",
      "diversity_loss_weight",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "tdnn_dim",
      "tdnn_kernel",
      "tdnn_dilation",
      "xvector_output_dim",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_adapter",
      "adapter_kernel_size",
      "adapter_stride",
      "num_adapter_layers",
      "output_hidden_size",
      "adapter_attn_dim"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "WAV2VEC2_KWARGS_DOCSTRING": [],
  "Wav2Vec2CTCTokenizerOutput": {},
  "Wav2Vec2CTCTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "bos_token",
      "eos_token",
      "unk_token",
      "pad_token",
      "word_delimiter_token",
      "replace_word_delimiter_char",
      "do_lower_case",
      "target_lang"
    ],
    "set_target_lang": [
      "self",
      "target_lang"
    ],
    "word_delimiter_token": [
      "self",
      "value"
    ],
    "word_delimiter_token_id": [
      "self",
      "value"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens",
      "group_tokens",
      "spaces_between_special_tokens",
      "output_char_offsets",
      "output_word_offsets"
    ],
    "clean_up_tokenization": [
      "out_string"
    ],
    "_compute_offsets": [
      "char_repetitions",
      "chars",
      "ctc_token"
    ],
    "_get_word_offsets": [
      "offsets",
      "word_delimiter_char"
    ],
    "prepare_for_tokenization": [
      "self",
      "text",
      "is_split_into_words"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "group_tokens",
      "spaces_between_special_tokens",
      "output_word_offsets",
      "output_char_offsets"
    ],
    "batch_decode": [
      "self",
      "sequences",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "output_char_offsets",
      "output_word_offsets"
    ],
    "decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces",
      "output_char_offsets",
      "output_word_offsets"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "WAV2VEC2_ADAPTER_PT_FILE": [],
  "WAV2VEC2_ADAPTER_SAFE_FILE": [],
  "Wav2Vec2ForPreTrainingOutput": {},
  "_sample_negative_indices": [
    "features_shape",
    "num_negatives",
    "mask_time_indices"
  ],
  "Wav2Vec2NoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2LayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2GroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2PositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2SamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2FeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "Wav2Vec2FeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2Attention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Wav2Vec2FeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Wav2Vec2EncoderLayerStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Wav2Vec2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2EncoderStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2GumbelVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_perplexity": [
      "probs",
      "mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask_time_indices"
    ]
  },
  "Wav2Vec2Adapter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2AdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2AttnAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths",
      "add_adapter"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask",
      "add_adapter"
    ],
    "_get_adapters": [
      "self"
    ],
    "init_adapter_layers": [
      "self"
    ],
    "load_adapter": [
      "self",
      "target_lang",
      "force_load"
    ]
  },
  "Wav2Vec2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2ForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "set_gumbel_temperature": [
      "self",
      "temperature"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "compute_contrastive_logits": [
      "target_features",
      "negative_features",
      "predicted_features",
      "temperature"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "sampled_negative_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2ForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "tie_weights": [
      "self"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2ForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2ForAudioFrameClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2ForXVector": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "_get_tdnn_output_lengths": [
      "self",
      "input_lengths"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2ProcessorKwargs": {
    "_defaults": []
  },
  "Wav2Vec2Processor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "audio",
      "text"
    ],
    "pad": [
      "self"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "VitDetEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "get_absolute_positions": [
      "self",
      "abs_pos_embeddings",
      "has_cls_token",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "get_rel_pos": [
    "q_size",
    "k_size",
    "rel_pos"
  ],
  "add_decomposed_relative_positions": [
    "attn",
    "queries",
    "rel_pos_h",
    "rel_pos_w",
    "q_size",
    "k_size"
  ],
  "VitDetAttention": {
    "__init__": [
      "self",
      "config",
      "input_size"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_attentions"
    ]
  },
  "VitDetDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "VitDetLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape",
      "eps"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VitDetResBottleneckBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "bottleneck_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VitDetMlp": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "hidden_features"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VitDetLayer": {
    "__init__": [
      "self",
      "config",
      "drop_path_rate",
      "window_size",
      "use_residual_block"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "VitDetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VitDetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VitDetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VitDetBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "VitDetConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "hidden_act",
      "dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "pretrain_image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "drop_path_rate",
      "window_block_indices",
      "residual_block_indices",
      "use_absolute_position_embeddings",
      "use_relative_position_embeddings",
      "window_size",
      "out_features",
      "out_indices"
    ]
  },
  "_pad_to_multiple": [
    "x",
    "block_len",
    "dim",
    "pad_value"
  ],
  "_split_into_blocks": [
    "x",
    "block_len",
    "dim"
  ],
  "_concatenate_3_blocks": [
    "x",
    "block_dim",
    "sequence_dim",
    "pad_value"
  ],
  "_make_3block_relative_position_ids": [
    "block_len"
  ],
  "_mask_local_attention_mask": [
    "local_attention_mask",
    "block_len"
  ],
  "_get_local_attention_mask": [
    "attention_mask",
    "block_len",
    "device"
  ],
  "_make_global_fixed_block_ids": [
    "attention_mask",
    "global_block_size"
  ],
  "_make_side_relative_position_ids": [
    "attention_mask",
    "global_block_size"
  ],
  "_create_global_aggregates": [
    "hidden_states",
    "block_ids",
    "global_seq_len"
  ],
  "LongT5LayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongT5DenseActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongT5DenseGatedActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongT5LayerFF": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongT5Attention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_values",
      "query_length",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "LongT5LocalAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "block_length"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "LongT5TransientGlobalAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "block_length"
    ],
    "compute_side_bias": [
      "self",
      "mask",
      "global_segment_ids"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "LongT5LayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "LongT5LayerLocalSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "LongT5LayerTransientGlobalSelfAttention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "output_attentions"
    ]
  },
  "LongT5LayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "position_bias",
      "past_key_values",
      "use_cache",
      "query_length",
      "output_attentions",
      "cache_position"
    ]
  },
  "LongT5Block": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "return_dict",
      "cache_position"
    ]
  },
  "LongT5PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_can_compile_fullgraph": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "LongT5Stack": {
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "LongT5Model": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "LongT5ForConditionalGeneration": {
    "_keys_to_ignore_on_load_unexpected": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "LongT5EncoderModel": {
    "_tied_weights_keys": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LongT5Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "d_kv",
      "d_ff",
      "num_layers",
      "num_decoder_layers",
      "num_heads",
      "local_radius",
      "global_block_size",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "dropout_rate",
      "layer_norm_epsilon",
      "initializer_factor",
      "feed_forward_proj",
      "is_encoder_decoder",
      "encoder_attention_type",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "is_decoder",
      "bos_token_id",
      "tie_word_embeddings"
    ]
  },
  "BarthezTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "slow_tokenizer_class": [],
    "__init__": [
      "self",
      "vocab",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space"
    ]
  },
  "RobertaPreLayerNormConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "classifier_dropout",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "RobertaPreLayerNormEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "RobertaPreLayerNormSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "RobertaPreLayerNormCrossAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "RobertaPreLayerNormSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RobertaPreLayerNormAttention": {
    "__init__": [
      "self",
      "config",
      "is_causal",
      "layer_idx",
      "is_cross_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "RobertaPreLayerNormIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RobertaPreLayerNormOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "RobertaPreLayerNormLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "RobertaPreLayerNormEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "RobertaPreLayerNormPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RobertaPreLayerNormPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RobertaPreLayerNormModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "cache_position"
    ],
    "_create_attention_masks": [
      "self",
      "attention_mask",
      "encoder_attention_mask",
      "embedding_output",
      "encoder_hidden_states",
      "cache_position",
      "past_key_values"
    ]
  },
  "RobertaPreLayerNormForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "RobertaPreLayerNormForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels"
    ]
  },
  "RobertaPreLayerNormLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "RobertaPreLayerNormForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "RobertaPreLayerNormForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "RobertaPreLayerNormForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels"
    ]
  },
  "RobertaPreLayerNormClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "RobertaPreLayerNormForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions"
    ]
  },
  "StableLmConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "intermediate_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "use_qkv_bias",
      "qk_layernorm",
      "use_parallel_residual",
      "hidden_dropout",
      "attention_dropout",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ]
  },
  "StableLmRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "StableLmMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "StableLmLayerNormPerHead": {
    "__init__": [
      "self",
      "dim",
      "num_heads",
      "eps",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "StableLmAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "StableLmDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "StableLmPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": []
  },
  "StableLmModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "StableLmForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "StableLmForSequenceClassification": {},
  "StableLmForTokenClassification": {},
  "SuperPointImageProcessorKwargs": {},
  "SuperPointImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_grayscale"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_grayscale",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_keypoint_detection": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "remove_keypoints_from_borders": [
    "keypoints",
    "scores",
    "border",
    "height",
    "width"
  ],
  "top_k_keypoints": [
    "keypoints",
    "scores",
    "k"
  ],
  "simple_nms": [
    "scores",
    "nms_radius"
  ],
  "SuperPointKeypointDescriptionOutput": {},
  "SuperPointConvBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "add_pooling"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SuperPointEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SuperPointInterestPointDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoded"
    ],
    "_get_pixel_scores": [
      "self",
      "encoded"
    ],
    "_extract_keypoints": [
      "self",
      "scores"
    ]
  },
  "SuperPointDescriptorDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "encoded",
      "keypoints"
    ],
    "_sample_descriptors": [
      "keypoints",
      "descriptors",
      "scale"
    ]
  },
  "SuperPointPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "extract_one_channel_pixel_values": [
      "self",
      "pixel_values"
    ]
  },
  "SuperPointForKeypointDetection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SuperPointImageProcessorFast": {
    "resample": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "_preprocess": [
      "self",
      "images",
      "size",
      "rescale_factor",
      "do_rescale",
      "do_resize",
      "interpolation",
      "do_grayscale",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_keypoint_detection": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "SuperPointConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "encoder_hidden_sizes",
      "decoder_hidden_size",
      "keypoint_decoder_dim",
      "descriptor_decoder_dim",
      "keypoint_threshold",
      "max_keypoints",
      "nms_radius",
      "border_removal_distance",
      "initializer_range"
    ]
  },
  "M2M100Tokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "spm_file",
      "src_lang",
      "tgt_lang",
      "bos_token",
      "eos_token",
      "sep_token",
      "pad_token",
      "unk_token",
      "language_codes",
      "sp_model_kwargs",
      "num_madeup_words"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "src_lang": [
      "self",
      "new_src_lang"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "prepare_seq2seq_batch": [
      "self",
      "src_texts",
      "src_lang",
      "tgt_texts",
      "tgt_lang"
    ],
    "_build_translation_inputs": [
      "self",
      "raw_inputs",
      "src_lang",
      "tgt_lang"
    ],
    "_switch_to_input_mode": [
      "self"
    ],
    "_switch_to_target_mode": [
      "self"
    ],
    "set_src_lang_special_tokens": [
      "self",
      "src_lang"
    ],
    "set_tgt_lang_special_tokens": [
      "self",
      "tgt_lang"
    ],
    "get_lang_token": [
      "self",
      "lang"
    ],
    "get_lang_id": [
      "self",
      "lang"
    ]
  },
  "M2M100Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "M2M100ScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "M2M100SinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "create_position_ids_from_inputs_embeds": [
      "inputs_embeds",
      "past_key_values_length",
      "padding_idx"
    ],
    "create_position_ids_from_input_ids": [
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "M2M100Attention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "M2M100EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "M2M100DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "M2M100PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "M2M100Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "M2M100Decoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "M2M100Model": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "M2M100ForConditionalGeneration": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "VitPoseBackbonePatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "VitPoseBackboneEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "VitPoseBackboneSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VitPoseBackboneSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "VitPoseBackboneAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VitPoseNaiveMoe": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "indices"
    ]
  },
  "VitPoseBackboneMoeMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "indices"
    ]
  },
  "VitPoseBackboneMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "VitPoseBackboneLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "dataset_index"
    ]
  },
  "VitPoseBackboneEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "dataset_index",
      "output_hidden_states"
    ]
  },
  "VitPoseBackbonePreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VitPoseBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "dataset_index",
      "output_hidden_states"
    ]
  },
  "VitPoseBackboneConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "num_experts",
      "part_features",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "qkv_bias",
      "out_features",
      "out_indices"
    ]
  },
  "LightOnOcrRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "LightOnOcrPatchMerger": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features",
      "image_sizes"
    ]
  },
  "LightOnOcrMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features",
      "image_sizes"
    ]
  },
  "LightOnOcrModelOutputWithPast": {},
  "LightOnOcrPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": []
  },
  "LightOnOcrModel": {
    "_checkpoint_conversion_mapping": [],
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "image_sizes"
    ]
  },
  "LightOnOcrCausalLMOutputWithPast": {},
  "LightOnOcrForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_sizes"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep",
      "image_sizes"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "LightOnOcrConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "spatial_merge_size",
      "image_token_id",
      "tie_word_embeddings",
      "vision_config",
      "text_config"
    ]
  },
  "LightOnOcrProcessorKwargs": {
    "_defaults": []
  },
  "LightOnOcrProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "patch_size",
      "spatial_merge_size",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "_num_image_tokens": [
    "image_size",
    "patch_size"
  ],
  "apply_depth_multiplier": [
    "config",
    "channels"
  ],
  "apply_tf_padding": [
    "features",
    "conv_layer"
  ],
  "MobileNetV2ConvLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "bias",
      "dilation",
      "use_normalization",
      "use_activation",
      "layer_norm_eps"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileNetV2InvertedResidual": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "dilation"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileNetV2Stem": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "expanded_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileNetV2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": []
  },
  "MobileNetV2Model": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileNetV2ForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "MobileNetV2DeepLabV3Plus": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileNetV2ForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileNetV2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "image_size",
      "depth_multiplier",
      "depth_divisible_by",
      "min_depth",
      "expand_ratio",
      "output_stride",
      "first_layer_is_expansion",
      "finegrained_output",
      "hidden_act",
      "tf_padding",
      "classifier_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "semantic_loss_ignore_index"
    ]
  },
  "MobileNetV2ImageProcessorKwargs": {},
  "MobileNetV2ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_reduce_labels"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "reduce_label": [
      "self",
      "label"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_reduce_labels",
      "do_resize",
      "do_rescale",
      "do_center_crop",
      "do_normalize",
      "size",
      "resample",
      "rescale_factor",
      "crop_size",
      "image_mean",
      "image_std",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_center_crop",
      "crop_size",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_reduce_labels",
      "do_resize",
      "size",
      "do_center_crop",
      "crop_size",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "MobileNetV2ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_reduce_labels": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "reduce_label": [
      "self",
      "labels"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_reduce_labels",
      "do_resize",
      "do_rescale",
      "do_center_crop",
      "do_normalize",
      "size",
      "interpolation",
      "rescale_factor",
      "crop_size",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "ChameleonTextKwargs": {},
  "ChameleonProcessorKwargs": {
    "_defaults": []
  },
  "ChameleonProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "image_seq_length",
      "image_token"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "ChameleonVQVAEConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "embed_dim",
      "num_embeddings",
      "double_latent",
      "latent_channels",
      "resolution",
      "in_channels",
      "base_channels",
      "channel_multiplier",
      "num_res_blocks",
      "attn_resolutions",
      "dropout",
      "attn_type",
      "initializer_range"
    ]
  },
  "ChameleonConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "model_parallel_size",
      "swin_norm",
      "vq_config",
      "vocabulary_map",
      "mlp_bias"
    ]
  },
  "ChameleonVQVAEModelOutput": {},
  "ChameleonRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ChameleonRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "ChameleonMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ChameleonLayerNorm": {
    "__init__": [
      "self",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChameleonAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "ChameleonDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "ChameleonSwinDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "ChameleonVQVAEVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ChameleonVQVAEEncoderConvDownsample": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChameleonVQVAEEncoderResnetBlock": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "conv_shortcut"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChameleonVQVAEEncoderAttnBlock": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ChameleonVQVAEEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "ChameleonImageVocabularyMapping": {
    "__init__": [
      "self",
      "vocab_map"
    ],
    "val2name": [
      "self"
    ],
    "image_tokens": [
      "self"
    ],
    "bpe2img": [
      "self"
    ],
    "img2bpe": [
      "self"
    ],
    "bpe2img_search_tensors": [
      "self"
    ],
    "img2bpe_mapping_tensor": [
      "self"
    ],
    "convert_img2bpe": [
      "self",
      "img_batch"
    ]
  },
  "ChameleonPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "ChameleonVQVAE": {
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "encode": [
      "self",
      "pixel_values"
    ]
  },
  "ChameleonModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_image_tokens": [
      "self",
      "pixel_values"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "ChameleonForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_image_tokens": [
      "self",
      "pixel_values"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "pixel_values",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "is_first_iteration"
    ]
  },
  "ChameleonImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "convert_to_rgb": [
      "self",
      "image"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ]
  },
  "ChameleonImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "blend_rgba": [
      "self",
      "image"
    ]
  },
  "SamHQVisionEncoderOutput": {},
  "SamHQMMaskDecoderOutputs": {},
  "SamHQImageSegmentationOutput": {},
  "SamHQVisionAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "get_rel_pos": [
      "self",
      "q_size",
      "k_size",
      "rel_pos"
    ],
    "get_decomposed_rel_pos": [
      "self",
      "query",
      "rel_pos_h",
      "rel_pos_w",
      "q_size",
      "k_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "SamHQMLPBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamHQVisionSdpaAttention": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_attentions"
    ]
  },
  "SAM_HQ_VISION_ATTENTION_CLASSES": [],
  "SamHQVisionLayer": {
    "__init__": [
      "self",
      "config",
      "window_size"
    ],
    "window_partition": [
      "self",
      "hidden_states",
      "window_size"
    ],
    "window_unpartition": [
      "self",
      "windows",
      "window_size",
      "padding_shape",
      "original_shape"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamHQPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_coords",
      "input_shape"
    ]
  },
  "SamHQPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_supports_sdpa": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SamHQPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SamHQVisionNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamHQVisionEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SamHQLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "SamHQAttention": {
    "__init__": [
      "self",
      "config",
      "downsample_rate"
    ],
    "_separate_heads": [
      "self",
      "hidden_states",
      "num_attention_heads"
    ],
    "_recombine_heads": [
      "self",
      "hidden_states",
      "point_batch_size"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_similarity"
    ]
  },
  "SamHQTwoWayAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "attention_downsample_rate",
      "skip_first_layer_pe"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "query_point_embedding",
      "key_point_embedding",
      "attention_similarity"
    ]
  },
  "SamHQTwoWayTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "point_embeddings",
      "image_embeddings",
      "image_positional_embeddings",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "SamHQFeedForward": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "sigmoid_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "SamHQMaskDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "sparse_prompt_embeddings",
      "dense_prompt_embeddings",
      "multimask_output",
      "hq_token_only",
      "intermediate_embeddings",
      "attention_similarity",
      "target_embedding"
    ]
  },
  "SamHQVisionModel": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "SamHQMaskEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "masks"
    ]
  },
  "SamHQPromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels",
      "pad"
    ],
    "_embed_boxes": [
      "self",
      "boxes"
    ],
    "forward": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ]
  },
  "SamHQModel": {
    "input_modalities": [],
    "_can_record_outputs": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_prompt_embeddings": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "input_boxes",
      "input_masks",
      "image_embeddings",
      "multimask_output",
      "hq_token_only",
      "attention_similarity",
      "target_embedding",
      "intermediate_embeddings"
    ]
  },
  "SamHQPromptEncoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "image_size",
      "patch_size",
      "mask_input_channels",
      "num_point_embeddings",
      "hidden_act",
      "layer_norm_eps"
    ]
  },
  "SamHQVisionConfig": {
    "base_config_key": [],
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "output_channels",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range",
      "qkv_bias",
      "mlp_ratio",
      "use_abs_pos",
      "use_rel_pos",
      "window_size",
      "global_attn_indexes",
      "num_pos_feats",
      "mlp_dim"
    ]
  },
  "SamHQMaskDecoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "hidden_act",
      "mlp_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "attention_downsample_rate",
      "num_multimask_outputs",
      "iou_head_depth",
      "iou_head_hidden_dim",
      "layer_norm_eps",
      "vit_dim"
    ]
  },
  "SamHQConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "prompt_encoder_config",
      "mask_decoder_config",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "SamHQImagesKwargs": {},
  "SamHQProcessorKwargs": {
    "_defaults": []
  },
  "SamHQProcessor": {
    "__init__": [
      "self",
      "image_processor"
    ],
    "__call__": [
      "self",
      "images"
    ],
    "_normalize_and_convert": [
      "self",
      "encoding_image_processor",
      "original_sizes",
      "input_points",
      "input_labels",
      "input_boxes",
      "return_tensors",
      "point_pad_value"
    ],
    "_pad_points_and_labels": [
      "self",
      "input_points",
      "input_labels",
      "point_pad_value"
    ],
    "_normalize_coordinates": [
      "self",
      "target_size",
      "coords",
      "original_size",
      "is_bounding_box"
    ],
    "_preprocess_input": [
      "self",
      "inputs",
      "error_message",
      "expected_nesting",
      "dtype"
    ],
    "_check_and_preprocess_points": [
      "self",
      "input_points",
      "input_labels",
      "input_boxes"
    ],
    "model_input_names": [
      "self"
    ],
    "post_process_masks": [
      "self"
    ],
    "_to_tensor": [
      "self",
      "array",
      "min_dim",
      "return_tensors"
    ],
    "_normalize_batch_coordinates": [
      "self",
      "inputs",
      "original_sizes",
      "is_bounding_box"
    ]
  },
  "XGLMConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "d_model",
      "ffn_dim",
      "num_layers",
      "attention_heads",
      "activation_function",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "layerdrop",
      "init_std",
      "scale_embedding",
      "use_cache",
      "decoder_start_token_id",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "XGLMScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "XGLMSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "position_ids",
      "past_key_values_length"
    ]
  },
  "XGLMAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "XGLMDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "XGLMPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "XGLMModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "XGLMForCausalLM": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "XGLMTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "add_prefix_space"
    ]
  },
  "get_target_size": [
    "size_dict"
  ],
  "reorder_patches_and_offsets": [
    "patches",
    "offsets"
  ],
  "EomtImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_split_image": [],
    "do_pad": [],
    "ignore_index": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "_split_image": [
      "self",
      "images",
      "size",
      "image_indices"
    ],
    "_pad": [
      "self",
      "images",
      "size"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_split_image",
      "do_pad",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "merge_image_patches": [
      "self",
      "segmentation_logits",
      "patch_offsets",
      "target_sizes",
      "size"
    ],
    "unpad_image": [
      "self",
      "segmentation_logits",
      "target_sizes",
      "size"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "size"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "stuff_classes",
      "size"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "threshold",
      "size"
    ]
  },
  "EomtImageProcessorKwargs": {},
  "EomtImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_split_image",
      "do_pad",
      "image_mean",
      "image_std",
      "ignore_index",
      "num_labels"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_split_image": [
      "self",
      "image",
      "size",
      "image_index"
    ],
    "_pad": [
      "self",
      "image",
      "size"
    ],
    "_preprocess_images": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_split_image",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_resize",
      "do_pad",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_split_image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_pad",
      "image_mean",
      "image_std",
      "ignore_index",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "encode_inputs": [
      "self",
      "pixel_values_list",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "ignore_index",
      "return_tensors",
      "input_data_format"
    ],
    "merge_image_patches": [
      "self",
      "segmentation_logits",
      "patch_offsets",
      "target_sizes",
      "size"
    ],
    "unpad_image": [
      "self",
      "segmentation_logits",
      "target_sizes",
      "size"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "size"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "stuff_classes",
      "size"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "target_sizes",
      "threshold",
      "size"
    ]
  },
  "EomtForUniversalSegmentationOutput": {},
  "EomtHungarianMatcher": {
    "__init__": [
      "self",
      "cost_class",
      "cost_mask",
      "cost_dice",
      "num_points"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels"
    ]
  },
  "EomtLoss": {
    "__init__": [
      "self",
      "config",
      "weight_dict"
    ],
    "_max_by_axis": [
      "self",
      "sizes"
    ],
    "_pad_images_to_max_in_batch": [
      "self",
      "tensors"
    ],
    "loss_labels": [
      "self",
      "class_queries_logits",
      "class_labels",
      "indices"
    ],
    "loss_masks": [
      "self",
      "masks_queries_logits",
      "mask_labels",
      "indices",
      "num_masks"
    ],
    "_get_predictions_permutation_indices": [
      "self",
      "indices"
    ],
    "_get_targets_permutation_indices": [
      "self",
      "indices"
    ],
    "calculate_uncertainty": [
      "self",
      "logits"
    ],
    "sample_points_using_uncertainty": [
      "self",
      "logits",
      "uncertainty_function",
      "num_points",
      "oversample_ratio",
      "importance_sample_ratio"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_predictions"
    ],
    "get_num_masks": [
      "self",
      "class_labels",
      "device"
    ]
  },
  "EomtPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "EomtEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "EomtAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "EomtLayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "EomtDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "EomtMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "EomtSwiGLUFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "EomtLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "EomtLayerNorm2d": {
    "__init__": [
      "self",
      "num_channels",
      "eps",
      "affine"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "EomtScaleLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EomtScaleBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EomtMaskHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "EomtPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "EomtForUniversalSegmentation": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_loss_dict": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_predictions"
    ],
    "get_loss": [
      "self",
      "loss_dict"
    ],
    "forward": [
      "self",
      "pixel_values",
      "mask_labels",
      "class_labels",
      "patch_offsets"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "predict": [
      "self",
      "logits"
    ],
    "_disable_attention_mask": [
      "attn_mask",
      "prob",
      "num_query_tokens",
      "encoder_start_tokens",
      "device"
    ]
  },
  "EomtConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "hidden_act",
      "hidden_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "layerscale_value",
      "drop_path_rate",
      "num_upscale_blocks",
      "attention_dropout",
      "use_swiglu_ffn",
      "num_blocks",
      "no_object_weight",
      "class_weight",
      "mask_weight",
      "dice_weight",
      "train_num_points",
      "oversample_ratio",
      "importance_sample_ratio",
      "num_queries",
      "num_register_tokens"
    ]
  },
  "_constrain_to_multiple_of": [
    "val",
    "multiple",
    "min_val",
    "max_val"
  ],
  "_get_resize_output_image_size": [
    "input_image",
    "output_size",
    "keep_aspect_ratio",
    "multiple"
  ],
  "PromptDepthAnythingImageProcessorFast": {
    "model_input_names": [],
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "keep_aspect_ratio": [],
    "ensure_multiple_of": [],
    "do_pad": [],
    "size_divisor": [],
    "prompt_scale_to_meter": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images",
      "prompt_depth"
    ],
    "resize_with_aspect_ratio": [
      "self",
      "image",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "interpolation"
    ],
    "pad_image": [
      "self",
      "image",
      "size_divisor"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "prompt_depth",
      "input_data_format",
      "device",
      "prompt_scale_to_meter",
      "return_tensors"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "keep_aspect_ratio",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "disable_grouping",
      "ensure_multiple_of",
      "return_tensors",
      "size_divisor"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "PromptDepthAnythingConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "patch_size",
      "initializer_range",
      "reassemble_hidden_size",
      "reassemble_factors",
      "neck_hidden_sizes",
      "fusion_hidden_size",
      "head_in_index",
      "head_hidden_size",
      "depth_estimation_type",
      "max_depth"
    ]
  },
  "PromptDepthAnythingLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "prompt_depth"
    ]
  },
  "PromptDepthAnythingFeatureFusionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "residual",
      "size",
      "prompt_depth"
    ]
  },
  "PromptDepthAnythingFeatureFusionStage": {
    "forward": [
      "self",
      "hidden_states",
      "size",
      "prompt_depth"
    ]
  },
  "PromptDepthAnythingDepthEstimationHead": {
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "PromptDepthAnythingPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": []
  },
  "PromptDepthAnythingReassembleLayer": {
    "__init__": [
      "self",
      "config",
      "channels",
      "factor"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "PromptDepthAnythingReassembleStage": {},
  "PromptDepthAnythingNeck": {
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width",
      "prompt_depth"
    ]
  },
  "PromptDepthAnythingForDepthEstimation": {
    "forward": [
      "self",
      "pixel_values",
      "prompt_depth",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PromptDepthAnythingPreActResidualLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "PromptDepthAnythingImageProcessorKwargs": {},
  "PromptDepthAnythingImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "prompt_scale_to_meter"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "pad_image": [
      "self",
      "image",
      "size_divisor",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "prompt_depth",
      "do_resize",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "size_divisor",
      "prompt_scale_to_meter",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "Glm4MoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "moe_intermediate_size",
      "num_experts_per_tok",
      "n_shared_experts",
      "n_routed_experts",
      "routed_scaling_factor",
      "n_group",
      "topk_group",
      "first_k_dense_replace",
      "norm_topk_prob",
      "use_qk_norm",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ]
  },
  "Glm4MoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Glm4MoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Glm4MoeMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Glm4MoeTopkRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4MoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Glm4MoeNaiveMoe": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Glm4MoeMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4MoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Glm4MoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_keep_in_fp32_modules_strict": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Glm4MoeModel": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "Glm4MoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MixtralExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "MixtralTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MixtralSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MixtralRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MixtralRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "MixtralAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "MixtralDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "MixtralPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MixtralModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "MixtralForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MixtralForSequenceClassification": {},
  "MixtralForTokenClassification": {},
  "MixtralForQuestionAnswering": {},
  "MixtralConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "sliding_window",
      "attention_dropout",
      "num_experts_per_tok",
      "num_local_experts",
      "output_router_logits",
      "router_aux_loss_coef",
      "router_jitter_noise",
      "rope_parameters"
    ]
  },
  "lowercase_and_remove_accent": [
    "text"
  ],
  "romanian_preprocessing": [
    "text"
  ],
  "XLMTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "unk_token",
      "bos_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "additional_special_tokens",
      "lang2id",
      "id2lang",
      "do_lowercase_and_remove_accent"
    ],
    "do_lower_case": [
      "self"
    ],
    "moses_punct_norm": [
      "self",
      "text",
      "lang"
    ],
    "moses_tokenize": [
      "self",
      "text",
      "lang"
    ],
    "moses_pipeline": [
      "self",
      "text",
      "lang"
    ],
    "ja_tokenize": [
      "self",
      "text"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text",
      "lang",
      "bypass_tokenizer"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ]
  },
  "XLMSquadHeadOutput": {},
  "XLMPoolerStartLogits": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "p_mask"
    ]
  },
  "XLMPoolerEndLogits": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_states",
      "start_positions",
      "p_mask"
    ]
  },
  "XLMPoolerAnswerClass": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_states",
      "start_positions",
      "cls_index"
    ]
  },
  "XLMSQuADHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "start_positions",
      "end_positions",
      "cls_index",
      "is_impossible",
      "p_mask",
      "return_dict"
    ]
  },
  "XLMSequenceSummary": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cls_index"
    ]
  },
  "XLMPreTrainedModel": {
    "base_model_prefix": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "XLMForQuestionAnsweringOutput": {},
  "XLMModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "XLMPredLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "y"
    ]
  },
  "XLMWithLMHeadModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "is_first_iteration"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "XLMForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLMForQuestionAnsweringSimple": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLMForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "is_impossible",
      "cls_index",
      "p_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLMForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLMForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "langs",
      "token_type_ids",
      "position_ids",
      "lengths",
      "cache",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "XLMConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "emb_dim",
      "n_layers",
      "n_heads",
      "dropout",
      "attention_dropout",
      "gelu_activation",
      "sinusoidal_embeddings",
      "causal",
      "asm",
      "n_langs",
      "use_lang_emb",
      "max_position_embeddings",
      "embed_init_std",
      "layer_norm_eps",
      "init_std",
      "unk_index",
      "mask_index",
      "is_encoder",
      "summary_type",
      "summary_use_proj",
      "summary_activation",
      "summary_proj_to_labels",
      "summary_first_dropout",
      "start_n_top",
      "end_n_top",
      "mask_token_id",
      "lang_id",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "Qwen2AudioEncoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_mel_bins",
      "encoder_layers",
      "encoder_attention_heads",
      "encoder_ffn_dim",
      "encoder_layerdrop",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_function",
      "activation_dropout",
      "scale_embedding",
      "initializer_range",
      "max_source_positions"
    ]
  },
  "Qwen2AudioConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "audio_config",
      "text_config",
      "audio_token_index"
    ]
  },
  "Qwen2AudioProcessorKwargs": {
    "_defaults": []
  },
  "Qwen2AudioProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer",
      "chat_template",
      "audio_token",
      "audio_bos_token",
      "audio_eos_token"
    ],
    "__call__": [
      "self",
      "text",
      "audio"
    ],
    "model_input_names": [
      "self"
    ],
    "default_chat_template": [
      "self"
    ]
  },
  "Qwen2AudioCausalLMOutputWithPast": {},
  "Qwen2AudioAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "layer_idx",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Qwen2AudioEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Qwen2AudioPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": []
  },
  "Qwen2AudioEncoder": {
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ]
  },
  "Qwen2AudioMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_features"
    ]
  },
  "Qwen2AudioForConditionalGeneration": {
    "__init__": [
      "self",
      "config"
    ],
    "padding_side": [
      "self",
      "padding_side"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "_merge_input_ids_with_audio_features": [
      "self",
      "audio_features",
      "num_audio_tokens",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "labels"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "attention_mask",
      "feature_attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ]
  },
  "GlmOcrVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "attention_bias",
      "attention_dropout",
      "num_heads",
      "in_channels",
      "image_size",
      "patch_size",
      "rms_norm_eps",
      "spatial_merge_size",
      "temporal_patch_size",
      "out_hidden_size",
      "intermediate_size",
      "initializer_range"
    ]
  },
  "GlmOcrTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "attention_dropout",
      "rope_parameters",
      "pad_token_id"
    ]
  },
  "GlmOcrConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "image_start_token_id",
      "image_end_token_id",
      "video_start_token_id",
      "video_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "GlmOcrRMSNorm": {},
  "GlmOcrVisionMlp": {
    "__init__": [
      "self",
      "config",
      "bias"
    ]
  },
  "GlmOcrTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "GlmOcrPreTrainedModel": {
    "_keys_to_ignore_on_load_unexpected": []
  },
  "GlmOcrModelOutputWithPast": {},
  "GlmOcrVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "GlmOcrVisionBlock": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "GlmOcrVisionPatchMerger": {},
  "GlmOcrVisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "GlmOcrModel": {},
  "GlmOcrForConditionalGeneration": {},
  "GlmOcrVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "GlmOcrTextMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GlmOcrTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "GlmOcrVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GlmOcrTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ],
    "apply_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ]
  },
  "GlmOcrTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "GlmOcrCausalLMOutputWithPast": {},
  "LAYOUTLMV2_ENCODE_KWARGS_DOCSTRING": [],
  "LAYOUTLMV2_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING": [],
  "LayoutLMv2Tokenizer": {
    "vocab_files_names": [],
    "model": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab",
      "do_lower_case",
      "unk_token",
      "sep_token",
      "pad_token",
      "cls_token",
      "mask_token",
      "cls_token_box",
      "sep_token_box",
      "pad_token_box",
      "pad_token_label",
      "only_label_first_subword",
      "tokenize_chinese_chars",
      "strip_accents"
    ],
    "__call__": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "encode_plus": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "is_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "padding_side",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_pad": [
      "self",
      "encoded_inputs",
      "max_length",
      "padding_strategy",
      "pad_to_multiple_of",
      "padding_side",
      "return_attention_mask"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "LayoutLMv2TokenizerFast": [],
  "LayoutLMv2ImageProcessorKwargs": {},
  "LayoutLMv2ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "apply_ocr",
      "ocr_lang",
      "tesseract_config"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "apply_ocr",
      "ocr_lang",
      "tesseract_config",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "LayoutLMv2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "max_2d_position_embeddings",
      "max_rel_pos",
      "rel_pos_bins",
      "fast_qkv",
      "max_rel_2d_pos",
      "rel_2d_pos_bins",
      "convert_sync_batchnorm",
      "image_feature_pool_shape",
      "coordinate_size",
      "shape_size",
      "has_relative_attention_bias",
      "has_spatial_attention_bias",
      "has_visual_segment_embedding",
      "detectron2_config_args"
    ],
    "get_default_detectron2_config": [
      "cls"
    ],
    "get_detectron2_config": [
      "self"
    ]
  },
  "LayoutLMv2Embeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "_calc_spatial_position_embeddings": [
      "self",
      "bbox"
    ]
  },
  "LayoutLMv2SelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "compute_qkv": [
      "self",
      "hidden_states"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "rel_pos",
      "rel_2d_pos"
    ]
  },
  "LayoutLMv2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "rel_pos",
      "rel_2d_pos"
    ]
  },
  "LayoutLMv2SelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LayoutLMv2Intermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayoutLMv2Output": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "LayoutLMv2Layer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "rel_pos",
      "rel_2d_pos"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "relative_position_bucket": [
    "relative_position",
    "bidirectional",
    "num_buckets",
    "max_distance"
  ],
  "LayoutLMv2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_calculate_1d_position_embeddings": [
      "self",
      "position_ids"
    ],
    "_calculate_2d_position_embeddings": [
      "self",
      "bbox"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "bbox",
      "position_ids"
    ]
  },
  "LayoutLMv2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "my_convert_sync_batchnorm": [
    "module",
    "process_group"
  ],
  "LayoutLMv2VisualBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "images"
    ],
    "synchronize_batch_norm": [
      "self"
    ]
  },
  "LayoutLMv2Pooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayoutLMv2ForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "image",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMv2ForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "image",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMv2ForQuestionAnswering": {
    "__init__": [
      "self",
      "config",
      "has_visual_segment_embedding"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "bbox",
      "image",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LayoutLMv2ImageProcessorFast": {
    "resample": [],
    "size": [],
    "rescale_factor": [],
    "do_resize": [],
    "apply_ocr": [],
    "ocr_lang": [],
    "tesseract_config": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "apply_ocr",
      "ocr_lang",
      "tesseract_config",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "LayoutLMv2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "text_pair",
      "boxes",
      "word_labels",
      "add_special_tokens",
      "padding",
      "truncation",
      "max_length",
      "stride",
      "pad_to_multiple_of",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose",
      "return_tensors"
    ],
    "get_overflowing_images": [
      "self",
      "images",
      "overflow_to_sample_mapping"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Speech2TextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_source_positions",
      "max_target_positions",
      "num_conv_layers",
      "conv_kernel_sizes",
      "conv_channels",
      "input_feat_per_channel",
      "input_channels",
      "tie_word_embeddings"
    ]
  },
  "Conv1dSubsampler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features"
    ]
  },
  "Speech2TextSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length"
    ],
    "create_position_ids_from_input_ids": [
      "self",
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "Speech2TextAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "Speech2TextEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "Speech2TextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "Speech2TextPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask"
    ]
  },
  "Speech2TextEncoder": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Speech2TextDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Speech2TextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Speech2TextForConditionalGeneration": {
    "input_modalities": [],
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Speech2TextProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self"
    ]
  },
  "MUSTC_LANGS": [],
  "Speech2TextTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "spm_file",
      "bos_token",
      "eos_token",
      "pad_token",
      "unk_token",
      "do_upper_case",
      "do_lower_case",
      "tgt_lang",
      "lang_codes",
      "additional_special_tokens",
      "sp_model_kwargs"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "tgt_lang": [
      "self",
      "new_tgt_lang"
    ],
    "set_tgt_lang_special_tokens": [
      "self",
      "tgt_lang"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "T5GemmaModuleConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "query_pre_attn_scalar",
      "sliding_window",
      "layer_types",
      "final_logit_softcapping",
      "attn_logit_softcapping",
      "is_decoder"
    ]
  },
  "T5GemmaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "encoder",
      "decoder",
      "is_encoder_decoder",
      "dropout_rate",
      "classifier_dropout_rate",
      "attention_dropout",
      "tie_word_embeddings",
      "vocab_size"
    ]
  },
  "T5GemmaRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "T5GemmaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "T5GemmaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "T5GemmaSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "T5GemmaCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values"
    ]
  },
  "T5GemmaEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids"
    ]
  },
  "T5GemmaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "T5GemmaClassificationHead": {
    "__init__": [
      "self",
      "hidden_size",
      "num_labels",
      "classifier_dropout_rate"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5GemmaLMHead": {
    "__init__": [
      "self",
      "hidden_size",
      "vocab_size",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "T5GemmaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "make_default_2d_attention_mask": [
    "token_ids",
    "hidden_states",
    "pad_token_id"
  ],
  "T5GemmaEncoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "T5GemmaDecoder": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "T5GemmaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "T5GemmaEncoderModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "T5GemmaForConditionalGeneration": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "T5GemmaForSequenceClassification": {
    "__init__": [
      "self",
      "config",
      "is_encoder_decoder"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "T5GemmaForTokenClassification": {
    "__init__": [
      "self",
      "config",
      "is_encoder_decoder"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_position_ids",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels"
    ]
  },
  "Dots1RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Dots1RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Dots1Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Dots1MLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Dots1TopkRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dots1NaiveMoe": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Dots1MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dots1DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Dots1PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_keep_in_fp32_modules_strict": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Dots1Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Dots1ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Dots1Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "moe_intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "n_shared_experts",
      "n_routed_experts",
      "n_group",
      "topk_group",
      "num_experts_per_tok",
      "first_k_dense_replace",
      "norm_topk_prob",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "routed_scaling_factor",
      "sliding_window",
      "max_window_layers",
      "layer_types",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Wav2Vec2BertProcessorKwargs": {
    "_defaults": []
  },
  "Wav2Vec2BertProcessor": {
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "audio",
      "text"
    ],
    "pad": [
      "self",
      "input_features",
      "labels"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Wav2Vec2BertRotaryPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Wav2Vec2BertRelPositionalEmbedding": {},
  "Wav2Vec2BertFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2BertFeedForward": {
    "__init__": [
      "self",
      "config",
      "act_fn",
      "hidden_size"
    ]
  },
  "Wav2Vec2BertConvolutionModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Wav2Vec2BertSelfAttention": {
    "__init__": [
      "self",
      "config",
      "is_adapter_attention"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "relative_position_embeddings",
      "output_attentions"
    ]
  },
  "Wav2Vec2BertEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "relative_position_embeddings",
      "output_attentions",
      "conv_attention_mask"
    ]
  },
  "Wav2Vec2BertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2BertAdapter": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_sub_sample_lengths_from_attention_mask": [
      "self",
      "seq_lens"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Wav2Vec2BertAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "sub_sampled_lengths"
    ]
  },
  "Wav2Vec2BertPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths",
      "add_adapter"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask",
      "add_adapter"
    ]
  },
  "Wav2Vec2BertBaseModelOutput": [],
  "Wav2Vec2BertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2BertForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2BertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2BertForAudioFrameClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2BertForXVector": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2BertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "feature_projection_input_dim",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "tdnn_dim",
      "tdnn_kernel",
      "tdnn_dilation",
      "xvector_output_dim",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_adapter",
      "adapter_kernel_size",
      "adapter_stride",
      "num_adapter_layers",
      "adapter_act",
      "use_intermediate_ffn_before_adapter",
      "output_hidden_size",
      "position_embeddings_type",
      "rotary_embedding_base",
      "max_source_positions",
      "left_max_position_embeddings",
      "right_max_position_embeddings",
      "conv_depthwise_kernel_size",
      "conformer_conv_dropout"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "DbrxAttentionConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "attn_pdrop",
      "clip_qkv",
      "kv_n_heads"
    ]
  },
  "DbrxFFNConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "ffn_act_fn",
      "ffn_hidden_size",
      "moe_num_experts",
      "moe_top_k",
      "moe_jitter_eps",
      "moe_loss_weight",
      "moe_normalize_expert_weights"
    ]
  },
  "DbrxConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "d_model",
      "n_heads",
      "n_layers",
      "max_seq_len",
      "vocab_size",
      "resid_pdrop",
      "emb_pdrop",
      "attn_config",
      "ffn_config",
      "use_cache",
      "initializer_range",
      "output_router_logits",
      "rope_parameters",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "DbrxRotaryEmbedding": {},
  "DbrxAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "past_key_values",
      "cache_position"
    ]
  },
  "DbrxExpertGLU": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "expert_w1",
      "expert_v1",
      "expert_w2"
    ]
  },
  "DbrxExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "DbrxRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DbrxFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DbrxNormAttentionNorm": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "DbrxBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "past_key_values",
      "cache_position"
    ]
  },
  "DbrxPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DbrxModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "DbrxForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MegatronBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "MegatronBertSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "MegatronBertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual"
    ]
  },
  "MegatronBertAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ]
  },
  "MegatronBertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MegatronBertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MegatronBertLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "cache_position"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "MegatronBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MegatronBertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MegatronBertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MegatronBertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MegatronBertOnlyMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "MegatronBertOnlyNSPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "MegatronBertPreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "MegatronBertPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MegatronBertForPreTrainingOutput": {},
  "MegatronBertModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MegatronBertForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config",
      "add_binary_head"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "next_sentence_label",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MegatronBertForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MegatronBertForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MegatronBertForNextSentencePrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MegatronBertForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MegatronBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MegatronBertForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MegatronBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MegatronBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "use_cache",
      "is_decoder",
      "add_cross_attention",
      "tie_word_embeddings"
    ]
  },
  "Qwen3VLMoeTextRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Qwen3VLMoeTextExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Qwen3VLMoeTextTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3VLMoeTextSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3VLMoeTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Qwen3VLMoeTextMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen3VLMoeTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen3VLMoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen3VLMoeVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Qwen3VLMoeVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen3VLMoeVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen3VLMoeVisionBlock": {
    "__init__": [
      "self",
      "config",
      "attn_implementation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen3VLMoeVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3VLMoeVisionPatchMerger": {
    "__init__": [
      "self",
      "config",
      "use_postshuffle_norm"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen3VLMoeVisionModel": {
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "fast_pos_embed_interpolate": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Qwen3VLMoeTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ],
    "apply_interleaved_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ]
  },
  "Qwen3VLMoeTextModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "visual_pos_masks",
      "deepstack_visual_embeds"
    ],
    "_deepstack_process": [
      "self",
      "hidden_states",
      "visual_pos_masks",
      "visual_embeds"
    ]
  },
  "Qwen3VLMoeCausalLMOutputWithPast": {},
  "Qwen3VLMoeModelOutputWithPast": {},
  "Qwen3VLMoeModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position"
    ]
  },
  "Qwen3VLMoeForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "Qwen3VLMoeTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "attention_bias",
      "attention_dropout",
      "decoder_sparse_step",
      "moe_intermediate_size",
      "num_experts_per_tok",
      "num_experts",
      "mlp_only_layers",
      "rope_parameters",
      "head_dim",
      "pad_token_id"
    ]
  },
  "Qwen3VLMoeVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "intermediate_size",
      "num_heads",
      "in_channels",
      "patch_size",
      "spatial_merge_size",
      "temporal_patch_size",
      "out_hidden_size",
      "num_position_embeddings",
      "deepstack_visual_indexes",
      "initializer_range"
    ]
  },
  "Qwen3VLMoeConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "vision_start_token_id",
      "vision_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "GptOssRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GptOssExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "_apply_gate": [
      "self",
      "gate_up"
    ],
    "forward": [
      "self",
      "hidden_states",
      "router_indices",
      "routing_weights"
    ]
  },
  "GptOssTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GptOssMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GptOssRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "_apply_rotary_emb": [
    "x",
    "cos",
    "sin"
  ],
  "GptOssAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "GptOssDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GptOssPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_keep_in_fp32_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GptOssModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "GptOssForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GptOssForSequenceClassification": {},
  "GptOssForTokenClassification": {},
  "GptOssConfig": {
    "model_type": [],
    "default_theta": [],
    "base_model_pp_plan": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "num_hidden_layers",
      "num_local_experts",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "head_dim",
      "num_attention_heads",
      "num_key_value_heads",
      "sliding_window",
      "tie_word_embeddings",
      "hidden_act",
      "initializer_range",
      "max_position_embeddings",
      "rms_norm_eps",
      "rope_parameters",
      "attention_dropout",
      "num_experts_per_tok",
      "router_aux_loss_coef",
      "output_router_logits",
      "use_cache",
      "layer_types",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ],
    "__setattr__": [
      "self",
      "key",
      "value"
    ]
  },
  "HerbertTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "sep_token",
      "vocab_file",
      "merges_file"
    ]
  },
  "Mamba2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "num_heads",
      "head_dim",
      "vocab_size",
      "hidden_size",
      "state_size",
      "num_hidden_layers",
      "layer_norm_epsilon",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "expand",
      "conv_kernel",
      "n_groups",
      "use_bias",
      "use_conv_bias",
      "hidden_act",
      "initializer_range",
      "residual_in_fp32",
      "time_step_rank",
      "time_step_min",
      "time_step_max",
      "time_step_floor",
      "time_step_limit",
      "rescale_prenorm_residual",
      "use_cache",
      "rms_norm",
      "chunk_size",
      "tie_word_embeddings"
    ]
  },
  "Mamba2Cache": {
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device"
    ],
    "update_conv_state": [
      "self",
      "layer_idx",
      "new_conv_state",
      "cache_init"
    ],
    "update_ssm_state": [
      "self",
      "layer_idx",
      "new_ssm_state"
    ],
    "reset": [
      "self"
    ]
  },
  "MambaRMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "Mamba2Mixer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "torch_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "Mamba2RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Mamba2Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "Mamba2PreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Mamba2Output": {},
  "Mamba2CausalLMOutput": {},
  "Mamba2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "load_hook": [
      "self",
      "state_dict",
      "prefix"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "use_cache",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "attention_mask"
    ]
  },
  "Mamba2ForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "inputs_embeds",
      "use_cache",
      "cache_params",
      "cache_position",
      "attention_mask",
      "is_first_iteration"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "labels",
      "output_hidden_states",
      "return_dict",
      "use_cache",
      "cache_position",
      "attention_mask",
      "logits_to_keep"
    ]
  },
  "Aimv2VisionConfig": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "rms_norm_eps",
      "attention_dropout",
      "qkv_bias",
      "mlp_bias",
      "hidden_act",
      "initializer_range",
      "use_head",
      "is_native"
    ]
  },
  "Aimv2TextConfig": {
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "rms_norm_eps",
      "attention_dropout",
      "qkv_bias",
      "mlp_bias",
      "hidden_act",
      "eos_token_id",
      "max_position_embeddings",
      "initializer_range"
    ]
  },
  "Aimv2Config": {
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "projection_dim",
      "logit_scale_init_value"
    ]
  },
  "Aimv2Output": {},
  "Aimv2RMSNorm": {},
  "Aimv2MLP": {},
  "Aimv2VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "build_2d_sincos_position_embedding": [
      "height",
      "width",
      "embed_dim",
      "temperature",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Aimv2TextEmbeddings": {},
  "Aimv2Attention": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Aimv2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Aimv2Encoder": {},
  "Aimv2AttentionPoolingHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Aimv2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Aimv2VisionModel": {
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Aimv2TextModel": {
    "main_input_name": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask"
    ]
  },
  "Aimv2Model": {
    "_supports_flash_attn": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask"
    ]
  },
  "VaultGemmaRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "VaultGemmaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VaultGemmaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "VaultGemmaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "VaultGemmaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "VaultGemmaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VaultGemmaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "VaultGemmaForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "VaultGemmaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_activation",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "query_pre_attn_scalar",
      "sliding_window",
      "layer_types",
      "final_logit_softcapping",
      "attn_logit_softcapping"
    ]
  },
  "VisualBertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "visual_embeds",
      "visual_token_type_ids",
      "image_text_alignment"
    ]
  },
  "VisualBertSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "VisualBertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "VisualBertAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "VisualBertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VisualBertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "VisualBertLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "feed_forward_chunk": [
      "self",
      "attention_output"
    ]
  },
  "VisualBertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VisualBertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VisualBertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VisualBertLMPredictionHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VisualBertPreTrainingHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "VisualBertPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VisualBertForPreTrainingOutput": {},
  "VisualBertModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "visual_embeds",
      "visual_attention_mask",
      "visual_token_type_ids",
      "image_text_alignment",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VisualBertForPreTraining": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "visual_embeds",
      "visual_attention_mask",
      "visual_token_type_ids",
      "image_text_alignment",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels",
      "sentence_image_labels"
    ]
  },
  "VisualBertForMultipleChoice": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "visual_embeds",
      "visual_attention_mask",
      "visual_token_type_ids",
      "image_text_alignment",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "VisualBertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "visual_embeds",
      "visual_attention_mask",
      "visual_token_type_ids",
      "image_text_alignment",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "VisualBertForVisualReasoning": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "visual_embeds",
      "visual_attention_mask",
      "visual_token_type_ids",
      "image_text_alignment",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "VisualBertRegionToPhraseAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "attention_mask"
    ]
  },
  "VisualBertForRegionToPhraseAlignment": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "visual_embeds",
      "visual_attention_mask",
      "visual_token_type_ids",
      "image_text_alignment",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "region_to_phrase_position",
      "labels"
    ]
  },
  "VisualBertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "visual_embedding_dim",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range",
      "layer_norm_eps",
      "bypass_transformer",
      "special_visual_initialize",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "GraniteMoeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "embedding_multiplier",
      "logits_scaling",
      "residual_multiplier",
      "attention_multiplier",
      "num_local_experts",
      "num_experts_per_tok",
      "output_router_logits",
      "router_aux_loss_coef"
    ]
  },
  "GraniteMoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GraniteMoeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "GraniteMoeParallelExperts": {
    "__init__": [
      "self",
      "num_experts",
      "input_size",
      "output_size"
    ],
    "forward": [
      "self",
      "inputs",
      "expert_size"
    ]
  },
  "GraniteMoeTopKGating": {
    "__init__": [
      "self",
      "input_size",
      "num_experts",
      "top_k"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteMoeMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "layer_input"
    ]
  },
  "GraniteMoeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "GraniteMoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GraniteMoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "GraniteMoeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "GraniteMoeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Glm4vMoeTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Glm4vMoeTextTopkRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4vMoeTextNaiveMoe": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Glm4vMoeTextMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4vMoeTextMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Glm4vMoeTextRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Glm4vMoeTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Glm4vMoePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_keep_in_fp32_modules_strict": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Glm4vMoeCausalLMOutputWithPast": {},
  "Glm4vMoeVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Glm4vMoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Glm4vMoeisionMlp": {
    "__init__": [
      "self",
      "config",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Glm4vMoeVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4vMoeVisionPatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "hidden_act",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Glm4vMoeVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "embeddings",
      "lengths",
      "image_shapes",
      "h_coords",
      "w_coords"
    ]
  },
  "Glm4vMoeVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Glm4vMoeVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Glm4vMoeVisionModel": {
    "input_modalities": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Glm4vMoeTextRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ],
    "apply_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ]
  },
  "Glm4vMoeTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Glm4vMoeModelOutputWithPast": {},
  "Glm4vMoeModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position"
    ]
  },
  "Glm4vMoeForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "Glm4vMoeTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "moe_intermediate_size",
      "num_experts_per_tok",
      "n_shared_experts",
      "n_routed_experts",
      "routed_scaling_factor",
      "n_group",
      "topk_group",
      "first_k_dense_replace",
      "norm_topk_prob",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "router_aux_loss_coef"
    ]
  },
  "Glm4vMoeConfig": {
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "image_start_token_id",
      "image_end_token_id",
      "video_start_token_id",
      "video_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "Glm4vMoeVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "attention_bias",
      "attention_dropout",
      "num_heads",
      "in_channels",
      "image_size",
      "patch_size",
      "rms_norm_eps",
      "spatial_merge_size",
      "temporal_patch_size",
      "out_hidden_size",
      "intermediate_size",
      "initializer_range"
    ]
  },
  "LlamaRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "LlamaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "LlamaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LlamaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "LlamaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "LlamaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "LlamaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "LlamaForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "LlamaForSequenceClassification": {},
  "LlamaForQuestionAnswering": {
    "base_model_prefix": []
  },
  "LlamaForTokenClassification": {},
  "LlamaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "pretraining_tp",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "head_dim"
    ]
  },
  "LlamaTokenizer": {
    "vocab_files_names": [],
    "padding_side": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "vocab",
      "merges",
      "clean_up_tokenization_spaces",
      "unk_token",
      "bos_token",
      "eos_token",
      "use_default_system_prompt",
      "legacy",
      "add_prefix_space"
    ]
  },
  "LlamaTokenizerFast": [],
  "DogeConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "hidden_dropout",
      "hidden_act",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "max_position_embeddings",
      "rope_parameters",
      "num_attention_heads",
      "num_key_value_heads",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "sliding_window",
      "keep_window_size",
      "is_moe",
      "num_experts",
      "num_experts_per_tok",
      "norm_topk_prob",
      "output_router_logits",
      "router_aux_loss_coef",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "DogeRMSNorm": {},
  "DogeRotaryEmbedding": {},
  "ALL_ATTENTION_FUNCTIONS": [],
  "DogeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "prepare_dynamic_mask": [
      "self",
      "hidden_states",
      "dt_states",
      "keep_window_size",
      "attention_mask"
    ]
  },
  "DogeMLP": {},
  "DogeCDMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DogeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "DogePreTrainedModel": {
    "_supports_flash_attn": [],
    "_can_compile_fullgraph": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DogeModel": {},
  "DogeForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep",
      "output_router_logits"
    ]
  },
  "DogeForSequenceClassification": {},
  "VideoLlavaVideoProcessor": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": []
  },
  "VideoLlavaImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_center_crop",
      "crop_size",
      "do_convert_rgb",
      "data_format",
      "input_data_format"
    ]
  },
  "VideoLlavaConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "video_token_index",
      "projector_hidden_act",
      "vision_feature_select_strategy",
      "vision_feature_layer",
      "image_seq_length",
      "video_seq_length",
      "multimodal_projector_bias",
      "tie_word_embeddings"
    ]
  },
  "VideoLlavaModelOutputWithPast": {},
  "VideoLlavaCausalLMOutputWithPast": {},
  "VideoLlavaMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "VideoLlavaPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VideoLlavaModel": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values_images",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "output_hidden_states"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "vision_feature_layer",
      "output_hidden_states"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values_images",
      "pixel_values_videos",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "VideoLlavaForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values_images",
      "vision_feature_layer",
      "vision_feature_select_strategy"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values_images",
      "pixel_values_videos",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values_images",
      "pixel_values_videos",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "VideoLlavaProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "video_processor",
      "tokenizer",
      "patch_size",
      "vision_feature_select_strategy",
      "image_token",
      "video_token",
      "chat_template",
      "num_additional_image_tokens"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "videos",
      "padding",
      "truncation",
      "max_length",
      "return_tensors"
    ]
  },
  "RegNetConfig": {
    "model_type": [],
    "layer_types": [],
    "__init__": [
      "self",
      "num_channels",
      "embedding_size",
      "hidden_sizes",
      "depths",
      "groups_width",
      "layer_type",
      "hidden_act"
    ]
  },
  "RegNetConvLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "activation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RegNetEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "RegNetShortCut": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "RegNetSELayer": {
    "__init__": [
      "self",
      "in_channels",
      "reduced_channels"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RegNetXLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RegNetYLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RegNetStage": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "depth"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "RegNetEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RegNetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RegNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RegNetForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DiaFeatureExtractor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "feature_size",
      "sampling_rate",
      "padding_value",
      "hop_length"
    ],
    "__call__": [
      "self",
      "raw_audio",
      "padding",
      "truncation",
      "max_length",
      "return_tensors",
      "sampling_rate"
    ]
  },
  "DiaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "main_input_name": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DiaMultiChannelEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "audio_codes"
    ]
  },
  "DiaMLP": {},
  "DiaRMSNorm": {},
  "DiaRotaryEmbedding": {},
  "DiaSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_causal"
    ]
  },
  "DiaCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cross_attention_states",
      "attention_mask",
      "past_key_values"
    ]
  },
  "DiaEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask"
    ]
  },
  "DiaEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "DiaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "DiaDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "DiaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_position_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "DiaForConditionalGeneration": {
    "base_model_prefix": [],
    "output_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_position_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "labels",
      "cache_position"
    ]
  },
  "DiaEncoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "max_position_embeddings",
      "num_hidden_layers",
      "hidden_size",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "intermediate_size",
      "norm_eps",
      "vocab_size",
      "hidden_act",
      "rope_parameters",
      "initializer_range"
    ]
  },
  "DiaDecoderConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "max_position_embeddings",
      "num_hidden_layers",
      "hidden_size",
      "intermediate_size",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "cross_num_attention_heads",
      "cross_head_dim",
      "cross_num_key_value_heads",
      "cross_hidden_size",
      "norm_eps",
      "vocab_size",
      "hidden_act",
      "num_channels",
      "rope_parameters",
      "initializer_range",
      "use_cache",
      "is_encoder_decoder"
    ]
  },
  "DiaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "encoder_config",
      "decoder_config",
      "norm_eps",
      "is_encoder_decoder",
      "pad_token_id",
      "eos_token_id",
      "bos_token_id",
      "delay_pattern",
      "initializer_range",
      "use_cache"
    ],
    "get_text_config": [
      "self"
    ]
  },
  "DiaGenerationMixin": {
    "_uses_cfg": [],
    "_get_logits_processor": [
      "self",
      "generation_config",
      "input_ids_seq_length",
      "encoder_input_ids",
      "prefix_allowed_tokens_fn",
      "logits_processor",
      "device",
      "model_kwargs",
      "negative_prompt_ids",
      "negative_prompt_attention_mask"
    ],
    "_prepare_generation_config": [
      "self",
      "generation_config"
    ],
    "_prepare_model_inputs": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "_prepare_decoder_input_ids_for_generation": [
      "self",
      "batch_size",
      "model_input_name",
      "model_kwargs",
      "decoder_start_token_id",
      "device"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "encoder_outputs",
      "decoder_delay_mask"
    ],
    "apply_delay_mask": [
      "input_ids",
      "pad_id",
      "delay_mask"
    ],
    "_main_generate_loop": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus",
      "assistant_model",
      "streamer",
      "negative_prompt_ids",
      "negative_prompt_attention_mask",
      "custom_generate"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus",
      "assistant_model",
      "streamer",
      "negative_prompt_ids",
      "negative_prompt_attention_mask",
      "custom_generate"
    ]
  },
  "DiaTokenizer": {
    "model_input_names": [],
    "__init__": [
      "self",
      "pad_token",
      "unk_token",
      "max_length",
      "offset"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ]
  },
  "DiaAudioKwargs": {},
  "DiaProcessorKwargs": {
    "_defaults": []
  },
  "DiaProcessor": {
    "audio_tokenizer_class": [],
    "__init__": [
      "self",
      "feature_extractor",
      "tokenizer",
      "audio_tokenizer"
    ],
    "__call__": [
      "self",
      "text",
      "audio",
      "output_labels"
    ],
    "batch_decode": [
      "self",
      "decoder_input_ids",
      "audio_prompt_len"
    ],
    "decode": [
      "self",
      "decoder_input_ids",
      "audio_prompt_len"
    ],
    "get_audio_prompt_len": [
      "self",
      "decoder_attention_mask"
    ],
    "save_audio": [
      "self",
      "audio",
      "saving_path"
    ],
    "build_indices": [
      "bsz",
      "seq_len",
      "num_channels",
      "delay_pattern",
      "revert"
    ],
    "apply_audio_delay": [
      "audio",
      "pad_token_id",
      "bos_token_id",
      "precomputed_idx"
    ]
  },
  "MaskFormerImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "rescale_factor": [],
    "do_normalize": [],
    "do_pad": [],
    "model_input_names": [],
    "size_divisor": [],
    "do_reduce_labels": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "reduce_label": [
      "self",
      "labels"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "size_divisor",
      "interpolation"
    ],
    "pad": [
      "self",
      "images",
      "padded_size",
      "segmentation_maps",
      "fill",
      "ignore_index"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_resize",
      "size",
      "pad_size",
      "size_divisor",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation",
      "return_binary_maps"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "MaskFormerSwinModelOutputWithPooling": {},
  "MaskFormerSwinBaseModelOutput": {},
  "MaskFormerSwinEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "interpolate_pos_encoding"
    ]
  },
  "MaskFormerSwinPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "maybe_pad": [
      "self",
      "pixel_values",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "MaskFormerSwinPatchMerging": {
    "__init__": [
      "self",
      "input_resolution",
      "dim",
      "norm_layer"
    ],
    "maybe_pad": [
      "self",
      "input_feature",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "input_feature",
      "input_dimensions"
    ]
  },
  "MaskFormerSwinDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "MaskFormerSwinSelfAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "create_relative_position_index": [
      "self"
    ]
  },
  "MaskFormerSwinSelfOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MaskFormerSwinAttention": {
    "__init__": [
      "self",
      "config",
      "dim",
      "num_heads",
      "window_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "MaskFormerSwinIntermediate": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MaskFormerSwinOutput": {
    "__init__": [
      "self",
      "config",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MaskFormerSwinLayer": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "num_heads",
      "drop_path_rate",
      "shift_size"
    ],
    "get_attn_mask": [
      "self",
      "input_resolution"
    ],
    "maybe_pad": [
      "self",
      "hidden_states",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions"
    ]
  },
  "MaskFormerSwinStage": {
    "__init__": [
      "self",
      "config",
      "dim",
      "input_resolution",
      "depth",
      "num_heads",
      "drop_path",
      "downsample"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "MaskFormerSwinEncoder": {
    "__init__": [
      "self",
      "config",
      "grid_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MaskFormerSwinPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MaskFormerSwinModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_attentions",
      "output_hidden_states",
      "interpolate_pos_encoding",
      "return_dict"
    ]
  },
  "MaskFormerSwinBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "MaskFormerImageProcessorKwargs": {},
  "get_maskformer_resize_output_image_size": [
    "image",
    "size",
    "max_size",
    "size_divisor",
    "default_to_square",
    "input_data_format"
  ],
  "MaskFormerImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "num_labels",
      "pad_size"
    ],
    "to_dict": [
      "self"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "size_divisor",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "convert_segmentation_map_to_binary_masks": [
      "self",
      "segmentation_map",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_resize",
      "size",
      "size_divisor",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "images",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "encode_inputs": [
      "self",
      "pixel_values_list",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels",
      "return_tensors",
      "input_data_format",
      "pad_size"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation",
      "return_binary_maps"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "MaskFormerSwinConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim",
      "depths",
      "num_heads",
      "window_size",
      "mlp_ratio",
      "qkv_bias",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "drop_path_rate",
      "hidden_act",
      "use_absolute_embeddings",
      "initializer_range",
      "layer_norm_eps",
      "out_features",
      "out_indices"
    ]
  },
  "MaskFormerConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "backbones_supported": [],
    "decoders_supported": [],
    "__init__": [
      "self",
      "fpn_feature_size",
      "mask_feature_size",
      "no_object_weight",
      "use_auxiliary_loss",
      "backbone_config",
      "decoder_config",
      "init_std",
      "init_xavier_std",
      "dice_weight",
      "cross_entropy_weight",
      "mask_weight",
      "output_auxiliary_logits"
    ]
  },
  "MaskFormerPixelLevelModuleOutput": {},
  "MaskFormerPixelDecoderOutput": {},
  "MaskFormerModelOutput": {},
  "MaskFormerForInstanceSegmentationOutput": {},
  "upsample_like": [
    "pixel_values",
    "like",
    "mode"
  ],
  "pair_wise_sigmoid_focal_loss": [
    "inputs",
    "labels",
    "alpha",
    "gamma"
  ],
  "MaskFormerHungarianMatcher": {
    "__init__": [
      "self",
      "cost_class",
      "cost_mask",
      "cost_dice"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels"
    ],
    "__repr__": [
      "self"
    ]
  },
  "MaskFormerLoss": {
    "__init__": [
      "self",
      "num_labels",
      "matcher",
      "weight_dict",
      "eos_coef"
    ],
    "_max_by_axis": [
      "self",
      "the_list"
    ],
    "_pad_images_to_max_in_batch": [
      "self",
      "tensors"
    ],
    "loss_labels": [
      "self",
      "class_queries_logits",
      "class_labels",
      "indices"
    ],
    "loss_masks": [
      "self",
      "masks_queries_logits",
      "mask_labels",
      "indices",
      "num_masks"
    ],
    "_get_predictions_permutation_indices": [
      "self",
      "indices"
    ],
    "_get_targets_permutation_indices": [
      "self",
      "indices"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_predictions"
    ],
    "get_num_masks": [
      "self",
      "class_labels",
      "device"
    ]
  },
  "MaskFormerFPNConvLayer": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "kernel_size",
      "padding"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "MaskFormerFPNLayer": {
    "__init__": [
      "self",
      "in_features",
      "lateral_features"
    ],
    "forward": [
      "self",
      "down",
      "left"
    ]
  },
  "MaskFormerFPNModel": {
    "__init__": [
      "self",
      "in_features",
      "lateral_widths",
      "feature_size"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MaskFormerPixelDecoder": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "features",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MaskFormerSinePositionEmbedding": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "PredictionBlock": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim",
      "activation"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "MaskformerMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "MaskFormerPixelLevelModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MaskFormerTransformerModule": {
    "__init__": [
      "self",
      "in_features",
      "config"
    ],
    "forward": [
      "self",
      "image_features",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "MaskFormerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MaskFormerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "MaskFormerForInstanceSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "get_loss_dict": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels",
      "auxiliary_logits"
    ],
    "get_loss": [
      "self",
      "loss_dict"
    ],
    "get_logits": [
      "self",
      "outputs"
    ],
    "forward": [
      "self",
      "pixel_values",
      "mask_labels",
      "class_labels",
      "pixel_mask",
      "output_auxiliary_logits",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "DeepseekV3RMSNorm": {},
  "DeepseekV3RotaryEmbedding": {},
  "DeepseekV3MLP": {},
  "DeepseekV3TopkRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeepseekV3NaiveMoe": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DeepseekV3MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "route_tokens_to_experts": [
      "self",
      "router_logits"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "DeepseekV3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "DeepseekV3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "DeepseekV3PreTrainedModel": {
    "_can_compile_fullgraph": [],
    "_keep_in_fp32_modules_strict": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DeepseekV3Model": {
    "_keys_to_ignore_on_load_unexpected": []
  },
  "DeepseekV3ForCausalLM": {},
  "DeepseekV3ForSequenceClassification": {},
  "DeepseekV3ForTokenClassification": {},
  "DEEPSEEK_PRETRAINED_CONFIG_ARCHIVE_MAP": [],
  "DeepseekV3Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "moe_intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "n_shared_experts",
      "n_routed_experts",
      "routed_scaling_factor",
      "kv_lora_rank",
      "q_lora_rank",
      "qk_rope_head_dim",
      "v_head_dim",
      "qk_nope_head_dim",
      "n_group",
      "topk_group",
      "num_experts_per_tok",
      "first_k_dense_replace",
      "norm_topk_prob",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "pretraining_tp",
      "tie_word_embeddings",
      "rope_parameters",
      "rope_interleave",
      "attention_bias",
      "attention_dropout"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "rwkv_cuda_kernel": [],
  "load_wkv_cuda_kernel": [
    "context_length"
  ],
  "RwkvLinearAttention": {
    "forward": [
      "ctx",
      "time_decay",
      "time_first",
      "key",
      "value",
      "state",
      "return_state"
    ],
    "backward": [
      "ctx",
      "g_output",
      "g_state"
    ]
  },
  "rwkv_linear_attention_cpu": [
    "time_decay",
    "time_first",
    "key",
    "value",
    "state",
    "return_state"
  ],
  "rwkv_linear_attention": [
    "time_decay",
    "time_first",
    "key",
    "value",
    "state",
    "return_state"
  ],
  "RwkvSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "extract_key_value": [
      "self",
      "hidden",
      "state"
    ],
    "forward": [
      "self",
      "hidden",
      "state",
      "use_cache"
    ]
  },
  "RwkvFeedForward": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden",
      "state"
    ]
  },
  "RwkvBlock": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden",
      "state",
      "use_cache",
      "output_attentions"
    ]
  },
  "RwkvPreTrainedModel": {
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_keep_in_fp32_modules": [],
    "supports_gradient_checkpointing": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "RwkvOutput": {},
  "RwkvCausalLMOutput": {},
  "RwkvModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "state",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "_rescale_layers": [
      "self"
    ],
    "_bnb_4bit_dequantize_and_rescale": [
      "self",
      "target_layer",
      "block_id"
    ]
  },
  "RwkvForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "state",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "RwkvConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "context_length",
      "hidden_size",
      "num_hidden_layers",
      "attention_hidden_size",
      "intermediate_size",
      "layer_norm_epsilon",
      "bos_token_id",
      "eos_token_id",
      "rescale_every",
      "tie_word_embeddings",
      "use_cache"
    ]
  },
  "Qwen2_5_VLProcessorKwargs": {
    "_defaults": []
  },
  "Qwen2_5_VLProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "video_processor",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes",
      "video_sizes"
    ],
    "post_process_image_text_to_text": [
      "self",
      "generated_outputs",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Qwen2_5_VLMLP": {
    "__init__": [
      "self",
      "config",
      "bias"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen2_5_VLPatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "spatial_merge_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen2_5_VLVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen2_5_VLVisionBlock": {
    "__init__": [
      "self",
      "config",
      "attn_implementation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen2_5_VLPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen2_5_VisionTransformerPretrainedModel": {
    "_no_split_modules": [],
    "_input_embed_layer": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "get_window_index": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "Qwen2_5_VLModelOutputWithPast": {},
  "Qwen2_5_VLRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Qwen2_5_VLAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen2_5_VLDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Qwen2_5_VLTextModel": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "Qwen2_5_VLModel": {
    "base_model_prefix": [],
    "_checkpoint_conversion_mapping": [],
    "accepts_loss_kwargs": [],
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "second_per_grid_ts",
      "attention_mask"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position",
      "second_per_grid_ts"
    ]
  },
  "Qwen2_5_VLCausalLMOutputWithPast": {},
  "Qwen2_5_VLForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "accepts_loss_kwargs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "cache_position",
      "second_per_grid_ts",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "second_per_grid_ts",
      "is_first_iteration"
    ],
    "_get_image_nums_and_video_nums": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "_expand_inputs_for_generation": [
      "self",
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ]
  },
  "Qwen2_5_VLVisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "depth",
      "hidden_size",
      "hidden_act",
      "intermediate_size",
      "num_heads",
      "in_channels",
      "patch_size",
      "spatial_merge_size",
      "temporal_patch_size",
      "tokens_per_second",
      "window_size",
      "out_hidden_size",
      "fullatt_block_indexes",
      "initializer_range"
    ]
  },
  "Qwen2_5_VLTextConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "use_sliding_window",
      "sliding_window",
      "max_window_layers",
      "layer_types",
      "attention_dropout",
      "rope_parameters",
      "bos_token_id",
      "eos_token_id",
      "pad_token_id"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "Qwen2_5_VLConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "vision_start_token_id",
      "vision_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "TrOCRLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "TrOCRScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "TrOCRSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim",
      "padding_idx"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim",
      "padding_idx"
    ],
    "forward": [
      "self",
      "input_ids",
      "past_key_values_length"
    ],
    "create_position_ids_from_input_ids": [
      "self",
      "input_ids",
      "padding_idx",
      "past_key_values_length"
    ]
  },
  "TrOCRAttention": {
    "__init__": [
      "self",
      "config",
      "embed_dim",
      "num_heads",
      "kdim",
      "vdim",
      "dropout",
      "is_decoder",
      "bias",
      "is_cross_attention",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "TrOCRDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "TrOCRPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": []
  },
  "TrOCRDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "TrOCRDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "TrOCRForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "TrOCRConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "decoder_layers",
      "decoder_attention_heads",
      "decoder_ffn_dim",
      "activation_function",
      "max_position_embeddings",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "decoder_start_token_id",
      "init_std",
      "decoder_layerdrop",
      "use_cache",
      "scale_embedding",
      "use_learned_position_embeddings",
      "layernorm_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "cross_attention_hidden_size",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "TrOCRProcessorKwargs": {
    "_defaults": []
  },
  "TrOCRProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "MobileViTConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_sizes",
      "neck_hidden_sizes",
      "num_attention_heads",
      "mlp_ratio",
      "expand_ratio",
      "hidden_act",
      "conv_kernel_size",
      "output_stride",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "classifier_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "qkv_bias",
      "aspp_out_channels",
      "atrous_rates",
      "aspp_dropout_prob",
      "semantic_loss_ignore_index"
    ]
  },
  "MobileVitImageProcessorKwargs": {},
  "MobileViTImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_center_crop",
      "crop_size",
      "do_flip_channel_order",
      "do_reduce_labels"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "flip_channel_order": [
      "self",
      "image",
      "data_format",
      "input_data_format"
    ],
    "reduce_label": [
      "self",
      "label"
    ],
    "__call__": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_reduce_labels",
      "do_resize",
      "do_rescale",
      "do_center_crop",
      "do_flip_channel_order",
      "size",
      "resample",
      "rescale_factor",
      "crop_size",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_center_crop",
      "crop_size",
      "do_flip_channel_order",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_reduce_labels",
      "do_resize",
      "size",
      "do_center_crop",
      "crop_size",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_center_crop",
      "crop_size",
      "do_flip_channel_order",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "MobileViTConvLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "bias",
      "dilation",
      "use_normalization",
      "use_activation"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTInvertedResidual": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "dilation"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTMobileNetLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "num_stages"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTSelfAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTSelfOutput": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTIntermediate": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTOutput": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "MobileViTTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTTransformer": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_stages"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "stride",
      "hidden_size",
      "num_stages",
      "dilation"
    ],
    "unfolding": [
      "self",
      "features"
    ],
    "folding": [
      "self",
      "patches",
      "info_dict"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileViTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MobileViTModel": {
    "__init__": [
      "self",
      "config",
      "expand_output"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileViTForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "MobileViTASPPPooling": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTASPP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileViTDeepLabV3": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "MobileViTForSemanticSegmentation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileViTImageProcessorFast": {
    "resample": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_flip_channel_order": [],
    "do_reduce_labels": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "reduce_label": [
      "self",
      "labels"
    ],
    "preprocess": [
      "self",
      "images",
      "segmentation_maps"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "segmentation_maps",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_reduce_labels",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_center_crop",
      "crop_size",
      "do_flip_channel_order",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ]
  },
  "HubertConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_layer_norm",
      "feat_proj_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "conv_pos_batch_norm",
      "do_stable_layer_norm",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "HubertPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HubertSamePadLayer": {},
  "HubertFeatureEncoder": {},
  "HubertFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HubertEncoder": {},
  "HubertEncoderStableLayerNorm": {},
  "HubertPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask"
    ]
  },
  "HubertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "HubertForCTC": {},
  "HubertForSequenceClassification": {},
  "HubertNoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HubertLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HubertGroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HubertAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "HubertFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HubertEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "HubertAttnAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "HubertEncoderLayerStableLayerNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "BlenderbotSmallConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "forced_eos_token_id",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "BlenderbotSmallLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "BlenderbotSmallAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "BlenderbotSmallEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "BlenderbotSmallDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "BlenderbotSmallPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "BlenderbotSmallEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BlenderbotSmallDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BlenderbotSmallModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BlenderbotSmallForConditionalGeneration": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BlenderbotSmallDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "BlenderbotSmallForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BlenderbotSmallTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "bos_token",
      "eos_token",
      "unk_token",
      "pad_token"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "FlexOlmoRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "FlexOlmoRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "FlexOlmoMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FlexOlmoAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "FlexOlmoExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "FlexOlmoTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FlexOlmoSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FlexOlmoDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "position_embeddings"
    ]
  },
  "FlexOlmoPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "FlexOlmoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "FlexOlmoForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "FlexOlmoConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "num_experts_per_tok",
      "num_experts",
      "output_router_logits",
      "router_aux_loss_coef",
      "norm_topk_prob"
    ]
  },
  "UMT5LayerNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UMT5DenseActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UMT5DenseGatedActDense": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UMT5LayerFF": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UMT5Attention": {
    "__init__": [
      "self",
      "config",
      "has_relative_attention_bias",
      "layer_idx"
    ],
    "_shape": [
      "self",
      "projection"
    ],
    "_relative_position_bucket": [
      "self",
      "relative_position"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length",
      "device",
      "cache_position"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "past_key_values",
      "attention_mask",
      "cache_position"
    ]
  },
  "UMT5LayerSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "UMT5LayerCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "UMT5Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cache_position"
    ]
  },
  "UMT5ClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "UMT5PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_can_compile_fullgraph": [],
    "_no_split_modules": [],
    "_keep_in_fp32_modules": [],
    "dummy_inputs": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ]
  },
  "UMT5Stack": {
    "__init__": [
      "self",
      "config"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ]
  },
  "UMT5Model": {
    "model_type": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "UMT5ForConditionalGeneration": {
    "model_type": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "UMT5EncoderModel": {
    "model_type": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UMT5ForSequenceClassification": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UMT5ForTokenClassification": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UMT5ForQuestionAnswering": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "start_positions",
      "end_positions",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "UMT5Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "d_model",
      "d_kv",
      "d_ff",
      "num_layers",
      "num_decoder_layers",
      "num_heads",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "dropout_rate",
      "layer_norm_epsilon",
      "initializer_factor",
      "feed_forward_proj",
      "is_encoder_decoder",
      "use_cache",
      "tokenizer_class",
      "pad_token_id",
      "eos_token_id",
      "decoder_start_token_id",
      "classifier_dropout",
      "is_decoder"
    ]
  },
  "MusicgenMelodyDecoderConfig": {
    "model_type": [],
    "base_config_key": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "num_hidden_layers",
      "ffn_dim",
      "num_attention_heads",
      "layerdrop",
      "use_cache",
      "activation_function",
      "hidden_size",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "initializer_factor",
      "scale_embedding",
      "num_codebooks",
      "audio_channels",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "is_decoder",
      "add_cross_attention"
    ]
  },
  "MusicgenMelodyConfig": {
    "model_type": [],
    "sub_configs": [],
    "has_no_defaults_at_init": [],
    "__init__": [
      "self",
      "text_encoder",
      "audio_encoder",
      "decoder",
      "num_chroma",
      "chroma_length"
    ],
    "sampling_rate": [
      "self"
    ]
  },
  "MusicgenMelodyOutputWithPast": {},
  "MusicgenMelodySinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "num_positions",
      "embedding_dim"
    ],
    "make_weights": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "get_embedding": [
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "past_key_values_length"
    ]
  },
  "MusicgenMelodyAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "MusicgenMelodyDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "MusicgenMelodyPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MusicgenMelodyDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "_update_cross_attn_mask": [
      "self",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "input_shape",
      "inputs_embeds"
    ]
  },
  "MusicgenMelodyModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "MusicgenMelodyForCausalLM": {
    "output_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels",
      "cache_position"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "use_cache",
      "delay_pattern_mask",
      "guidance_scale"
    ],
    "build_delay_pattern_mask": [
      "self",
      "input_ids",
      "pad_token_id",
      "max_length"
    ],
    "apply_delay_pattern_mask": [
      "input_ids",
      "decoder_pad_token_mask"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "synced_gpus",
      "streamer"
    ]
  },
  "MusicgenMelodyForConditionalGeneration": {
    "main_input_name": [],
    "output_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "__init__": [
      "self",
      "config",
      "text_encoder",
      "audio_encoder",
      "decoder"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "from_sub_models_pretrained": [
      "cls",
      "text_encoder_pretrained_model_name_or_path",
      "audio_encoder_pretrained_model_name_or_path",
      "decoder_pretrained_model_name_or_path"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "input_features",
      "decoder_input_ids",
      "decoder_attention_mask",
      "past_key_values",
      "encoder_hidden_states",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "decoder_input_ids",
      "encoder_hidden_states",
      "past_key_values",
      "attention_mask",
      "decoder_attention_mask",
      "use_cache",
      "decoder_delay_pattern_mask",
      "guidance_scale"
    ],
    "_prepare_decoder_input_ids_for_generation": [
      "self",
      "batch_size",
      "model_input_name",
      "model_kwargs",
      "decoder_start_token_id",
      "bos_token_id",
      "device"
    ],
    "_prepare_encoder_hidden_states_kwargs_for_generation": [
      "self",
      "inputs_tensor",
      "model_kwargs",
      "model_input_name",
      "generation_config"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ],
    "resize_token_embeddings": [
      "self"
    ],
    "_maybe_initialize_input_ids_for_generation": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "freeze_audio_encoder": [
      "self"
    ],
    "freeze_text_encoder": [
      "self"
    ],
    "_get_decoder_start_token_id": [
      "self",
      "decoder_start_token_id",
      "bos_token_id"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "synced_gpus",
      "streamer"
    ]
  },
  "FalconMambaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "state_size",
      "num_hidden_layers",
      "layer_norm_epsilon",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "expand",
      "conv_kernel",
      "use_bias",
      "use_conv_bias",
      "hidden_act",
      "initializer_range",
      "residual_in_fp32",
      "time_step_rank",
      "time_step_scale",
      "time_step_min",
      "time_step_max",
      "time_step_init_scheme",
      "time_step_floor",
      "rescale_prenorm_residual",
      "use_cache",
      "use_falcon_mambapy",
      "mixer_rms_eps",
      "tie_word_embeddings"
    ]
  },
  "FalconMambaCache": {},
  "rms_forward": [
    "hidden_states",
    "variance_epsilon"
  ],
  "FalconMambaMixer": {
    "warn_slow_implementation": [
      "self"
    ],
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "slow_forward": [
      "self",
      "input_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "FalconMambaRMSNorm": {
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "FalconMambaBlock": {},
  "FalconMambaPreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "FalconMambaOutput": {},
  "FalconMambaCausalLMOutput": {},
  "FalconMambaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "load_hook": [
      "self",
      "state_dict",
      "prefix"
    ]
  },
  "FalconMambaForCausalLM": {},
  "SeedOssRMSNorm": {},
  "SeedOssMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SeedOssAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "SeedOssDecoderLayer": {},
  "SeedOssPreTrainedModel": {},
  "SeedOssModel": {},
  "SeedOssForCausalLM": {
    "forward": [
      "self"
    ]
  },
  "SeedOssForSequenceClassification": {},
  "SeedOssForTokenClassification": {},
  "SeedOssForQuestionAnswering": {},
  "SeedOssConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "pretraining_tp",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_out_bias",
      "attention_dropout",
      "residual_dropout",
      "mlp_bias",
      "head_dim"
    ]
  },
  "SeedOssRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "_get_feat_extract_output_lengths": [
    "input_lengths"
  ],
  "Qwen3OmniMoeAudioEncoderConfig": {
    "__init__": [
      "self",
      "num_mel_bins",
      "encoder_layers",
      "encoder_attention_heads",
      "encoder_ffn_dim",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_function",
      "activation_dropout",
      "scale_embedding",
      "initializer_range",
      "max_source_positions",
      "n_window",
      "output_dim",
      "n_window_infer",
      "conv_chunksize",
      "downsample_hidden_size"
    ]
  },
  "Qwen3OmniMoeVisionEncoderConfig": {},
  "Qwen3OmniMoeTextConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "rope_parameters",
      "attention_bias",
      "sliding_window",
      "attention_dropout",
      "decoder_sparse_step",
      "moe_intermediate_size",
      "num_experts_per_tok",
      "num_experts",
      "norm_topk_prob",
      "output_router_logits",
      "router_aux_loss_coef",
      "mlp_only_layers",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Qwen3OmniMoeThinkerConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "audio_config",
      "vision_config",
      "text_config",
      "audio_token_id",
      "image_token_id",
      "video_token_id",
      "position_id_per_seconds",
      "audio_start_token_id",
      "user_token_id",
      "initializer_range",
      "tie_word_embeddings"
    ]
  },
  "Qwen3OmniMoeTalkerCodePredictorConfig": {
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "sliding_window",
      "max_window_layers",
      "layer_types",
      "attention_dropout",
      "num_code_groups",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Qwen3OmniMoeTalkerTextConfig": {
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "sliding_window",
      "attention_dropout",
      "decoder_sparse_step",
      "moe_intermediate_size",
      "num_experts_per_tok",
      "num_experts",
      "norm_topk_prob",
      "output_router_logits",
      "router_aux_loss_coef",
      "mlp_only_layers",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id"
    ]
  },
  "Qwen3OmniMoeTalkerConfig": {
    "sub_configs": [],
    "__init__": [
      "self",
      "code_predictor_config",
      "text_config",
      "num_code_groups",
      "thinker_hidden_size",
      "codec_eos_token_id",
      "accept_hidden_layer",
      "codec_nothink_id",
      "codec_think_bos_id",
      "codec_think_eos_id",
      "codec_pad_id",
      "codec_bos_id",
      "audio_token_id",
      "image_token_id",
      "video_token_id",
      "vision_start_token_id",
      "position_id_per_seconds",
      "audio_start_token_id",
      "speaker_id"
    ]
  },
  "Qwen3OmniMoeCode2WavConfig": {
    "__init__": [
      "self",
      "codebook_size",
      "hidden_size",
      "max_position_embeddings",
      "rope_parameters",
      "num_attention_heads",
      "num_key_value_heads",
      "attention_bias",
      "sliding_window",
      "intermediate_size",
      "hidden_act",
      "layer_scale_initial_scale",
      "rms_norm_eps",
      "num_hidden_layers",
      "num_quantizers",
      "upsample_rates",
      "upsampling_ratios",
      "decoder_dim",
      "attention_dropout",
      "initializer_range"
    ],
    "layer_types": [
      "self"
    ]
  },
  "Qwen3OmniMoeConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "thinker_config",
      "talker_config",
      "code2wav_config",
      "enable_audio_output",
      "im_start_token_id",
      "im_end_token_id",
      "tts_pad_token_id",
      "tts_bos_token_id",
      "tts_eos_token_id",
      "system_token_id",
      "user_token_id",
      "assistant_token_id"
    ],
    "get_text_config": [
      "self",
      "decoder"
    ]
  },
  "Qwen3OmniMoePreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Qwen3OmniMoePreTrainedModelForConditionalGeneration": {
    "get_llm_pos_ids_for_vision": [
      "self",
      "start_idx",
      "vision_idx",
      "spatial_merge_size",
      "t_index",
      "grid_hs",
      "grid_ws"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask",
      "use_audio_in_video",
      "audio_seqlens",
      "second_per_grids"
    ]
  },
  "Qwen3OmniMoeAudioAttention": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Qwen3OmniMoeAudioEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_features",
      "feature_lens",
      "aftercnn_lens"
    ]
  },
  "Qwen3OmniMoeVisionAttention": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Qwen3OmniMoeVisionPatchMerger": {
    "__init__": [
      "self",
      "config",
      "use_postshuffle_norm"
    ],
    "forward": [
      "self",
      "hidden"
    ]
  },
  "Qwen3OmniMoeVisionRotaryEmbedding": {},
  "Qwen3OmniMoeVisionEncoder": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "deepstack_merger_list": [
      "self"
    ]
  },
  "Qwen3OmniMoeThinkerTextRotaryEmbedding": {},
  "Qwen3OmniMoeThinkerTextExperts": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Qwen3OmniMoeThinkerTextSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "Qwen3OmniMoeThinkerTextAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Qwen3OmniMoeThinkerTextDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Qwen3OmniMoeThinkerTextPreTrainedModel": {
    "config_class": [],
    "config": []
  },
  "Qwen3OmniMoeThinkerTextModel": {
    "config_class": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ]
  },
  "Qwen3OmniMoeThinkerCausalLMOutputWithPast": {},
  "Qwen3OmniMoeThinkerForConditionalGeneration": {
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_audio_features": [
      "self",
      "input_features",
      "feature_attention_mask",
      "audio_feature_lengths"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_features",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask",
      "feature_attention_mask",
      "audio_feature_lengths",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "rope_deltas",
      "labels",
      "use_cache",
      "output_router_logits",
      "use_audio_in_video",
      "cache_position",
      "video_second_per_grid"
    ]
  },
  "Qwen3OmniMoeTalkerResizeMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen3OmniMoeTalkerCodePredictorOutputWithPast": {},
  "Qwen3OmniMoeTalkerCodePredictorAttention": {},
  "Qwen3OmniMoeTalkerCodePredictorDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Qwen3OmniMoeRotaryEmbedding": {},
  "Qwen3OmniMoeTalkerCodePredictorModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Qwen3OmniMoeTalkerCodePredictorModelForConditionalGeneration": {
    "config_class": [],
    "base_model_prefix": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "generation_steps"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ]
  },
  "Qwen3OmniMoeTalkerOutputWithPast": {},
  "Qwen3OmniMoeTalkerRotaryEmbedding": {},
  "Qwen3OmniMoeTalkerTextMLP": {},
  "Qwen3OmniMoeTalkerTextSparseMoeBlock": {},
  "Qwen3OmniMoeTalkerDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Qwen3OmniMoeTalkerModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ]
  },
  "Qwen3OmniMoeTalkerForConditionalGeneration": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "config_class": [],
    "base_model_prefix": [],
    "_no_split_modules": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask",
      "use_audio_in_video",
      "audio_seqlens",
      "second_per_grids"
    ],
    "get_llm_pos_ids_for_vision": [
      "self",
      "start_idx",
      "vision_idx",
      "spatial_merge_size",
      "t_index",
      "grid_hs",
      "grid_ws"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "use_audio_in_video",
      "audio_feature_lengths",
      "video_second_per_grid",
      "image_grid_thw",
      "video_grid_thw",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_router_logits",
      "cache_position",
      "residual_codes",
      "trailing_text_hidden",
      "tts_pad_embed",
      "generation_step",
      "talker_input_ids"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "is_first_iteration"
    ]
  },
  "Qwen3OmniMoeCausalConvNet": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "dilation",
      "stride",
      "groups"
    ],
    "_get_extra_padding_for_conv1d": [
      "self",
      "hidden_state"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen3OmniMoeCausalTransConvNet": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen3OmniMoeConvNeXtBlock": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3OmniMoeCode2WavAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ]
  },
  "Qwen3OmniMoeCode2WavMlp": {},
  "Qwen3OmniMoeCode2WavRMSNorm": {},
  "Qwen3OmniMoeCode2WavLayerScale": {},
  "Qwen3OmniMoeCode2WavTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position"
    ]
  },
  "Qwen3OmniMoeCode2WavTransformerModel": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Qwen3OmniMoeCode2WavDecoderResidualUnit": {
    "__init__": [
      "self",
      "dim",
      "dilation"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen3OmniMoeCode2WavDecoderBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden"
    ]
  },
  "Qwen3OmniMoeCode2Wav": {
    "input_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "codes"
    ],
    "chunked_decode": [
      "self",
      "codes",
      "chunk_size",
      "left_context_size"
    ]
  },
  "Qwen3OmniMoeForConditionalGeneration": {
    "config_class": [],
    "output_modalities": [],
    "__init__": [
      "self",
      "config"
    ],
    "enable_talker": [
      "self"
    ],
    "disable_talker": [
      "self"
    ],
    "_get_talker_user_parts": [
      "self",
      "im_start_index",
      "segment_end_index",
      "multimodal_mask",
      "thinker_hidden",
      "thinker_embed"
    ],
    "_get_talker_assistant_parts": [
      "self",
      "im_start_index",
      "segment_end_index",
      "speaker_id",
      "thinker_embed",
      "tts_pad_embed",
      "tts_bos_embed",
      "tts_eos_embed"
    ],
    "generate": [
      "self",
      "input_ids",
      "speaker",
      "use_audio_in_video",
      "return_audio",
      "thinker_max_new_tokens",
      "thinker_eos_token_id",
      "talker_max_new_tokens",
      "talker_do_sample",
      "talker_top_k",
      "talker_top_p",
      "talker_temperature",
      "talker_repetition_penalty"
    ]
  },
  "Qwen3OmniMoeProcessorKwargs": {
    "_defaults": []
  },
  "Qwen3OmniMoeProcessor": {
    "replace_multimodal_special_tokens": [
      "self",
      "text",
      "audio_lengths",
      "image_grid_thw",
      "video_grid_thw",
      "video_second_per_grid",
      "use_audio_in_video",
      "position_id_per_seconds",
      "seconds_per_chunk"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "videos",
      "audio"
    ],
    "apply_chat_template": [
      "self",
      "conversations",
      "chat_template"
    ]
  },
  "Qwen3OmniMoeAudioEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "attention_mask"
    ]
  },
  "Qwen3OmniMoeTextTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3OmniMoeVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Qwen3OmniMoeVisionBlock": {
    "__init__": [
      "self",
      "config",
      "attn_implementation"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "position_embeddings"
    ]
  },
  "Qwen3OmniMoeVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3OmniMoeThinkerTextTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3OmniMoeThinkerTextRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Qwen3OmniMoeThinkerTextMLP": {
    "__init__": [
      "self",
      "config",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen3OmniMoeTextRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Qwen3OmniMoeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Qwen3OmniMoeMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Qwen3OmniMoeTalkerTextExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "Qwen3OmniMoeTalkerTextTopKRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3OmniMoeVideosKwargs": {},
  "ZoeDepthImageProcessorKwargs": {},
  "ZoeDepthImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_resize",
      "size",
      "resample",
      "keep_aspect_ratio",
      "ensure_multiple_of"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "pad_image": [
      "self",
      "image",
      "mode",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_resize",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "resample",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "source_sizes",
      "target_sizes",
      "outputs_flipped",
      "do_remove_padding"
    ]
  },
  "ZoeDepthDepthEstimatorOutput": {},
  "ZoeDepthReassembleStage": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "ZoeDepthReassembleLayer": {
    "__init__": [
      "self",
      "config",
      "channels",
      "factor"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ZoeDepthFeatureFusionStage": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ZoeDepthPreActResidualLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ZoeDepthFeatureFusionLayer": {
    "__init__": [
      "self",
      "config",
      "align_corners"
    ],
    "forward": [
      "self",
      "hidden_state",
      "residual"
    ]
  },
  "ZoeDepthNeck": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "patch_height",
      "patch_width"
    ]
  },
  "ZoeDepthRelativeDepthEstimationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "log_binom": [
    "n",
    "k",
    "eps"
  ],
  "LogBinomialSoftmax": {
    "__init__": [
      "self",
      "n_classes",
      "act"
    ],
    "forward": [
      "self",
      "probabilities",
      "temperature",
      "eps"
    ]
  },
  "ZoeDepthConditionalLogBinomialSoftmax": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "condition_dim",
      "n_classes",
      "bottleneck_factor"
    ],
    "forward": [
      "self",
      "main_feature",
      "condition_feature"
    ]
  },
  "ZoeDepthSeedBinRegressor": {
    "__init__": [
      "self",
      "config",
      "n_bins",
      "mlp_dim",
      "min_depth",
      "max_depth"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "inv_attractor": [
    "dx",
    "alpha",
    "gamma"
  ],
  "ZoeDepthAttractorLayer": {
    "__init__": [
      "self",
      "config",
      "n_bins",
      "n_attractors",
      "min_depth",
      "max_depth",
      "memory_efficient"
    ],
    "forward": [
      "self",
      "x",
      "prev_bin",
      "prev_bin_embedding",
      "interpolate"
    ]
  },
  "ZoeDepthAttractorLayerUnnormed": {
    "__init__": [
      "self",
      "config",
      "n_bins",
      "n_attractors",
      "min_depth",
      "max_depth",
      "memory_efficient"
    ],
    "forward": [
      "self",
      "x",
      "prev_bin",
      "prev_bin_embedding",
      "interpolate"
    ]
  },
  "ZoeDepthProjector": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "mlp_dim"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ZoeDepthMultiheadAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "dropout"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "values",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ZoeDepthTransformerEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "dropout",
      "activation"
    ],
    "forward": [
      "self",
      "src",
      "src_mask"
    ]
  },
  "ZoeDepthPatchTransformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "positional_encoding_1d": [
      "self",
      "batch_size",
      "sequence_length",
      "embedding_dim",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ZoeDepthMLPClassifier": {
    "__init__": [
      "self",
      "in_features",
      "out_features"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "ZoeDepthMultipleMetricDepthEstimationHeads": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "outconv_activation",
      "bottleneck",
      "feature_blocks",
      "relative_depth"
    ]
  },
  "ZoeDepthMetricDepthEstimationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "outconv_activation",
      "bottleneck",
      "feature_blocks",
      "relative_depth"
    ]
  },
  "ZoeDepthPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ZoeDepthForDepthEstimation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ZoeDepthImageProcessorFast": {
    "do_pad": [],
    "do_rescale": [],
    "do_normalize": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "size": [],
    "resample": [],
    "keep_aspect_ratio": [],
    "ensure_multiple_of": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "resize": [
      "self",
      "images",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "interpolation"
    ],
    "_pad_images": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "keep_aspect_ratio",
      "ensure_multiple_of",
      "interpolation",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "post_process_depth_estimation": [
      "self",
      "outputs",
      "source_sizes",
      "target_sizes",
      "outputs_flipped",
      "do_remove_padding"
    ]
  },
  "ZOEDEPTH_PRETRAINED_CONFIG_ARCHIVE_MAP": [],
  "ZoeDepthConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "hidden_act",
      "initializer_range",
      "batch_norm_eps",
      "readout_type",
      "reassemble_factors",
      "neck_hidden_sizes",
      "fusion_hidden_size",
      "head_in_index",
      "use_batch_norm_in_fusion_residual",
      "use_bias_in_fusion_residual",
      "num_relative_features",
      "add_projection",
      "bottleneck_features",
      "num_attractors",
      "bin_embedding_dim",
      "attractor_alpha",
      "attractor_gamma",
      "attractor_kind",
      "min_temp",
      "max_temp",
      "bin_centers_type",
      "bin_configurations",
      "num_patch_transformer_layers",
      "patch_transformer_hidden_size",
      "patch_transformer_intermediate_size",
      "patch_transformer_num_attention_heads"
    ]
  },
  "Idefics3ProcessorKwargs": {
    "_defaults": []
  },
  "Idefics3Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "image_seq_len",
      "chat_template"
    ],
    "_extract_images_from_prompts": [
      "self",
      "prompts"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "image_seq_len"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "Idefics3ImageProcessorKwargs": {},
  "Idefics3ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_convert_rgb",
      "do_resize",
      "size",
      "resample",
      "do_image_splitting",
      "max_image_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "split_image": [
      "self",
      "image",
      "max_image_size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize_for_vision_encoder": [
      "self",
      "image",
      "vision_encoder_max_size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "images",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_convert_rgb",
      "do_resize",
      "size",
      "resample",
      "do_image_splitting",
      "do_rescale",
      "max_image_size",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "return_tensors",
      "return_row_col_info",
      "data_format",
      "input_data_format"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "Idefics3ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "max_image_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_image_splitting": [],
    "do_pad": [],
    "return_row_col_info": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "_prepare_images_structure": [
      "self",
      "images",
      "expected_ndims"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation",
      "antialias"
    ],
    "split_images": [
      "self",
      "images",
      "max_image_size",
      "interpolation"
    ],
    "resize_for_vision_encoder": [
      "self",
      "image",
      "vision_encoder_max_size",
      "interpolation"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "fill",
      "return_pixel_mask"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "do_image_splitting",
      "max_image_size",
      "return_row_col_info",
      "disable_grouping",
      "return_tensors"
    ],
    "to_dict": [
      "self"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width",
      "images_kwargs"
    ]
  },
  "Idefics3BaseModelOutputWithPast": {},
  "Idefics3CausalLMOutputWithPast": {},
  "Idefics3VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "patch_attention_mask"
    ]
  },
  "Idefics3VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Idefics3VisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Idefics3SimpleMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Idefics3EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Idefics3Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "Idefics3RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Idefics3Connector": {
    "__init__": [
      "self",
      "config"
    ],
    "pixel_shuffle": [
      "self",
      "x",
      "scale_factor"
    ],
    "forward": [
      "self",
      "image_hidden_states"
    ]
  },
  "Idefics3PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": []
  },
  "Idefics3VisionTransformer": {
    "input_modalities": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "pixel_values",
      "patch_attention_mask"
    ]
  },
  "Idefics3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "inputs_merger": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_hidden_states"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_attention_mask",
      "image_hidden_states",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "return_dict"
    ]
  },
  "Idefics3ForConditionalGeneration": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "pixel_attention_mask"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "pixel_attention_mask",
      "image_hidden_states",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "return_dict",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "pixel_values",
      "pixel_attention_mask",
      "image_hidden_states",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "Idefics3VisionConfig": {
    "model_type": [],
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "layer_norm_eps",
      "attention_dropout",
      "initializer_range"
    ]
  },
  "Idefics3Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "use_cache",
      "image_token_id",
      "tie_word_embeddings",
      "vision_config",
      "text_config",
      "scale_factor",
      "pad_token_id"
    ]
  },
  "ViltProcessorKwargs": {
    "_defaults": []
  },
  "ViltProcessor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ]
  },
  "ViltConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "type_vocab_size",
      "modality_type_vocab_size",
      "max_position_embeddings",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "max_image_length",
      "tie_word_embeddings",
      "num_images",
      "pad_token_id"
    ]
  },
  "MAX_LONGER_EDGE": [],
  "MAX_SHORTER_EDGE": [],
  "ViltImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "size_divisor": [],
    "do_pad": [],
    "default_to_square": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "size_divisor",
      "do_pad",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "resize": [
      "self",
      "images",
      "size",
      "interpolation",
      "size_divisor"
    ],
    "_pad_batch": [
      "self",
      "images",
      "return_tensors",
      "disable_grouping"
    ]
  },
  "ViltForImagesAndTextClassificationOutput": {},
  "ViltEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "visual_embed": [
      "self",
      "pixel_values",
      "pixel_mask",
      "max_image_length"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "image_token_type_idx"
    ]
  },
  "TextEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "ViltPatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "ViltSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ViltSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViltAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ViltIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViltOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "ViltLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "ViltEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ViltPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "ViltModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "image_token_type_idx",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ViltPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViltForMaskedLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ViltPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "ViltMLMHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ViltForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ViltForImageAndTextRetrieval": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ViltForImagesAndTextClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ViltForTokenClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "pixel_values",
      "pixel_mask",
      "inputs_embeds",
      "image_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ViltImageProcessorKwargs": {},
  "ViltImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "size_divisor",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "images",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "size_divisor",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_pad",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "get_oneformer_resize_output_image_size": [
    "image",
    "size",
    "max_size",
    "default_to_square"
  ],
  "OneFormerImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "crop_size": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "default_to_square": [],
    "do_center_crop": [],
    "do_convert_rgb": [],
    "rescale_factor": [],
    "ignore_index": [],
    "do_reduce_labels": [],
    "repo_path": [],
    "class_info_file": [],
    "num_text": [],
    "num_labels": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images",
      "task_inputs",
      "segmentation_maps",
      "instance_id_to_semantic_id"
    ],
    "_preprocess_image_like_inputs": [
      "self",
      "images",
      "task_inputs",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_convert_rgb",
      "input_data_format",
      "device"
    ],
    "_preprocess": [
      "self",
      "images",
      "task_inputs",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "disable_grouping",
      "return_tensors"
    ],
    "_pad_image_fast": [
      "self",
      "image",
      "output_size",
      "constant_values"
    ],
    "pad": [
      "self",
      "images",
      "return_pixel_mask",
      "return_tensors"
    ],
    "convert_segmentation_map_to_binary_masks": [
      "self",
      "segmentation_map",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels"
    ],
    "get_semantic_annotations": [
      "self",
      "label",
      "num_class_obj"
    ],
    "get_instance_annotations": [
      "self",
      "label",
      "num_class_obj"
    ],
    "get_panoptic_annotations": [
      "self",
      "label",
      "num_class_obj"
    ],
    "_encode_inputs_fast": [
      "self",
      "pixel_values_list",
      "task_inputs",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels",
      "return_tensors"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "task_type",
      "is_demo",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "_get_clones": [
    "module",
    "N"
  ],
  "OneFormerHungarianMatcher": {
    "__init__": [
      "self",
      "cost_class",
      "cost_mask",
      "cost_dice",
      "num_points"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "mask_labels",
      "class_labels"
    ]
  },
  "OneFormerLoss": {
    "__init__": [
      "self",
      "num_classes",
      "matcher",
      "weight_dict",
      "eos_coef",
      "num_points",
      "oversample_ratio",
      "importance_sample_ratio",
      "contrastive_temperature"
    ],
    "_max_by_axis": [
      "self",
      "the_list"
    ],
    "_pad_images_to_max_in_batch": [
      "self",
      "tensors"
    ],
    "loss_contrastive": [
      "self",
      "contrastive_queries_logits",
      "text_queries"
    ],
    "loss_labels": [
      "self",
      "class_queries_logits",
      "class_labels",
      "indices"
    ],
    "loss_masks": [
      "self",
      "masks_queries_logits",
      "mask_labels",
      "indices",
      "num_masks"
    ],
    "calculate_uncertainty": [
      "self",
      "logits"
    ],
    "sample_points_using_uncertainty": [
      "self",
      "logits",
      "uncertainty_function",
      "num_points",
      "oversample_ratio",
      "importance_sample_ratio"
    ],
    "_get_predictions_permutation_indices": [
      "self",
      "indices"
    ],
    "_get_targets_permutation_indices": [
      "self",
      "indices"
    ],
    "forward": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "contrastive_queries_logits",
      "mask_labels",
      "class_labels",
      "text_queries",
      "auxiliary_predictions",
      "calculate_contrastive_loss"
    ],
    "get_num_masks": [
      "self",
      "class_labels",
      "device"
    ]
  },
  "OneFormerTransformerDecoderOutput": {},
  "OneFormerPixelDecoderOutput": {},
  "OneFormerPixelLevelModuleOutput": {},
  "OneFormerModelOutput": {},
  "OneFormerForUniversalSegmentationOutput": {},
  "OneFormerPixelDecoderEncoderMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "n_levels",
      "n_points"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "level_start_index",
      "output_attentions"
    ]
  },
  "OneFormerPixelDecoderEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "level_start_index",
      "output_attentions"
    ]
  },
  "OneFormerPixelDecoderEncoderOnly": {
    "__init__": [
      "self",
      "config"
    ],
    "get_reference_points": [
      "spatial_shapes",
      "valid_ratios",
      "device"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "position_embeddings",
      "spatial_shapes",
      "level_start_index",
      "valid_ratios",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OneFormerPixelDecoder": {
    "__init__": [
      "self",
      "config",
      "feature_channels"
    ],
    "get_valid_ratio": [
      "self",
      "mask",
      "dtype"
    ],
    "forward": [
      "self",
      "features",
      "encoder_outputs",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OneFormerPixelLevelModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "OneFormerAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "batch_size"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "key_value_states",
      "key_value_position_embeddings",
      "output_attentions"
    ]
  },
  "OneFormerTransformerDecoderSelfAttentionLayer": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "activation",
      "normalize_before",
      "layer_norm_eps"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "pos"
    ],
    "forward_post": [
      "self",
      "output",
      "output_mask",
      "output_key_padding_mask",
      "query_pos"
    ],
    "forward_pre": [
      "self",
      "output",
      "output_mask",
      "output_key_padding_mask",
      "query_pos"
    ],
    "forward": [
      "self",
      "output",
      "output_mask",
      "output_key_padding_mask",
      "query_pos"
    ]
  },
  "OneFormerTransformerDecoderCrossAttentionLayer": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "activation",
      "normalize_before",
      "layer_norm_eps"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "pos"
    ],
    "forward_post": [
      "self",
      "output",
      "memory",
      "memory_mask",
      "memory_key_padding_mask",
      "pos",
      "query_pos"
    ],
    "forward_pre": [
      "self",
      "output",
      "memory",
      "memory_mask",
      "memory_key_padding_mask",
      "pos",
      "query_pos"
    ],
    "forward": [
      "self",
      "output",
      "memory",
      "memory_mask",
      "memory_key_padding_mask",
      "pos",
      "query_pos"
    ]
  },
  "OneFormerTransformerDecoderFFNLayer": {
    "__init__": [
      "self",
      "d_model",
      "dim_feedforward",
      "dropout",
      "activation",
      "normalize_before",
      "layer_norm_eps"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "pos"
    ],
    "forward_post": [
      "self",
      "output"
    ],
    "forward_pre": [
      "self",
      "output"
    ],
    "forward": [
      "self",
      "output"
    ]
  },
  "OneFormerMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "OneFormerTransformerDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "index",
      "output",
      "multi_stage_features",
      "multi_stage_positional_embeddings",
      "attention_mask",
      "query_embeddings",
      "output_attentions"
    ]
  },
  "OneFormerTransformerDecoderQueryTransformerDecoder": {
    "__init__": [
      "self",
      "decoder_layer",
      "num_layers",
      "norm",
      "return_intermediate"
    ],
    "forward": [
      "self",
      "output",
      "memory",
      "output_mask",
      "memory_mask",
      "output_key_padding_mask",
      "memory_key_padding_mask",
      "pos",
      "query_pos"
    ]
  },
  "OneFormerTransformerDecoderQueryTransformerDecoderLayer": {
    "__init__": [
      "self",
      "d_model",
      "nhead",
      "dim_feedforward",
      "dropout",
      "activation",
      "normalize_before",
      "layer_norm_eps"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "pos"
    ],
    "forward_post": [
      "self",
      "output",
      "memory",
      "output_mask",
      "memory_mask",
      "output_key_padding_mask",
      "memory_key_padding_mask",
      "pos",
      "query_pos"
    ],
    "forward_pre": [
      "self",
      "output",
      "memory",
      "output_mask",
      "memory_mask",
      "output_key_padding_mask",
      "memory_key_padding_mask",
      "pos",
      "query_pos"
    ],
    "forward": [
      "self",
      "output",
      "memory",
      "output_mask",
      "memory_mask",
      "output_key_padding_mask",
      "memory_key_padding_mask",
      "pos",
      "query_pos"
    ]
  },
  "OneFormerTransformerDecoderQueryTransformer": {
    "__init__": [
      "self",
      "d_model",
      "nhead",
      "num_decoder_layers",
      "dim_feedforward",
      "dropout",
      "activation",
      "normalize_before",
      "return_intermediate_dec",
      "layer_norm_eps"
    ],
    "forward": [
      "self",
      "src",
      "mask",
      "query_embed",
      "pos_embed",
      "task_token"
    ]
  },
  "OneFormerTransformerDecoder": {
    "__init__": [
      "self",
      "in_channels",
      "config"
    ],
    "forward": [
      "self",
      "task_token",
      "multi_stage_features",
      "multi_stage_positional_embeddings",
      "mask_features",
      "query_features",
      "query_embeddings",
      "query_embedder",
      "size_list",
      "output_attentions"
    ],
    "forward_prediction_heads": [
      "self",
      "output",
      "mask_features",
      "attention_mask_target_size"
    ],
    "_get_aux_predictions": [
      "self",
      "outputs_class",
      "outputs_seg_masks"
    ]
  },
  "OneFormerTransformerModule": {
    "__init__": [
      "self",
      "in_features",
      "config"
    ],
    "forward": [
      "self",
      "multi_scale_features",
      "mask_features",
      "task_token",
      "output_attentions"
    ]
  },
  "OneFormerSinePositionEmbedding": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "shape",
      "device",
      "dtype",
      "mask"
    ]
  },
  "OneFormerTextMapperAttention": {
    "__init__": [
      "self",
      "dim",
      "num_heads",
      "qkv_bias",
      "qk_scale",
      "attn_drop",
      "proj_drop"
    ],
    "forward": [
      "self",
      "q",
      "k",
      "v"
    ]
  },
  "OneFormerTextTransformerDecoderLayer": {
    "__init__": [
      "self",
      "d_model",
      "nhead",
      "dropout",
      "layer_norm_eps"
    ],
    "forward": [
      "self",
      "hidden_state",
      "mem"
    ]
  },
  "OneFormerTextContextDecoder": {
    "__init__": [
      "self",
      "transformer_width",
      "transformer_heads",
      "transformer_layers",
      "visual_dim",
      "dropout",
      "layer_norm_eps"
    ],
    "forward": [
      "self",
      "text",
      "visual"
    ]
  },
  "OneFormerTextMLP": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "output_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "OneFormerTextTransformerLayer": {
    "__init__": [
      "self",
      "width",
      "heads",
      "attn_mask",
      "layer_norm_eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_padding_mask"
    ]
  },
  "OneFormerTextTransformer": {
    "__init__": [
      "self",
      "width",
      "layers",
      "heads",
      "attn_mask",
      "use_checkpoint",
      "layer_norm_eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "OneFormerTextEncoder": {
    "__init__": [
      "self",
      "context_length",
      "width",
      "layers",
      "vocab_size",
      "use_checkpoint",
      "layer_norm_eps"
    ],
    "build_attention_mask": [
      "self"
    ],
    "forward": [
      "self",
      "text"
    ]
  },
  "OneFormerTextMapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ],
    "encode_text": [
      "self",
      "text"
    ]
  },
  "OneFormerTaskModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "OneFormerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "OneFormerModel": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "task_inputs",
      "text_inputs",
      "pixel_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "OneFormerForUniversalSegmentation": {
    "main_input_name": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_loss_dict": [
      "self",
      "masks_queries_logits",
      "class_queries_logits",
      "contrastive_queries_logits",
      "mask_labels",
      "class_labels",
      "text_queries",
      "auxiliary_predictions",
      "calculate_contrastive_loss"
    ],
    "get_loss": [
      "self",
      "loss_dict"
    ],
    "forward": [
      "self",
      "pixel_values",
      "task_inputs",
      "text_inputs",
      "mask_labels",
      "class_labels",
      "pixel_mask",
      "output_auxiliary_logits",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "OneFormerConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "ignore_value",
      "num_queries",
      "no_object_weight",
      "class_weight",
      "mask_weight",
      "dice_weight",
      "contrastive_weight",
      "contrastive_temperature",
      "train_num_points",
      "oversample_ratio",
      "importance_sample_ratio",
      "init_std",
      "init_xavier_std",
      "layer_norm_eps",
      "is_training",
      "use_auxiliary_loss",
      "output_auxiliary_logits",
      "strides",
      "task_seq_len",
      "text_encoder_width",
      "text_encoder_context_length",
      "text_encoder_num_layers",
      "text_encoder_vocab_size",
      "text_encoder_proj_layers",
      "text_encoder_n_ctx",
      "conv_dim",
      "mask_dim",
      "hidden_dim",
      "encoder_feedforward_dim",
      "norm",
      "encoder_layers",
      "decoder_layers",
      "use_task_norm",
      "num_attention_heads",
      "dropout",
      "dim_feedforward",
      "pre_norm",
      "enforce_input_proj",
      "query_dec_layers",
      "common_stride"
    ]
  },
  "OneFormerImageProcessorKwargs": {},
  "prepare_metadata": [
    "class_info"
  ],
  "load_metadata": [
    "repo_id",
    "class_info_file"
  ],
  "OneFormerImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "repo_path",
      "class_info_file",
      "num_text",
      "num_labels"
    ],
    "to_dict": [
      "self"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "convert_segmentation_map_to_binary_masks": [
      "self",
      "segmentation_map",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels"
    ],
    "__call__": [
      "self",
      "images",
      "task_inputs",
      "segmentation_maps"
    ],
    "_preprocess": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "input_data_format"
    ],
    "_preprocess_image": [
      "self",
      "image",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "data_format",
      "input_data_format"
    ],
    "_preprocess_mask": [
      "self",
      "segmentation_map",
      "do_resize",
      "size",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "task_inputs",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "ignore_index",
      "do_reduce_labels",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "constant_values",
      "data_format",
      "input_data_format"
    ],
    "pad": [
      "self",
      "images",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "get_semantic_annotations": [
      "self",
      "label",
      "num_class_obj"
    ],
    "get_instance_annotations": [
      "self",
      "label",
      "num_class_obj"
    ],
    "get_panoptic_annotations": [
      "self",
      "label",
      "num_class_obj"
    ],
    "encode_inputs": [
      "self",
      "pixel_values_list",
      "task_inputs",
      "segmentation_maps",
      "instance_id_to_semantic_id",
      "ignore_index",
      "do_reduce_labels",
      "return_tensors",
      "input_data_format"
    ],
    "post_process_semantic_segmentation": [
      "self",
      "outputs",
      "target_sizes"
    ],
    "post_process_instance_segmentation": [
      "self",
      "outputs",
      "task_type",
      "is_demo",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "target_sizes",
      "return_coco_annotation"
    ],
    "post_process_panoptic_segmentation": [
      "self",
      "outputs",
      "threshold",
      "mask_threshold",
      "overlap_mask_area_threshold",
      "label_ids_to_fuse",
      "target_sizes"
    ]
  },
  "OneFormerProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "max_seq_length",
      "task_seq_length"
    ],
    "_preprocess_text": [
      "self",
      "text_list",
      "max_length"
    ],
    "__call__": [
      "self",
      "images",
      "task_inputs",
      "segmentation_maps"
    ],
    "encode_inputs": [
      "self",
      "images",
      "task_inputs",
      "segmentation_maps"
    ],
    "post_process_semantic_segmentation": [
      "self"
    ],
    "post_process_instance_segmentation": [
      "self"
    ],
    "post_process_panoptic_segmentation": [
      "self"
    ]
  },
  "BambaFlashAttentionKwargs": {},
  "BambaRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "BambaAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "BambaRMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "BambaMixer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask",
      "seq_idx"
    ],
    "torch_forward": [
      "self",
      "input_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask",
      "seq_idx"
    ]
  },
  "BambaMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BambaRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "BambaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "layer_type"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "BambaPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "BambaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ],
    "_update_causal_mask": [
      "self",
      "attention_mask",
      "input_tensor",
      "cache_position",
      "past_key_values",
      "output_attentions"
    ],
    "_prepare_4d_causal_attention_mask_with_cache_position": [
      "attention_mask",
      "sequence_length",
      "target_length",
      "dtype",
      "cache_position",
      "batch_size"
    ],
    "_update_mamba_mask": [
      "self",
      "attention_mask",
      "cache_position"
    ]
  },
  "BambaForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "is_first_iteration"
    ]
  },
  "BambaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "tie_word_embeddings",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "num_logits_to_keep",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "max_position_embeddings",
      "attention_dropout",
      "attn_layer_indices",
      "mamba_n_heads",
      "mamba_d_head",
      "mamba_n_groups",
      "mamba_d_state",
      "mamba_d_conv",
      "mamba_expand",
      "mamba_chunk_size",
      "mamba_conv_bias",
      "mamba_proj_bias",
      "time_step_min",
      "time_step_max",
      "time_step_limit",
      "z_loss_coefficient",
      "rope_parameters"
    ],
    "layers_block_type": [
      "self"
    ]
  },
  "ApertusConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout"
    ]
  },
  "ApertusMLP": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "ApertusRMSNorm": {},
  "ApertusRotaryEmbedding": {},
  "ApertusAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ApertusDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "ApertusPreTrainedModel": {},
  "ApertusModel": {},
  "ApertusForCausalLM": {
    "forward": [
      "self"
    ]
  },
  "ApertusForTokenClassification": {},
  "Wav2Vec2ConformerForPreTrainingOutput": {},
  "Wav2Vec2ConformerSamePadLayer": {
    "__init__": [
      "self",
      "num_conv_pos_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerPositionalConvEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerRotaryPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerRelPositionalEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "extend_pe": [
      "self",
      "x",
      "pe"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerNoLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerLayerNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerGroupNormConvLayer": {
    "__init__": [
      "self",
      "config",
      "layer_id"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerFeatureEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "_freeze_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_values"
    ]
  },
  "Wav2Vec2ConformerFeatureProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerConvolutionModule": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "relative_position_embeddings",
      "output_attentions"
    ],
    "_apply_rotary_embedding": [
      "self",
      "hidden_states",
      "relative_position_embeddings"
    ],
    "_apply_relative_embeddings": [
      "self",
      "query",
      "key",
      "relative_position_embeddings"
    ]
  },
  "Wav2Vec2ConformerEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "relative_position_embeddings",
      "output_attentions"
    ]
  },
  "Wav2Vec2ConformerEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2ConformerGumbelVectorQuantizer": {
    "__init__": [
      "self",
      "config"
    ],
    "_compute_perplexity": [
      "probs",
      "mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mask_time_indices"
    ]
  },
  "Wav2Vec2ConformerAdapter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerAdapterLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Wav2Vec2ConformerPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_get_feat_extract_output_lengths": [
      "self",
      "input_lengths",
      "add_adapter"
    ],
    "_get_feature_vector_attention_mask": [
      "self",
      "feature_vector_length",
      "attention_mask",
      "add_adapter"
    ]
  },
  "Wav2Vec2ConformerBaseModelOutput": [],
  "Wav2Vec2ConformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "_mask_hidden_states": [
      "self",
      "hidden_states",
      "mask_time_indices",
      "attention_mask"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2ConformerForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "set_gumbel_temperature": [
      "self",
      "temperature"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "compute_contrastive_logits": [
      "target_features",
      "negative_features",
      "predicted_features",
      "temperature"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "mask_time_indices",
      "sampled_negative_indices",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2ConformerForCTC": {
    "__init__": [
      "self",
      "config",
      "target_lang"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2ConformerForSequenceClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2ConformerForAudioFrameClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "Wav2Vec2ConformerForXVector": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_feature_encoder": [
      "self"
    ],
    "freeze_base_model": [
      "self"
    ],
    "_get_tdnn_output_lengths": [
      "self",
      "input_lengths"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "Wav2Vec2ConformerConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout",
      "activation_dropout",
      "attention_dropout",
      "feat_proj_dropout",
      "feat_quantizer_dropout",
      "final_dropout",
      "layerdrop",
      "initializer_range",
      "layer_norm_eps",
      "feat_extract_norm",
      "feat_extract_activation",
      "conv_dim",
      "conv_stride",
      "conv_kernel",
      "conv_bias",
      "num_conv_pos_embeddings",
      "num_conv_pos_embedding_groups",
      "apply_spec_augment",
      "mask_time_prob",
      "mask_time_length",
      "mask_time_min_masks",
      "mask_feature_prob",
      "mask_feature_length",
      "mask_feature_min_masks",
      "num_codevectors_per_group",
      "num_codevector_groups",
      "contrastive_logits_temperature",
      "num_negatives",
      "codevector_dim",
      "proj_codevector_dim",
      "diversity_loss_weight",
      "ctc_loss_reduction",
      "ctc_zero_infinity",
      "use_weighted_layer_sum",
      "classifier_proj_size",
      "tdnn_dim",
      "tdnn_kernel",
      "tdnn_dilation",
      "xvector_output_dim",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "add_adapter",
      "adapter_kernel_size",
      "adapter_stride",
      "num_adapter_layers",
      "output_hidden_size",
      "position_embeddings_type",
      "rotary_embedding_base",
      "max_source_positions",
      "conv_depthwise_kernel_size",
      "conformer_conv_dropout"
    ],
    "inputs_to_logits_ratio": [
      "self"
    ]
  },
  "WAV2VEC2_CONFORMER_START_DOCSTRING": [],
  "MobileNetV1ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": []
  },
  "MobileNetV1ImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "MobileNetV1ConvLayer": {
    "__init__": [
      "self",
      "config",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "bias",
      "use_normalization",
      "use_activation"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "MobileNetV1PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": []
  },
  "MobileNetV1Model": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MobileNetV1ForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "labels",
      "return_dict"
    ]
  },
  "MobileNetV1Config": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "image_size",
      "depth_multiplier",
      "min_depth",
      "hidden_act",
      "tf_padding",
      "classifier_dropout_prob",
      "initializer_range",
      "layer_norm_eps"
    ]
  },
  "BaseModelOutputWithVisualIndicatorFeatures": {},
  "Ovis2ModelOutputWithPast": {},
  "Ovis2CausalLMOutputWithPast": {},
  "Ovis2RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Ovis2VisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Ovis2VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Ovis2VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Ovis2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Ovis2VisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Ovis2VisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "Ovis2VisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "attention_mask"
    ]
  },
  "Ovis2VisualEmbeddingTable": {
    "forward": [
      "self",
      "visual_tokens"
    ]
  },
  "Ovis2PreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_cache_class": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Ovis2VisionModel": {
    "_can_record_outputs": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Ovis2Model": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Ovis2ForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "Ovis2ImageProcessorKwargs": {},
  "compute_patch_covering_area": [
    "left",
    "upper",
    "right",
    "lower",
    "side"
  ],
  "split_image_into_grid": [
    "h",
    "w",
    "grid"
  ],
  "get_min_tile_covering_grid": [
    "image_size",
    "target_patch_size",
    "max_image_tiles",
    "covering_threshold"
  ],
  "Ovis2ImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "crop_to_patches",
      "min_patches",
      "max_patches",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "use_covering_area_grid"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_to_patches",
      "min_patches",
      "max_patches",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "do_convert_rgb",
      "data_format",
      "input_data_format",
      "use_covering_area_grid"
    ],
    "crop_image_to_patches": [
      "self",
      "images",
      "min_patches",
      "max_patches",
      "use_covering_area_grid",
      "patch_size",
      "data_format",
      "covering_threshold"
    ]
  },
  "Ovis2ImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "crop_to_patches": [],
    "min_patches": [],
    "max_patches": [],
    "use_covering_area_grid": [],
    "valid_kwargs": [],
    "preprocess": [
      "self",
      "images"
    ],
    "crop_image_to_patches": [
      "self",
      "images",
      "min_patches",
      "max_patches",
      "use_covering_area_grid",
      "covering_threshold",
      "patch_size",
      "interpolation"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "crop_to_patches",
      "min_patches",
      "max_patches",
      "use_covering_area_grid",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "Ovis2ProcessorKwargs": {
    "_defaults": []
  },
  "Ovis2Processor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "image_token",
      "image_seq_length"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_expand_image_tokens": [
      "self",
      "text",
      "grids"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "Ovis2VisionConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "rms_norm_eps",
      "attention_dropout",
      "qkv_bias",
      "mlp_bias",
      "hidden_act",
      "vocab_size",
      "hidden_stride",
      "num_visual_indicator_tokens",
      "initializer_range",
      "tokenize_function"
    ]
  },
  "Ovis2Config": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_id",
      "visual_indicator_token_ids",
      "vocab_size",
      "hidden_size",
      "tie_word_embeddings"
    ]
  },
  "VitsConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "window_size",
      "use_bias",
      "ffn_dim",
      "layerdrop",
      "ffn_kernel_size",
      "flow_size",
      "spectrogram_bins",
      "hidden_act",
      "hidden_dropout",
      "attention_dropout",
      "activation_dropout",
      "initializer_range",
      "layer_norm_eps",
      "use_stochastic_duration_prediction",
      "num_speakers",
      "speaker_embedding_size",
      "upsample_initial_channel",
      "upsample_rates",
      "upsample_kernel_sizes",
      "resblock_kernel_sizes",
      "resblock_dilation_sizes",
      "leaky_relu_slope",
      "depth_separable_channels",
      "depth_separable_num_layers",
      "duration_predictor_flow_bins",
      "duration_predictor_tail_bound",
      "duration_predictor_kernel_size",
      "duration_predictor_dropout",
      "duration_predictor_num_flows",
      "duration_predictor_filter_channels",
      "prior_encoder_num_flows",
      "prior_encoder_num_wavenet_layers",
      "posterior_encoder_num_wavenet_layers",
      "wavenet_kernel_size",
      "wavenet_dilation_rate",
      "wavenet_dropout",
      "speaking_rate",
      "noise_scale",
      "noise_scale_duration",
      "sampling_rate",
      "pad_token_id"
    ]
  },
  "VitsModelOutput": {},
  "VitsTextEncoderOutput": {},
  "fused_add_tanh_sigmoid_multiply": [
    "input_a",
    "input_b",
    "num_channels"
  ],
  "_unconstrained_rational_quadratic_spline": [
    "inputs",
    "unnormalized_widths",
    "unnormalized_heights",
    "unnormalized_derivatives",
    "reverse",
    "tail_bound",
    "min_bin_width",
    "min_bin_height",
    "min_derivative"
  ],
  "_rational_quadratic_spline": [
    "inputs",
    "unnormalized_widths",
    "unnormalized_heights",
    "unnormalized_derivatives",
    "reverse",
    "tail_bound",
    "min_bin_width",
    "min_bin_height",
    "min_derivative"
  ],
  "VitsWaveNet": {
    "__init__": [
      "self",
      "config",
      "num_layers"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "VitsPosteriorEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning"
    ]
  },
  "VitsHifiGan": {
    "__init__": [
      "self",
      "config"
    ],
    "apply_weight_norm": [
      "self"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "forward": [
      "self",
      "spectrogram",
      "global_conditioning"
    ]
  },
  "VitsResidualCouplingLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning",
      "reverse"
    ]
  },
  "VitsResidualCouplingBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning",
      "reverse"
    ]
  },
  "VitsDilatedDepthSeparableConv": {
    "__init__": [
      "self",
      "config",
      "dropout_rate"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning"
    ]
  },
  "VitsConvFlow": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning",
      "reverse"
    ]
  },
  "VitsElementwiseAffine": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning",
      "reverse"
    ]
  },
  "VitsStochasticDurationPredictor": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning",
      "durations",
      "reverse",
      "noise_scale"
    ]
  },
  "VitsDurationPredictor": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs",
      "padding_mask",
      "global_conditioning"
    ]
  },
  "VitsAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ],
    "_get_relative_embeddings": [
      "self",
      "relative_embeddings",
      "length"
    ],
    "_relative_position_to_absolute_position": [
      "self",
      "x"
    ],
    "_absolute_position_to_relative_position": [
      "self",
      "x"
    ]
  },
  "VitsFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask"
    ]
  },
  "VitsEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask",
      "attention_mask",
      "output_attentions"
    ]
  },
  "VitsEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "padding_mask",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VitsTextEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "padding_mask",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "VitsPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "VitsModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "speaker_id",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "has_non_roman_characters": [
    "input_string"
  ],
  "VitsTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "pad_token",
      "unk_token",
      "language",
      "add_blank",
      "normalize",
      "phonemize",
      "is_uroman"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "normalize_text": [
      "self",
      "input_string"
    ],
    "_preprocess_char": [
      "self",
      "text"
    ],
    "prepare_for_tokenization": [
      "self",
      "text",
      "is_split_into_words",
      "normalize"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ]
  },
  "FocalNetConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim",
      "use_conv_embed",
      "hidden_sizes",
      "depths",
      "focal_levels",
      "focal_windows",
      "hidden_act",
      "mlp_ratio",
      "hidden_dropout_prob",
      "drop_path_rate",
      "use_layerscale",
      "layerscale_value",
      "use_post_layernorm",
      "use_post_layernorm_in_modulation",
      "normalize_modulator",
      "initializer_range",
      "layer_norm_eps",
      "encoder_stride",
      "out_features",
      "out_indices"
    ]
  },
  "FocalNetEncoderOutput": {},
  "FocalNetModelOutput": {},
  "FocalNetMaskedImageModelingOutput": {},
  "FocalNetImageClassifierOutput": {},
  "FocalNetEmbeddings": {
    "__init__": [
      "self",
      "config",
      "use_mask_token"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "FocalNetPatchEmbeddings": {
    "__init__": [
      "self",
      "config",
      "image_size",
      "patch_size",
      "num_channels",
      "embed_dim",
      "add_norm",
      "use_conv_embed",
      "is_stem"
    ],
    "maybe_pad": [
      "self",
      "pixel_values",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "FocalNetDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "FocalNetModulation": {
    "__init__": [
      "self",
      "config",
      "index",
      "dim",
      "focal_factor",
      "bias",
      "projection_dropout"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "FocalNetMlp": {
    "__init__": [
      "self",
      "config",
      "in_features",
      "hidden_features",
      "out_features",
      "drop"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "FocalNetLayer": {
    "__init__": [
      "self",
      "config",
      "index",
      "dim",
      "input_resolution",
      "drop_path"
    ],
    "forward": [
      "self",
      "hidden_state",
      "input_dimensions"
    ]
  },
  "FocalNetStage": {
    "__init__": [
      "self",
      "config",
      "index",
      "input_resolution"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions"
    ]
  },
  "FocalNetEncoder": {
    "__init__": [
      "self",
      "config",
      "grid_size"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_dimensions",
      "output_hidden_states",
      "output_hidden_states_before_downsampling",
      "return_dict"
    ]
  },
  "FocalNetPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "FocalNetModel": {
    "__init__": [
      "self",
      "config",
      "add_pooling_layer",
      "use_mask_token"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FocalNetForMaskedImageModeling": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FocalNetForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "FocalNetBackbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PegasusXConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "pad_token_id",
      "eos_token_id",
      "forced_eos_token_id",
      "num_global_tokens",
      "block_size",
      "stagger_local_blocks",
      "tie_word_embeddings"
    ]
  },
  "DimensionInfo": {},
  "PegasusXScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "PegasusXSinusoidalPositionalEmbedding": {
    "__init__": [
      "self",
      "embed_dim",
      "max_scale"
    ],
    "forward": [
      "self",
      "input_embeds",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "PegasusXAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "PegasusXGlobalLocalAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "block_size",
      "dropout",
      "is_decoder"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "token_hidden_states",
      "global_hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "compute_global_attention_representations": [
      "self",
      "global_q",
      "global_k",
      "global_v",
      "local_k",
      "local_v",
      "mask",
      "dim"
    ],
    "compute_local_attention_representations": [
      "self",
      "global_k",
      "global_v",
      "local_q",
      "local_k",
      "local_v",
      "mask",
      "dim"
    ]
  },
  "PegasusXEncoderLayer": {
    "__init__": [
      "self",
      "stagger_blocks_this_layer",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "global_hidden_states",
      "attention_mask",
      "output_attentions"
    ],
    "pad_local_tokens": [
      "cls",
      "hidden_states",
      "attention_mask",
      "block_size"
    ],
    "unpad_local_tokens": [
      "cls",
      "padded_hidden_states",
      "block_size"
    ]
  },
  "PegasusXDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "PegasusXPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": []
  },
  "PegasusXEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PegasusXDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "PegasusXModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "PegasusXForConditionalGeneration": {
    "base_model_prefix": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_position_embeddings": [
      "self",
      "new_num_position_embeddings"
    ],
    "get_position_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ],
    "prepare_decoder_input_ids_from_labels": [
      "self",
      "labels"
    ]
  },
  "PegasusXDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "PatchTSTConfig": {
    "model_type": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "num_input_channels",
      "context_length",
      "distribution_output",
      "loss",
      "patch_length",
      "patch_stride",
      "num_hidden_layers",
      "d_model",
      "num_attention_heads",
      "share_embedding",
      "channel_attention",
      "ffn_dim",
      "norm_type",
      "norm_eps",
      "attention_dropout",
      "positional_dropout",
      "path_dropout",
      "ff_dropout",
      "bias",
      "activation_function",
      "pre_norm",
      "positional_encoding_type",
      "use_cls_token",
      "init_std",
      "share_projection",
      "scaling",
      "do_mask_input",
      "mask_type",
      "random_mask_ratio",
      "num_forecast_mask_patches",
      "channel_consistent_masking",
      "unmasked_channel_indices",
      "mask_value",
      "pooling_type",
      "head_dropout",
      "prediction_length",
      "num_targets",
      "output_range",
      "num_parallel_samples"
    ]
  },
  "PatchTSTAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "PatchTSTBatchNorm": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "PatchTSTPatchify": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values"
    ]
  },
  "PatchTSTMasking": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "patch_input"
    ]
  },
  "PatchTSTEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_attentions"
    ]
  },
  "PatchTSTPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "module",
      "value"
    ]
  },
  "PatchTSTEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "patch_input"
    ]
  },
  "PatchTSTPositionalEncoding": {
    "__init__": [
      "self",
      "config",
      "num_patches"
    ],
    "_init_pe": [
      "config",
      "num_patches"
    ],
    "forward": [
      "self",
      "patch_input"
    ]
  },
  "PatchTSTEncoder": {
    "__init__": [
      "self",
      "config",
      "num_patches"
    ],
    "forward": [
      "self",
      "patch_input",
      "output_hidden_states",
      "output_attentions"
    ]
  },
  "PatchTSTModelOutput": {},
  "PatchTSTForPretrainingOutput": {},
  "PatchTSTForRegressionOutput": {},
  "PatchTSTForPredictionOutput": {},
  "PatchTSTForClassificationOutput": {},
  "SamplePatchTSTOutput": {},
  "PatchTSTStdScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "PatchTSTMeanScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "PatchTSTNOPScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "PatchTSTScaler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "data",
      "observed_indicator"
    ]
  },
  "PatchTSTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "past_observed_mask",
      "future_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "PatchTSTMaskPretrainHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "embedding"
    ]
  },
  "PatchTSTForPretraining": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "past_observed_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "PatchTSTClassificationHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "embedding"
    ]
  },
  "PatchTSTForClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "target_values",
      "past_observed_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "PatchTSTPredictionHead": {
    "__init__": [
      "self",
      "config",
      "num_patches",
      "distribution_output"
    ],
    "forward": [
      "self",
      "embedding"
    ]
  },
  "PatchTSTForPrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "past_observed_mask",
      "future_values",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ],
    "generate": [
      "self",
      "past_values",
      "past_observed_mask"
    ]
  },
  "PatchTSTRegressionHead": {
    "__init__": [
      "self",
      "config",
      "distribution_output"
    ],
    "forward": [
      "self",
      "embedding"
    ]
  },
  "PatchTSTForRegression": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "past_values",
      "target_values",
      "past_observed_mask",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ],
    "generate": [
      "self",
      "past_values",
      "past_observed_mask"
    ]
  },
  "LlavaImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "size": [],
    "default_to_square": [],
    "crop_size": [],
    "do_pad": [],
    "do_resize": [],
    "do_center_crop": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "pad_to_square": [
      "self",
      "images",
      "background_color"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "interpolation",
      "do_pad",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "LlavaProcessorKwargs": {
    "_defaults": []
  },
  "LlavaProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "patch_size",
      "vision_feature_select_strategy",
      "chat_template",
      "image_token",
      "num_additional_image_tokens"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ]
  },
  "LlavaConfig": {
    "model_type": [],
    "attribute_map": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "text_config",
      "image_token_index",
      "projector_hidden_act",
      "vision_feature_select_strategy",
      "vision_feature_layer",
      "image_seq_length",
      "multimodal_projector_bias",
      "tie_word_embeddings"
    ]
  },
  "LlavaModelOutputWithPast": {},
  "LlavaCausalLMOutputWithPast": {},
  "LlavaMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "image_features"
    ]
  },
  "LlavaPreTrainedModel": {
    "base_model_prefix": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_can_compile_fullgraph": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": []
  },
  "LlavaModel": {
    "_checkpoint_conversion_mapping": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "output_hidden_states"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "cache_position",
      "image_sizes"
    ]
  },
  "LlavaForConditionalGeneration": {
    "_checkpoint_conversion_mapping": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "vision_feature_layer",
      "vision_feature_select_strategy"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "vision_feature_layer",
      "vision_feature_select_strategy",
      "labels",
      "cache_position",
      "logits_to_keep",
      "image_sizes"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "LlavaImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_pad",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "pad_to_square": [
      "self",
      "image",
      "background_color",
      "data_format",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_pad",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "Sam3TrackerVideoPromptEncoderConfig": {
    "base_config_key": [],
    "__init__": [
      "self",
      "hidden_size",
      "image_size",
      "patch_size",
      "mask_input_channels",
      "num_point_embeddings",
      "hidden_act",
      "layer_norm_eps",
      "scale"
    ]
  },
  "Sam3TrackerVideoProcessor": {},
  "Sam3TrackerVideoMaskDecoderConfig": {},
  "Sam3TrackerVideoConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "vision_config",
      "prompt_encoder_config",
      "mask_decoder_config",
      "initializer_range",
      "num_maskmem",
      "image_size",
      "sigmoid_scale_for_mem_enc",
      "sigmoid_bias_for_mem_enc",
      "enable_occlusion_spatial_embedding",
      "multimask_output_in_sam",
      "multimask_min_pt_num",
      "multimask_max_pt_num",
      "multimask_output_for_tracking",
      "max_object_pointers_in_encoder",
      "max_cond_frame_num",
      "enable_temporal_pos_encoding_for_object_pointers",
      "memory_attention_hidden_size",
      "memory_attention_num_layers",
      "memory_attention_num_attention_heads",
      "memory_attention_downsample_rate",
      "memory_attention_feed_forward_hidden_size",
      "memory_attention_feed_forward_hidden_act",
      "memory_attention_dropout",
      "memory_attention_rope_theta",
      "memory_attention_rope_feat_sizes",
      "memory_attention_rope_dropout",
      "memory_encoder_hidden_size",
      "memory_encoder_output_channels",
      "mask_downsampler_embed_dim",
      "mask_downsampler_kernel_size",
      "mask_downsampler_stride",
      "mask_downsampler_padding",
      "mask_downsampler_total_stride",
      "mask_downsampler_hidden_act",
      "memory_fuser_num_layers",
      "memory_fuser_embed_dim",
      "memory_fuser_intermediate_dim",
      "memory_fuser_kernel_size",
      "memory_fuser_padding",
      "memory_fuser_layer_scale_init_value",
      "memory_fuser_hidden_act"
    ],
    "image_size": [
      "self",
      "value"
    ]
  },
  "Sam3TrackerVideoInferenceCache": {},
  "Sam3TrackerVideoInferenceSession": {},
  "Sam3TrackerVideoLayerNorm": {},
  "Sam3TrackerVideoPositionEmbeddingSine": {},
  "Sam3TrackerVideoAttention": {},
  "Sam3TrackerVideoTwoWayAttentionBlock": {},
  "Sam3TrackerVideoFeedForward": {},
  "Sam3TrackerVideoImageSegmentationOutput": {},
  "Sam3TrackerVideoSegmentationOutput": {},
  "Sam3TrackerVideoPreTrainedModel": {},
  "Sam3TrackerVideoVisionRotaryEmbedding": {},
  "Sam3TrackerVideoRoPEAttention": {},
  "Sam3TrackerVideoMemoryAttentionLayer": {},
  "Sam3TrackerVideoMemoryAttention": {},
  "Sam3TrackerVideoMemoryFuserCXBlock": {},
  "Sam3TrackerVideoMemoryFuser": {},
  "Sam3TrackerVideoMaskDownSamplerLayer": {},
  "Sam3TrackerVideoMaskDownSampler": {},
  "Sam3TrackerVideoMemoryEncoder": {},
  "Sam3TrackerVideoVisionEncoderOutput": {},
  "Sam3TrackerVideoPositionalEmbedding": {},
  "Sam3TrackerVideoMaskEmbedding": {},
  "Sam3TrackerVideoPromptEncoder": {},
  "Sam3TrackerVideoTwoWayTransformer": {},
  "Sam3TrackerVideoMaskDecoder": {},
  "Sam3TrackerVideoModel": {
    "_checkpoint_conversion_mapping": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config",
      "remove_vision_encoder"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ]
  },
  "VitPoseImageProcessorFast": {
    "image_mean": [],
    "image_std": [],
    "size": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_affine_transform": [],
    "normalize_factor": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "torch_affine_transform": [
      "self",
      "image",
      "center",
      "scale",
      "rotation",
      "size"
    ],
    "preprocess": [
      "self",
      "images",
      "boxes"
    ],
    "_preprocess": [
      "self",
      "images",
      "boxes",
      "do_affine_transform",
      "size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ],
    "keypoints_from_heatmaps": [
      "self",
      "heatmaps",
      "center",
      "scale",
      "kernel"
    ],
    "post_process_pose_estimation": [
      "self",
      "outputs",
      "boxes",
      "kernel_size",
      "threshold",
      "target_sizes"
    ]
  },
  "VitPoseConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "backbone_config",
      "initializer_range",
      "scale_factor",
      "use_simple_decoder"
    ]
  },
  "VitPoseEstimatorOutput": {},
  "VitPosePreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "flip_back": [
    "output_flipped",
    "flip_pairs",
    "target_type"
  ],
  "VitPoseSimpleDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "flip_pairs"
    ]
  },
  "VitPoseClassicDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "flip_pairs"
    ]
  },
  "VitPoseForPoseEstimation": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "dataset_index",
      "flip_pairs",
      "labels"
    ]
  },
  "VitPoseImageProcessorKwargs": {},
  "box_to_center_and_scale": [
    "box",
    "image_width",
    "image_height",
    "normalize_factor",
    "padding_factor"
  ],
  "coco_to_pascal_voc": [
    "bboxes"
  ],
  "get_keypoint_predictions": [
    "heatmaps"
  ],
  "post_dark_unbiased_data_processing": [
    "coords",
    "batch_heatmaps",
    "kernel"
  ],
  "transform_preds": [
    "coords",
    "center",
    "scale",
    "output_size"
  ],
  "get_warp_matrix": [
    "theta",
    "size_input",
    "size_dst",
    "size_target"
  ],
  "scipy_warp_affine": [
    "src",
    "M",
    "size"
  ],
  "VitPoseImageProcessor": {
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "do_affine_transform",
      "size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "affine_transform": [
      "self",
      "image",
      "center",
      "scale",
      "rotation",
      "size",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "boxes",
      "do_affine_transform",
      "size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format",
      "input_data_format"
    ],
    "keypoints_from_heatmaps": [
      "self",
      "heatmaps",
      "center",
      "scale",
      "kernel"
    ],
    "post_process_pose_estimation": [
      "self",
      "outputs",
      "boxes",
      "kernel_size",
      "threshold",
      "target_sizes"
    ]
  },
  "GroundingDinoImageProcessorKwargs": {},
  "GroundingDinoImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "format",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_annotations",
      "do_pad",
      "pad_size"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "size",
      "resample"
    ],
    "rescale": [
      "self",
      "image",
      "rescale_factor",
      "data_format",
      "input_data_format"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "_pad_image": [
      "self",
      "image",
      "output_size",
      "annotation",
      "constant_values",
      "data_format",
      "input_data_format",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "images",
      "annotations",
      "constant_values",
      "return_pixel_mask",
      "return_tensors",
      "data_format",
      "input_data_format",
      "update_bboxes",
      "pad_size"
    ],
    "preprocess": [
      "self",
      "images",
      "annotations",
      "return_segmentation_masks",
      "masks_path",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "format",
      "return_tensors",
      "data_format",
      "input_data_format",
      "pad_size"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ]
  },
  "GroundingDinoDecoderOutput": {},
  "GroundingDinoEncoderOutput": {},
  "GroundingDinoModelOutput": {},
  "GroundingDinoObjectDetectionOutput": {},
  "GroundingDinoFrozenBatchNorm2d": {
    "__init__": [
      "self",
      "n"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GroundingDinoConvEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "GroundingDinoConvModel": {
    "__init__": [
      "self",
      "conv_encoder",
      "position_embedding"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "GroundingDinoSinePositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "GroundingDinoLearnedPositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "pixel_mask"
    ]
  },
  "GroundingDinoMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "config",
      "num_heads",
      "n_points"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "output_attentions"
    ]
  },
  "GroundingDinoTextEnhancerLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "with_pos_embed": [
      "self",
      "hidden_state",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_masks",
      "position_embeddings"
    ]
  },
  "GroundingDinoBiMultiHeadAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_reshape": [
      "self",
      "tensor",
      "seq_len",
      "batch_size"
    ],
    "forward": [
      "self",
      "vision_features",
      "text_features",
      "vision_attention_mask",
      "text_attention_mask"
    ]
  },
  "GroundingDinoDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GroundingDinoFusionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_features",
      "text_features",
      "attention_mask_vision",
      "attention_mask_text"
    ]
  },
  "GroundingDinoDeformableLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "output_attentions"
    ]
  },
  "GroundingDinoEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "get_text_position_embeddings": [
      "self",
      "text_features",
      "text_position_embedding",
      "text_position_ids"
    ],
    "forward": [
      "self",
      "vision_features",
      "vision_position_embedding",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "key_padding_mask",
      "reference_points",
      "text_features",
      "text_attention_mask",
      "text_position_embedding",
      "text_self_attention_masks",
      "text_position_ids"
    ]
  },
  "GroundingDinoMultiheadAttention": {
    "__init__": [
      "self",
      "config",
      "num_attention_heads"
    ],
    "forward": [
      "self",
      "queries",
      "keys",
      "values",
      "attention_mask",
      "output_attentions"
    ]
  },
  "GroundingDinoDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "position_embeddings"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "vision_encoder_hidden_states",
      "vision_encoder_attention_mask",
      "text_encoder_hidden_states",
      "text_encoder_attention_mask",
      "self_attn_mask",
      "output_attentions"
    ]
  },
  "GroundingDinoContrastiveEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_hidden_state",
      "text_hidden_state",
      "text_token_mask"
    ]
  },
  "GroundingDinoPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "module",
      "value"
    ]
  },
  "GroundingDinoEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "get_reference_points": [
      "spatial_shapes_list",
      "valid_ratios",
      "device"
    ],
    "forward": [
      "self",
      "vision_features",
      "vision_attention_mask",
      "vision_position_embedding",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios",
      "text_features",
      "text_attention_mask",
      "text_position_embedding",
      "text_self_attention_masks",
      "text_position_ids",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GroundingDinoDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "vision_encoder_hidden_states",
      "vision_encoder_attention_mask",
      "text_encoder_hidden_states",
      "text_encoder_attention_mask",
      "reference_points",
      "spatial_shapes",
      "spatial_shapes_list",
      "level_start_index",
      "valid_ratios",
      "self_attn_mask",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GroundingDinoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "freeze_backbone": [
      "self"
    ],
    "unfreeze_backbone": [
      "self"
    ],
    "get_valid_ratio": [
      "self",
      "mask"
    ],
    "generate_encoder_output_proposals": [
      "self",
      "enc_output",
      "padding_mask",
      "spatial_shapes_list"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "pixel_mask",
      "encoder_outputs",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GroundingDinoMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GroundingDinoForObjectDetection": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "pixel_mask",
      "encoder_outputs",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels"
    ]
  },
  "GroundingDinoImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "format": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_pad": [],
    "size": [],
    "default_to_square": [],
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self"
    ],
    "prepare_annotation": [
      "self",
      "image",
      "target",
      "format",
      "return_segmentation_masks",
      "masks_path",
      "input_data_format"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "interpolation"
    ],
    "resize_annotation": [
      "self",
      "annotation",
      "orig_size",
      "target_size",
      "threshold",
      "interpolation"
    ],
    "normalize_annotation": [
      "self",
      "annotation",
      "image_size"
    ],
    "_update_annotation_for_padded_image": [
      "self",
      "annotation",
      "input_image_size",
      "output_image_size",
      "padding",
      "update_bboxes"
    ],
    "pad": [
      "self",
      "image",
      "padded_size",
      "annotation",
      "update_bboxes",
      "fill"
    ],
    "_preprocess": [
      "self",
      "images",
      "annotations",
      "masks_path",
      "return_segmentation_masks",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "do_convert_annotations",
      "image_mean",
      "image_std",
      "do_pad",
      "pad_size",
      "format",
      "return_tensors"
    ],
    "post_process_object_detection": [
      "self",
      "outputs",
      "threshold",
      "target_sizes"
    ]
  },
  "GroundingDinoConfig": {
    "model_type": [],
    "sub_configs": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "backbone_config",
      "text_config",
      "num_queries",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "auxiliary_loss",
      "position_embedding_type",
      "num_feature_levels",
      "encoder_n_points",
      "decoder_n_points",
      "two_stage",
      "class_cost",
      "bbox_cost",
      "giou_cost",
      "bbox_loss_coefficient",
      "giou_loss_coefficient",
      "focal_alpha",
      "disable_custom_kernels",
      "max_text_len",
      "text_enhancer_dropout",
      "fusion_droppath",
      "fusion_dropout",
      "embedding_init_target",
      "query_dim",
      "decoder_bbox_embed_share",
      "two_stage_bbox_embed_share",
      "positional_embedding_temperature",
      "init_std",
      "layer_norm_eps",
      "tie_word_embeddings"
    ]
  },
  "get_phrases_from_posmap": [
    "posmaps",
    "input_ids"
  ],
  "_is_list_of_candidate_labels": [
    "text"
  ],
  "_merge_candidate_labels_text": [
    "text"
  ],
  "DictWithDeprecationWarning": {
    "message": [],
    "__getitem__": [
      "self",
      "key"
    ],
    "get": [
      "self",
      "key"
    ]
  },
  "GroundingDinoProcessorKwargs": {
    "_defaults": []
  },
  "GroundingDinoProcessor": {
    "valid_processor_kwargs": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_preprocess_input_text": [
      "self",
      "text"
    ],
    "post_process_grounded_object_detection": [
      "self",
      "outputs",
      "input_ids",
      "threshold",
      "text_threshold",
      "target_sizes",
      "text_labels"
    ]
  },
  "Glm46VVideoProcessorInitKwargs": {},
  "Glm46VVideoProcessor": {
    "resample": [],
    "size": [],
    "max_image_size": [],
    "image_mean": [],
    "image_std": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "do_sample_frames": [],
    "patch_size": [],
    "temporal_patch_size": [],
    "max_duration": [],
    "merge_size": [],
    "valid_kwargs": [],
    "num_frames": [],
    "fps": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size"
    ],
    "sample_frames": [
      "self",
      "metadata",
      "fps"
    ],
    "_preprocess": [
      "self",
      "videos",
      "do_convert_rgb",
      "do_resize",
      "size",
      "interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "return_tensors"
    ]
  },
  "Glm46VConfig": {
    "model_type": [],
    "sub_configs": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "image_token_id",
      "video_token_id",
      "image_start_token_id",
      "image_end_token_id",
      "video_start_token_id",
      "video_end_token_id",
      "tie_word_embeddings"
    ]
  },
  "Glm46VPreTrainedModel": {
    "_can_record_outputs": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Glm46VModel": {
    "_no_split_modules": [],
    "__init__": [
      "self",
      "config"
    ]
  },
  "Glm46VForConditionalGeneration": {},
  "Glm46VProcessor": {
    "replace_frame_token_id": [
      "self",
      "timestamp_sec"
    ]
  },
  "Glm46VImageProcessor": {},
  "Glm46VImageProcessorFast": {},
  "Glm46VProcessorKwargs": {
    "_defaults": []
  },
  "Glm46VModelOutputWithPast": {},
  "Glm46VCausalLMOutputWithPast": {},
  "Glm46VImageProcessorKwargs": {},
  "Dinov2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "mlp_ratio",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "initializer_range",
      "layer_norm_eps",
      "image_size",
      "patch_size",
      "num_channels",
      "qkv_bias",
      "layerscale_value",
      "drop_path_rate",
      "use_swiglu_ffn",
      "out_features",
      "out_indices",
      "apply_layernorm",
      "reshape_hidden_states",
      "use_mask_token"
    ]
  },
  "Dinov2Embeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "interpolate_pos_encoding": [
      "self",
      "embeddings",
      "height",
      "width"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos"
    ]
  },
  "Dinov2PatchEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Dinov2SelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dinov2SelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "Dinov2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dinov2LayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Dinov2DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Dinov2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Dinov2SwiGLUFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Dinov2Layer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dinov2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "output_hidden_states"
    ]
  },
  "Dinov2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_sdpa": [],
    "_supports_flash_attn": [],
    "_supports_flex_attn": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "Dinov2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "bool_masked_pos",
      "output_hidden_states"
    ]
  },
  "Dinov2ForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels"
    ]
  },
  "Dinov2Backbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "BlenderbotLearnedPositionalEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_ids_shape",
      "past_key_values_length",
      "position_ids"
    ]
  },
  "BlenderbotScaledWordEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "embed_scale"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "BlenderbotAttention": {
    "__init__": [
      "self",
      "embed_dim",
      "num_heads",
      "dropout",
      "is_decoder",
      "bias",
      "is_causal",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "key_value_states",
      "past_key_values",
      "attention_mask",
      "output_attentions",
      "cache_position"
    ]
  },
  "BlenderbotEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_attentions"
    ]
  },
  "BlenderbotDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position"
    ]
  },
  "BlenderbotPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "dummy_inputs": [
      "self"
    ]
  },
  "BlenderbotEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BlenderbotDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BlenderbotModel": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BlenderbotForConditionalGeneration": {
    "base_model_prefix": [],
    "_keys_to_ignore_on_load_missing": [],
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "resize_token_embeddings": [
      "self",
      "new_num_tokens",
      "pad_to_multiple_of",
      "mean_resizing"
    ],
    "_resize_final_logits_bias": [
      "self",
      "new_num_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "inputs_embeds",
      "decoder_inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position"
    ]
  },
  "BlenderbotDecoderWrapper": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "BlenderbotForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "BlenderbotTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "model": [],
    "__init__": [
      "self",
      "bos_token",
      "eos_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "pad_token",
      "mask_token",
      "add_prefix_space",
      "vocab",
      "merges"
    ]
  },
  "BlenderbotConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "attribute_map": [],
    "__init__": [
      "self",
      "vocab_size",
      "max_position_embeddings",
      "encoder_layers",
      "encoder_ffn_dim",
      "encoder_attention_heads",
      "decoder_layers",
      "decoder_ffn_dim",
      "decoder_attention_heads",
      "encoder_layerdrop",
      "decoder_layerdrop",
      "use_cache",
      "is_encoder_decoder",
      "activation_function",
      "d_model",
      "dropout",
      "attention_dropout",
      "activation_dropout",
      "init_std",
      "decoder_start_token_id",
      "scale_embedding",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "encoder_no_repeat_ngram_size",
      "forced_eos_token_id",
      "is_decoder",
      "tie_word_embeddings"
    ]
  },
  "ArceeMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ArceeRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ArceeRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "ArceeAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "ArceeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "ArceePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "ArceeModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "ArceeForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "ArceeForSequenceClassification": {},
  "ArceeForQuestionAnswering": {
    "base_model_prefix": []
  },
  "ArceeForTokenClassification": {},
  "ArceeConfig": {
    "model_type": [],
    "base_model_tp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "head_dim"
    ]
  },
  "LongcatFlashRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "LongcatFlashRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "LongcatFlashMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LongcatFlashTopkRouter": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "get_topk_indices": [
      "self",
      "scores"
    ]
  },
  "LongcatFlashExperts": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "top_k_index",
      "top_k_weights"
    ]
  },
  "LongcatFlashMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LongcatFlashMLA": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "LongcatFlashDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "LongcatFlashPreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "LongcatFlashModel": {
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "cache_position",
      "use_cache"
    ]
  },
  "LongcatFlashForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "LongcatFlashConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "default_theta": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "num_hidden_layers",
      "num_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "ffn_hidden_size",
      "q_lora_rank",
      "kv_lora_rank",
      "qk_nope_head_dim",
      "qk_rope_head_dim",
      "head_dim",
      "v_head_dim",
      "qk_head_dim",
      "moe_topk",
      "n_routed_experts",
      "zero_expert_num",
      "expert_ffn_hidden_size",
      "routed_scaling_factor"
    ],
    "convert_rope_params_to_dict": [
      "self",
      "ignore_keys_at_rope_validation"
    ]
  },
  "HGNetV2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "embedding_size",
      "depths",
      "hidden_sizes",
      "hidden_act",
      "out_features",
      "out_indices",
      "stem_channels",
      "stage_in_channels",
      "stage_mid_channels",
      "stage_out_channels",
      "stage_num_blocks",
      "stage_downsample",
      "stage_light_block",
      "stage_kernel_size",
      "stage_numb_of_layers",
      "use_learnable_affine_block",
      "initializer_range"
    ]
  },
  "HGNetV2PreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "HGNetV2LearnableAffineBlock": {
    "__init__": [
      "self",
      "scale_value",
      "bias_value"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "HGNetV2ConvLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "activation",
      "use_learnable_affine_block"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "HGNetV2ConvLayerLight": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "use_learnable_affine_block"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "HGNetV2Embeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "HGNetV2BasicLayer": {
    "__init__": [
      "self",
      "in_channels",
      "middle_channels",
      "out_channels",
      "layer_num",
      "kernel_size",
      "residual",
      "light_block",
      "drop_path",
      "use_learnable_affine_block"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "HGNetV2Stage": {
    "__init__": [
      "self",
      "config",
      "stage_index",
      "drop_path"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "HGNetV2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_state",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "HGNetV2Backbone": {
    "has_attentions": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "HGNetV2ForImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "labels",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DeepseekVLHybridImageProcessorKwargs": {},
  "DeepseekVLHybridImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "high_res_size",
      "min_size",
      "resample",
      "high_res_resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "high_res_image_mean",
      "high_res_image_std",
      "do_convert_rgb",
      "do_pad"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "high_res_size",
      "resample",
      "high_res_resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "high_res_image_mean",
      "high_res_image_std",
      "return_tensors",
      "data_format",
      "input_data_format",
      "do_convert_rgb",
      "do_pad",
      "background_color"
    ],
    "pad_to_square": [
      "self",
      "image",
      "background_color",
      "data_format",
      "input_data_format"
    ]
  },
  "DEEPSEEK_VL_COMMON_CUSTOM_ARGS": [],
  "DeepseekVLHybridConfig": {
    "model_type": [],
    "sub_configs": [],
    "__init__": [
      "self",
      "text_config",
      "vision_config",
      "high_res_vision_config",
      "image_token_id",
      "tie_word_embeddings"
    ]
  },
  "BaseModelOutputWithHighResVisionEncodings": {},
  "DeepseekVLHybridBaseModelOutputWithPast": {},
  "DeepseekVLHybridCausalLMOutputWithPast": {},
  "DeepseekVLHybridLayerNorm": {},
  "DeepseekVLSamVisionNeck": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "DeepseekVLSamVisionProj": {
    "__init__": [
      "self",
      "config",
      "output_size"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DeepseekVLHybridAligner": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "vision_encodings",
      "high_res_vision_encodings"
    ]
  },
  "DeepseekVLHybridPreTrainedModel": {
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DeepseekVLHybridModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_low_res_image_features": [
      "self",
      "pixel_values"
    ],
    "get_high_res_image_features": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "high_res_pixel_values"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "high_res_pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "inputs_embeds",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "DeepseekVLHybridForConditionalGeneration": {
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "high_res_pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "inputs_embeds",
      "labels",
      "use_cache",
      "logits_to_keep"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "high_res_pixel_values",
      "attention_mask",
      "cache_position",
      "logits_to_keep",
      "is_first_iteration"
    ]
  },
  "DeepseekVLHybridImageProcessorFast": {
    "high_res_image_mean": [],
    "high_res_image_std": [],
    "high_res_size": [],
    "high_res_resample": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "_further_process_kwargs": [
      "self",
      "size",
      "high_res_size",
      "default_to_square",
      "image_mean",
      "image_std",
      "high_res_image_mean",
      "high_res_image_std",
      "data_format"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "high_res_size",
      "min_size",
      "interpolation",
      "high_res_interpolation",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "high_res_image_mean",
      "high_res_image_std",
      "disable_grouping",
      "return_tensors",
      "do_pad"
    ]
  },
  "DeepseekVLHybridProcessorKwargs": {},
  "DeepseekVLHybridProcessor": {
    "__call__": [
      "self",
      "text",
      "images"
    ]
  },
  "PixtralVisionConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_channels",
      "image_size",
      "patch_size",
      "hidden_act",
      "attention_dropout",
      "rope_parameters",
      "initializer_range"
    ]
  },
  "PixtralImageProcessorFast": {
    "resample": [],
    "image_mean": [],
    "image_std": [],
    "patch_size": [],
    "size": [],
    "default_to_square": [],
    "do_resize": [],
    "do_rescale": [],
    "do_normalize": [],
    "do_convert_rgb": [],
    "valid_kwargs": [],
    "model_input_names": [],
    "__init__": [
      "self"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "patch_size",
      "interpolation"
    ],
    "_pad_for_batching": [
      "self",
      "pixel_values",
      "image_sizes"
    ],
    "_preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "patch_size",
      "interpolation",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "disable_grouping",
      "return_tensors"
    ]
  },
  "position_ids_in_meshgrid": [
    "patch_embeds_list",
    "max_width"
  ],
  "PixtralRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device",
      "layer_type"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PixtralAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "output_attentions"
    ]
  },
  "PixtralMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "PixtralRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "PixtralAttentionLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "output_attentions"
    ]
  },
  "PixtralTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "position_embeddings",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PixtralPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "supports_gradient_checkpointing": [],
    "_supports_attention_backend": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_no_split_modules": []
  },
  "generate_block_attention_mask": [
    "patch_embeds_list",
    "tensor"
  ],
  "PixtralVisionModel": {
    "base_model_prefix": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "image_sizes",
      "output_hidden_states",
      "output_attentions",
      "return_dict"
    ]
  },
  "PixtralProcessorKwargs": {
    "_defaults": []
  },
  "PixtralProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "patch_size",
      "spatial_merge_size",
      "chat_template",
      "image_token",
      "image_break_token",
      "image_end_token"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "_get_num_multimodal_tokens": [
      "self",
      "image_sizes"
    ],
    "model_input_names": [
      "self"
    ]
  },
  "PixtralImageProcessorKwargs": {},
  "PixtralImageProcessor": {
    "model_input_names": [],
    "valid_kwargs": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "patch_size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "patch_size",
      "resample",
      "data_format",
      "input_data_format"
    ],
    "_pad_for_batching": [
      "self",
      "pixel_values",
      "image_sizes",
      "data_format",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "patch_size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "return_tensors",
      "data_format",
      "input_data_format"
    ]
  },
  "GraniteAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "GraniteRMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GraniteMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GraniteDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "output_attentions",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "GranitePreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "GraniteRotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "GraniteModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position"
    ]
  },
  "GraniteForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "GraniteConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "attention_bias",
      "attention_dropout",
      "mlp_bias",
      "embedding_multiplier",
      "logits_scaling",
      "residual_multiplier",
      "attention_multiplier"
    ]
  },
  "DINOv3ConvNextDropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "DINOv3ConvNextLayerNorm": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DINOv3ConvNextLayer": {
    "__init__": [
      "self",
      "config",
      "channels",
      "drop_path"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DINOv3ConvNextStage": {
    "__init__": [
      "self",
      "config",
      "stage_idx"
    ],
    "forward": [
      "self",
      "features"
    ]
  },
  "DINOv3ConvNextPreTrainedModel": {
    "base_model_prefix": [],
    "main_input_name": [],
    "input_modalities": [],
    "_no_split_modules": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "DINOv3ConvNextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "DINOv3ConvNextBackbone": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "output_hidden_states"
    ]
  },
  "DINOv3ConvNextConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_channels",
      "hidden_sizes",
      "depths",
      "hidden_act",
      "initializer_range",
      "layer_norm_eps",
      "layer_scale_init_value",
      "drop_path_rate",
      "image_size",
      "out_features",
      "out_indices"
    ],
    "num_stages": [
      "self"
    ]
  },
  "Ministral3Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "base_model_tp_plan": [],
    "base_model_pp_plan": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "rope_parameters",
      "sliding_window",
      "attention_dropout"
    ]
  },
  "_get_llama_4_attn_scale": [
    "positions_ids",
    "beta",
    "max_position_embeddings"
  ],
  "Ministral3Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "past_key_values",
      "cache_position"
    ]
  },
  "Ministral3MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Ministral3RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "Ministral3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "use_cache",
      "cache_position",
      "position_embeddings"
    ]
  },
  "Ministral3PreTrainedModel": {
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_supports_flash_attn": [],
    "_supports_sdpa": [],
    "_supports_flex_attn": [],
    "_can_compile_fullgraph": [],
    "_supports_attention_backend": [],
    "_can_record_outputs": []
  },
  "Ministral3RotaryEmbedding": {
    "__init__": [
      "self",
      "config",
      "device"
    ],
    "compute_default_rope_parameters": [
      "config",
      "device",
      "seq_len"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Ministral3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position"
    ]
  },
  "Ministral3ForCausalLM": {
    "_tied_weights_keys": [],
    "_tp_plan": [],
    "_pp_plan": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "Ministral3ForTokenClassification": {},
  "Ministral3ForSequenceClassification": {},
  "Ministral3ForQuestionAnswering": {}
}