{
  "graph_cache": [
    "key_fn",
    "maxsize"
  ],
  "jit": [
    "heur_modes"
  ],
  "graph": [
    "handle",
    "name",
    "io_data_type",
    "intermediate_data_type",
    "compute_data_type"
  ],
  "is_windows": [],
  "module_name": [],
  "_pybind_module": [],
  "torch_available": [],
  "_torch_to_cudnn_data_type_dict": [],
  "cutlass_available": [],
  "_torch_to_cutlass_data_type_dict": [],
  "is_torch_available": [],
  "is_cutlass_available": [],
  "_torch_to_cudnn_data_type": [
    "torch_data_type"
  ],
  "_torch_to_cutlass_data_type": [
    "data_type",
    "interpret_uint8_as_fp4x2"
  ],
  "_convert_to_cutlass_data_type": [
    "data_type",
    "interpret_uint8_as_fp4x2"
  ],
  "_cudnn_to_torch_data_type": [
    "cudnn_data_type"
  ],
  "_library_type": [
    "input_type"
  ],
  "_is_torch_tensor": [
    "input_tensor"
  ],
  "symbols_to_import": [],
  "__version__": [],
  "_tensor": [
    "self",
    "dim",
    "stride",
    "data_type",
    "is_virtual",
    "is_pass_by_value",
    "ragged_offset",
    "reordering_type",
    "name",
    "uid"
  ],
  "_set_data_type": [
    "self",
    "data_type"
  ],
  "_library_device_pointer": [
    "input_tensor"
  ],
  "_execute": [
    "self",
    "tensor_to_device_buffer",
    "workspace",
    "handle",
    "override_uids",
    "override_shapes",
    "override_strides"
  ],
  "_execute_plan_at_index": [
    "self",
    "tensor_to_device_buffer",
    "workspace",
    "index",
    "handle",
    "override_uids",
    "override_shapes",
    "override_strides"
  ],
  "load_cudnn": [],
  "_dlopen_cudnn": [],
  "__getattr__": [
    "name"
  ],
  "ceil_div": [
    "a",
    "b"
  ],
  "is_power_of_2": [
    "n"
  ],
  "APIBase": {
    "__init__": [
      "self"
    ],
    "check_support": [
      "self"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "_ensure_support_checked": [
      "self"
    ],
    "_get_default_stream": [
      "self",
      "stream"
    ],
    "_pad_tensor_to_ndim": [
      "self",
      "tensor",
      "ndim",
      "name"
    ],
    "_unpad_tensor_to_ndim": [
      "self",
      "tensor",
      "ndim",
      "name"
    ],
    "_is_fp4x2": [
      "self",
      "tensor_or_dtype"
    ],
    "_is_fp8": [
      "self",
      "tensor_or_dtype"
    ],
    "_get_innermost_stride_dim": [
      "self",
      "tensor",
      "name"
    ],
    "_tensor_shape": [
      "self",
      "tensor",
      "name"
    ],
    "_tensor_stride": [
      "self",
      "tensor",
      "name"
    ],
    "_check_tensor_shape": [
      "self",
      "tensor_or_shape",
      "shape",
      "name"
    ],
    "_check_tensor_stride": [
      "self",
      "tensor_or_stride",
      "stride",
      "stride_order",
      "name",
      "extra_error_msg"
    ],
    "_check_dtype": [
      "self",
      "tensor_or_dtype",
      "dtype",
      "name",
      "extra_error_msg"
    ],
    "_value_error_if": [
      "self",
      "condition",
      "error_msg"
    ],
    "_not_implemented_error_if": [
      "self",
      "condition",
      "error_msg"
    ],
    "_runtime_error_if": [
      "self",
      "condition",
      "error_msg"
    ],
    "_make_cute_pointer": [
      "self",
      "tensor",
      "assumed_align"
    ],
    "_make_cute_tensor_descriptor": [
      "self",
      "tensor",
      "assumed_align",
      "name"
    ]
  },
  "TupleDict": {
    "__init__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ]
  },
  "__all__": [],
  "CudnnHandle": [],
  "_default_cudnn_handle": [],
  "logger": [],
  "_graph_tensor": [
    "graph",
    "tensor"
  ],
  "_find_tensor": [
    "tensor",
    "tensor_map",
    "dlpack_map"
  ],
  "_extract_tensor": [
    "name",
    "tensor",
    "arg_dict"
  ],
  "_tensor_like": [
    "cudnn_tensor",
    "tensor_type"
  ],
  "get_default_handle": [
    "stream"
  ],
  "destroy_default_handle": [],
  "Graph": {
    "__init__": [
      "self"
    ],
    "__del__": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "tb"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "__call__": [
      "self"
    ],
    "__call_with_positional_args": [
      "self"
    ],
    "__call_with_tensor_dict": [
      "self",
      "tensor_dict"
    ],
    "set_io_tuples": [
      "self",
      "inputs",
      "outputs"
    ]
  },
  "LOG2_E": [],
  "PersistentDenseGemmKernel": {
    "__init__": [
      "self",
      "acc_dtype",
      "use_2cta_instrs",
      "mma_tiler_mn",
      "cluster_shape_mn"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "a",
      "b",
      "ab12",
      "c",
      "alpha",
      "max_active_clusters",
      "stream",
      "epilogue_op"
    ],
    "kernel": [
      "self",
      "tiled_mma",
      "tma_atom_a",
      "mA_mkl",
      "tma_atom_b",
      "mB_nkl",
      "tma_atom_ab12",
      "tma_atom_c",
      "mAB12_mnl",
      "mC_mnl",
      "cluster_layout_vmnk",
      "a_smem_layout_staged",
      "b_smem_layout_staged",
      "ab12_smem_layout_staged",
      "c_smem_layout_staged",
      "epi_tile",
      "epi_tile_c",
      "tile_sched_params",
      "epilogue_op",
      "alpha"
    ],
    "epilog_tmem_copy_and_partition": [
      "self",
      "tidx",
      "tAcc",
      "gAB12_mnl",
      "gC_mnl",
      "epi_tile",
      "epi_tile_c",
      "use_2cta_instrs"
    ],
    "epilog_smem_copy_and_partition": [
      "self",
      "tiled_copy_t2r",
      "tTR_rAB12",
      "tTR_rAB12_1",
      "tTR_rC",
      "tidx",
      "sAB12",
      "sC"
    ],
    "epilog_gmem_copy_and_partition": [
      "self",
      "tidx",
      "atom1",
      "atom2",
      "gAB12_mnl",
      "gC_mnl",
      "epi_tile",
      "epi_tile_c",
      "sAB12",
      "sC"
    ],
    "_compute_stages": [
      "tiled_mma",
      "mma_tiler_mnk",
      "a_dtype",
      "b_dtype",
      "epi_tile",
      "epi_tile_c",
      "ab12_dtype",
      "ab12_layout",
      "c_dtype",
      "c_layout",
      "smem_capacity",
      "occupancy"
    ],
    "_compute_grid": [
      "ab12",
      "cta_tile_shape_mnk",
      "cluster_shape_mn",
      "max_active_clusters"
    ],
    "_compute_num_tmem_alloc_cols": [
      "tiled_mma",
      "mma_tiler",
      "num_acc_stage"
    ]
  },
  "PersistentDenseGemmKernelNoDlpack": {
    "__init__": [
      "self",
      "acc_dtype",
      "use_2cta_instrs",
      "mma_tiler_mn",
      "cluster_shape_mn"
    ],
    "__call__": [
      "self",
      "a_ptr",
      "a_shape",
      "a_order",
      "b_ptr",
      "b_shape",
      "b_order",
      "ab12_ptr",
      "ab12_shape",
      "ab12_order",
      "c_cute",
      "alpha",
      "max_active_clusters",
      "stream",
      "epilogue_op"
    ]
  },
  "GemmSwigluSm100": {
    "__init__": [
      "self",
      "sample_a",
      "sample_b",
      "sample_ab12",
      "sample_c",
      "alpha",
      "acc_dtype",
      "mma_tiler_mn",
      "cluster_shape_mn",
      "sample_sfa",
      "sample_sfb",
      "sample_amax",
      "sample_sfc",
      "sample_norm_const",
      "sf_vec_size",
      "vector_f32",
      "ab12_stages"
    ],
    "check_support": [
      "self"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self",
      "a_tensor",
      "b_tensor",
      "ab12_tensor",
      "c_tensor",
      "sfa_tensor",
      "sfb_tensor",
      "amax_tensor",
      "sfc_tensor",
      "norm_const_tensor",
      "alpha",
      "current_stream",
      "skip_compile"
    ]
  },
  "_logger": [],
  "_cache_of_GemmSwigluSm100Objects": [],
  "gemm_swiglu_wrapper_sm100": [
    "a_tensor",
    "b_tensor",
    "alpha",
    "c_major",
    "ab12_dtype",
    "c_dtype",
    "acc_dtype",
    "mma_tiler_mn",
    "cluster_shape_mn",
    "sfa_tensor",
    "sfb_tensor",
    "norm_const_tensor",
    "sf_vec_size",
    "vector_f32",
    "ab12_stages",
    "stream"
  ],
  "sigmoid_f32": [
    "a",
    "fastmath"
  ],
  "silu_f32": [
    "a",
    "fastmath"
  ],
  "Sm100BlockScaledPersistentDenseGemmKernel": {
    "__init__": [
      "self",
      "sf_vec_size",
      "mma_tiler_mn",
      "cluster_shape_mn",
      "vector_f32",
      "ab12_stages"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "a_tensor",
      "b_tensor",
      "sfa_tensor",
      "sfb_tensor",
      "c_tensor",
      "ab12_tensor",
      "amax_tensor",
      "sfc_tensor",
      "norm_const_tensor",
      "alpha",
      "max_active_clusters",
      "stream",
      "epilogue_op"
    ],
    "kernel": [
      "self",
      "tiled_mma",
      "tiled_mma_sfb",
      "tma_atom_a",
      "mA_mkl",
      "tma_atom_b",
      "mB_nkl",
      "tma_atom_sfa",
      "mSFA_mkl",
      "tma_atom_sfb",
      "mSFB_nkl",
      "tma_atom_c",
      "mC_mnl",
      "tma_atom_ab12",
      "mAB12_mnl",
      "mAmax_tensor",
      "mSFC_mnl",
      "norm_const_tensor",
      "cluster_layout_vmnk",
      "cluster_layout_sfb_vmnk",
      "a_smem_layout_staged",
      "b_smem_layout_staged",
      "sfa_smem_layout_staged",
      "sfb_smem_layout_staged",
      "c_smem_layout_staged",
      "ab12_smem_layout_staged",
      "epi_tile",
      "epi_tile_ab12",
      "tile_sched_params",
      "epilogue_op",
      "alpha"
    ],
    "mainloop_s2t_copy_and_partition": [
      "self",
      "sSF",
      "tSF"
    ],
    "epilog_tmem_copy_and_partition": [
      "self",
      "tidx",
      "tAcc",
      "gC_mnl",
      "epi_tile",
      "use_2cta_instrs"
    ],
    "epilog_smem_copy_and_partition": [
      "self",
      "tiled_copy_t2r",
      "tTR_rC",
      "tidx",
      "sC"
    ],
    "epilog_gmem_copy_and_partition": [
      "self",
      "tidx",
      "atom",
      "gC_mnl",
      "epi_tile",
      "sC"
    ],
    "_compute_stages": [
      "tiled_mma",
      "mma_tiler_mnk",
      "a_dtype",
      "b_dtype",
      "epi_tile",
      "c_dtype",
      "c_layout",
      "sf_dtype",
      "sf_vec_size",
      "ab12_dtype",
      "ab12_layout",
      "epi_tile_ab12",
      "smem_capacity",
      "occupancy",
      "ab12_stages"
    ],
    "_compute_grid": [
      "c",
      "cta_tile_shape_mnk",
      "cluster_shape_mn",
      "max_active_clusters"
    ],
    "get_dtype_rcp_limits": [
      "dtype"
    ],
    "get_amax_smem_size": []
  },
  "Sm100BlockScaledPersistentDenseGemmKernelNoDlpack": {
    "__init__": [
      "self",
      "sf_vec_size",
      "mma_tiler_mn",
      "cluster_shape_mn",
      "vector_f32",
      "ab12_stages"
    ],
    "__call__": [
      "self",
      "a_ptr",
      "a_shape",
      "a_order",
      "b_ptr",
      "b_shape",
      "b_order",
      "sfa_ptr",
      "sfa_shape",
      "sfa_order",
      "sfb_ptr",
      "sfb_shape",
      "sfb_order",
      "c_ptr",
      "c_shape",
      "c_order",
      "ab12_ptr",
      "ab12_shape",
      "ab12_order",
      "amax_ptr",
      "amax_shape",
      "amax_order",
      "sfc_ptr",
      "sfc_shape",
      "sfc_order",
      "norm_const_ptr",
      "norm_const_shape",
      "norm_const_order",
      "alpha",
      "max_active_clusters",
      "stream",
      "epilogue_op"
    ]
  },
  "fmin": [
    "a",
    "b"
  ],
  "GemmAmaxSm100": {
    "__init__": [
      "self",
      "sample_a",
      "sample_b",
      "sample_sfa",
      "sample_sfb",
      "sample_c",
      "sample_amax",
      "acc_dtype",
      "mma_tiler_mn",
      "cluster_shape_mn",
      "sf_vec_size"
    ],
    "check_support": [
      "self"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self",
      "a_tensor",
      "b_tensor",
      "sfa_tensor",
      "sfb_tensor",
      "c_tensor",
      "amax_tensor",
      "current_stream",
      "skip_compile"
    ]
  },
  "_cache_of_GemmAmaxSm100Objects": [],
  "gemm_amax_wrapper_sm100": [
    "a_tensor",
    "b_tensor",
    "sfa_tensor",
    "sfb_tensor",
    "c_major",
    "c_dtype",
    "acc_dtype",
    "mma_tiler_mn",
    "cluster_shape_mn",
    "sf_vec_size",
    "stream"
  ],
  "make_tensor_strided_like": [
    "q_tensor",
    "o_shape",
    "dtype",
    "device"
  ],
  "NSANamespace": {
    "SelectionAttention": [],
    "selection_attention_wrapper": [],
    "SlidingWindowAttention": [],
    "sliding_window_attention_wrapper": [],
    "CompressionAttention": [],
    "compression_attention_wrapper": [],
    "TopKReduction": [],
    "topk_reduction_wrapper": []
  },
  "NSA": [],
  "TopKReduction": {
    "__init__": [
      "self",
      "sample_q",
      "sample_k",
      "sample_lse",
      "sample_topk_scores",
      "sample_topk_indices",
      "sample_cum_seqlen_q",
      "sample_cum_seqlen_k",
      "max_s_q",
      "max_s_k",
      "acc_dtype",
      "k_value",
      "selection_block_size",
      "compress_stride",
      "is_causal",
      "mma_tiler_mn",
      "scale_softmax"
    ],
    "check_support": [
      "self"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self",
      "q_tensor",
      "k_tensor",
      "lse_tensor",
      "topk_scores_tensor",
      "topk_indices_tensor",
      "cumulative_s_q_tensor",
      "cumulative_s_k_tensor",
      "skip_compile",
      "current_stream"
    ]
  },
  "_cache_of_TopKReductionObjects": [],
  "topk_reduction_wrapper": [
    "q_tensor",
    "k_tensor",
    "lse_tensor",
    "cum_seqlen_q_tensor",
    "cum_seqlen_k_tensor",
    "max_s_q",
    "max_s_k",
    "acc_dtype",
    "k_value",
    "selection_block_size",
    "compress_stride",
    "is_causal",
    "mma_tiler_mn",
    "scale_softmax",
    "current_stream"
  ],
  "FineGrainedReductionQK": {
    "__init__": [
      "self",
      "element_dtype",
      "acc_dtype",
      "k_value",
      "selection_block_size",
      "compress_block_sliding_stride",
      "mma_tiler",
      "is_causal"
    ],
    "__call__": [
      "self",
      "problem_size",
      "Q",
      "K",
      "LSE",
      "Topk_scores",
      "Topk_indices",
      "softmax_scale_log2_e",
      "cumulative_s_q",
      "cumulative_s_k",
      "stream"
    ],
    "make_and_init_load_mma_Q_pipeline": [
      "self",
      "load_mma_Q_mbar_ptr"
    ],
    "make_and_init_load_mma_K_pipeline": [
      "self",
      "load_mma_K_mbar_ptr"
    ],
    "make_and_init_load_compute_LSE_pipeline": [
      "self",
      "load_compute_lse_mbar_ptr"
    ],
    "make_and_init_mma_compute_S_pipeline": [
      "self",
      "mma_compute_S_mbar_ptr"
    ],
    "kernel": [
      "self",
      "problem_size",
      "QK_tiled_mma",
      "tma_atom_Q",
      "tma_tensor_Q",
      "tma_atom_K",
      "tma_tensor_K",
      "LSE",
      "cumulative_s_q",
      "cumulative_s_k",
      "Topk_scores",
      "Topk_indices",
      "Q_smem_layout_staged",
      "K_smem_layout_staged",
      "LSE_smem_layout",
      "softmax_scale_log2_e"
    ],
    "topk_step": [
      "self",
      "tiled_scores",
      "scores_heap_rf",
      "idx_heap_rf",
      "query_index",
      "k_tile_idx",
      "heap_size"
    ]
  },
  "SlidingWindowAttention": {
    "__init__": [
      "self",
      "sample_q",
      "sample_k",
      "sample_v",
      "sample_o",
      "sample_stats",
      "left_bound",
      "right_bound",
      "sample_seq_len_q",
      "sample_seq_len_kv",
      "sample_q_ragged_offset",
      "sample_k_ragged_offset",
      "sample_v_ragged_offset",
      "sample_o_ragged_offset",
      "sample_stats_ragged_offset",
      "max_seq_len_q",
      "max_seq_len_kv",
      "attn_scale",
      "intermediate_data_type",
      "compute_data_type",
      "cudnn_handle"
    ],
    "_calculate_ragged_offsets": [
      "self",
      "seq_len_q",
      "seq_len_kv",
      "sample_q",
      "sample_k",
      "sample_v",
      "sample_o",
      "sample_stats"
    ],
    "check_support": [
      "self"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self",
      "q_tensor",
      "k_tensor",
      "v_tensor",
      "o_tensor",
      "stats_tensor",
      "seq_len_q_tensor",
      "seq_len_kv_tensor",
      "q_ragged_offset_tensor",
      "k_ragged_offset_tensor",
      "v_ragged_offset_tensor",
      "o_ragged_offset_tensor",
      "stats_ragged_offset_tensor",
      "current_stream",
      "cudnn_handle",
      "skip_compile"
    ],
    "__call__": [
      "self"
    ]
  },
  "_cache_of_SlidingWindowAttentionObjects": [],
  "sliding_window_attention_wrapper": [
    "q_tensor",
    "k_tensor",
    "v_tensor",
    "seq_len_q_tensor",
    "seq_len_kv_tensor",
    "q_ragged_offset_tensor",
    "k_ragged_offset_tensor",
    "v_ragged_offset_tensor",
    "o_ragged_offset_tensor",
    "stats_ragged_offset_tensor",
    "left_bound",
    "right_bound",
    "is_infer",
    "attn_scale",
    "o_dtype",
    "intermediate_data_type",
    "compute_data_type",
    "cudnn_handle",
    "stream"
  ],
  "SelectionAttention": {
    "__init__": [
      "self",
      "sample_q",
      "sample_k",
      "sample_v",
      "sample_o",
      "sample_l",
      "sample_m",
      "sample_block_indices",
      "sample_block_counts",
      "sample_cum_seqlen_q",
      "sample_cum_seqlen_k",
      "max_s_q",
      "max_s_k",
      "acc_dtype",
      "block_size",
      "scale_softmax"
    ],
    "check_support": [
      "self"
    ],
    "_reshape_tensors": [
      "self",
      "q",
      "k",
      "v",
      "o",
      "l",
      "m"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self",
      "q_tensor",
      "k_tensor",
      "v_tensor",
      "o_tensor",
      "l_tensor",
      "m_tensor",
      "block_indices_tensor",
      "block_counts_tensor",
      "cum_seqlen_q_tensor",
      "cum_seqlen_k_tensor",
      "scale_softmax",
      "current_stream",
      "skip_compile"
    ]
  },
  "_cache_of_SelectionAttentionObjects": [],
  "selection_attention_wrapper": [
    "q_tensor",
    "k_tensor",
    "v_tensor",
    "block_indices_tensor",
    "block_counts_tensor",
    "cum_seqlen_q_tensor",
    "cum_seqlen_k_tensor",
    "block_size",
    "scale_softmax",
    "o_dtype",
    "acc_dtype",
    "max_s_q",
    "max_s_k",
    "stream"
  ],
  "HopperSelectAttentionFwd": {
    "__init__": [
      "self",
      "head_dim",
      "value_dim",
      "GQA_group_size",
      "block_size",
      "dtype",
      "acc_dtype"
    ],
    "__call__": [
      "self",
      "Q",
      "K",
      "V",
      "O",
      "L",
      "M",
      "block_indices",
      "block_counts",
      "max_length",
      "seq_offsets",
      "softmax_scale",
      "stream"
    ],
    "_threadquad_reduce": [
      "self",
      "val",
      "op",
      "mask"
    ],
    "_threadquad_reduce_max": [
      "self",
      "val",
      "mask"
    ],
    "_threadquad_reduce_sum": [
      "self",
      "val",
      "mask"
    ],
    "_make_acc_tensor_mn_view": [
      "self",
      "acc"
    ],
    "_exp2f": [
      "self",
      "x"
    ],
    "kernel": [
      "self",
      "tma_atom_Q",
      "mQ",
      "tma_atom_K",
      "mK",
      "tma_atom_V",
      "mV",
      "tma_atom_O",
      "mO",
      "mL",
      "L_smem_layout",
      "mM",
      "M_smem_layout",
      "seq_offsets",
      "block_indices",
      "block_counts",
      "tiled_mma_QK",
      "tiled_mma_PV",
      "cta_layout_mnk",
      "Q_smem_layout_staged",
      "K_smem_layout_staged",
      "V_smem_layout_staged",
      "Vt_smem_layout_staged",
      "O_smem_layout_staged",
      "softmax_scale"
    ],
    "copy_reg_to_gmem": [
      "self",
      "dest_layout",
      "dest_dtype",
      "gmem_tensor_partition",
      "tma_atom",
      "tiled_mma",
      "acc_tensor",
      "acc_dtype",
      "smem_tensor",
      "tidx",
      "warp_idx"
    ]
  },
  "make_thread_cooperative_group": [
    "size"
  ],
  "BlackwellFusedMultiHeadAttentionForward": {
    "__init__": [
      "self",
      "qk_acc_dtype",
      "pv_acc_dtype",
      "mma_tiler",
      "is_persistent",
      "mask_type"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "q_iter",
      "q_stride",
      "k_iter",
      "k_stride",
      "v_iter",
      "v_stride",
      "o_iter",
      "o_stride",
      "problem_size",
      "cum_seqlen_q",
      "cum_seqlen_k",
      "lse_iter",
      "lse_stride",
      "scale_softmax_log2",
      "scale_softmax",
      "scale_output",
      "window_size_left",
      "window_size_right",
      "stream"
    ],
    "kernel": [
      "self",
      "qk_tiled_mma",
      "pv_tiled_mma",
      "tma_atom_q",
      "mQ_qdl",
      "tma_atom_k",
      "mK_kdl",
      "tma_atom_v",
      "mV_dkl",
      "tma_atom_o",
      "mO_qdl",
      "cum_seqlen_q",
      "cum_seqlen_k",
      "mLSE",
      "scale_softmax_log2",
      "scale_softmax",
      "scale_output",
      "window_size_left",
      "window_size_right",
      "q_smem_layout_staged",
      "k_smem_layout_staged",
      "p_tmem_layout_staged",
      "v_smem_layout_staged",
      "o_smem_layout_staged",
      "tile_sched_params"
    ],
    "softmax_step": [
      "self",
      "stage",
      "need_apply_mask",
      "iter_args",
      "value_args",
      "pipeline_args",
      "atom_args",
      "tensor_args"
    ],
    "softmax": [
      "self",
      "stage",
      "seqlen_k",
      "seqlen_q",
      "cum_seqlen_q",
      "cum_seqlen_k",
      "scale_softmax_log2",
      "qk_thr_mma",
      "tStS",
      "tStSi",
      "window_size_left",
      "window_size_right",
      "mma_si_consumer",
      "si_corr_producer",
      "s0_s1_sequence_consumer",
      "s0_s1_sequence_producer",
      "tile_sched_params"
    ],
    "correction_rescale": [
      "self",
      "thr_mma",
      "tOtO",
      "scale"
    ],
    "correction_epilog": [
      "self",
      "thr_mma",
      "tOtO",
      "mLSE",
      "tTMEM_LOAD_VECrS",
      "row_idx",
      "cuseqlen_q",
      "seqlen_q",
      "blk_coord",
      "scale_softmax",
      "scale",
      "sO"
    ]
  },
  "FmhaStaticTileSchedulerParams": {
    "__init__": [
      "self",
      "is_persistent",
      "problem_shape_mbh"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "FmhaStaticTileScheduler": {
    "__init__": [
      "self",
      "params",
      "current_work_linear_idx",
      "blk_coord",
      "grid_shape"
    ],
    "get_grid_shape": [
      "params"
    ],
    "check_valid_work_for_seqlen_q": [
      "q_tiler",
      "current_idx",
      "seqlen_q"
    ],
    "get_current_work": [
      "self"
    ],
    "initial_work_tile_info": [
      "self"
    ],
    "advance_to_next_work": [
      "self"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "create_fmha_static_tile_scheduler": [
    "params",
    "blk_coord",
    "grid_shape"
  ],
  "create_fmha_static_tile_scheduler_params": [
    "is_persistent",
    "problem_shape_mbh"
  ],
  "compute_grid": [
    "o_shape",
    "cta_tiler",
    "is_persistent"
  ],
  "MaskType": {
    "RESIDUAL_MASK": [],
    "WINDOW_MASK": [],
    "WINDOW_MASK_INFERENCE": [],
    "COMPRESSED_CAUSAL_MASK": []
  },
  "FusedMask": {
    "get_trip_count": [
      "mask_type",
      "blk_coord",
      "tile_shape",
      "seqlen_q",
      "seqlen_k",
      "window_size_left",
      "window_size_right"
    ],
    "get_trip_start": [
      "mask_type",
      "blk_coord",
      "tile_shape",
      "seqlen_q",
      "seqlen_k",
      "window_size_left"
    ],
    "get_leading_mask_id": [
      "mask_type",
      "blk_coord",
      "tile_shape",
      "seqlen_q",
      "seqlen_k",
      "window_size_left",
      "window_size_right"
    ],
    "get_trailing_mask_id": [
      "mask_type",
      "blk_coord",
      "tile_shape",
      "seqlen_q",
      "seqlen_k",
      "window_size_left",
      "window_size_right"
    ],
    "get_masked_leading_count": [
      "mask_type",
      "blk_coord",
      "tile_shape",
      "seqlen_q",
      "seqlen_k",
      "window_size_left",
      "window_size_right"
    ],
    "get_masked_trailing_count": [
      "mask_type",
      "blk_coord",
      "tile_shape",
      "seqlen_q",
      "seqlen_k",
      "window_size_left",
      "window_size_right",
      "rem_count"
    ],
    "get_unmasked_trip_count": [
      "mask_type",
      "blk_coord",
      "tile_shape",
      "seqlen_q",
      "seqlen_k",
      "window_size_left",
      "window_size_right"
    ],
    "apply_mask": [
      "mask_type",
      "acc_qk",
      "index_qk",
      "seqlen_q",
      "seqlen_k",
      "window_size_left",
      "window_size_right"
    ]
  },
  "CompressionAttention": {
    "__init__": [
      "self",
      "sample_q",
      "sample_k",
      "sample_v",
      "sample_o",
      "sample_lse",
      "sample_cum_seqlen_q",
      "sample_cum_seqlen_k",
      "qk_acc_dtype",
      "pv_acc_dtype",
      "mma_tiler_mn",
      "is_persistent",
      "scale_q",
      "scale_k",
      "scale_v",
      "inv_scale_o",
      "scale_softmax"
    ],
    "check_support": [
      "self"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self",
      "q_tensor",
      "k_tensor",
      "v_tensor",
      "o_tensor",
      "lse_tensor",
      "cum_seqlen_q_tensor",
      "cum_seqlen_k_tensor",
      "current_stream",
      "skip_compile",
      "scale_q",
      "scale_k",
      "scale_v",
      "inv_scale_o",
      "scale_softmax"
    ]
  },
  "_cache_of_CompressionAttentionObjects": [],
  "compression_attention_wrapper": [
    "q_tensor",
    "k_tensor",
    "v_tensor",
    "cum_seqlen_q_tensor",
    "cum_seqlen_k_tensor",
    "enable_lse",
    "o_dtype",
    "qk_acc_dtype",
    "pv_acc_dtype",
    "mma_tiler_mn",
    "is_persistent",
    "scale_q",
    "scale_k",
    "scale_v",
    "inv_scale_o",
    "scale_softmax",
    "stream"
  ],
  "warp_redux_sync": [
    "value",
    "kind",
    "mask_and_clamp",
    "abs",
    "nan"
  ],
  "atomic_max_float32": [
    "ptr",
    "value"
  ],
  "atomic_add_float32": [
    "ptr",
    "value"
  ],
  "WorkTileInfo": {
    "__init__": [
      "self",
      "tile_idx",
      "is_valid_tile"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ],
    "is_valid_tile": [
      "self"
    ],
    "tile_idx": [
      "self"
    ]
  },
  "PersistentTileSchedulerParams": {
    "__init__": [
      "self",
      "problem_shape_ntile_mnl",
      "cluster_shape_mnk",
      "raster_along_m",
      "swizzle_size"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ],
    "get_grid_shape": [
      "self",
      "max_active_clusters"
    ]
  },
  "StaticPersistentTileScheduler": {
    "__init__": [
      "self",
      "params",
      "num_persistent_clusters",
      "current_work_linear_idx",
      "cta_id_in_cluster",
      "num_tiles_executed"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ],
    "create": [
      "params",
      "block_idx",
      "grid_dim"
    ],
    "get_grid_shape": [
      "params",
      "max_active_clusters"
    ],
    "_get_current_work_for_linear_idx": [
      "self",
      "current_work_linear_idx"
    ],
    "_get_cluster_work_idx_with_fastdivmod": [
      "self",
      "current_work_linear_idx"
    ],
    "get_current_work": [
      "self"
    ],
    "initial_work_tile_info": [
      "self"
    ],
    "advance_to_next_work": [
      "self"
    ],
    "num_tiles_executed": [
      "self"
    ]
  },
  "StaticPersistentRuntimeTileScheduler": {
    "__init__": [
      "self",
      "params",
      "num_persistent_clusters",
      "current_work_linear_idx",
      "cta_id_in_cluster",
      "num_tiles_executed",
      "inner_mode"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ],
    "create": [
      "params",
      "block_idx",
      "grid_dim",
      "inner_mode"
    ],
    "_get_current_work_for_linear_idx": [
      "self",
      "current_work_linear_idx"
    ]
  },
  "BlockScaledContiguousGroupedGemmKernel": {
    "__init__": [
      "self",
      "sf_vec_size",
      "acc_dtype",
      "use_2cta_instrs",
      "mma_tiler_mn",
      "cluster_shape_mn",
      "vector_f32",
      "generate_sfd",
      "discrete_col_sfd"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "a",
      "b",
      "c",
      "d",
      "d_col",
      "sfa",
      "sfb",
      "sfd_row_tensor",
      "sfd_col_tensor",
      "amax_tensor",
      "norm_const_tensor",
      "tile_idx_to_expert_idx",
      "num_non_exiting_tiles",
      "m_split_cumsum",
      "alpha",
      "prob",
      "max_active_clusters",
      "stream",
      "epilogue_op"
    ],
    "mainloop_s2t_copy_and_partition": [
      "self",
      "sSF",
      "tSF"
    ],
    "amax_reduction_per_thread": [
      "self",
      "vec_fp32",
      "amax_fp32"
    ],
    "amax_reduction_per_warp_and_cta": [
      "self",
      "amax_fp32",
      "warp_idx",
      "amax_smem",
      "amax_gmem"
    ],
    "store_c": [
      "self",
      "tiled_copy_r2s",
      "tma_atom_c",
      "warp_idx",
      "tTR_rAcc",
      "tTR_rAcc_gate",
      "tRS_rC",
      "tRS_sC",
      "bSG_gC",
      "bSG_sC",
      "c_pipeline",
      "prev_subtile_idx",
      "real_subtile_idx"
    ],
    "quant_sfd_row": [
      "self",
      "tile_idx",
      "tiled_copy_r2s",
      "src",
      "pvscale",
      "norm_const",
      "rcp_limit",
      "tRSrD",
      "tile_info"
    ],
    "quant_sfd_col": [
      "self",
      "tile_idx",
      "tiled_copy_r2s",
      "src",
      "pvscale",
      "norm_const",
      "rcp_limit",
      "tRSrD",
      "tile_info"
    ],
    "tile_info_to_mn_idx": [
      "self",
      "tile_info"
    ],
    "create_and_partition_new_SFDCol": [
      "self",
      "tile_info",
      "mSFDCol_mnl"
    ],
    "kernel": [
      "self",
      "tiled_mma",
      "tiled_mma_sfb",
      "tma_atom_a",
      "mA_mkl",
      "tma_atom_b",
      "mB_nkl",
      "tma_atom_sfa",
      "mSFA_mkl",
      "tma_atom_sfb",
      "mSFB_nkl",
      "tma_atom_c",
      "mC_mnl",
      "tma_atom_d",
      "mD_mnl",
      "tma_atom_d_col",
      "mD_col_mnl",
      "mSFDRow_mnl",
      "mSFDCol_mnl",
      "norm_const_tensor",
      "mAmax_tensor",
      "tile_idx_to_expert_idx",
      "num_non_exiting_tiles",
      "m_split_cumsum",
      "alpha",
      "prob",
      "cluster_layout_vmnk",
      "cluster_layout_sfb_vmnk",
      "a_smem_layout_staged",
      "b_smem_layout_staged",
      "sfa_smem_layout_staged",
      "sfb_smem_layout_staged",
      "c_smem_layout_staged",
      "d_smem_layout_staged",
      "epi_tile",
      "tile_sched_params",
      "epilogue_op"
    ],
    "epilog_tmem_copy_and_partition": [
      "self",
      "tidx",
      "tAcc",
      "gD_mnl",
      "epi_tile",
      "use_2cta_instrs"
    ],
    "epilog_smem_copy_and_partition": [
      "self",
      "tiled_copy_t2r",
      "tTR_rC",
      "tidx",
      "sD"
    ],
    "epilog_gmem_copy_and_partition": [
      "self",
      "tidx",
      "atom",
      "gD_mnl",
      "epi_tile",
      "sD"
    ],
    "_compute_stages": [
      "tiled_mma",
      "mma_tiler_mnk",
      "a_dtype",
      "b_dtype",
      "epi_tile",
      "epi_tile_c",
      "c_dtype",
      "c_layout",
      "d_dtype",
      "d_layout",
      "sf_dtype",
      "sf_vec_size",
      "num_smem_capacity",
      "occupancy",
      "generate_sfd"
    ],
    "_compute_grid": [
      "output_shape",
      "cta_tile_shape_mnk",
      "cluster_shape_mn",
      "max_active_clusters"
    ],
    "_get_tma_atom_kind": [
      "atom_sm_cnt",
      "mcast"
    ],
    "get_dtype_rcp_limits": [
      "dtype"
    ],
    "get_amax_smem_size": []
  },
  "BlockScaledContiguousGroupedGemmKernelNoDlpack": {
    "__init__": [
      "self",
      "sf_vec_size",
      "acc_dtype",
      "use_2cta_instrs",
      "mma_tiler_mn",
      "cluster_shape_mn",
      "vector_f32",
      "generate_sfd",
      "discrete_col_sfd"
    ],
    "__call__": [
      "self",
      "a_ptr",
      "a_shape",
      "a_order",
      "b_ptr",
      "b_shape",
      "b_order",
      "c_ptr",
      "c_shape",
      "c_order",
      "d_ptr",
      "d_shape",
      "d_order",
      "d_col_ptr",
      "d_col_shape",
      "d_col_order",
      "sfa_ptr",
      "sfa_shape",
      "sfa_order",
      "sfb_ptr",
      "sfb_shape",
      "sfb_order",
      "sfd_row_ptr",
      "sfd_row_shape",
      "sfd_row_order",
      "sfd_col_ptr",
      "sfd_col_shape",
      "sfd_col_order",
      "amax_ptr",
      "amax_shape",
      "amax_order",
      "norm_const_ptr",
      "norm_const_shape",
      "norm_const_order",
      "tile_idx_to_expert_idx_ptr",
      "tile_idx_to_expert_idx_shape",
      "tile_idx_to_expert_idx_order",
      "num_non_exiting_tiles_ptr",
      "num_non_exiting_tiles_shape",
      "num_non_exiting_tiles_order",
      "m_split_cumsum_ptr",
      "m_split_cumsum_shape",
      "m_split_cumsum_order",
      "alpha_ptr",
      "alpha_shape",
      "alpha_order",
      "prob_ptr",
      "prob_shape",
      "prob_order",
      "max_active_clusters",
      "stream",
      "epilogue_op"
    ]
  },
  "cvt_sf_MKL_to_M32x4xrm_K4xrk_L": [
    "sf_ref_tensor",
    "sf_mma_tensor"
  ],
  "GroupedGemmSwigluSm100": {
    "__init__": [
      "self",
      "sample_a",
      "sample_b",
      "sample_c",
      "sample_d",
      "sample_sfa",
      "sample_sfb",
      "sample_tile_idx_to_expert_idx",
      "sample_num_non_exiting_tiles",
      "sample_alpha",
      "sample_d_col",
      "sample_sfd_row",
      "sample_sfd_col",
      "sample_amax",
      "sample_norm_const",
      "sample_prob",
      "sample_m_split_cumsum",
      "acc_dtype",
      "mma_tiler_mn",
      "cluster_shape_mn",
      "sf_vec_size",
      "vector_f32",
      "m_aligned",
      "discrete_col_sfd"
    ],
    "check_support": [
      "self"
    ],
    "compile": [
      "self",
      "current_stream"
    ],
    "execute": [
      "self",
      "a_tensor",
      "b_tensor",
      "c_tensor",
      "d_tensor",
      "sfa_tensor",
      "sfb_tensor",
      "tile_idx_to_expert_idx",
      "num_non_exiting_tiles",
      "alpha_tensor",
      "d_col_tensor",
      "sfd_row_tensor",
      "sfd_col_tensor",
      "amax_tensor",
      "norm_const_tensor",
      "prob_tensor",
      "m_split_cumsum",
      "current_stream",
      "skip_compile"
    ]
  },
  "_cache_of_GroupedGemmSwigluSm100Objects": [],
  "grouped_gemm_swiglu_wrapper_sm100": [
    "a_tensor",
    "b_tensor",
    "sfa_tensor",
    "sfb_tensor",
    "tile_idx_to_expert_idx",
    "num_non_exiting_tiles",
    "alpha_tensor",
    "norm_const_tensor",
    "prob_tensor",
    "m_split_cumsum",
    "acc_dtype",
    "c_dtype",
    "d_dtype",
    "cd_major",
    "mma_tiler_mn",
    "cluster_shape_mn",
    "sf_vec_size",
    "vector_f32",
    "m_aligned",
    "discrete_col_sfd",
    "current_stream"
  ]
}