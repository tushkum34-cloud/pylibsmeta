{
  "__all__": [],
  "IGNORED_TOKENIZER_CHECKING": [],
  "IGNORED_TOKENIZER_NAMES": [],
  "keynames": [],
  "IS_COLAB_ENVIRONMENT": [],
  "IS_KAGGLE_ENVIRONMENT": [],
  "KAGGLE_TMP": [],
  "try_fix_tokenizer": [
    "tokenizer",
    "prepend"
  ],
  "get_sorted_dict": [
    "dictionary"
  ],
  "convert_to_fast_tokenizer": [
    "slow_tokenizer",
    "temporary_location"
  ],
  "mistral_template": [],
  "llama_template": [],
  "assert_same_tokenization": [
    "slow_tokenizer",
    "fast_tokenizer"
  ],
  "fix_sentencepiece_tokenizer": [
    "old_tokenizer",
    "new_tokenizer",
    "token_mapping",
    "temporary_location"
  ],
  "fix_sentencepiece_gguf": [
    "saved_location"
  ],
  "_load_correct_tokenizer": [
    "tokenizer_name",
    "model_max_length",
    "padding_side",
    "token",
    "trust_remote_code",
    "cache_dir",
    "fix_tokenizer"
  ],
  "load_correct_tokenizer": [
    "tokenizer_name",
    "model_max_length",
    "padding_side",
    "token",
    "trust_remote_code",
    "cache_dir",
    "fix_tokenizer"
  ],
  "_find_end_position": [
    "template",
    "endfor",
    "endif"
  ],
  "_fix_chat_template": [
    "chat_template"
  ],
  "fix_chat_template": [
    "tokenizer"
  ],
  "check_tokenizer": [
    "model",
    "tokenizer",
    "model_name",
    "model_max_length",
    "padding_side",
    "token",
    "_reload"
  ],
  "patch_sft_trainer_tokenizer": [],
  "logger": [],
  "_AUTO_PADDING_FREE_ENV_DISABLED": [],
  "PADDING_FREE_BLOCKLIST": [],
  "_should_pack": [
    "config"
  ],
  "_should_auto_padding_free": [
    "config"
  ],
  "_disable_sample_packing": [
    "config"
  ],
  "_AUTO_PACK_SKIP_MESSAGES": [],
  "_should_skip_auto_packing_error": [
    "exc"
  ],
  "UnslothTrainingArguments": {
    "__init__": [
      "self",
      "embedding_learning_rate"
    ]
  },
  "_create_unsloth_optimizer": [
    "model",
    "optimizer_cls",
    "optimizer_kwargs",
    "embedding_lr"
  ],
  "UnslothTrainer": {
    "create_optimizer": [
      "self"
    ]
  },
  "_resolve_trainer_params": [
    "trainer_class",
    "init_fn"
  ],
  "_backwards_compatible_trainer": [
    "trainer_class",
    "config_class"
  ],
  "_patch_sft_trainer_auto_packing": [
    "trl_module"
  ],
  "_patch_trl_trainer": [],
  "UNSLOTH_ENABLE_LOGGING": [],
  "Version": [
    "version"
  ],
  "HideLoggingMessage": {
    "__slots__": [],
    "__init__": [
      "self",
      "text"
    ],
    "filter": [
      "self",
      "x"
    ]
  },
  "HidePrintMessage": {
    "__init__": [
      "self",
      "original_stream"
    ],
    "add_filter": [
      "self",
      "text"
    ],
    "write": [
      "self",
      "message"
    ],
    "flush": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ]
  },
  "fix_message_factory_issue": [],
  "fix_xformers_performance_issue": [],
  "patch_vllm_for_notebooks": [],
  "fix_vllm_aimv2_issue": [],
  "fix_vllm_guided_decoding_params": [],
  "ignore_logger_messages": [],
  "patch_ipykernel_hf_xet": [],
  "patch_trackio": [],
  "patch_datasets": [],
  "check_fbgemm_gpu_version": [],
  "patch_enable_input_require_grads": [],
  "_is_custom_torch_build": [
    "raw_version_str"
  ],
  "_infer_required_torchvision": [
    "torch_major",
    "torch_minor"
  ],
  "torchvision_compatibility_check": [],
  "fix_openenv_no_vllm": [],
  "fix_executorch": [],
  "fix_diffusers_warnings": [],
  "fix_huggingface_hub": [],
  "fix_triton_compiled_kernel_missing_attrs": [],
  "fix_rocm_triton_key_error": [],
  "check_vllm_torch_sm100_compatibility": [],
  "fix_vllm_pdl_blackwell": [],
  "patch_openspiel_env_async": [],
  "patch_torchcodec_audio_decoder": [],
  "disable_torchcodec_if_broken": [],
  "is_hip": [],
  "get_device_type": [],
  "DEVICE_TYPE_TORCH": [],
  "get_device_count": [],
  "critical_modules": [],
  "already_imported": [],
  "LLAMA_CPP_TARGETS": [],
  "LLAMA_WEIGHTS": [],
  "LLAMA_LAYERNORMS": [],
  "ALLOWED_QUANTS": [],
  "has_curl": [],
  "CURL_FLAG": [],
  "print_quantization_methods": [],
  "check_if_sentencepiece_model": [
    "model",
    "temporary_location"
  ],
  "_free_cached_model": [
    "model"
  ],
  "_merge_lora": [
    "layer",
    "name"
  ],
  "fast_save_pickle": [
    "shard",
    "name"
  ],
  "unsloth_save_model": [
    "model",
    "tokenizer",
    "save_directory",
    "save_method",
    "push_to_hub",
    "token",
    "is_main_process",
    "state_dict",
    "save_function",
    "max_shard_size",
    "safe_serialization",
    "variant",
    "save_peft_format",
    "use_temp_dir",
    "commit_message",
    "private",
    "create_pr",
    "revision",
    "commit_description",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "install_llama_cpp_clone_non_blocking": [],
  "install_llama_cpp_make_non_blocking": [],
  "install_python_non_blocking": [
    "packages"
  ],
  "try_execute": [
    "commands",
    "force_complete"
  ],
  "install_llama_cpp_old": [
    "version"
  ],
  "install_llama_cpp_blocking": [
    "use_cuda"
  ],
  "get_executable": [
    "executables"
  ],
  "save_to_gguf": [
    "model_name",
    "model_type",
    "model_dtype",
    "is_sentencepiece",
    "model_directory",
    "quantization_method",
    "first_conversion",
    "is_vlm",
    "is_gpt_oss"
  ],
  "unsloth_save_pretrained_merged": [
    "self",
    "save_directory",
    "tokenizer",
    "save_method",
    "push_to_hub",
    "token",
    "is_main_process",
    "state_dict",
    "save_function",
    "max_shard_size",
    "safe_serialization",
    "variant",
    "save_peft_format",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "unsloth_push_to_hub_merged": [
    "self",
    "repo_id",
    "tokenizer",
    "save_method",
    "use_temp_dir",
    "commit_message",
    "private",
    "token",
    "max_shard_size",
    "create_pr",
    "safe_serialization",
    "revision",
    "commit_description",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "MODEL_CARD": [],
  "_determine_username": [
    "save_directory",
    "old_username",
    "token"
  ],
  "create_huggingface_repo": [
    "model",
    "save_directory",
    "token",
    "private"
  ],
  "upload_to_huggingface": [
    "model",
    "save_directory",
    "token",
    "method",
    "extra",
    "file_location",
    "old_username",
    "private",
    "create_config"
  ],
  "fix_tokenizer_bos_token": [
    "tokenizer"
  ],
  "create_ollama_modelfile": [
    "tokenizer",
    "base_model_name",
    "model_location"
  ],
  "create_ollama_model": [
    "username",
    "model_name",
    "tag",
    "modelfile_path"
  ],
  "push_to_ollama_hub": [
    "username",
    "model_name",
    "tag"
  ],
  "push_to_ollama": [
    "tokenizer",
    "gguf_location",
    "username",
    "model_name",
    "tag"
  ],
  "unsloth_save_pretrained_gguf": [
    "self",
    "save_directory",
    "tokenizer",
    "quantization_method",
    "first_conversion",
    "push_to_hub",
    "token",
    "private",
    "is_main_process",
    "state_dict",
    "save_function",
    "max_shard_size",
    "safe_serialization",
    "variant",
    "save_peft_format",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "unsloth_push_to_hub_gguf": [
    "self",
    "repo_id",
    "tokenizer",
    "quantization_method",
    "first_conversion",
    "use_temp_dir",
    "commit_message",
    "private",
    "token",
    "max_shard_size",
    "create_pr",
    "safe_serialization",
    "revision",
    "commit_description",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "save_lora_to_custom_dir": [
    "model",
    "tokenizer",
    "save_directory"
  ],
  "unsloth_convert_lora_to_ggml_and_push_to_hub": [
    "self",
    "tokenizer",
    "repo_id",
    "use_temp_dir",
    "commit_message",
    "private",
    "token",
    "create_pr",
    "revision",
    "commit_description",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "unsloth_convert_lora_to_ggml_and_save_locally": [
    "self",
    "save_directory",
    "tokenizer",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "save_to_gguf_generic": [
    "model",
    "save_directory",
    "tokenizer",
    "quantization_method",
    "quantization_type",
    "repo_id",
    "token"
  ],
  "unsloth_generic_save": [
    "model",
    "tokenizer",
    "save_directory",
    "save_method",
    "push_to_hub",
    "token",
    "is_main_process",
    "state_dict",
    "save_function",
    "max_shard_size",
    "safe_serialization",
    "variant",
    "save_peft_format",
    "use_temp_dir",
    "commit_message",
    "private",
    "create_pr",
    "revision",
    "commit_description",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "unsloth_generic_save_pretrained_merged": [
    "self",
    "save_directory",
    "tokenizer",
    "save_method",
    "push_to_hub",
    "token",
    "is_main_process",
    "state_dict",
    "save_function",
    "max_shard_size",
    "safe_serialization",
    "variant",
    "save_peft_format",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "unsloth_generic_push_to_hub_merged": [
    "self",
    "repo_id",
    "tokenizer",
    "save_method",
    "use_temp_dir",
    "commit_message",
    "private",
    "token",
    "max_shard_size",
    "create_pr",
    "safe_serialization",
    "revision",
    "commit_description",
    "tags",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "_unsloth_save_torchao_with_attached_config": [
    "model",
    "save_directory",
    "tokenizer",
    "push_to_hub",
    "token"
  ],
  "_unsloth_save_torchao_with_given_config": [
    "model",
    "save_directory",
    "tokenizer",
    "torchao_config",
    "push_to_hub",
    "token"
  ],
  "unsloth_save_pretrained_torchao": [
    "self",
    "save_directory",
    "tokenizer",
    "torchao_config",
    "push_to_hub",
    "token"
  ],
  "not_implemented_save": [],
  "patch_saving_functions": [
    "model",
    "vision"
  ],
  "v": [],
  "cuda": [],
  "is_ampere": [],
  "USE_ABI": [],
  "x": [],
  "standardize_sharegpt": [],
  "CHAT_TEMPLATES": [],
  "DEFAULT_SYSTEM_MESSAGE": [],
  "_ollama_template": [
    "name"
  ],
  "unsloth_template": [],
  "unsloth_ollama": [],
  "unsloth_eos_token": [],
  "zephyr_template": [],
  "zephyr_ollama": [],
  "zephyr_eos_token": [],
  "chatml_template": [],
  "chatml_ollama": [],
  "chatml_eos_token": [],
  "mistral_ollama": [],
  "mistral_eos_token": [],
  "llama_ollama": [],
  "llama_eos_token": [],
  "vicuna_template": [],
  "vicuna_ollama": [],
  "vicuna_eos_token": [],
  "vicuna_old_template": [],
  "vicuna_old_ollama": [],
  "vicuna_old_eos_token": [],
  "alpaca_template": [],
  "alpaca_ollama": [],
  "alpaca_eos_token": [],
  "gemma_template": [],
  "gemma_ollama": [],
  "gemma_eos_token": [],
  "gemma_chatml_template": [],
  "gemma_chatml_ollama": [],
  "gemma_chatml_eos_token": [],
  "gemma2_template": [],
  "gemma2_ollama": [],
  "gemma2_eos_token": [],
  "gemma2_chatml_template": [],
  "gemma2_chatml_ollama": [],
  "gemma2_chatml_eos_token": [],
  "llama3_template": [],
  "llama3_ollama": [],
  "llama3_template_eos_token": [],
  "phi3_template": [],
  "phi3_ollama": [],
  "phi3_template_eos_token": [],
  "llama31_template": [],
  "llama31_ollama": [],
  "llama31_template_eos_token": [],
  "qwen25_template": [],
  "qwen25_ollama": [],
  "qwen25_template_eos_token": [],
  "qwen25_default_system_message": [],
  "phi4_template": [],
  "_phi4_ollama_template": [],
  "phi4_ollama": [],
  "phi4_template_eos_token": [],
  "gemma3_template": [],
  "gemma3_ollama": [],
  "gemma3_template_eos_token": [],
  "qwen3_template": [],
  "qwen3_ollama": [],
  "qwen3_template_eos_token": [],
  "gemma3n_template": [],
  "gemma3n_ollama": [],
  "gemma3n_template_eos_token": [],
  "gptoss_template": [],
  "gptoss_ollama": [],
  "gptoss_template_template_eos_token": [],
  "qwen3_instruct_template": [],
  "qwen3_thinking_template": [],
  "liquid_lfm2_template": [],
  "liquid_lfm2_template_eos_token": [],
  "starling_template": [],
  "starling_ollama": [],
  "starling_template_eos_token": [],
  "yi_chat_template": [],
  "yi_chat_ollama": [],
  "yi_chat_template_eos_token": [],
  "_change_system_message": [
    "template",
    "type_chat_template",
    "system_message"
  ],
  "get_chat_template": [
    "tokenizer",
    "chat_template",
    "mapping",
    "map_eos_token",
    "system_message"
  ],
  "remove_special_tokens": [
    "tokenizer",
    "prompt"
  ],
  "_parse_combined_prompt": [
    "combined_prompt",
    "dataset"
  ],
  "_create_formatter": [
    "possible_columns",
    "final_optional_prompts",
    "user_column_name"
  ],
  "to_sharegpt": [
    "dataset",
    "merged_prompt",
    "merged_column_name",
    "output_column_name",
    "remove_unused_columns",
    "conversation_extension",
    "random_state"
  ],
  "get_ollama_eos_tokens": [
    "tokenizer",
    "extra_eos_tokens"
  ],
  "construct_chat_template": [
    "tokenizer",
    "chat_template",
    "default_system_message",
    "extra_eos_tokens"
  ],
  "test_construct_chat_template": [],
  "apply_chat_template": [
    "dataset",
    "tokenizer",
    "chat_template",
    "default_system_message",
    "extra_eos_tokens"
  ],
  "create_stopping_criteria": [
    "tokenizer",
    "stop_word"
  ],
  "test_chat_templates": [],
  "test_hf_gguf_equivalence": [
    "tokenizer",
    "gguf_model"
  ],
  "OLLAMA_TEMPLATES": [],
  "mistral_v03_ollama": [],
  "mistral_small_ollama": [],
  "mistral_small_31_ollama": [],
  "mistral_small_32_ollama": [],
  "mixtral_ollama": [],
  "mistral_nemo_ollama": [],
  "codestral_ollama": [],
  "devstral_ollama": [],
  "magistral_ollama": [],
  "llama_31_storm_ollama": [],
  "llama_31_nemotron_ollama": [],
  "llama_32_vision_ollama": [],
  "tinyllama_ollama": [],
  "qwen_25_coder_ollama": [],
  "qwen_25_vl_ollama": [],
  "openthinker_ollama": [],
  "phi_4_ollama": [],
  "phi_4_reasoning_ollama": [],
  "phi_4_mini_ollama": [],
  "phi_4_mini_reasoning_ollama": [],
  "gemma3_270m_ollama": [],
  "granite_32_ollama": [],
  "granite_32_vision_ollama": [],
  "OLLAMA_TEMPLATE_TO_MODEL_MAPPER": [],
  "MODEL_TO_OLLAMA_TEMPLATE_MAPPER": [],
  "torch_matmul": [],
  "weight_dequant_kernel": [
    "x_ptr",
    "s_ptr",
    "y_ptr",
    "M",
    "N",
    "BLOCK_SIZE"
  ],
  "weight_dequant_block": [
    "x",
    "s",
    "block_size",
    "dtype"
  ],
  "weight_dequant": [
    "x",
    "s",
    "dtype"
  ],
  "act_quant_kernel": [
    "x_ptr",
    "y_ptr",
    "s_ptr",
    "BLOCK_SIZE"
  ],
  "act_quant": [
    "x",
    "block_size"
  ],
  "_w8a8_block_fp8_matmul": [
    "A",
    "B",
    "C",
    "As",
    "Bs",
    "M",
    "N",
    "K",
    "group_n",
    "group_k",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "stride_As_m",
    "stride_As_k",
    "stride_Bs_k",
    "stride_Bs_n",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M"
  ],
  "w8a8_block_fp8_matmul_triton": [
    "A",
    "B",
    "As",
    "Bs",
    "block_size",
    "output_dtype"
  ],
  "torchao_block_matmul": [
    "act_q",
    "weight_q",
    "act_scale",
    "weight_scale",
    "block_size",
    "output_dtype"
  ],
  "fp8_block_matmul": [],
  "FP8BlockQuantLinear": {
    "forward": [
      "ctx",
      "X",
      "weight",
      "weight_scale"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "fp8_torch_block_quant_forward": [
    "X",
    "weight",
    "weight_scale"
  ],
  "FbgemmFp8Linear_matmul": {
    "forward": [
      "ctx",
      "x",
      "weight",
      "weight_scale",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "fbgemm_fp8_linear": [
    "X",
    "weight",
    "weight_scale",
    "bias"
  ],
  "FP8_fbgemm_block_linear": {
    "forward": [
      "ctx",
      "X",
      "weight",
      "weight_scale",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "fp8_fbgemm_block_linear": [
    "X",
    "weight",
    "weight_scale",
    "bias"
  ],
  "test_has_fbgemm": [],
  "fp8_block_quant_linear": [],
  "fp8_linear": [
    "X",
    "weight",
    "weight_scale",
    "bias"
  ],
  "module_forward_patch": [
    "forward_function",
    "scale_attr"
  ],
  "LoRA_MLP": {
    "forward": [
      "ctx",
      "X",
      "gateW",
      "gateW_quant",
      "gateA",
      "gateB",
      "gateS",
      "upW",
      "upW_quant",
      "upA",
      "upB",
      "upS",
      "downW",
      "downW_quant",
      "downA",
      "downB",
      "downS",
      "_forward_function",
      "_backward_function",
      "inplace"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "apply_lora_mlp_swiglu": [
    "self",
    "X",
    "inplace"
  ],
  "apply_lora_mlp_geglu_exact": [
    "self",
    "X",
    "inplace"
  ],
  "apply_lora_mlp_geglu_approx": [
    "self",
    "X"
  ],
  "LoRA_QKV": {
    "forward": [
      "ctx",
      "X",
      "QW",
      "QW_quant",
      "QA",
      "QB",
      "QS",
      "KW",
      "KW_quant",
      "KA",
      "KB",
      "KS",
      "VW",
      "VW_quant",
      "VA",
      "VB",
      "VS",
      "inplace"
    ],
    "backward": [
      "ctx",
      "dQ",
      "dK",
      "dV"
    ]
  },
  "apply_lora_qkv": [
    "self",
    "X",
    "inplace"
  ],
  "LoRA_W": {
    "forward": [
      "ctx",
      "X",
      "W",
      "W_quant",
      "A",
      "B",
      "S"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "apply_lora_o": [
    "self",
    "X"
  ],
  "IDENTITY_DROPOUT": [],
  "fast_lora_forward": [
    "self",
    "x"
  ],
  "next_power_of_2": [],
  "torch_Tensor": [],
  "is_cdna": [],
  "calculate_settings": [
    "n"
  ],
  "HAS_CUDA_STREAM": [],
  "get_ptr": [],
  "c_void_p": [],
  "_get_tensor_stream": [
    "tensor"
  ],
  "ctypes_c_int": [],
  "ctypes_c_int32": [],
  "cdequantize_blockwise_fp32": [],
  "cdequantize_blockwise_fp16_nf4": [],
  "cdequantize_blockwise_bf16_nf4": [],
  "torch_device_stream": [],
  "torch_mm": [],
  "torch_mv": [],
  "torch_addmm": [],
  "torch_empty": [],
  "torch_float32": [],
  "torch_float16": [],
  "torch_bfloat16": [],
  "QUANT_STATE": [
    "W"
  ],
  "get_lora_parameters": [
    "proj"
  ],
  "get_lora_parameters_bias": [
    "proj"
  ],
  "_maybe_fake_quantize_activations": [
    "X",
    "proj"
  ],
  "fast_linear_forward": [
    "proj",
    "X",
    "temp_lora",
    "out"
  ],
  "matmul_lora": [
    "X",
    "W",
    "W_quant",
    "A",
    "B",
    "s",
    "out"
  ],
  "NUM_INT32_ELEMENTS": [],
  "SAFE_INT32_BUFFER_MULTIPLIER": [],
  "BLOCK_SIZE": [],
  "INT32_SAFETY_BUFFER": [],
  "_exact_forward_kernel": [
    "e",
    "g",
    "h",
    "n_elements",
    "BLOCK_SIZE",
    "LONG_INDEXING"
  ],
  "geglu_exact_forward_kernel": [
    "gate",
    "up"
  ],
  "_exact_backward_kernel": [
    "DW",
    "e",
    "g",
    "n_elements",
    "BLOCK_SIZE",
    "LONG_INDEXING"
  ],
  "geglu_exact_backward_kernel": [
    "DW",
    "e",
    "g"
  ],
  "_approx_forward_kernel": [
    "e",
    "g",
    "h",
    "n_elements",
    "BLOCK_SIZE",
    "LONG_INDEXING"
  ],
  "geglu_approx_forward_kernel": [
    "gate",
    "up"
  ],
  "_approx_backward_kernel": [
    "DW",
    "e",
    "g",
    "n_elements",
    "BLOCK_SIZE",
    "LONG_INDEXING"
  ],
  "geglu_approx_backward_kernel": [
    "DW",
    "e",
    "g"
  ],
  "_rms_layernorm_forward": [
    "Y",
    "Y_row_stride",
    "X",
    "X_row_stride",
    "W",
    "W_row_stride",
    "r",
    "r_row_stride",
    "n_cols",
    "eps",
    "BLOCK_SIZE"
  ],
  "_rms_layernorm_backward": [],
  "_gemma_rms_layernorm_forward": [
    "Y",
    "Y_row_stride",
    "X",
    "X_row_stride",
    "W",
    "W_row_stride",
    "r",
    "r_row_stride",
    "n_cols",
    "eps",
    "BLOCK_SIZE"
  ],
  "Fast_RMS_Layernorm": {
    "forward": [
      "ctx",
      "X",
      "W",
      "eps",
      "gemma"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "fast_rms_layernorm": [
    "layernorm",
    "X",
    "gemma"
  ],
  "Unsloth_LlamaRMSNorm": {
    "forward": [
      "self",
      "X"
    ]
  },
  "patch_rms_layernorm": [],
  "unpatch_rms_layernorm": [],
  "test_rms_layernorm": [
    "dim",
    "eps",
    "dtype",
    "bsz",
    "random_state",
    "seqlen"
  ],
  "testing_suite_layernorm": [],
  "_fg_kernel": [
    "e",
    "g",
    "h",
    "n_elements",
    "BLOCK_SIZE",
    "LONG_INDEXING"
  ],
  "swiglu_fg_kernel": [
    "e",
    "g"
  ],
  "_DWf_DW_dfg_kernel": [
    "DW",
    "e",
    "g",
    "n_elements",
    "BLOCK_SIZE",
    "LONG_INDEXING"
  ],
  "swiglu_DWf_DW_dfg_kernel": [
    "DW",
    "e",
    "g"
  ],
  "torch_compile_options": [],
  "torch_tanh": [],
  "torch_nn_functional_softmax": [],
  "slow_inference_attention_softcapping": [
    "Q",
    "K",
    "V",
    "causal_mask",
    "self",
    "bsz",
    "q_len"
  ],
  "_rope_embedding_QK": [],
  "_rope_embedding": [],
  "Fast_RoPE_Embedding": {
    "forward": [
      "ctx",
      "Q",
      "cos",
      "sin"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "fast_rope_embedding": [
    "Q",
    "K",
    "cos",
    "sin",
    "rope_embedding_indices"
  ],
  "Fast_RoPE_Embedding_QK": {
    "forward": [
      "ctx",
      "Q",
      "K",
      "cos",
      "sin",
      "rope_indices"
    ],
    "backward": [
      "ctx",
      "dQ",
      "dK"
    ]
  },
  "Slow_RoPE_Embedding": {
    "forward": [
      "ctx",
      "Q",
      "cos",
      "sin",
      "position_ids"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "inplace_rope_embedding": [
    "Q",
    "K",
    "cos",
    "sin",
    "position_ids"
  ],
  "_cross_entropy_forward": [],
  "_chunked_cross_entropy_forward": [],
  "_cross_entropy_backward": [],
  "MAX_FUSED_SIZE": [],
  "Fast_CrossEntropyLoss": {
    "forward": [
      "ctx",
      "logits",
      "labels",
      "logit_softcapping",
      "logit_scaling"
    ],
    "backward": [
      "ctx",
      "dlosses"
    ]
  },
  "fast_cross_entropy_loss": [
    "logits",
    "labels",
    "logit_softcapping",
    "logit_scaling",
    "n_items"
  ],
  "patch_loss_functions": [
    "torch_compile"
  ],
  "layernorm_forward": [
    "Y",
    "Y_row_stride",
    "X",
    "X_row_stride",
    "W",
    "b",
    "r",
    "mu",
    "n_cols",
    "eps",
    "BLOCK_SIZE"
  ],
  "layernorm_backward": [
    "dY",
    "dY_row_stride",
    "X",
    "X_row_stride",
    "W",
    "b",
    "r",
    "mu",
    "n_cols",
    "eps",
    "BLOCK_SIZE"
  ],
  "Fast_Layernorm": {
    "forward": [
      "ctx",
      "X",
      "W",
      "b",
      "eps"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "fast_layernorm": [
    "layernorm",
    "X"
  ],
  "test_layernorm": [
    "dim",
    "eps",
    "dtype",
    "bsz",
    "random_state",
    "seqlen"
  ],
  "_get_cache_key": [
    "num_experts",
    "hidden_dim",
    "intermediate_dim",
    "top_k",
    "dtype",
    "device_capability",
    "seq_len"
  ],
  "_get_cache_file_path": [
    "cache_key"
  ],
  "load_cached_config": [
    "cache_key"
  ],
  "save_cached_config": [
    "cache_key",
    "config_fwd",
    "config_bwd_dx",
    "config_bwd_dw",
    "metadata"
  ],
  "get_or_autotune_moe_kernels": [
    "num_experts",
    "hidden_dim",
    "intermediate_dim",
    "top_k",
    "dtype",
    "force_autotune",
    "seq_len"
  ],
  "_run_moe_autotuning": [
    "num_experts",
    "hidden_dim",
    "intermediate_dim",
    "top_k",
    "dtype",
    "seq_len"
  ],
  "_get_heuristic_configs": [],
  "_get_default_configs": [],
  "clear_cache": [],
  "is_autotuning_completed": [
    "cache_key"
  ],
  "SEED": [],
  "create_merged_results": [
    "df",
    "mode",
    "seqlen",
    "dtype",
    "autotune"
  ],
  "post_process_results": [
    "results",
    "mode",
    "seqlen",
    "dtype",
    "autotune"
  ],
  "save_results": [
    "df",
    "results_dir",
    "mode",
    "seqlen",
    "dtype",
    "autotune"
  ],
  "create_kernel_configs": [
    "args",
    "permute_x",
    "permute_y"
  ],
  "power_of_two_range": [
    "start",
    "end"
  ],
  "multiples_of_range": [
    "start",
    "end",
    "step"
  ],
  "map_key_to_args": [
    "key",
    "mode"
  ],
  "save_autotune_results": [
    "autotune_cache",
    "mode",
    "ref_time",
    "fused_time",
    "results_dir"
  ],
  "get_autotuner": [
    "mode"
  ],
  "postprocess_autotune_results": [
    "autotuner",
    "mode",
    "ref_time",
    "fused_time",
    "results_dir"
  ],
  "LLAMA4_ID": [],
  "QWEN3_MODEL_ID": [],
  "run_benchmark_forward": [
    "ref_model",
    "tt_model",
    "config",
    "seqlen",
    "dtype",
    "autotune",
    "kernel_config_fwd",
    "bs"
  ],
  "run_benchmark_backward": [
    "ref_model",
    "tt_model",
    "config",
    "seqlen",
    "dtype",
    "bs"
  ],
  "setup_model": [
    "config",
    "dtype",
    "permute_x",
    "permute_y",
    "autotune",
    "kernel_config_fwd",
    "kernel_config_bwd_dW",
    "kernel_config_bwd_dX",
    "dX_only",
    "dW_only",
    "overlap_router_shared",
    "device"
  ],
  "run_benchmark": [
    "mode",
    "model_config",
    "seqlen",
    "dtype",
    "permute_x",
    "permute_y",
    "autotune",
    "kernel_config_fwd",
    "kernel_config_bwd_dW",
    "kernel_config_bwd_dX",
    "overlap_router_shared",
    "results_dir"
  ],
  "print_delimiter": [
    "char",
    "length"
  ],
  "delimiter_context": [],
  "make_inputs": [
    "M",
    "N",
    "K",
    "E",
    "topk",
    "dtype",
    "requires_grad"
  ],
  "DataConfig": {},
  "ModelConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "GroupedGEMMTestConfig": {},
  "TOLERANCE": [],
  "assert_equal": [
    "ref",
    "tri"
  ],
  "assert_close": [
    "ref",
    "tri",
    "maxtol",
    "rmstol",
    "description",
    "verbose"
  ],
  "assert_indx_equal": [
    "ref",
    "tri"
  ],
  "get_kernel_test_configs": [
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "num_warps",
    "num_stages"
  ],
  "remove_feature_flags": [
    "kernel_configs",
    "permute_x",
    "permute_y",
    "tma_loads",
    "tma_store"
  ],
  "TOPK": [],
  "NUM_EXPERTS": [],
  "TEST_MODEL_SIZES": [],
  "SMALL_MODEL_CONFIGS": [],
  "LLAMA_MODEL_CONFIG": [],
  "QWEN_MODEL_CONFIG": [],
  "SEQLENS": [],
  "DTYPE": [],
  "DATA_CONFIGS": [],
  "rebind_experts_to_shared_buffer": [
    "moe_block",
    "config"
  ],
  "get_expert_metadata": [
    "model_id"
  ],
  "clone_experts": [
    "moe_block",
    "config",
    "copy"
  ],
  "ForwardResult": {},
  "BackwardResult": {},
  "check_down_proj_grad": [
    "moe_block",
    "grouped_gemm_block",
    "atol",
    "rtol"
  ],
  "check_gate_up_proj_grad": [
    "moe_block",
    "grouped_gemm_block",
    "atol",
    "rtol"
  ],
  "check_gate_grad": [
    "moe_block",
    "grouped_gemm_block",
    "atol",
    "rtol"
  ],
  "check_wgrad": [
    "moe_block",
    "grouped_gemm_block",
    "atol",
    "rtol"
  ],
  "check_tensor_allclose": [
    "X_ref",
    "X_test",
    "atol",
    "rtol",
    "name",
    "verbose"
  ],
  "check_expert_grads": [
    "ref_result",
    "test_result",
    "atol",
    "rtol",
    "verbose"
  ],
  "check_grads": [
    "ref_result",
    "test_result",
    "atol",
    "rtol",
    "verbose"
  ],
  "check_fwd": [
    "ref_result",
    "test_result",
    "atol",
    "rtol",
    "verbose"
  ],
  "check_grouped_gemm_results": [
    "grouped_result",
    "fused_result",
    "permute_y",
    "atol",
    "rtol",
    "verbose"
  ],
  "run_forward": [
    "model",
    "X",
    "is_grouped_gemm"
  ],
  "run_backward": [
    "model",
    "grad_output",
    "output",
    "X"
  ],
  "Qwen3MoeFusedGroupedGEMMBlock": {
    "__init__": [
      "self",
      "config",
      "gate",
      "gate_up_proj",
      "down_proj",
      "permute_x",
      "permute_y",
      "autotune",
      "kernel_config_fwd",
      "kernel_config_bwd_dW",
      "kernel_config_bwd_dX"
    ],
    "from_hf": [
      "cls",
      "moe_block",
      "permute_x",
      "permute_y",
      "autotune",
      "kernel_config_fwd",
      "kernel_config_bwd_dW",
      "kernel_config_bwd_dX"
    ],
    "forward": [
      "self",
      "hidden_states",
      "debug"
    ]
  },
  "TOLERANCES": [],
  "LLAMA4_SCOUT_ID": [],
  "SEQ_LENS": [],
  "DTYPES": [],
  "NUM_AUTOTUNE_CONFIGS": [],
  "annotated_context": [
    "prelude",
    "epilogue",
    "char",
    "num_chars"
  ],
  "get_text_config": [
    "model_id"
  ],
  "prep_triton_kernel_traits": [
    "autotune"
  ],
  "sparse_to_dense": [
    "t"
  ],
  "_check_diff": [
    "t1",
    "t2",
    "atol",
    "rtol",
    "precision",
    "verbose",
    "msg"
  ],
  "run_backwards": [
    "y",
    "grad_output",
    "module"
  ],
  "_check_grads": [
    "m1",
    "m2",
    "atol",
    "rtol",
    "precision",
    "verbose",
    "msg"
  ],
  "model_config": [],
  "test_llama4_ref": [
    "dtype",
    "seqlen",
    "autotune",
    "permute_x",
    "permute_y",
    "overlap_router_shared",
    "model_config",
    "bs",
    "device",
    "precision",
    "verbose"
  ],
  "check_valid_config": [
    "permute_x",
    "permute_y",
    "use_W1",
    "fuse_mul_post",
    "is_backward",
    "verbose"
  ],
  "_test_grouped_gemm_forward": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "fuse_mul_post",
    "flatten",
    "use_tma_load_w",
    "use_tma_load_x",
    "use_tma_store",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "num_warps",
    "num_stages",
    "autotune",
    "num_autotune_configs",
    "allow_tma_store",
    "use_autograd"
  ],
  "test_grouped_gemm_forward_manual": [
    "data_config",
    "model_config",
    "kernel_config",
    "use_W1"
  ],
  "test_grouped_gemm_forward_manual_autograd": [
    "data_config",
    "model_config",
    "kernel_config",
    "use_W1"
  ],
  "test_grouped_gemm_forward_autotune": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "num_autotune_configs"
  ],
  "test_grouped_gemm_forward_autotune_autograd": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "num_autotune_configs"
  ],
  "_test_grouped_gemm_backward_dX": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_tma_load_dy",
    "use_tma_load_w",
    "use_tma_store",
    "use_W1",
    "autotune",
    "num_autotune_configs",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "num_warps",
    "num_stages",
    "flatten",
    "allow_tma_store",
    "use_autograd",
    "fuse_mul_post"
  ],
  "test_grouped_gemm_backward_dX_manual": [
    "data_config",
    "model_config",
    "kernel_config",
    "use_W1"
  ],
  "test_grouped_gemm_backward_dX_manual_autograd": [
    "data_config",
    "model_config",
    "kernel_config",
    "use_W1"
  ],
  "test_grouped_gemm_backward_dX_autotune": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "num_autotune_configs"
  ],
  "test_grouped_gemm_backward_dX_autotune_autograd": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "num_autotune_configs"
  ],
  "_test_grouped_gemm_backward_dW": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "use_tma_load_dy",
    "use_tma_load_x",
    "use_tma_store",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "num_warps",
    "num_stages",
    "flatten",
    "autotune",
    "num_autotune_configs",
    "allow_tma_store",
    "debug",
    "fuse_mul_post",
    "use_autograd"
  ],
  "test_grouped_gemm_backward_dW_manual": [
    "data_config",
    "model_config",
    "kernel_config",
    "use_W1",
    "debug"
  ],
  "test_grouped_gemm_backward_dW_manual_autograd": [
    "data_config",
    "model_config",
    "kernel_config",
    "use_W1",
    "debug"
  ],
  "test_grouped_gemm_backward_dW_autotune": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "num_autotune_configs"
  ],
  "test_grouped_gemm_backward_dW_autotune_autograd": [
    "data_config",
    "model_config",
    "permute_x",
    "permute_y",
    "use_W1",
    "num_autotune_configs"
  ],
  "model_id": [],
  "config": [
    "model_id"
  ],
  "test_qwen3_moe": [
    "config",
    "seqlen",
    "dtype",
    "permute_x",
    "permute_y",
    "autotune"
  ],
  "formatter": [],
  "ch": [],
  "_check_tma_support": [],
  "_SUPPORTS_TMA": [],
  "_HAS_SET_ALLOCATOR": [],
  "supports_tma": [],
  "_is_tracing": [],
  "_per_device_alloc_fns": [],
  "get_per_device_per_stream_alloc_fn": [
    "device"
  ],
  "log_kernel_info": [
    "compiled_kernel",
    "best_config"
  ],
  "grouped_gemm_forward": [
    "X",
    "W",
    "topk",
    "m_sizes",
    "gather_indices",
    "topk_weights",
    "permute_x",
    "permute_y",
    "fuse_mul_post",
    "autotune",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "num_warps",
    "num_stages",
    "use_tma_load_w",
    "use_tma_load_x",
    "use_tma_store",
    "flatten",
    "debug"
  ],
  "grouped_gemm_dX": [
    "dY",
    "W",
    "gather_indices",
    "m_sizes",
    "topk",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "debug",
    "permute_x",
    "permute_y",
    "use_tma_load_w",
    "use_tma_load_dy",
    "use_tma_store",
    "num_warps",
    "num_stages",
    "flatten",
    "fuse_mul_pre",
    "fuse_mul_post",
    "autotune"
  ],
  "grouped_gemm_dW": [
    "X",
    "dY",
    "m_sizes",
    "gather_indices",
    "topk",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "permute_x",
    "permute_y",
    "use_tma_load_dy",
    "use_tma_load_x",
    "use_tma_store",
    "fuse_mul_pre",
    "fuse_mul_post",
    "num_warps",
    "num_stages",
    "flatten",
    "autotune",
    "debug"
  ],
  "GroupedGemm": {
    "forward": [
      "ctx",
      "X",
      "W",
      "m_sizes",
      "topk",
      "gather_indices",
      "permute_x",
      "permute_y",
      "topk_weights",
      "fuse_mul_post",
      "kernel_config_fwd",
      "kernel_config_bwd_dX",
      "kernel_config_bwd_dW",
      "autotune",
      "dX_only",
      "dW_only"
    ],
    "backward": [
      "ctx",
      "dY"
    ]
  },
  "check_valid_config_fwd": [
    "permute_x",
    "permute_y",
    "use_tma_load_x",
    "use_tma_load_w",
    "use_tma_store",
    "fuse_mul_post",
    "is_first_gemm"
  ],
  "check_valid_config_bwd_dW": [
    "permute_x",
    "permute_y",
    "use_tma_load_dY",
    "use_tma_load_x",
    "use_tma_store",
    "fuse_mul_post",
    "is_first_gemm"
  ],
  "check_valid_config_bwd_dX": [
    "permute_x",
    "permute_y",
    "use_tma_load_dY",
    "use_tma_load_w",
    "use_tma_store",
    "fuse_mul_post",
    "is_first_gemm"
  ],
  "grouped_gemm": [
    "X",
    "W",
    "m_sizes",
    "topk",
    "gather_indices",
    "permute_x",
    "permute_y",
    "topk_weights",
    "fuse_mul_post",
    "kernel_config_fwd",
    "kernel_config_bwd_dX",
    "kernel_config_bwd_dW",
    "autotune",
    "is_first_gemm",
    "dX_only",
    "dW_only"
  ],
  "DEFAULT_M_BLOCK_SIZES": [],
  "DEFAULT_N_BLOCK_SIZES": [],
  "DEFAULT_K_BLOCK_SIZES": [],
  "DEFAULT_NUM_CTAS": [],
  "DEFAULT_NUM_WARPS": [],
  "DEFAULT_NUM_STAGES": [],
  "BOOLS": [],
  "val_to_list": [
    "val"
  ],
  "convert_args_to_list": [
    "args"
  ],
  "_triton_supports_tma": [],
  "_TRITON_HAS_TMA": [],
  "get_forward_configs": [
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "TMA_LOAD_X",
    "TMA_LOAD_W",
    "TMA_STORE",
    "num_warps",
    "num_stages",
    "num_ctas"
  ],
  "get_dX_kernel_configs": [
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "TMA_LOAD_dY",
    "TMA_LOAD_W",
    "TMA_STORE",
    "num_warps",
    "num_stages",
    "num_ctas"
  ],
  "get_dW_kernel_configs": [
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "num_warps",
    "num_stages",
    "num_ctas",
    "TMA_LOAD_dY",
    "TMA_LOAD_X",
    "TMA_STORE"
  ],
  "estimate_smem_reqs": [
    "num_stages",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "dtype"
  ],
  "exceeds_smem_capacity": [
    "num_stages",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "dtype",
    "smem_size",
    "slack"
  ],
  "common_prune_criteria": [
    "config",
    "kwargs",
    "dtype"
  ],
  "maybe_disable_tma": [
    "config"
  ],
  "prune_kernel_configs_fwd": [
    "configs",
    "args"
  ],
  "prune_dX_configs": [
    "configs",
    "args"
  ],
  "prune_kernel_configs_backward_dW": [
    "configs",
    "args"
  ],
  "_grouped_gemm_dX_kernel": [
    "dY_ptr",
    "w_ptr",
    "dX_ptr",
    "gather_indices_ptr",
    "m_sizes_ptr",
    "NUM_EXPERTS",
    "NUM_TOKENS",
    "TOPK",
    "N",
    "K",
    "NUM_SMS",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "PERMUTE_X",
    "PERMUTE_Y",
    "USE_TMA_LOAD_W",
    "USE_TMA_LOAD_dY",
    "USE_TMA_STORE",
    "FLATTEN"
  ],
  "_autotuned_grouped_gemm_dX_kernel": [],
  "_grouped_gemm_dW_kernel": [
    "x_ptr",
    "dY_ptr",
    "dW_ptr",
    "m_sizes_ptr",
    "gather_indices_ptr",
    "NUM_TOKENS",
    "TOPK",
    "NUM_EXPERTS",
    "N",
    "K",
    "NUM_SMS",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "BLOCK_SIZE_M",
    "PERMUTE_X",
    "PERMUTE_Y",
    "USE_TMA_LOAD_dY",
    "USE_TMA_LOAD_X",
    "USE_TMA_STORE",
    "FLATTEN",
    "acc_dtype"
  ],
  "_autotuned_grouped_gemm_dW_kernel": [],
  "DeviceProperties": {},
  "get_device_properties": [],
  "KernelConfig": {
    "to_string": [
      "self",
      "include_tuning_params",
      "include_tma"
    ]
  },
  "KernelConfigForward": {},
  "KernelConfigBackward_dW": {},
  "KernelConfigBackward_dX": {},
  "KernelResult": {
    "to_dict": [
      "self"
    ],
    "to_dataframe": [
      "results",
      "sort_by",
      "ascending"
    ],
    "to_csv": [
      "results",
      "sort_by",
      "ascending",
      "filename"
    ],
    "print_table": [
      "results",
      "sort_by",
      "ascending",
      "num_results"
    ]
  },
  "get_kernel_configs": [
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "num_warps",
    "num_stages",
    "use_tma_loads",
    "fuse_permute"
  ],
  "prune_kernel_configs_backward_dX": [
    "configs"
  ],
  "TritonTuningContext": {
    "__init__": [
      "self",
      "kernel_config"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_grouped_gemm_forward_kernel": [
    "x_ptr",
    "w_ptr",
    "y_ptr",
    "m_sizes_ptr",
    "gather_indices_ptr",
    "topk_weights_ptr",
    "NUM_EXPERTS",
    "NUM_TOKENS",
    "TOPK",
    "N",
    "K",
    "NUM_SMS",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "PERMUTE_X",
    "PERMUTE_Y",
    "FUSE_MUL_PRE",
    "FUSE_MUL_POST",
    "USE_FAST_ACCUM",
    "USE_TMA_LOAD_W",
    "USE_TMA_LOAD_X",
    "USE_TMA_STORE",
    "acc_dtype",
    "FLATTEN"
  ],
  "_autotuned_grouped_gemm_forward_kernel": [],
  "permute": [
    "X",
    "gather_indices",
    "topk"
  ],
  "unpermute": [
    "X",
    "gather_indices"
  ],
  "calculate_topk": [
    "gating_output",
    "top_k",
    "use_sigmoid",
    "renormalize",
    "pre_act",
    "post_act"
  ],
  "get_routing_indices": [
    "selected_experts",
    "num_experts",
    "return_scatter_indices"
  ],
  "torch_grouped_gemm": [
    "X",
    "W",
    "m_sizes",
    "transpose"
  ],
  "Llama4MoeResult": {},
  "Llama4GroupedGemmTextMoe": {
    "EXPERT_WEIGHT_NAMES": [],
    "__init__": [
      "self",
      "config",
      "overlap_router_shared",
      "verbose",
      "debug"
    ],
    "copy_weights": [
      "self",
      "other"
    ],
    "check_weights": [
      "self",
      "other"
    ],
    "act_and_mul": [
      "self",
      "x"
    ],
    "run_router": [
      "self",
      "hidden_states"
    ],
    "get_token_counts_and_gather_indices": [
      "self",
      "selected_experts"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4TritonTextMoe": {
    "__init__": [
      "self",
      "config",
      "overlap_router_shared",
      "permute_x",
      "permute_y",
      "autotune",
      "kernel_config_fwd",
      "kernel_config_bwd_dW",
      "kernel_config_bwd_dX",
      "dW_only",
      "dX_only",
      "verbose"
    ],
    "copy_weights": [
      "self",
      "other"
    ],
    "check_weights": [
      "self",
      "other"
    ],
    "act_and_mul": [
      "self",
      "x"
    ],
    "run_router": [
      "self",
      "hidden_states"
    ],
    "get_token_counts_and_gather_indices": [
      "self",
      "selected_experts"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "GroupedGEMMResult": {},
  "Qwen3MoeGroupedGEMMBlock": {
    "__init__": [
      "self",
      "config",
      "gate",
      "gate_up_proj",
      "down_proj"
    ],
    "extract_hf_weights": [
      "moe_block"
    ],
    "from_hf": [
      "cls",
      "moe_block"
    ],
    "check_weights": [
      "self",
      "moe_block"
    ],
    "act_and_mul": [
      "self",
      "x"
    ],
    "run_router": [
      "self",
      "hidden_states"
    ],
    "get_token_counts_and_gather_indices": [
      "self",
      "selected_experts"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "terminate_tree": [
    "proc",
    "timeout"
  ],
  "PipeCapture": {
    "__init__": [
      "self",
      "pipe",
      "keep_lines",
      "echo",
      "name",
      "text",
      "encoding",
      "errors",
      "ready_regex"
    ],
    "_reader": [
      "self"
    ],
    "wait_for_ready": [
      "self",
      "timeout"
    ],
    "has_closed": [
      "self"
    ],
    "wait_until_closed": [
      "self",
      "timeout"
    ],
    "tail": [
      "self",
      "n"
    ]
  },
  "SyntheticDataKit": {
    "__init__": [
      "self",
      "model_name",
      "max_seq_length",
      "gpu_memory_utilization",
      "float8_kv_cache",
      "conservativeness",
      "token",
      "timeout"
    ],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "gpu_memory_utilization",
      "float8_kv_cache",
      "conservativeness",
      "token"
    ],
    "check_vllm_status": [],
    "cleanup": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ],
    "__del__": [
      "self"
    ],
    "chunk_data": [
      "self",
      "filename"
    ],
    "prepare_qa_generation": [
      "self",
      "output_folder",
      "max_generation_tokens",
      "temperature",
      "top_p",
      "overlap",
      "default_num_pairs",
      "cleanup_threshold",
      "cleanup_batch_size",
      "cleanup_temperature"
    ]
  },
  "synthetic_qa_config": [],
  "SUPPORTED_FORMATS": [],
  "RawTextDataLoader": {
    "__init__": [
      "self",
      "tokenizer",
      "chunk_size",
      "stride",
      "return_tokenized"
    ],
    "detect_format": [
      "self",
      "file_path"
    ],
    "load_from_file": [
      "self",
      "file_path",
      "return_tokenized"
    ],
    "load_from_files": [
      "self",
      "file_paths",
      "return_tokenized"
    ],
    "chunk_text": [
      "self",
      "text",
      "return_tokenized"
    ],
    "create_causal_dataset": [
      "self",
      "chunks"
    ],
    "smart_chunk_text": [
      "self",
      "text",
      "chunk_size",
      "stride",
      "return_tokenized"
    ],
    "_read_file_by_format": [
      "self",
      "file_path",
      "file_format"
    ],
    "_extract_text_from_json": [
      "self",
      "data"
    ],
    "_extract_text_from_csv_row": [
      "self",
      "row"
    ]
  },
  "TextPreprocessor": {
    "clean_text": [
      "self",
      "text"
    ],
    "extract_sections": [
      "self",
      "text",
      "patterns"
    ],
    "add_structure_tokens": [
      "self",
      "text"
    ],
    "validate_dataset": [
      "self",
      "dataset"
    ]
  },
  "_XFORMERS_MASK_CACHE_MAXSIZE": [],
  "_window_cache_key": [
    "sliding_window"
  ],
  "_get_cached_block_mask": [
    "lengths",
    "sliding_window"
  ],
  "_TrlPackingWarningFilter": {
    "to_filter": [],
    "filter": [
      "self",
      "record"
    ]
  },
  "_TRL_FILTER_INSTALLED": [],
  "_ensure_trl_warning_filter": [],
  "mark_allow_overlength": [
    "module"
  ],
  "configure_sample_packing": [
    "config"
  ],
  "configure_padding_free": [
    "config"
  ],
  "enable_sample_packing": [
    "model",
    "trainer"
  ],
  "enable_padding_free_metadata": [
    "model",
    "trainer"
  ],
  "get_packed_info_from_kwargs": [
    "kwargs",
    "device"
  ],
  "build_xformers_block_causal_mask": [
    "seq_info"
  ],
  "build_sdpa_packed_attention_mask": [
    "seq_info"
  ],
  "_normalize_packed_lengths": [
    "seq_lengths"
  ],
  "mask_packed_sequence_boundaries": [
    "shift_labels",
    "seq_lengths"
  ],
  "POPULARITY_PROPERTIES": [],
  "THOUSAND": [],
  "MILLION": [],
  "BILLION": [],
  "formatted_int": [
    "value"
  ],
  "get_model_info": [
    "model_id",
    "properties"
  ],
  "list_models": [
    "properties",
    "full",
    "sort",
    "author",
    "search",
    "limit"
  ],
  "HAS_XFORMERS": [],
  "SDPA_HAS_GQA": [],
  "FLASH_VARLEN": [],
  "FLASH_DENSE": [],
  "XFORMERS": [],
  "SDPA": [],
  "XFORMERS_BLOCK_DIAG_CLS": [],
  "AttentionConfig": {},
  "AttentionContext": {},
  "select_attention_backend": [
    "use_varlen"
  ],
  "run_attention": [],
  "_ARE_MODELS_REGISTERED": [],
  "register_models": [],
  "search_models": [
    "org",
    "base_name",
    "version",
    "size",
    "quant_types",
    "search_pattern"
  ],
  "_IS_QWEN_2_5_REGISTERED": [],
  "_IS_QWEN_2_5_VL_REGISTERED": [],
  "_IS_QWEN_QWQ_REGISTERED": [],
  "QwenModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "QwenVLModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "QwenQwQModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "QwenQVQPreviewModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "Qwen_2_5_Meta": [],
  "Qwen_2_5_VLMeta": [],
  "QwenQwQMeta": [],
  "QwenQVQPreviewMeta": [],
  "register_qwen_2_5_models": [
    "include_original_model"
  ],
  "register_qwen_2_5_vl_models": [
    "include_original_model"
  ],
  "register_qwen_qwq_models": [
    "include_original_model"
  ],
  "register_qwen_models": [
    "include_original_model"
  ],
  "_IS_GEMMA_3_BASE_REGISTERED": [],
  "_IS_GEMMA_3_INSTRUCT_REGISTERED": [],
  "GemmaModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "GemmaMeta3Base": [],
  "GemmaMeta3Instruct": [],
  "register_gemma_3_base_models": [
    "include_original_model"
  ],
  "register_gemma_3_instruct_models": [
    "include_original_model"
  ],
  "register_gemma_models": [
    "include_original_model"
  ],
  "_IS_PHI_4_REGISTERED": [],
  "_IS_PHI_4_INSTRUCT_REGISTERED": [],
  "PhiModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "PhiMeta4": [],
  "PhiInstructMeta4": [],
  "register_phi_4_models": [
    "include_original_model"
  ],
  "register_phi_4_instruct_models": [
    "include_original_model"
  ],
  "register_phi_models": [
    "include_original_model"
  ],
  "_IS_DEEPSEEK_V3_REGISTERED": [],
  "_IS_DEEPSEEK_V3_0324_REGISTERED": [],
  "_IS_DEEPSEEK_R1_REGISTERED": [],
  "_IS_DEEPSEEK_R1_ZERO_REGISTERED": [],
  "_IS_DEEPSEEK_R1_DISTILL_LLAMA_REGISTERED": [],
  "_IS_DEEPSEEK_R1_DISTILL_QWEN_REGISTERED": [],
  "DeepseekV3ModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "DeepseekR1ModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "DeepseekV3Meta": [],
  "DeepseekV3_0324Meta": [],
  "DeepseekR1Meta": [],
  "DeepseekR1ZeroMeta": [],
  "DeepseekR1DistillLlamaMeta": [],
  "DeepseekR1DistillQwenMeta": [],
  "register_deepseek_v3_models": [
    "include_original_model"
  ],
  "register_deepseek_v3_0324_models": [
    "include_original_model"
  ],
  "register_deepseek_r1_models": [
    "include_original_model"
  ],
  "register_deepseek_r1_zero_models": [
    "include_original_model"
  ],
  "register_deepseek_r1_distill_llama_models": [
    "include_original_model"
  ],
  "register_deepseek_r1_distill_qwen_models": [
    "include_original_model"
  ],
  "register_deepseek_models": [
    "include_original_model"
  ],
  "_list_deepseek_r1_distill_models": [],
  "_IS_LLAMA_3_1_REGISTERED": [],
  "_IS_LLAMA_3_2_REGISTERED": [],
  "_IS_LLAMA_3_2_VISION_REGISTERED": [],
  "LlamaModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "LlamaVisionModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "LlamaMeta_3_1": [],
  "LlamaMeta_3_2_Base": [],
  "LlamaMeta_3_2_Instruct": [],
  "LlamaMeta_3_2_Vision": [],
  "register_llama_3_1_models": [
    "include_original_model"
  ],
  "register_llama_3_2_models": [
    "include_original_model"
  ],
  "register_llama_3_2_vision_models": [
    "include_original_model"
  ],
  "register_llama_models": [
    "include_original_model"
  ],
  "_IS_MISTRAL_SMALL_REGISTERED": [],
  "_MISTRAL_SMALL_03_25_VERSION": [],
  "_MISTRAL_SMALL_01_25_VERSION": [],
  "_MISTRAL_SMALL_09_24_VERSION": [],
  "MistralSmallModelInfo": {
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag"
    ]
  },
  "MistralSmall_2503_Base_Meta": [],
  "MistralSmall_2503_Instruct_Meta": [],
  "MistralSmall_2501_Base_Meta": [],
  "MistralSmall_2501_Instruct_Meta": [],
  "register_mistral_small_models": [
    "include_original_model"
  ],
  "register_mistral_models": [
    "include_original_model"
  ],
  "QuantType": {
    "BNB": [],
    "UNSLOTH": [],
    "GGUF": [],
    "NONE": [],
    "BF16": []
  },
  "BNB_QUANTIZED_TAG": [],
  "UNSLOTH_DYNAMIC_QUANT_TAG": [],
  "GGUF_TAG": [],
  "BF16_TAG": [],
  "QUANT_TAG_MAP": [],
  "ModelInfo": {
    "__post_init__": [
      "self"
    ],
    "append_instruct_tag": [
      "key",
      "instruct_tag"
    ],
    "append_quant_type": [
      "key",
      "quant_type"
    ],
    "construct_model_name": [
      "cls",
      "base_name",
      "version",
      "size",
      "quant_type",
      "instruct_tag",
      "key"
    ],
    "model_path": [
      "self"
    ]
  },
  "ModelMeta": {},
  "register_model": [
    "model_info_cls",
    "org",
    "base_name",
    "version",
    "size",
    "instruct_tag",
    "quant_type",
    "is_multimodal",
    "name"
  ],
  "_check_model_info": [
    "model_id",
    "properties"
  ],
  "_register_models": [
    "model_meta",
    "include_original_model"
  ],
  "__version__": [],
  "platform_system": [],
  "apply_unsloth_gradient_checkpointing": [
    "use_gradient_checkpointing",
    "max_seq_length",
    "dtype"
  ],
  "prefer_flex_attn_if_supported": [
    "model_class",
    "config"
  ],
  "TORCHAO_MSG": [],
  "ReplaceWarningMessage": {
    "_rules": [],
    "_original_showwarning": [],
    "_installed": [],
    "add_rule": [
      "cls",
      "match_text",
      "replacement",
      "category"
    ],
    "_install": [
      "cls"
    ]
  },
  "_RaiseUninitialized": {
    "__init__": [
      "self"
    ],
    "emit": [
      "self",
      "record"
    ]
  },
  "RaiseUninitialized": {
    "__init__": [
      "self"
    ],
    "remove": [
      "self"
    ]
  },
  "extract_quant_model_param_count": [
    "model"
  ],
  "get_model_param_count": [
    "model",
    "trainable_only"
  ],
  "patch_mistral_nemo_config": [
    "config"
  ],
  "model_architectures": [],
  "torch_version": [],
  "SUPPORTS_BFLOAT16": [],
  "HAS_FLASH_ATTENTION": [],
  "HAS_FLASH_ATTENTION_SOFTCAPPING": [],
  "UNSLOTH_COMPILE_DEBUG": [],
  "UNSLOTH_COMPILE_MAXIMUM": [],
  "UNSLOTH_COMPILE_IGNORE_ERRORS": [],
  "is_big_gpu": [
    "index"
  ],
  "torch_compile_kwargs": [],
  "patch_regional_compilation": [],
  "prepare_model_for_kbit_training": [
    "model",
    "use_gradient_checkpointing",
    "use_reentrant"
  ],
  "USE_MODELSCOPE": [],
  "has_internet": [
    "host",
    "port",
    "timeout"
  ],
  "_get_statistics": [
    "statistics",
    "force_download"
  ],
  "get_statistics": [
    "local_files_only"
  ],
  "BitsAndBytesConfig__init__": [],
  "length_spaces": [],
  "move_to_device": [
    "target_device"
  ],
  "offload_to_disk": [
    "W",
    "model",
    "name",
    "temporary_location"
  ],
  "offload_input_embeddings": [
    "model",
    "temporary_location"
  ],
  "offload_output_embeddings": [
    "model",
    "temporary_location"
  ],
  "is_bfloat16_supported": [],
  "is_vLLM_available": [],
  "patch_linear_scaling": [
    "model_name",
    "rope_module",
    "scaled_rope_module",
    "attention_module"
  ],
  "patch_llama_rope_scaling": [
    "model_name",
    "rope_module",
    "scaled_rope_module",
    "extended_rope_module",
    "attention_module",
    "longrope_module"
  ],
  "create_boolean_mask": [
    "n",
    "sliding_window"
  ],
  "test_mask_creation": [],
  "_unsloth_pre_compute_loss": [
    "self",
    "model",
    "inputs"
  ],
  "patch_gradient_accumulation_fix": [
    "Trainer"
  ],
  "patch_tokenizer": [
    "model",
    "tokenizer"
  ],
  "patch_fast_lora": [],
  "unsloth_compile_transformers": [
    "dtype",
    "model_name",
    "model_types",
    "token",
    "revision",
    "trust_remote_code",
    "sdpa_dynamic_mask",
    "sdpa_bool_masks",
    "sdpa_gqa_replace",
    "sdpa_dynamic_compile",
    "compile_attention",
    "disable_causal_masks",
    "compile_torch_modules",
    "compile_custom_modules",
    "compile_function_calls",
    "fuse_lm_head",
    "gradient_checkpointing",
    "manual_replacements",
    "fast_lora_forwards",
    "fast_residual_stream",
    "accurate_accumulation",
    "epilogue_fusion",
    "max_autotune",
    "shape_padding",
    "cudagraphs",
    "debug",
    "fullgraph",
    "import_from_cache",
    "disable",
    "return_logits",
    "unsloth_force_compile"
  ],
  "LOGITS_ERROR_STRING": [],
  "raise_logits_error": [],
  "return_none": [],
  "EmptyLogits": {
    "__init__": [
      "self"
    ],
    "raise_getattr_error": [
      "self",
      "attr"
    ],
    "__getitem__": [],
    "__getattr__": [],
    "__repr__": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "EMPTY_LOGITS": [],
  "functions": [],
  "validate_loftq_config": [
    "loftq_config",
    "lora_dropout",
    "bias",
    "init_lora_weights",
    "model"
  ],
  "fast_inference_setup": [
    "model_name",
    "model_config"
  ],
  "patch_peft_fast_inference": [
    "model"
  ],
  "error_out_no_vllm": [],
  "TorchAOConfig": {},
  "_untie_input_output_embeddings": [
    "model"
  ],
  "_filter_fn_to_fqns": [
    "model",
    "filter_fn"
  ],
  "_convert_torchao_model": [
    "model"
  ],
  "_prepare_model_for_qat": [
    "model",
    "qat_scheme"
  ],
  "patch_hf_quantizer": [],
  "verify_fp8_support_if_applicable": [
    "model_config"
  ],
  "_get_inference_mode_context_manager": [
    "model"
  ],
  "hf_login": [
    "token"
  ],
  "is_moe_model": [
    "model"
  ],
  "get_moe_target_parameters": [
    "model",
    "target_modules"
  ],
  "make_fast_generate_wrapper": [
    "original_generate"
  ],
  "FastQwen2Model": {
    "pre_patch": [],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "model_patcher",
      "tokenizer_name",
      "trust_remote_code"
    ]
  },
  "MistralAttention_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "MistralForCausalLM_fast_forward": [
    "self",
    "input_ids",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "num_logits_to_keep",
    "logits_to_keep"
  ],
  "patch_mistral_nemo_attention": [
    "function"
  ],
  "FastMistralModel": {
    "pre_patch": [],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "model_patcher",
      "tokenizer_name",
      "trust_remote_code"
    ]
  },
  "fast_layernorm_inference": [
    "self",
    "X",
    "out_weight"
  ],
  "CohereAttention_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "CohereDecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "KV_CACHE_INCREMENT": [],
  "CohereAttention_fast_forward_inference": [
    "self",
    "hidden_states",
    "past_key_value",
    "position_ids",
    "do_prefill",
    "attention_mask"
  ],
  "CohereModel_fast_forward_inference": [
    "self",
    "input_ids",
    "past_key_values",
    "position_ids",
    "attention_mask"
  ],
  "FastCohereModel": {
    "pre_patch": []
  },
  "torch_nn_functional_gelu": [],
  "fast_geglu_inference": [
    "self",
    "X"
  ],
  "GemmaDecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask"
  ],
  "GemmaModel_fast_forward_inference": [
    "self",
    "input_ids",
    "past_key_values",
    "position_ids",
    "attention_mask"
  ],
  "GemmaFixedRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "device",
      "config"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "seq_len"
    ],
    "get_cached": [
      "self",
      "seq_len",
      "device_index"
    ],
    "extend_rope_embedding": [
      "self",
      "x",
      "seq_len"
    ]
  },
  "GemmaFixedLinearScalingRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "device",
      "scaling_factor",
      "config"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "device",
      "dtype"
    ]
  },
  "FastGemmaModel": {
    "pre_patch": [],
    "post_patch": [
      "model",
      "tokenizer",
      "correct_dtype"
    ]
  },
  "HAS_GROUPED_GEMM": [],
  "torch_nn_functional_silu": [],
  "Glm4MoeLiteMoE_fast_forward": [
    "self",
    "hidden_states"
  ],
  "Glm4MoeLiteNaiveMoe_fast_forward": [
    "self",
    "hidden_states",
    "top_k_index",
    "top_k_weights"
  ],
  "Glm4MoeLiteDecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "use_cache",
    "cache_position",
    "position_embeddings"
  ],
  "Glm4MoeLiteMLP_fast_forward": [
    "self",
    "x"
  ],
  "FastGLM47Model": {
    "pre_patch": [],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "model_patcher",
      "tokenizer_name",
      "trust_remote_code"
    ]
  },
  "GraniteAttention_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "GraniteDecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "GraniteAttention_fast_forward_inference": [
    "self",
    "hidden_states",
    "past_key_value",
    "position_ids",
    "do_prefill",
    "attention_mask",
    "use_sliding_window",
    "position_embeddings"
  ],
  "GraniteModel_fast_forward_inference": [
    "self",
    "input_ids",
    "past_key_values",
    "position_ids",
    "attention_mask"
  ],
  "GraniteRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "patched_init": [
    "original_init"
  ],
  "FastGraniteModel": {
    "pre_patch": [],
    "post_patch": [
      "model",
      "tokenizer",
      "correct_dtype"
    ]
  },
  "__INT_TO_FLOAT_MAPPER": [],
  "INT_TO_FLOAT_MAPPER": [],
  "FLOAT_TO_INT_MAPPER": [],
  "MAP_TO_UNSLOTH_16bit": [],
  "FLOAT_TO_FP8_BLOCK_MAPPER": [],
  "FLOAT_TO_FP8_ROW_MAPPER": [],
  "RL_EXTRA_ARGS": [],
  "RL_FUNCTIONS": [],
  "RL_PRE_ITEMS": [],
  "RL_CONFIG_CHANGES": [],
  "RL_METRICS_CHANGES": [],
  "RL_ADDITIONAL_FUNCTIONS": [],
  "sft_trainer_fix_untrained_tokens": [
    "call_args",
    "extra_args"
  ],
  "grpo_config_fix_vllm_top_k": [
    "old_RLTrainer_source",
    "old_RLConfig_source"
  ],
  "dpo_trainer_fix_columns": [
    "call_args",
    "extra_args"
  ],
  "sft_trainer_prepare_dataset": [
    "function_name",
    "function"
  ],
  "sft_trainer_compute_loss": [
    "function_name",
    "function"
  ],
  "sft_trainer_push_to_hub_token": [
    "function_name",
    "function"
  ],
  "grpo_trainer__prepare_inputs": [
    "function_name",
    "function"
  ],
  "grpo_trainer__generate_single_turn": [
    "function_name",
    "function"
  ],
  "grpo_trainer__generate_and_score_completions": [
    "function_name",
    "function"
  ],
  "grpo_trainer_fix_maybe_apply_chat_template": [
    "function_name",
    "function"
  ],
  "grpo_trainer__move_model_to_vllm": [
    "function_name",
    "function"
  ],
  "grpo_trainer__get_per_token_logps": [
    "function_name",
    "function"
  ],
  "grpo_trainer__get_per_token_logps_and_entropies": [
    "function_name",
    "function"
  ],
  "grpo_compute_loss": [],
  "grpo_compute_loss_slow": [],
  "UnslothEfficientGRPO": [],
  "grpo_accumulated_loss": [],
  "grpo_update_SamplingParams": [],
  "grpo_trainer_compute_loss": [
    "function_name",
    "function"
  ],
  "kto_trainer_get_batch_logps": [
    "function_name",
    "function"
  ],
  "grpo_trainer_fix_batch_size": [
    "RLTrainer_source",
    "RLConfig_source"
  ],
  "grpo_trainer_metrics": [
    "RLTrainer_source",
    "RLConfig_source"
  ],
  "openenv_vllm_reload_weights": [],
  "vLLMSamplingParams": [],
  "PatchRL": [
    "FastLanguageModel"
  ],
  "grpo_selective_log_softmax": [],
  "selective_log_softmax": [],
  "calculate_pad_tokens_in_prompt": [],
  "create_completion_attention_mask": [],
  "left_pack_padding": [],
  "align_logprobs_with_mask": [],
  "autotune_batch_and_chunks": [],
  "RLTrainer_replacement": [],
  "_wrap_grpo_generate_and_score": [
    "trainer_cls"
  ],
  "_patch_trl_rl_trainers": [
    "trainer_file"
  ],
  "patch_functions": [
    "RLTrainer",
    "trainer_file",
    "RLTrainer_name",
    "all_imports",
    "imports"
  ],
  "patch_trl_rl_trainers": [],
  "patch_trl_openenv": [],
  "PatchFastRL": [
    "algorithm",
    "FastLanguageModel"
  ],
  "Gemma2Attention_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask"
  ],
  "Gemma2DecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask"
  ],
  "Gemma2Attention_fast_forward_inference": [
    "self",
    "hidden_states",
    "past_key_value",
    "position_ids",
    "do_prefill",
    "attention_mask",
    "use_sliding_window"
  ],
  "Gemma2Model_fast_forward_inference": [
    "self",
    "input_ids",
    "past_key_values",
    "position_ids",
    "attention_mask"
  ],
  "FastGemma2Model": {
    "pre_patch": [],
    "post_patch": [
      "model",
      "tokenizer",
      "correct_dtype"
    ]
  },
  "Qwen3MoeSparseMoeBlock_fast_forward": [
    "self",
    "X",
    "temp_gate",
    "temp_up"
  ],
  "Qwen3MoeDecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "output_router_logits",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "FastQwen3MoeModel": {
    "pre_patch": [],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "model_patcher",
      "tokenizer_name",
      "trust_remote_code"
    ]
  },
  "transformers_version": [],
  "SUPPORTS_FOURBIT": [],
  "SUPPORTS_GEMMA": [],
  "SUPPORTS_GEMMA2": [],
  "SUPPORTS_LLAMA31": [],
  "SUPPORTS_LLAMA32": [],
  "SUPPORTS_GRANITE": [],
  "SUPPORTS_QWEN3": [],
  "SUPPORTS_QWEN3_MOE": [],
  "SUPPORTS_FALCON_H1": [],
  "SUPPORTS_GEMMA3N": [],
  "SUPPORTS_GPTOSS": [],
  "FORCE_FLOAT32": [],
  "DISABLE_COMPILE_MODEL_NAMES": [],
  "DISABLE_SDPA_MODEL_NAMES": [],
  "FastLanguageModel": {
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "load_in_8bit",
      "load_in_16bit",
      "full_finetuning",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "trust_remote_code",
      "use_gradient_checkpointing",
      "resize_model_vocab",
      "revision",
      "use_exact_model_name",
      "offload_embedding",
      "float32_mixed_precision",
      "fast_inference",
      "gpu_memory_utilization",
      "float8_kv_cache",
      "random_state",
      "max_lora_rank",
      "disable_log_stats",
      "qat_scheme",
      "load_in_fp8",
      "unsloth_tiled_mlp"
    ]
  },
  "FastModel": {
    "_prepare_for_qat": [
      "model",
      "qat_scheme"
    ],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "load_in_8bit",
      "load_in_16bit",
      "full_finetuning",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "trust_remote_code",
      "use_gradient_checkpointing",
      "resize_model_vocab",
      "revision",
      "return_logits",
      "fullgraph",
      "use_exact_model_name",
      "auto_model",
      "whisper_language",
      "whisper_task",
      "unsloth_force_compile",
      "offload_embedding",
      "float32_mixed_precision",
      "fast_inference",
      "gpu_memory_utilization",
      "float8_kv_cache",
      "random_state",
      "max_lora_rank",
      "disable_log_stats",
      "qat_scheme",
      "load_in_fp8",
      "unsloth_tiled_mlp",
      "target_parameters"
    ]
  },
  "FastVisionModel": {},
  "FastTextModel": {},
  "FalconH1Attention_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "FalconH1Attention_fast_forward_inference": [
    "self",
    "hidden_states",
    "past_key_value",
    "position_ids",
    "do_prefill",
    "attention_mask"
  ],
  "FalconH1DecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "mamba_attention_mask",
    "position_ids",
    "cache_position",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "_FalconH1_fast_forward_inference": [
    "attention_fast_forward_inference",
    "mlp_fast_forward_inference"
  ],
  "_fast_prepare_inputs_for_generation": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "inputs_embeds",
    "cache_position",
    "position_ids",
    "use_cache"
  ],
  "fix_prepare_inputs_for_generation": [
    "module"
  ],
  "FastFalconH1Model": {
    "pre_patch": [],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "model_patcher",
      "tokenizer_name",
      "trust_remote_code"
    ]
  },
  "PatchDPOTrainer": [],
  "PatchKTOTrainer": [],
  "_save_pretrained_torchao": [
    "self",
    "save_directory",
    "tokenizer",
    "torchao_config",
    "push_to_hub",
    "token"
  ],
  "_save_pretrained_gguf": [
    "self",
    "save_directory",
    "tokenizer",
    "quantization_method",
    "first_conversion",
    "push_to_hub",
    "token",
    "max_shard_size",
    "temporary_location",
    "maximum_memory_usage"
  ],
  "_push_to_hub_gguf": [
    "self",
    "repo_id",
    "tokenizer",
    "quantization_method",
    "first_conversion",
    "token",
    "private",
    "commit_message",
    "commit_description",
    "max_shard_size",
    "temporary_location",
    "maximum_memory_usage",
    "create_pr",
    "revision",
    "tags"
  ],
  "FastSentenceTransformer": {
    "_read_pooling_mode": [
      "model_name",
      "token"
    ],
    "_patch_mpnet_v4": [],
    "_patch_mpnet_v5": [],
    "_patch_distilbert_v4": [],
    "_has_add_pooling_layer": [
      "config",
      "auto_model_class"
    ],
    "_patch_distilbert_v5": [],
    "_add_unsloth_tags": [
      "repo_id",
      "token",
      "tags"
    ],
    "_add_unsloth_branding": [
      "save_directory"
    ],
    "_module_path": [
      "model_name",
      "token"
    ],
    "_create_transformer_module": [
      "model_name",
      "model",
      "tokenizer",
      "max_seq_length",
      "trust_remote_code"
    ],
    "_load_modules": [
      "model_name",
      "token",
      "model",
      "tokenizer",
      "max_seq_length",
      "pooling_mode",
      "trust_remote_code"
    ],
    "ENCODER_MODEL_TYPES": [],
    "_estimate_compile_threshold": [
      "model",
      "batch_size",
      "grad_accum",
      "max_seq_length"
    ],
    "_apply_torch_compile": [
      "model",
      "mode"
    ],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "load_in_8bit",
      "load_in_16bit",
      "full_finetuning",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "trust_remote_code",
      "use_gradient_checkpointing",
      "resize_model_vocab",
      "revision",
      "use_exact_model_name",
      "offload_embedding",
      "random_state",
      "max_lora_rank",
      "disable_log_stats",
      "qat_scheme",
      "unsloth_tiled_mlp",
      "pooling_mode",
      "for_inference"
    ],
    "get_peft_model": [
      "model",
      "r",
      "target_modules",
      "lora_alpha",
      "lora_dropout",
      "bias",
      "layers_to_transform",
      "layers_pattern",
      "use_gradient_checkpointing",
      "random_state",
      "max_seq_length",
      "use_rslora",
      "modules_to_save",
      "init_lora_weights",
      "loftq_config"
    ]
  },
  "_patch_sentence_transformer_trainer": [],
  "LOCAL_RANK_KEYS": [],
  "WORLD_SIZE_KEYS": [],
  "BAD_MAPPINGS": [],
  "_get_env_int": [
    "keys"
  ],
  "_infer_distributed_ranks": [],
  "is_distributed": [],
  "prepare_device_map": [],
  "__get_model_name": [
    "model_name",
    "load_in_4bit",
    "INT_TO_FLOAT_MAPPER",
    "FLOAT_TO_INT_MAPPER",
    "MAP_TO_UNSLOTH_16bit",
    "load_in_fp8",
    "FLOAT_TO_FP8_BLOCK_MAPPER",
    "FLOAT_TO_FP8_ROW_MAPPER"
  ],
  "_get_new_mapper": [],
  "get_model_name": [
    "model_name",
    "load_in_4bit",
    "load_in_fp8"
  ],
  "_offline_quantize_to_fp8": [
    "model_name",
    "fp8_mode"
  ],
  "_tag_model_with_fp8_torchao_config": [
    "model",
    "fp8_mode"
  ],
  "_get_fp8_mode_and_check_settings": [
    "load_in_fp8",
    "fast_inference",
    "full_finetuning",
    "load_in_4bit",
    "load_in_8bit",
    "load_in_16bit"
  ],
  "IS_ATTENTION_REFACTOR": [],
  "BlockDiagonalCausalMask": [],
  "original_apply_qkv": [
    "self",
    "X"
  ],
  "original_apply_o": [
    "self",
    "X"
  ],
  "_offload_frozen_module_for_training": [
    "module",
    "device_type",
    "offload_device"
  ],
  "LlamaAttention_fast_forward_inference": [
    "self",
    "hidden_states",
    "past_key_value",
    "position_ids",
    "do_prefill",
    "attention_mask"
  ],
  "fast_swiglu_inference": [
    "self",
    "X",
    "temp_gate",
    "temp_up",
    "gate_multiplier",
    "down_multiplier"
  ],
  "torch_square": [],
  "torch_mean": [],
  "fast_rms_layernorm_inference": [
    "self",
    "X",
    "XX",
    "XX2",
    "variance"
  ],
  "fast_rms_layernorm_inference_gemma": [
    "self",
    "X",
    "out_weight"
  ],
  "fast_layernorm_compiled": [
    "layernorm",
    "X"
  ],
  "LlamaAttention_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "LlamaDecoderLayer_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "__DTYPE_MAP": [],
  "LlamaModel_fast_forward": [
    "self",
    "input_ids",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "_LlamaModel_fast_forward_inference": [
    "attention_fast_forward_inference",
    "mlp_fast_forward_inference"
  ],
  "LlamaModel_fast_forward_inference": [],
  "CausalLM_fast_forward": [
    "fast_forward_inference"
  ],
  "PeftModel_fast_forward": [
    "self",
    "input_ids",
    "causal_mask",
    "attention_mask",
    "inputs_embeds",
    "labels",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "task_ids",
    "num_logits_to_keep",
    "logits_to_keep"
  ],
  "_get_rope_theta": [
    "config",
    "default"
  ],
  "LlamaRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "device",
      "config"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "seq_len"
    ],
    "get_cached": [
      "self",
      "seq_len",
      "device_index"
    ],
    "extend_rope_embedding": [
      "self",
      "x",
      "seq_len"
    ]
  },
  "LlamaLinearScalingRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "device",
      "scaling_factor",
      "config"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "device",
      "dtype"
    ]
  },
  "LlamaExtendedRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "device",
      "config"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "seq_len"
    ],
    "get_cached": [
      "self",
      "seq_len",
      "device_index"
    ],
    "extend_rope_embedding": [
      "self",
      "x",
      "seq_len"
    ],
    "apply_scaling": [
      "self",
      "freqs"
    ]
  },
  "LongRopeRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "original_max_position_embeddings",
      "base",
      "short_factor",
      "long_factor",
      "device",
      "config"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x",
      "position_ids",
      "seq_len"
    ],
    "get_cached": [
      "self",
      "seq_len",
      "device_index"
    ],
    "extend_rope_embedding": [
      "self",
      "x",
      "seq_len"
    ]
  },
  "unsloth_fast_generate": [
    "self"
  ],
  "FastLlamaModel": {
    "_prepare_for_qat": [
      "model",
      "qat_scheme"
    ],
    "pre_patch": [],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "model_patcher",
      "tokenizer_name",
      "trust_remote_code",
      "revision",
      "fast_inference",
      "gpu_memory_utilization",
      "float8_kv_cache",
      "random_state",
      "max_lora_rank",
      "disable_log_stats",
      "unsloth_vllm_standby",
      "num_labels",
      "qat_scheme",
      "load_in_fp8"
    ],
    "post_patch": [
      "model",
      "tokenizer",
      "correct_dtype"
    ],
    "get_peft_model": [
      "model",
      "r",
      "target_modules",
      "lora_alpha",
      "lora_dropout",
      "bias",
      "layers_to_transform",
      "layers_pattern",
      "use_gradient_checkpointing",
      "random_state",
      "max_seq_length",
      "use_rslora",
      "modules_to_save",
      "init_lora_weights",
      "loftq_config",
      "temporary_location",
      "qat_scheme",
      "target_parameters",
      "ensure_weight_tying"
    ],
    "patch_peft_model": [
      "model",
      "use_gradient_checkpointing"
    ],
    "for_inference": [
      "model"
    ],
    "for_training": [
      "model",
      "use_gradient_checkpointing"
    ]
  },
  "NUM_LOGITS_TO_KEEP": [],
  "VLLM_SUPPORTED_VLM": [],
  "VLLM_NON_LORA_VLM": [],
  "PRE_COMPILE_INFERENCE": [],
  "HAS_TORCH_DTYPE": [],
  "_compile_config": [],
  "unsloth_base_fast_generate": [
    "self"
  ],
  "_construct_vlm_processor_fallback": [
    "tokenizer_name",
    "model_type",
    "token",
    "trust_remote_code"
  ],
  "FastBaseModel": {
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "load_in_8bit",
      "load_in_16bit",
      "full_finetuning",
      "token",
      "device_map",
      "trust_remote_code",
      "model_types",
      "tokenizer_name",
      "auto_model",
      "use_gradient_checkpointing",
      "supports_sdpa",
      "whisper_language",
      "whisper_task",
      "auto_config",
      "offload_embedding",
      "float32_mixed_precision",
      "fast_inference",
      "gpu_memory_utilization",
      "float8_kv_cache",
      "random_state",
      "max_lora_rank",
      "disable_log_stats",
      "unsloth_vllm_standby",
      "load_in_fp8"
    ],
    "get_peft_model": [
      "model",
      "r",
      "target_modules",
      "lora_alpha",
      "lora_dropout",
      "bias",
      "finetune_vision_layers",
      "finetune_language_layers",
      "finetune_attention_modules",
      "finetune_mlp_modules",
      "layers_to_transform",
      "layers_pattern",
      "use_gradient_checkpointing",
      "random_state",
      "max_seq_length",
      "use_rslora",
      "modules_to_save",
      "init_lora_weights",
      "loftq_config",
      "task_type",
      "temporary_location",
      "qat_scheme",
      "target_parameters",
      "ensure_weight_tying"
    ],
    "post_patch_model": [
      "model",
      "use_gradient_checkpointing",
      "trust_remote_code",
      "model_type",
      "tokenizer",
      "float32_mixed_precision"
    ],
    "for_inference": [
      "model"
    ],
    "for_training": [
      "model",
      "use_gradient_checkpointing"
    ]
  },
  "Qwen3Attention_fast_forward": [
    "self",
    "hidden_states",
    "causal_mask",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask",
    "position_embeddings"
  ],
  "Qwen3Attention_fast_forward_inference": [
    "self",
    "hidden_states",
    "past_key_value",
    "position_ids",
    "do_prefill",
    "attention_mask"
  ],
  "FastQwen3Model": {
    "pre_patch": [],
    "from_pretrained": [
      "model_name",
      "max_seq_length",
      "dtype",
      "load_in_4bit",
      "token",
      "device_map",
      "rope_scaling",
      "fix_tokenizer",
      "model_patcher",
      "tokenizer_name",
      "trust_remote_code"
    ]
  },
  "HERE": [],
  "main": [
    "argv"
  ],
  "enforce_spacing": [
    "text"
  ],
  "remove_redundant_passes": [
    "text"
  ],
  "process_file": [
    "path"
  ]
}