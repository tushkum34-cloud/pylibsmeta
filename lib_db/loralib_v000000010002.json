{
  "layer": [],
  "conv": [],
  "grouped_conv": [],
  "input_tensor": [],
  "mark_only_lora_as_trainable": [
    "model",
    "bias"
  ],
  "lora_state_dict": [
    "model",
    "bias"
  ],
  "name": [],
  "LoRALayer": {
    "__init__": [
      "self",
      "r",
      "lora_alpha",
      "lora_dropout",
      "merge_weights"
    ]
  },
  "Embedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "r",
      "lora_alpha",
      "merge_weights"
    ],
    "reset_parameters": [
      "self"
    ],
    "train": [
      "self",
      "mode"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Linear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out",
      "merge_weights"
    ],
    "reset_parameters": [
      "self"
    ],
    "train": [
      "self",
      "mode"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MergedLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "r",
      "lora_alpha",
      "lora_dropout",
      "enable_lora",
      "fan_in_fan_out",
      "merge_weights"
    ],
    "reset_parameters": [
      "self"
    ],
    "zero_pad": [
      "self",
      "x"
    ],
    "merge_AB": [
      "self"
    ],
    "train": [
      "self",
      "mode"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConvLoRA": {
    "__init__": [
      "self",
      "conv_module",
      "in_channels",
      "out_channels",
      "kernel_size",
      "r",
      "lora_alpha",
      "lora_dropout",
      "merge_weights"
    ],
    "reset_parameters": [
      "self"
    ],
    "train": [
      "self",
      "mode"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Conv2d": {
    "__init__": [
      "self"
    ]
  },
  "Conv1d": {
    "__init__": [
      "self"
    ]
  },
  "Conv3d": {
    "__init__": [
      "self"
    ]
  }
}