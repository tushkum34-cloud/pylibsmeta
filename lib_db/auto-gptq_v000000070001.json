{
  "__version__": [],
  "logger": [],
  "get_device": [
    "obj"
  ],
  "move_to_device": [
    "obj",
    "device"
  ],
  "find_layers": [
    "module",
    "layers",
    "name"
  ],
  "get_module_by_name_prefix": [
    "model",
    "module_name"
  ],
  "get_module_by_name_suffix": [
    "model",
    "module_name"
  ],
  "make_quant": [
    "module",
    "names",
    "bits",
    "group_size",
    "name",
    "use_triton",
    "use_marlin",
    "disable_exllama",
    "disable_exllamav2",
    "use_qigen",
    "use_cuda_fp16",
    "desc_act",
    "trainable"
  ],
  "preprocess_checkpoint_qigen": [
    "module",
    "names",
    "bits",
    "group_size",
    "checkpoint",
    "name"
  ],
  "pack_model": [
    "model",
    "quantizers",
    "bits",
    "group_size",
    "use_triton",
    "use_cuda_fp16",
    "desc_act",
    "warmup_triton",
    "force_layer_back_to_cpu"
  ],
  "check_and_get_model_type": [
    "model_dir",
    "trust_remote_code"
  ],
  "simple_dispatch_model": [
    "model",
    "device_map"
  ],
  "autogptq_post_init": [
    "model",
    "use_act_order",
    "max_input_length"
  ],
  "make_sure_no_tensor_in_meta_device": [
    "model",
    "use_triton",
    "desc_act",
    "group_size",
    "bits",
    "disable_exllama",
    "disable_exllamav2",
    "use_marlin"
  ],
  "awq_reverse_reorder_int_tensor": [
    "int_tensor",
    "bits"
  ],
  "unpack_awq": [
    "awq_qweight",
    "awq_qzeros",
    "awq_scales",
    "bits",
    "group_size"
  ],
  "pack_from_tensors": [
    "unpacked_qweight",
    "unpacked_qzeros",
    "awq_scales",
    "bits",
    "group_size"
  ],
  "get_checkpoints": [
    "model_name_or_path",
    "extensions",
    "possible_model_basenames"
  ],
  "__all__": [],
  "Qwen2GPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "MistralGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "GPTBigCodeGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "YiGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "fused_attn_module_type": [],
    "fused_mlp_module_type": []
  },
  "GemmaGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "GPTNeoXGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "lm_head_name": []
  },
  "DeciLMGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "fused_attn_module_type": [],
    "fused_mlp_module_type": []
  },
  "OPTGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "BaiChuanGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "GPTJGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "fused_attn_module_type": []
  },
  "LongLlamaGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "fused_attn_module_type": [],
    "fused_mlp_module_type": []
  },
  "XverseGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "fused_attn_module_type": [],
    "fused_mlp_module_type": []
  },
  "MixtralGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "handler": [],
  "formatter": [],
  "SYNONYMS": [],
  "BaseQuantizeConfig": {
    "__post_init__": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_dir"
    ],
    "from_pretrained": [
      "cls",
      "save_dir"
    ],
    "to_dict": [
      "self"
    ]
  },
  "BaseGPTQForCausalLM": {
    "__init__": [
      "self",
      "model",
      "quantized",
      "quantize_config",
      "is_triton_backend",
      "injected_fused_attention",
      "injected_fused_mlp",
      "trainable"
    ],
    "quantized": [
      "self"
    ],
    "hf_device_map": [
      "self"
    ],
    "_prepare_examples_for_quantization": [
      "self",
      "examples",
      "batch_size"
    ],
    "quantize": [
      "self",
      "examples",
      "batch_size",
      "use_triton",
      "use_cuda_fp16",
      "autotune_warmup_after_quantized",
      "cache_examples_on_gpu"
    ],
    "device": [
      "self"
    ],
    "to": [
      "self",
      "device"
    ],
    "forward": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ],
    "push_to_hub": [
      "self",
      "repo_id",
      "save_dir",
      "use_safetensors",
      "safetensors_metadata",
      "commit_message",
      "use_auth_token",
      "private",
      "token",
      "create_pr"
    ],
    "save_quantized": [
      "self",
      "save_dir",
      "use_safetensors",
      "safetensors_metadata"
    ],
    "save_pretrained": [
      "self",
      "save_dir",
      "use_safetensors",
      "safetensors_metadata"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "quantize_config",
      "max_memory",
      "trust_remote_code",
      "torch_dtype"
    ],
    "from_quantized": [
      "cls",
      "model_name_or_path",
      "device_map",
      "max_memory",
      "device",
      "low_cpu_mem_usage",
      "use_triton",
      "use_qigen",
      "use_marlin",
      "torch_dtype",
      "inject_fused_attention",
      "inject_fused_mlp",
      "use_cuda_fp16",
      "quantize_config",
      "model_basename",
      "use_safetensors",
      "trust_remote_code",
      "warmup_triton",
      "trainable",
      "disable_exllama",
      "disable_exllamav2"
    ],
    "warmup_triton": [
      "self",
      "enabled"
    ],
    "enable_trainable_mode": [
      "self",
      "enabled"
    ],
    "disable_trainable_mode": [
      "self"
    ],
    "make_sure_compatible_with_peft": [
      "model",
      "use_triton",
      "desc_act",
      "group_size",
      "bits",
      "disable_exllama",
      "disable_exllamav2",
      "use_marlin",
      "use_qigen"
    ],
    "__getattr__": [
      "self",
      "item"
    ]
  },
  "GPT2GPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "GPTQ_CAUSAL_LM_MODEL_MAP": [],
  "AutoGPTQForCausalLM": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "quantize_config",
      "max_memory",
      "trust_remote_code"
    ],
    "from_quantized": [
      "cls",
      "model_name_or_path",
      "device_map",
      "max_memory",
      "device",
      "low_cpu_mem_usage",
      "use_triton",
      "inject_fused_attention",
      "inject_fused_mlp",
      "use_cuda_fp16",
      "quantize_config",
      "model_basename",
      "use_safetensors",
      "trust_remote_code",
      "warmup_triton",
      "trainable",
      "disable_exllama",
      "disable_exllamav2",
      "use_marlin"
    ]
  },
  "CodeGenGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "QwenGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "StableLMEpochGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "fused_attn_module_type": [],
    "fused_mlp_module_type": []
  },
  "LlamaGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": [],
    "fused_attn_module_type": [],
    "fused_mlp_module_type": []
  },
  "BloomGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "CPU": [],
  "CUDA_0": [],
  "SUPPORTED_MODELS": [],
  "EXLLAMA_DEFAULT_MAX_INPUT_LENGTH": [],
  "InternLMGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "RWGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "MOSSGPTQForCausalLM": {
    "layer_type": [],
    "layers_block_name": [],
    "outside_layer_modules": [],
    "inside_layer_modules": []
  },
  "Perplexity": {
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "dataset_path",
      "dataset_name",
      "split",
      "text_column"
    ],
    "_get_device": [
      "self"
    ],
    "_prepare_data": [
      "self"
    ],
    "softmax": [
      "logits"
    ],
    "calculate_perplexity": [
      "self",
      "n_ctx",
      "n_batch"
    ],
    "_process_batch": [
      "self",
      "i",
      "n_ctx",
      "n_batch",
      "tokens",
      "nll",
      "count"
    ],
    "_compute_batch_logits": [
      "self",
      "tokens",
      "batch_start",
      "batch_size"
    ]
  },
  "dynamically_import_QuantLinear": [
    "use_triton",
    "desc_act",
    "group_size",
    "bits",
    "disable_exllama",
    "disable_exllamav2",
    "use_qigen",
    "disable_marlin"
  ],
  "compare_transformers_version": [
    "version",
    "op"
  ],
  "compare_pytorch_version": [
    "version",
    "op"
  ],
  "prepare_model_for_marlin_load": [
    "model_name_or_path",
    "model",
    "quantize_config",
    "quant_linear_class",
    "torch_dtype",
    "current_model_save_name",
    "device_map"
  ],
  "_get_cached_marlin_save_name": [
    "model_name_or_path"
  ],
  "_validate_marlin_device_support": [],
  "_validate_marlin_compatibility": [
    "quantization_config"
  ],
  "convert_to_marlin": [
    "model",
    "model_quantlinear",
    "quantization_config",
    "repack",
    "strict"
  ],
  "recurse_getattr": [
    "obj",
    "attr"
  ],
  "recurse_setattr": [
    "module",
    "name",
    "value"
  ],
  "LinearLayer": [],
  "GPTQLoraConfig": {},
  "_get_linear_feature_count": [
    "linear_layer"
  ],
  "_get_weight": [
    "linear_layer"
  ],
  "GPTQLoraLinear": {
    "__init__": [
      "self",
      "adapter_name",
      "linear_module",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out"
    ],
    "reset_lora_parameters": [
      "self",
      "adapter_name"
    ],
    "merge": [
      "self"
    ],
    "unmerge": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GPTQLoraModel": {
    "_replace_module": [
      "self",
      "parent_module",
      "child_name",
      "new_module",
      "old_module"
    ],
    "_create_new_module": [
      "lora_config",
      "adapter_name",
      "target"
    ],
    "merge_adapter": [
      "self"
    ],
    "unmerge_adapter": [
      "self"
    ],
    "merge_and_unload": [
      "self"
    ]
  },
  "GPTQAdaLoraConfig": {},
  "GPTQSVDLinear": {
    "__init__": [
      "self",
      "adapter_name",
      "linear_module",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out"
    ],
    "merge": [
      "self"
    ],
    "unmerge": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "reset_lora_parameters": [
      "self",
      "adapter_name"
    ]
  },
  "GPTQAdaLoraModel": {
    "_replace_module": [
      "self",
      "parent_module",
      "child_name",
      "new_module",
      "old_module"
    ],
    "_create_new_module": [
      "lora_config",
      "adapter_name",
      "target"
    ],
    "merge_adapter": [
      "self"
    ],
    "unmerge_adapter": [
      "self"
    ],
    "merge_and_unload": [
      "self"
    ]
  },
  "find_all_linear_names": [
    "model",
    "ignore",
    "ignore_lm_head"
  ],
  "hijack_peft_mappings": [],
  "get_gptq_peft_model": [
    "model",
    "peft_config",
    "model_id",
    "adapter_name",
    "auto_find_all_linears",
    "train_mode"
  ],
  "exllama_set_max_input_length": [
    "model",
    "max_input_length"
  ],
  "make_data_block": [
    "samples",
    "prompt_col_name",
    "label_col_name",
    "tokenizer",
    "preprocess_fn",
    "sample_max_len",
    "block_max_len",
    "add_eos_token",
    "truncate_prompt",
    "merge_prompt_label"
  ],
  "collate_data": [
    "blocks",
    "pad_token_id"
  ],
  "get_dataloader": [
    "data_path_or_name",
    "prompt_col_name",
    "label_col_name",
    "tokenizer",
    "load_fn",
    "preprocess_fn",
    "num_samples",
    "sample_max_len",
    "block_max_len",
    "add_eos_token",
    "truncate_prompt",
    "merge_prompt_label",
    "load_fn_kwargs",
    "preprocess_fn_kwargs"
  ],
  "FusedBaseModule": {
    "inject_to_model": [
      "cls"
    ]
  },
  "FusedBaseAttentionModule": {
    "inject_to_model": [
      "cls",
      "model",
      "use_triton",
      "group_size",
      "use_cuda_fp16",
      "desc_act",
      "trainable"
    ],
    "warmup": [
      "cls",
      "model",
      "transpose",
      "seqlen"
    ]
  },
  "FusedBaseMLPModule": {
    "inject_to_model": [
      "cls",
      "model",
      "use_triton"
    ]
  },
  "FusedLlamaAttentionForQuantizedModel": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "qkv_proj",
      "o_proj",
      "rotary_emb",
      "layer_idx"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "past_key_value",
      "attention_mask",
      "position_ids",
      "output_attentions",
      "use_cache"
    ],
    "inject_to_model": [
      "cls",
      "model",
      "use_triton",
      "group_size",
      "use_cuda_fp16",
      "desc_act",
      "trainable",
      "bits",
      "disable_exllama",
      "disable_exllamav2"
    ]
  },
  "FusedLlamaMLPForQuantizedModel": {
    "__init__": [
      "self",
      "gate_proj",
      "down_proj",
      "up_proj"
    ],
    "forward": [
      "self",
      "x"
    ],
    "triton_llama_mlp": [
      "self",
      "x"
    ],
    "inject_to_model": [
      "cls",
      "model",
      "use_triton"
    ],
    "warmup": [
      "cls",
      "model",
      "transpose",
      "seqlen"
    ]
  },
  "fixed_pos_embedding": [
    "x",
    "seq_dim",
    "seq_len"
  ],
  "rotate_every_two": [
    "x"
  ],
  "duplicate_interleave": [
    "m"
  ],
  "apply_rotary_pos_emb": [
    "x",
    "sincos",
    "offset"
  ],
  "FusedGPTJAttentionForQuantizedModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_split_heads": [
      "self",
      "qkv"
    ],
    "_merge_heads": [
      "self",
      "tensor",
      "num_attention_heads",
      "attn_head_size"
    ],
    "_attn": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "head_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "position_ids",
      "head_mask",
      "use_cache",
      "output_attentions"
    ],
    "inject_to_model": [
      "cls",
      "model",
      "use_triton",
      "group_size",
      "use_cuda_fp16",
      "desc_act",
      "trainable",
      "bits",
      "disable_exllama",
      "disable_exllamav2"
    ]
  },
  "none_tensor": [],
  "ext_make_q4": [
    "qweight",
    "qzeros",
    "scales",
    "g_idx",
    "device"
  ],
  "ext_q4_matmul": [
    "x",
    "q4",
    "q4_width"
  ],
  "QuantLinear": {
    "QUANT_TYPE": [],
    "__init__": [
      "self",
      "bits",
      "group_size",
      "infeatures",
      "outfeatures",
      "bias",
      "trainable"
    ],
    "post_init": [
      "self"
    ],
    "pack": [
      "self",
      "linear",
      "scales",
      "zeros",
      "g_idx"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "mem_model": [
    "N",
    "M",
    "T",
    "mu",
    "tu",
    "bits",
    "l1",
    "p",
    "gs"
  ],
  "params": [],
  "compute_reductions": [
    "x",
    "gs",
    "cpp"
  ],
  "process_zeros_scales": [
    "zeros",
    "scales",
    "bits",
    "M"
  ],
  "_torch_device": [
    "idx"
  ],
  "ext_gemm_half_q_half": [
    "x",
    "q_handle",
    "q4_width",
    "force_cuda"
  ],
  "ext_make_q_matrix": [
    "w",
    "temp_dq",
    "key"
  ],
  "ExLlamaV2DeviceTensors": {
    "__init__": [
      "self",
      "device_idx",
      "scratch_bytes"
    ],
    "prepare": [
      "self"
    ],
    "get_scratch_slice": [
      "self",
      "size_bytes"
    ]
  },
  "GeneralQuantLinear": {
    "__init__": [
      "self",
      "quant_linear_module"
    ],
    "inject_to_model": [
      "cls",
      "model",
      "target_module_type"
    ]
  },
  "mul": [
    "A",
    "B",
    "C",
    "s",
    "workspace",
    "thread_k",
    "thread_n",
    "sms",
    "max_par"
  ],
  "_get_perms": [],
  "unpack_4bit_to_32bit_signed": [
    "qweight",
    "qzeros"
  ],
  "unpack_qzeros": [
    "qzeros"
  ],
  "dequantize_weight": [
    "layer"
  ],
  "dequantize_qzeros": [
    "layer"
  ],
  "CustomizedTritonAutoTuner": {
    "__init__": [
      "self",
      "fn",
      "arg_names",
      "configs",
      "key",
      "reset_to_zero",
      "prune_configs_by",
      "nearest_power_of_two"
    ],
    "_bench": [
      "self"
    ],
    "run": [
      "self"
    ],
    "prune_configs": [
      "self",
      "kwargs"
    ],
    "warmup": [
      "self"
    ]
  },
  "autotune": [
    "configs",
    "key",
    "prune_configs_by",
    "reset_to_zero",
    "nearest_power_of_two"
  ],
  "matmul248_kernel_config_pruner": [
    "configs",
    "nargs"
  ],
  "TritonModuleMixin": {
    "warmup": [
      "cls",
      "model",
      "transpose",
      "seqlen"
    ]
  },
  "quant_matmul_248_kernel": [
    "a_ptr",
    "b_ptr",
    "c_ptr",
    "scales_ptr",
    "zeros_ptr",
    "g_ptr",
    "M",
    "N",
    "K",
    "bits",
    "maxq",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "stride_scales",
    "stride_zeros",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M"
  ],
  "transpose_quant_matmul_248_kernel": [
    "a_ptr",
    "b_ptr",
    "c_ptr",
    "scales_ptr",
    "zeros_ptr",
    "g_ptr",
    "M",
    "N",
    "K",
    "bits",
    "maxq",
    "stride_am",
    "stride_ak",
    "stride_bk",
    "stride_bn",
    "stride_cm",
    "stride_cn",
    "stride_scales",
    "stride_zeros",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M"
  ],
  "silu": [
    "x"
  ],
  "quant_matmul_248": [
    "input",
    "qweight",
    "scales",
    "qzeros",
    "g_idx",
    "bits",
    "maxq"
  ],
  "transpose_quant_matmul_248": [
    "input",
    "qweight",
    "scales",
    "qzeros",
    "g_idx",
    "bits",
    "maxq"
  ],
  "QuantLinearFunction": {
    "forward": [
      "ctx",
      "input",
      "qweight",
      "scales",
      "qzeros",
      "g_idx",
      "bits",
      "maxq"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "quant_matmul_inference_only_248": [
    "input",
    "qweight",
    "scales",
    "qzeros",
    "g_idx",
    "bits",
    "maxq"
  ],
  "QuantLinearInferenceOnlyFunction": {
    "forward": [
      "ctx",
      "input",
      "qweight",
      "scales",
      "qzeros",
      "g_idx",
      "bits",
      "maxq"
    ]
  },
  "TextSummarizationTask": {
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "data_name_or_path",
      "prompt_col_name",
      "label_col_name",
      "device"
    ],
    "_predict": [
      "self",
      "batch_data"
    ],
    "_parse_labels": [
      "self",
      "label_ids"
    ],
    "_metric": [
      "self",
      "pred",
      "label"
    ],
    "run": [
      "self",
      "generation_config"
    ]
  },
  "BaseTask": {
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "data_name_or_path",
      "prompt_col_name",
      "label_col_name",
      "device"
    ],
    "_predict": [
      "self",
      "batch_data"
    ],
    "_parse_labels": [
      "self",
      "label_ids"
    ],
    "_metric": [
      "self",
      "pred",
      "label"
    ],
    "run": [
      "self"
    ]
  },
  "get_predictions": [
    "input_ids",
    "output_ids",
    "num_return_sequences",
    "tokenizer",
    "classes"
  ],
  "SequenceClassificationTask": {
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "classes",
      "data_name_or_path",
      "prompt_col_name",
      "label_col_name",
      "device"
    ],
    "_predict": [
      "self",
      "batch_data"
    ],
    "_parse_labels": [
      "self",
      "label_ids"
    ],
    "_metric": [
      "self",
      "pred",
      "label"
    ],
    "run": [
      "self",
      "generation_config"
    ]
  },
  "LanguageModelingTask": {
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "data_name_or_path",
      "prompt_col_name",
      "label_col_name",
      "device"
    ],
    "_predict": [
      "self",
      "batch_data"
    ],
    "_parse_labels": [
      "self",
      "label_ids"
    ],
    "_metric": [
      "self",
      "pred",
      "label"
    ],
    "run": [
      "self"
    ]
  },
  "postprocess_generation_ids": [
    "input_ids",
    "output_ids",
    "num_return_sequences",
    "tokenizer",
    "pad_token_ids"
  ],
  "levenshtein_distance": [
    "seq1",
    "seq2"
  ],
  "get_closest_label": [
    "pred",
    "classes"
  ],
  "quantize": [
    "x",
    "scale",
    "zero",
    "maxq"
  ],
  "Quantizer": {
    "__init__": [
      "self",
      "shape"
    ],
    "configure": [
      "self",
      "bits",
      "perchannel",
      "sym",
      "mse",
      "norm",
      "grid",
      "maxshrink",
      "trits"
    ],
    "find_params": [
      "self",
      "x",
      "weight"
    ],
    "quantize": [
      "self",
      "x"
    ],
    "enabled": [
      "self"
    ],
    "ready": [
      "self"
    ]
  },
  "GPTQ": {
    "__init__": [
      "self",
      "layer"
    ],
    "add_batch": [
      "self",
      "inp",
      "out"
    ],
    "fasterquant": [
      "self",
      "blocksize",
      "percdamp",
      "group_size",
      "actorder",
      "static_groups"
    ],
    "free": [
      "self"
    ]
  },
  "gen_quant4": [
    "k",
    "n",
    "groupsize"
  ],
  "TestRepacking": {
    "test_marlin_fast_repacking": [
      "self"
    ]
  },
  "TestQuantization": {
    "test_quantize": [
      "self"
    ]
  },
  "TestShardedLoading": {
    "test_loading": [
      "self"
    ],
    "test_loading_large": [
      "self"
    ]
  },
  "TestAwqCompatibility": {
    "test_generation_cuda_old_fp32_pytorch": [
      "self"
    ],
    "test_generation_cuda_old_cuda_256": [
      "self"
    ],
    "test_generation_cuda_old_cuda_64": [
      "self"
    ],
    "test_generation_exllama": [
      "self"
    ],
    "test_generation_exllamav2": [
      "self"
    ]
  },
  "TestSerialization": {
    "MODEL_ID": [],
    "setUp": [
      "self"
    ],
    "test_marlin_local_serialization": [
      "self"
    ],
    "test_marlin_hf_cache_serialization": [
      "self"
    ]
  },
  "MODEL_NAME": [],
  "TestPeftConversion": {
    "check_model_trainable": [
      "self",
      "model_lora",
      "tokenizer"
    ],
    "test_lora_conversion": [
      "self"
    ],
    "test_adalora_conversion": [
      "self"
    ]
  },
  "group_size": [],
  "bits": [],
  "k": [],
  "n": [],
  "device": [],
  "linear_class": [],
  "linear_gptq": [],
  "num_runs": [],
  "lines": [],
  "seqlens": [],
  "header": [],
  "get_diff": [
    "a",
    "ref"
  ],
  "CUDA_OLD_REFERENCE": [],
  "TestsQ4Exllama": {
    "test_exllama": [
      "self"
    ],
    "test_exllama_buffer_size": [
      "self"
    ],
    "test_generation_no_act_order": [
      "self"
    ],
    "test_generation_with_act_order": [
      "self"
    ],
    "test_multigpu": [
      "self"
    ]
  },
  "TestsQ4CUDA": {
    "REFERENCE_OLD_HALF": [],
    "REFERENCE_OLD_NO_HALF": [],
    "test_cuda_old": [
      "self",
      "use_half2"
    ],
    "test_generation_with_act_order": [
      "self",
      "torch_dtype",
      "device"
    ],
    "test_generation_no_act_order": [
      "self",
      "torch_dtype",
      "device"
    ]
  },
  "TestsQ4ExllamaV2": {
    "test_exllamav2": [
      "self"
    ],
    "test_generation_no_act_order": [
      "self"
    ],
    "test_generation_with_act_order": [
      "self"
    ],
    "test_exllama_v2_buffer_size": [
      "self"
    ]
  },
  "TestsMixtral": {
    "test_mixtral_generation": [
      "self"
    ]
  },
  "TestQ4Marlin": {
    "test_generation": [
      "self"
    ],
    "test_bias": [
      "self"
    ]
  }
}