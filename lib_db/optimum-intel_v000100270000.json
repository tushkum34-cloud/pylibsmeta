{
  "REGISTER_COMMANDS": [],
  "logger": [],
  "parse_args_openvino": [
    "parser"
  ],
  "no_compression_parameter_provided": [
    "args"
  ],
  "no_quantization_parameter_provided": [
    "args"
  ],
  "OVExportCommand": {
    "COMMAND": [],
    "parse_args": [
      "parser"
    ],
    "run": [
      "self"
    ]
  },
  "prepare_wc_config": [
    "args",
    "default_configs"
  ],
  "prepare_q_config": [
    "args"
  ],
  "INCCommand": {
    "COMMAND": [],
    "SUBCOMMANDS": []
  },
  "parse_args_inc_quantize": [
    "parser"
  ],
  "INCQuantizeCommand": {
    "parse_args": [
      "parser"
    ],
    "run": [
      "self"
    ]
  },
  "InputInfo": [],
  "OV_XML_FILE_NAME": [],
  "_MAX_UNCOMPRESSED_SIZE": [],
  "is_torch_model": [
    "model"
  ],
  "flattenize_inputs": [
    "inputs"
  ],
  "_get_input_info": [
    "model",
    "config",
    "dummy_inputs"
  ],
  "remove_none_from_dummy_inputs": [
    "dummy_inputs"
  ],
  "clear_class_registry": [],
  "_get_open_clip_submodels_fn_and_export_configs": [
    "model",
    "library_name",
    "task",
    "preprocessors",
    "custom_export_configs",
    "fn_get_submodels"
  ],
  "MULTI_MODAL_TEXT_GENERATION_MODELS": [],
  "SSM_MODELS": [],
  "save_config": [
    "config",
    "save_dir"
  ],
  "deduce_diffusers_dtype": [
    "model_name_or_path"
  ],
  "save_preprocessors": [
    "preprocessors",
    "config",
    "output",
    "trust_remote_code"
  ],
  "COMPLEX_CHAT_TEMPLATES": [],
  "set_simplified_chat_template": [
    "ov_tokenizer_model",
    "processor_chat_template"
  ],
  "SKIP_CHECK_TRACE_MODELS": [],
  "allow_skip_tracing_check": [
    "library_name",
    "model_type"
  ],
  "load_preprocessors": [
    "src_name_or_path",
    "subfolder",
    "trust_remote_code",
    "model_type"
  ],
  "patch_qwenvl_configs": [],
  "__all__": [],
  "_set_runtime_options": [
    "models_and_export_configs",
    "task",
    "library_name",
    "quantized_model"
  ],
  "_save_model": [
    "model",
    "path",
    "ov_config",
    "library_name",
    "config"
  ],
  "export": [
    "model",
    "config",
    "output",
    "opset",
    "device",
    "input_shapes",
    "model_kwargs",
    "ov_config",
    "stateful",
    "patch_16bit_model",
    "library_name"
  ],
  "export_pytorch_via_onnx": [
    "model",
    "config",
    "opset",
    "output",
    "device",
    "input_shapes",
    "model_kwargs",
    "ov_config",
    "library_name"
  ],
  "export_pytorch": [
    "model",
    "config",
    "opset",
    "output",
    "device",
    "input_shapes",
    "model_kwargs",
    "ov_config",
    "stateful",
    "patch_16bit_model",
    "library_name"
  ],
  "export_models": [
    "models_and_export_configs",
    "output_dir",
    "opset",
    "output_names",
    "device",
    "input_shapes",
    "model_kwargs",
    "ov_config",
    "stateful",
    "patch_16bit_model",
    "library_name"
  ],
  "export_from_model": [
    "model",
    "output",
    "task",
    "ov_config",
    "stateful",
    "opset",
    "model_kwargs",
    "custom_export_configs",
    "fn_get_submodels",
    "preprocessors",
    "device",
    "trust_remote_code",
    "patch_16bit_model"
  ],
  "export_tokenizer": [
    "tokenizer",
    "output",
    "suffix",
    "task",
    "processor_chat_template"
  ],
  "_add_runtime_options_to_rt_info": [
    "model",
    "options"
  ],
  "_add_version_info_to_model": [
    "model",
    "library_name"
  ],
  "_get_multi_modal_submodels_and_export_configs": [
    "model",
    "task",
    "library_name",
    "int_dtype",
    "float_dtype",
    "preprocessors",
    "model_kwargs",
    "stateful"
  ],
  "_get_submodels_and_export_configs": [
    "model",
    "task",
    "monolith",
    "custom_export_configs",
    "custom_architecture",
    "_variant",
    "library_name",
    "int_dtype",
    "float_dtype",
    "fn_get_submodels",
    "preprocessors",
    "model_kwargs",
    "exporter",
    "stateful"
  ],
  "get_diffusion_models_for_export_ext": [
    "pipeline",
    "int_dtype",
    "float_dtype",
    "exporter"
  ],
  "get_ltx_video_models_for_export": [
    "pipeline",
    "exporter",
    "int_dtype",
    "float_dtype"
  ],
  "get_sana_models_for_export": [
    "pipeline",
    "exporter",
    "int_dtype",
    "float_dtype"
  ],
  "get_sd3_models_for_export": [
    "pipeline",
    "exporter",
    "int_dtype",
    "float_dtype"
  ],
  "get_flux_models_for_export": [
    "pipeline",
    "exporter",
    "int_dtype",
    "float_dtype"
  ],
  "_get_encoder_decoder_stateful_models_for_export": [
    "model",
    "task",
    "_variant",
    "library_name",
    "int_dtype",
    "float_dtype",
    "preprocessors"
  ],
  "_get_speecht5_tss_model_for_export": [
    "model",
    "task",
    "library_name",
    "int_dtype",
    "float_dtype",
    "preprocessors",
    "model_kwargs"
  ],
  "init_model_configs": [],
  "register_in_tasks_manager": [],
  "BaichaunOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "Qwen2OpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "Qwen2MoEOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "Qwen3OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": [],
    "inputs": [
      "self"
    ]
  },
  "Qwen3MoEOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "MiniCPMOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "MAX_TRANSFORMERS_VERSION": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "OVMiniCPM3DummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "MiniCPM3OpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "MAX_TRANSFORMERS_VERSION": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "StableLMOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "ChatGLM2DummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "ChatGLM2OpenVINOConfig": {
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "_MODEL_PATCHER": [],
    "MAX_TRANSFORMERS_VERSION": [],
    "generate_dummy_inputs": [
      "self",
      "framework"
    ],
    "add_past_key_values": [
      "self",
      "inputs_or_outputs",
      "direction"
    ]
  },
  "MixtralOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "GemmaOpenVINOConfig": {
    "_MODEL_PATCHER": [],
    "inputs": [
      "self"
    ]
  },
  "LlamaOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "GptOssOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": []
  },
  "BitnetOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": []
  },
  "ExaoneOpenVINOConfig": {},
  "Exaone4OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "ArceeOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "QwenDummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "QwenOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "MAX_TRANSFORMERS_VERSION": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "_MODEL_PATCHER": [],
    "generate_dummy_inputs": [
      "self",
      "framework"
    ],
    "add_past_key_values": [
      "self",
      "inputs_or_outputs",
      "direction"
    ]
  },
  "Starcoder2OpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "InternLM2OpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "OrionOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "OlmoOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": []
  },
  "MPTOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "Phi3OpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "PhiMoEOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": []
  },
  "PhiOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "OVFalconDummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ]
  },
  "FalconOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "PersimmonOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "BioGPTOpenVINOConfig": {
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "GPTNeoOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "GPTJOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "BloomOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "CohereOpenVINOConfig": {},
  "XGLMConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "AquilaDummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "AquilaMOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "XverseMOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "InternLMOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "CodeGenOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "DBRXOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "JaisOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "ArcticOpenVINOConfig": {
    "MAX_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": []
  },
  "OVMistralDummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "MistralOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "GPTNeoxOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "GPTNeoxJapaneseOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "Gemma2OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": []
  },
  "Gemma3TextOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "DeciDummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DeciOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "OpenCLIPOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ],
    "generate_dummy_inputs": [
      "self",
      "framework"
    ],
    "generate_dummy_inputs_for_validation": [
      "self",
      "reference_model_inputs",
      "onnx_input_names"
    ]
  },
  "OpenCLIPTextOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ],
    "generate_dummy_inputs": [
      "self",
      "framework"
    ]
  },
  "OpenCLIPVisualOpenVINOConfig": {
    "DEFAULT_ONNX_OPSET": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ]
  },
  "CLIPOpenVINOConfig": {},
  "CLIPTextOpenVINOConfig": {},
  "CLIPTextWithProjectionOpenVINOConfig": {},
  "CLIPVisionModelOpenVINOConfig": {},
  "IBertOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "LMInputEmbedsConfigHelper": {
    "__init__": [
      "self",
      "export_config",
      "patcher_cls",
      "dummy_input_generator",
      "inputs_update"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ],
    "outputs": [
      "self"
    ],
    "inputs": [
      "self"
    ],
    "generate_dummy_inputs": [
      "self",
      "framework"
    ]
  },
  "InputEmbedOpenvVINOConfig": {
    "NORMALIZED_CONFIG_CLASS": [],
    "_MODEL_PATCHER": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ]
  },
  "get_vlm_internal_text_generation_config": [
    "model_type",
    "model_config",
    "int_dtype",
    "float_dtype"
  ],
  "get_vlm_text_embeddings_config": [
    "model_type",
    "model_config",
    "int_dtype",
    "float_dtype"
  ],
  "get_vlm_text_generation_config": [
    "model_type",
    "model_config",
    "int_dtype",
    "float_dtype",
    "model_patcher",
    "dummy_input_generator",
    "inputs_update"
  ],
  "VLMConfigBehavior": {
    "VISION_EMBEDDINGS": [],
    "TEXT_EMBEDDINGS": [],
    "LANGUAGE": []
  },
  "BaseVLMOpenVINOConfig": {
    "SUPPORTED_BEHAVIORS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "SUPPORTS_PAST": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "get_model_for_behavior": [
      "self",
      "model",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ]
  },
  "LlavaOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ],
    "generate_dummy_inputs": [
      "self",
      "framework"
    ]
  },
  "LlavaNextOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "DummyLLavaMultiModalProjectorInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "random_batch_size_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "LLavaMultimodalProjectorOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ]
  },
  "LlavaNextVideoConfigBehavior": {
    "LANGUAGE": [],
    "VISION_EMBEDDINGS": [],
    "VISION_RESAMPLER": [],
    "MULTI_MODAL_PROJECTOR": [],
    "TEXT_EMBEDDINGS": []
  },
  "LlavaNextVideoOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "SUPPORTED_BEHAVIORS": [],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "get_model_for_behavior": [
      "self",
      "model",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ]
  },
  "MairaOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "SUPPORTS_PAST": [],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ],
    "get_model_for_behavior": [
      "self",
      "model",
      "behavior"
    ]
  },
  "InternVLChatOpenVINOConfig": {
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "get_model_for_behavior": [
      "model",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ]
  },
  "LlavaQwen2OpenVINOConfig": {
    "SUPPORTS_PAST": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "MAX_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors",
      "use_past"
    ],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "get_model_for_behavior": [
      "model",
      "behavior"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ]
  },
  "PooledProjectionsDummyInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "random_batch_size_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyTransformerTimestpsInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyUnetVisionInputGenerator": {
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyUnetTimestepInputGenerator": {
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummySanaTimestepInputGenerator": {
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyUnetEncoderInputGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "num_choices",
      "random_batch_size_range",
      "random_sequence_length_range",
      "random_num_choices_range"
    ]
  },
  "UNetOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "inputs": [
      "self"
    ]
  },
  "SD3TransformerOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "inputs": [
      "self"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ]
  },
  "T5EncoderOpenVINOConfig": {},
  "Gemma2TextEncoderOpenVINOConfig": {
    "_MODEL_PATCHER": [],
    "inputs": [
      "self"
    ]
  },
  "DummySanaSeq2SeqDecoderTextWithEncMaskInputGenerator": {
    "SUPPORTED_INPUT_NAMES": []
  },
  "DummySanaTransformerVisionInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "SanaTransformerOpenVINOConfig": {
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "inputs": [
      "self"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ]
  },
  "DcaeEncoderOpenVINOConfig": {
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ]
  },
  "DcaeDecoderOpenVINOConfig": {
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ]
  },
  "DummyFluxTransformerInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyFluxTextInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "FluxTransformerOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "_MODEL_PATCHER": [],
    "inputs": [
      "self"
    ]
  },
  "LTXVaeDummyInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height",
      "num_frames"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "LTXVaeEncoderOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ]
  },
  "LTXVaeDecoderOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ]
  },
  "LTXTransformerDummyInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height",
      "num_frames",
      "frame_rate"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "LTXVideoTransformerOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ]
  },
  "DummyMiniCPMVImageInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyMiniCPMVResampleInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "MiniCPMVConfigBehavior": {
    "RESAMPLER": [],
    "LANGUAGE": [],
    "VISION_EMBEDDINGS": [],
    "TEXT_EMBEDDINGS": []
  },
  "MiniCPMVOpenVINOConfig": {
    "SUPPORTED_BEHAVIORS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "get_model_for_behavior": [
      "model",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ]
  },
  "MiniCPMOOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "MAX_TRANSFORMERS_VERSION": []
  },
  "Phi3VisionConfigBehavior": {
    "LANGUAGE": [],
    "VISION_PROJECTION": [],
    "VISION_EMBEDDINGS": [],
    "TEXT_EMBEDDINGS": []
  },
  "DummyPhi3VisionProjectionInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height",
      "crop_size"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "Phi3VisionOpenVINOConfig": {
    "SUPPORTED_BEHAVIORS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "MAX_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "get_model_for_behavior": [
      "model",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ]
  },
  "DummyAudioPhi4MMInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "signal_length"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyVisionPositionIdsPhi4InputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ],
    "get_vision_position_ids": [
      "self"
    ]
  },
  "Phi4MMConfigBehavior": {
    "AUDIO_EMBEDDINGS": [],
    "AUDIO_ENCODER": [],
    "AUDIO_FORWARD_EMBEDDINGS": [],
    "AUDIO_VISION_PROJECTION": [],
    "AUDIO_SPEECH_PROJECTION": [],
    "LANGUAGE": [],
    "TEXT_EMBEDDINGS": [],
    "VISION_PROJECTION": [],
    "VISION_EMBEDDINGS": []
  },
  "Phi4MMOpenVINOConfig": {
    "SUPPORTED_BEHAVIORS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "MAX_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "get_model_for_behavior": [
      "model",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ],
    "rename_ambiguous_inputs": [
      "self",
      "inputs"
    ]
  },
  "DummyQwen2VLLMInputGenerator": {
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "DummyQwen2VLVisionEmbedInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "Qwen2VLConfigBehavior": {
    "LANGUAGE": [],
    "VISION_EMBEDDINGS": [],
    "VISION_EMBEDDINGS_MERGER": [],
    "TEXT_EMBEDDINGS": []
  },
  "Qwen2VLOpenVINOConfig": {
    "SUPPORTED_BEHAVIORS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "get_model_for_behavior": [
      "model",
      "behavior"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ]
  },
  "Qwen2_5_VLOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "inputs": [
      "self"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ]
  },
  "GLMOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "GLM4OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "GraniteOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "GraniteMoEOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": []
  },
  "WhisperOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "T5OpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "MT5OpenVINOConfig": {},
  "LongT5OpenVINOConfig": {},
  "BartOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "MBartOpenVINOConfig": {},
  "M2M100OpenVINOConfig": {},
  "DeepseekOpenVINOConfig": {
    "MAX_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": []
  },
  "GotOCR2OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ]
  },
  "Gemma3OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ]
  },
  "DummyVisionPositionIdsInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "num_channels",
      "width",
      "height"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "Idefics3OpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "behavior",
      "preprocessors"
    ],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ],
    "inputs": [
      "self"
    ],
    "get_model_for_behavior": [
      "self",
      "model",
      "behavior"
    ]
  },
  "SmolVLMOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": []
  },
  "BlenderbotOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "BlenderbotSmallOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "PegasusOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "MarianOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "DummySpeechT5OpenVINOInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "sequence_length"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "SpeechT5ConfigBehavior": {
    "ENCODER": [],
    "DECODER": [],
    "POSTNET": [],
    "VOCODER": []
  },
  "SpeechT5OpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "_MODEL_PATCHER": [],
    "__init__": [
      "self",
      "config",
      "task",
      "int_dtype",
      "float_dtype",
      "use_past",
      "use_past_in_inputs",
      "behavior",
      "preprocessors"
    ],
    "add_past_key_values": [
      "self",
      "inputs_or_outputs",
      "direction"
    ],
    "inputs": [
      "self"
    ],
    "outputs": [
      "self"
    ],
    "with_behavior": [
      "self",
      "behavior"
    ]
  },
  "Llama4TextOpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "_MODEL_PATCHER": []
  },
  "Llama4OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "patch_model_for_export": [
      "self",
      "model",
      "model_kwargs"
    ]
  },
  "MambaCacheDummyInputGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "MambaOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": [],
    "inputs": [
      "self"
    ],
    "add_past_key_values": [
      "self",
      "inputs_or_outputs",
      "direction"
    ],
    "generate_dummy_inputs": [
      "self",
      "framework"
    ]
  },
  "GPT2OpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "VisionEncoderDecoderOpenVINOConfig": {
    "_MODEL_PATCHER": []
  },
  "Zamba2DummyPastKeyValuesGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "Zamba2OpenVINOConfig": {
    "PAD_ATTENTION_MASK_TO_PAST": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": [],
    "add_past_key_values": [
      "self",
      "inputs_or_outputs",
      "direction"
    ],
    "inputs": [
      "self"
    ]
  },
  "Lfm2DummyPastKeyValuesGenerator": {
    "SUPPORTED_INPUT_NAMES": [],
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "LFM2OpenVINOConfig": {
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": [],
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "add_past_key_values": [
      "self",
      "inputs_or_outputs",
      "direction"
    ],
    "inputs": [
      "self"
    ]
  },
  "GraniteMoeHybridOpenVINOConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "MIN_TRANSFORMERS_VERSION": [],
    "_MODEL_PATCHER": [],
    "add_past_key_values": [
      "self",
      "inputs_or_outputs",
      "direction"
    ],
    "inputs": [
      "self"
    ]
  },
  "model_has_state": [
    "ov_model"
  ],
  "model_has_input_output_name": [
    "ov_model",
    "name"
  ],
  "fuse_cache_reorder": [
    "ov_model",
    "not_kv_inputs",
    "key_value_input_names",
    "gather_dim"
  ],
  "build_state_initializer": [
    "ov_model",
    "batch_dim"
  ],
  "make_stateful": [
    "ov_model",
    "not_kv_inputs",
    "key_value_input_names",
    "key_value_output_names",
    "batch_dim",
    "num_beams_and_batch"
  ],
  "ensure_stateful_is_available": [
    "warn"
  ],
  "_ENCODER_DECODER_TASKS_WITH_PAST": [],
  "_DECODER_TASKS_WITH_PAST": [],
  "ensure_export_task_support_stateful": [
    "task"
  ],
  "ensure_model_type_support_stateful": [
    "model_type"
  ],
  "remove_parameters_by_names": [
    "model",
    "names"
  ],
  "get_input_nodes": [
    "node"
  ],
  "find_dependent_nodes": [
    "model",
    "sources"
  ],
  "get_read_value_ops": [
    "model"
  ],
  "get_shape_of_ops": [
    "model"
  ],
  "get_consumer_nodes": [
    "node"
  ],
  "find_output_nodes_of_dependent_subgraph": [
    "model",
    "sources"
  ],
  "insert_state_for_nodes": [
    "model",
    "nodes"
  ],
  "patch_stateful_hybrid_ssm": [
    "ov_model"
  ],
  "patch_stateful": [
    "config",
    "ov_model"
  ],
  "patch_stateful_decoder": [
    "config",
    "ov_model"
  ],
  "patch_stateful_encoder_decoder": [
    "config",
    "ov_model"
  ],
  "patch_update_causal_mask": [
    "model",
    "transformers_version",
    "inner_model_name",
    "patch_fn",
    "patch_extrnal_model"
  ],
  "_update_causal_mask_patched": [
    "self",
    "attention_mask",
    "input_tensor",
    "cache_position",
    "past_key_values",
    "output_attentions"
  ],
  "patch_cos_sin_cached_fp32": [
    "model"
  ],
  "eager_mask_without_vmap": [],
  "OVDecoderModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_mixtral_sparse_moe_block_forward": [
    "self",
    "hidden_states"
  ],
  "MixtralModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "ArcticModelPatcher": {
    "__enter__": [
      "self"
    ]
  },
  "_chatglm_transformer_forward": [
    "self",
    "input_ids",
    "position_ids",
    "attention_mask",
    "full_attention_mask",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_hidden_states",
    "return_dict"
  ],
  "_chatglm2_get_context_layer": [
    "query_layer",
    "key_layer",
    "value_layer"
  ],
  "_chatglm2_core_attention_forward": [
    "self",
    "query_layer",
    "key_layer",
    "value_layer",
    "attention_mask"
  ],
  "_glm4_core_attention_forward": [
    "self",
    "query_layer",
    "key_layer",
    "value_layer",
    "attention_mask"
  ],
  "ChatGLMModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "llama_gemma_rotary_emb_forward": [
    "self",
    "x",
    "position_ids",
    "seq_len"
  ],
  "create_sinusoidal_positions": [
    "num_pos",
    "dim",
    "base",
    "inv_freq"
  ],
  "create_embed_positions_buffer": [
    "rotary_emb",
    "max_position_embeddings"
  ],
  "_mistral_update_causal_mask": [
    "self",
    "attention_mask",
    "input_tensor",
    "cache_position",
    "past_key_values",
    "use_cache",
    "output_attentions"
  ],
  "MistralModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "SUPPORT_SDPA": [],
  "_qwen_rotate_half": [
    "x"
  ],
  "_qwen_apply_rotary_pos_emb": [
    "t",
    "freqs"
  ],
  "_qwen_quantize_cache_v": [
    "fdata",
    "bits",
    "qmax",
    "qmin"
  ],
  "_qwen_attention_forward": [
    "self",
    "hidden_states",
    "rotary_pos_emb_list",
    "layer_past",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "output_attentions",
    "use_cache"
  ],
  "QwenModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_baichuan13b_atten_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "_baichuan7b_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "BaichuanModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_mpt_sdpa_attention_forward": [
    "self",
    "hidden_states",
    "position_bias",
    "past_key_value",
    "attention_mask",
    "cache_position"
  ],
  "_mpt_block_forward": [
    "self",
    "hidden_states",
    "position_bias",
    "attention_mask",
    "layer_past",
    "use_cache",
    "output_attentions",
    "cache_position"
  ],
  "MPTModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_internlm2_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "InternLM2Patcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "phi3_442_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "_phi3_self_attn_sdpa_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position"
  ],
  "Phi3ModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_phi_moe_sparse_moe_block_forward": [
    "self",
    "hidden_states"
  ],
  "PhiMoEModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_aquila_self_attn_sdpa_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "AquilaModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_xverse_self_attn_sdpa_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "_internlm_self_attn_sdpa_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "XverseModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "InternLMModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "codegen_wrapped_scaled_dot_product": [
    "self",
    "query",
    "key",
    "value",
    "attention_mask",
    "head_mask"
  ],
  "_codegen_wrapped_scaled_dot_product_legacy": [
    "self",
    "query",
    "key",
    "value",
    "attention_mask",
    "head_mask"
  ],
  "CodeGenModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_dbrx_experts_forward": [
    "self",
    "x",
    "weights",
    "top_weights",
    "top_experts"
  ],
  "_dbrx_update_causal_mask": [
    "self",
    "attention_mask",
    "input_tensor",
    "cache_position",
    "past_key_values",
    "output_attentions"
  ],
  "DBRXModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_persimmon_self_attn_sdpa_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position",
    "position_embeddings"
  ],
  "PersimmonModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_jais_attn_forward": [
    "self",
    "hidden_states",
    "layer_past",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "use_cache",
    "output_attentions",
    "position_bias"
  ],
  "_jais_attn": [
    "self",
    "query",
    "key",
    "value",
    "attention_mask",
    "head_mask",
    "position_bias"
  ],
  "JaisModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_falcon_prepare_4d_causal_attention_mask_with_cache_position": [
    "attention_mask",
    "sequence_length",
    "target_length",
    "dtype",
    "device",
    "cache_position",
    "batch_size"
  ],
  "_falcon_update_causal_mask": [
    "self",
    "attention_mask",
    "input_tensor",
    "cache_position",
    "past_key_values",
    "output_attentions",
    "head_mask",
    "alibi"
  ],
  "FalconModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "GptNeoxModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_gptj_attn": [
    "self",
    "query",
    "key",
    "value",
    "attention_mask",
    "head_mask"
  ],
  "gptj_attn_forward": [
    "self",
    "hidden_states",
    "layer_past",
    "attention_mask",
    "position_ids",
    "head_mask",
    "use_cache",
    "output_attentions",
    "cache_position"
  ],
  "GptJModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_bloom_attn_forward": [
    "self",
    "hidden_states",
    "residual",
    "alibi",
    "attention_mask",
    "layer_past",
    "head_mask",
    "use_cache",
    "output_attentions",
    "cache_position"
  ],
  "BloomModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_gpt_neo_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "layer_past",
    "head_mask",
    "use_cache",
    "output_attentions",
    "cache_position"
  ],
  "_gpt_neo_attn_sdpa": [
    "self",
    "query",
    "key",
    "value",
    "attention_mask",
    "head_mask"
  ],
  "GptNeoModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "Gemma2ModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ]
  },
  "_decilm_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "DeciLMModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "IBertModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ]
  },
  "InternVLChatImageEmbeddingModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "InternVL2ChatLangModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "patched_forward": [
      "self",
      "fn"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "llava_vision_embed_forward": [
    "self",
    "pixel_values"
  ],
  "llava_next_video_vision_embed_forward": [
    "self",
    "pixel_values"
  ],
  "maira_vision_embed_forward": [
    "self",
    "pixel_values"
  ],
  "LlavaImageEmbeddingModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "MairaImageEmbeddingModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "LlavaNextVideoImageEmbeddingModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_embednb_forward": [
    "self",
    "ids"
  ],
  "FluxTransfromerModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_minicpmv_resampler_forward": [
    "self",
    "image_feature",
    "pos_embed",
    "key_padding_mask"
  ],
  "_minicpmv_siglip_vis_embed_forward": [
    "self",
    "pixel_values",
    "patch_attention_mask",
    "tgt_sizes",
    "position_ids"
  ],
  "_minicpmv_siglip_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "output_attentions"
  ],
  "_minicpmv_siglip_transformer_forward": [
    "self",
    "pixel_values",
    "patch_attention_mask",
    "tgt_sizes",
    "position_ids",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "MiniCPMVResamplerModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "MiniCPMVImageEmbeddingsModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "LlavaQwen2ImageEmbeddingsModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "InputEmbeddingPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "phi3_vision_embeddings_forward": [
    "self",
    "pixel_values"
  ],
  "Phi3VisionImageEmbeddingsPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "minicpm3_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "MiniCPM3Patcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "DeepseekPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "deepseek_v3_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "deepseek_v2_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "deepseek_moe_infer": [
    "self",
    "x",
    "topk_ids",
    "topk_weight"
  ],
  "Qwen2VLLanguageModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "patch_qwen2vl_vision_blocks": [
    "model",
    "force_new_behaviour"
  ],
  "Qwen2VLVisionEmbMergerPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "Qwen2_5_VLVisionEmbMergerPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_granite_moe_topk_gating_forward": [
    "self",
    "hidden_states"
  ],
  "_granite_moe_parallel_experts_forward": [
    "self",
    "inputs",
    "expert_size"
  ],
  "GraniteMoEModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "OVSeq2SeqModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "SanaTextEncoderModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "MiniCPMModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ]
  },
  "CommonImageEmbeddingsModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_gemma3_mm_update_causal_mask": [
    "self",
    "attention_mask",
    "token_type_ids",
    "past_key_values",
    "cache_position",
    "input_tensor",
    "is_training"
  ],
  "Gemma3LMModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "Idefics3ImageEmbeddingsModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_blenderbot_attn_forward_legacy": [
    "self",
    "hidden_states",
    "key_value_states",
    "past_key_value",
    "attention_mask",
    "layer_head_mask",
    "output_attentions"
  ],
  "_blenderbot_attn_forward_new": [
    "self",
    "hidden_states",
    "key_value_states",
    "past_key_value",
    "attention_mask",
    "layer_head_mask",
    "output_attentions",
    "cache_position"
  ],
  "modulewise_patch": [
    "model",
    "module_cls",
    "patch_forward"
  ],
  "modulewise_unpatch": [
    "model",
    "module_cls"
  ],
  "BlenderbotModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "BlenderbotSmallModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "PegasusModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "_qwen2moe_sparse_block_forward": [
    "self",
    "hidden_states"
  ],
  "Qwen2MoEPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "MarianModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "speecht5_decoder_prenet_forward": [
    "self",
    "input_values",
    "speaker_embeddings"
  ],
  "speecht5_attention_forward": [
    "self",
    "hidden_states",
    "key_value_states",
    "past_key_value",
    "attention_mask",
    "layer_head_mask",
    "position_bias",
    "output_attentions",
    "serialize"
  ],
  "speecht5_decoder_layer_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "layer_head_mask",
    "cross_attn_layer_head_mask",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "serialize"
  ],
  "OVSpeechT5ModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ],
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ]
  },
  "Phi4MMLanguageModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "Phi4MMAudioForwardEmbeddingsPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "Phi4MMAudioEncoderPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "Phi4MMVisionEmbeddingsPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "Llama4ImageEmbeddingsModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "llama4_rope_forward": [
    "self",
    "x",
    "position_ids"
  ],
  "llama4_apply_rotary_emb": [
    "xq",
    "xk",
    "cos",
    "sin"
  ],
  "llama4_attn_forward": [
    "self",
    "hidden_states",
    "position_embeddings",
    "attention_mask",
    "past_key_value",
    "past_key_values",
    "cache_position"
  ],
  "llama4_moe_forward": [
    "self",
    "hidden_states"
  ],
  "Llama4TextModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "ConvSequenceTransform": {
    "__init__": [
      "self",
      "conv_kernel_size",
      "use_conv_bias",
      "conv1",
      "act",
      "conv_bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_position",
      "conv_state"
    ]
  },
  "SelectiveScan": {
    "forward": [
      "self",
      "ssm",
      "u",
      "dt",
      "A",
      "B",
      "C",
      "D"
    ]
  },
  "mamba_mixer_forward": [
    "self",
    "input_states",
    "cache_params",
    "cache_position",
    "attention_mask"
  ],
  "MambaPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "qwen3_moe_forward_patched": [
    "self",
    "hidden_states"
  ],
  "Qwen3MoeModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "zamba2_mamba_mixer": [
    "self",
    "hidden_states",
    "cache_params",
    "cache_position",
    "attention_mask"
  ],
  "Zamba2ModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "lfm2_short_conv_forward_patched": [
    "self",
    "x",
    "past_key_values",
    "cache_position",
    "attention_mask"
  ],
  "Lfm2ModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "GptOssModelPatcher": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "granite_moe_hybrid_update_causal_mask": [
    "self",
    "attention_mask",
    "input_tensor",
    "cache_position",
    "past_key_values",
    "output_attentions"
  ],
  "GraniteMoeHybridModelPatcher": {
    "__init__": [
      "self",
      "config",
      "model",
      "model_kwargs"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "FORCE_ATTN_MODEL_CLASSES": [],
  "core": [],
  "infer_task": [
    "task",
    "model_name_or_path",
    "subfolder",
    "revision",
    "cache_dir",
    "token",
    "library_name",
    "trust_remote_code"
  ],
  "infer_library_name": [
    "model_name_or_path",
    "subfolder",
    "revision",
    "cache_dir",
    "token"
  ],
  "main_export": [
    "model_name_or_path",
    "output",
    "task",
    "device",
    "framework",
    "cache_dir",
    "trust_remote_code",
    "pad_token_id",
    "subfolder",
    "revision",
    "force_download",
    "local_files_only",
    "use_auth_token",
    "token",
    "model_kwargs",
    "custom_export_configs",
    "fn_get_submodels",
    "ov_config",
    "stateful",
    "convert_tokenizer",
    "library_name",
    "model_loading_kwargs",
    "variant"
  ],
  "_main_quantize": [
    "model_name_or_path",
    "task",
    "library_name",
    "quantization_config",
    "output",
    "cache_dir",
    "trust_remote_code",
    "subfolder",
    "revision",
    "token",
    "model_kwargs"
  ],
  "maybe_convert_tokenizers": [
    "library_name",
    "output",
    "model",
    "preprocessors",
    "task"
  ],
  "_apply_model_size_based_quantization": [
    "submodel_paths",
    "ov_config",
    "output"
  ],
  "_merge_move": [
    "src",
    "dest"
  ],
  "IPEXCacheLayer": {
    "is_compileable": [],
    "is_sliding": [],
    "__init__": [
      "self",
      "key_cache_shape",
      "value_cache_shape",
      "device",
      "dtype",
      "supports_flash_decoding"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_seq_length": [
      "self",
      "cache_position"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "reset": [
      "self"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ]
  },
  "IPEXPagedCache": {
    "__init__": [
      "self",
      "config",
      "max_batch_size",
      "max_cache_len",
      "device",
      "dtype"
    ],
    "reshape_and_cache": [
      "self",
      "key",
      "value",
      "key_cache",
      "value_cache",
      "slots"
    ],
    "alloc_slot_for_prefill": [
      "self",
      "input_lens",
      "batch_size"
    ],
    "alloc_slot_for_decode": [
      "self",
      "batch_size"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx"
    ],
    "get_seq_length": [
      "self"
    ],
    "get_max_length": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "reorder_cache": [
      "self",
      "beam_idx"
    ],
    "crop": [
      "self",
      "maximum_length"
    ]
  },
  "_IPEX_MINIMUM_VERSION_FOR_PATCHING": [],
  "_accelerate_added_attributes": [],
  "_remove_hooks_for_ipex": [
    "module",
    "recurse"
  ],
  "_ipex_rms_layer_norm_forward": [
    "self",
    "hidden_states"
  ],
  "_falcon_for_causal_lm_forward": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "position_ids",
    "head_mask",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position",
    "logits_to_keep"
  ],
  "_gpt2_lm_head_model_forward": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "token_type_ids",
    "position_ids",
    "head_mask",
    "inputs_embeds",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "_llama_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "_falcon_model_forward": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "position_ids",
    "head_mask",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "_gpt2_model_forward": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "token_type_ids",
    "position_ids",
    "head_mask",
    "inputs_embeds",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "_qwen2_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "_mistral_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "_IPEXAttention": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "qkv_gemm": [
      "self",
      "hidden_states"
    ],
    "rope": [
      "self",
      "query",
      "key"
    ],
    "postprocess_attention_output": [
      "self",
      "attn_output"
    ],
    "has_flash_attn": [
      "self"
    ],
    "attention_interface": [
      "self",
      "query",
      "key_cache",
      "value_cache",
      "key",
      "value",
      "past_key_value",
      "attention_mask",
      "input_lens",
      "past_key_values_length",
      "seq_len_tensor",
      "query_len_tensor",
      "max_input_lens",
      "query_max_len"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_value",
      "output_attentions"
    ]
  },
  "_IPEXLlamaAttention": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "qkv_gemm": [
      "self",
      "hidden_states"
    ]
  },
  "_IPEXFalconAttention": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "qkv_gemm": [
      "self",
      "hidden_states"
    ]
  },
  "_IPEXGPT2Attention": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "qkv_gemm": [
      "self",
      "hidden_states"
    ],
    "rope": [
      "self",
      "query",
      "key"
    ],
    "postprocess_attention_output": [
      "self",
      "attn_output"
    ]
  },
  "_IPEXLlamaMLP": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual"
    ]
  },
  "_IPEXFalconMLP": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_output",
      "residual"
    ]
  },
  "_IPEXGPT2MLP": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "_IPEXLlamaDecoderLayer": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "_IPEXFalconDecoderLayer": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "_IPEXGPT2Block": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "head_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions"
    ]
  },
  "_IPEXQwen2DecoderLayer": {
    "__init__": [
      "self"
    ]
  },
  "_IPEXMistralDecoderLayer": {
    "__init__": [
      "self"
    ]
  },
  "_IPEXIntermediate": {
    "__init__": [
      "self",
      "module",
      "device",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "_TRANSFORMERS_MIN_VERSION": [],
  "_TRANSFORMERS_MAX_VERSION": [],
  "_IPEX_EXPORTED_GENERATION_TASKS": [],
  "convert_func": [
    "m",
    "func_name",
    "new_function"
  ],
  "convert_functions": [
    "m",
    "target_m",
    "new_function_name",
    "new_function"
  ],
  "convert_class": [
    "m",
    "target_m",
    "new_class",
    "device",
    "config"
  ],
  "patch_op": [
    "m",
    "target_m",
    "new_op_name",
    "new_op"
  ],
  "_patch_llama_model": [
    "model"
  ],
  "_patch_falcon_model": [
    "model"
  ],
  "_patch_gpt2_model": [
    "model"
  ],
  "_patch_qwen2_model": [
    "model"
  ],
  "_patch_mistral_model": [
    "model"
  ],
  "_patch_bert_model": [
    "model"
  ],
  "_patch_vit_model": [
    "model"
  ],
  "_patch_model": [
    "model"
  ],
  "IPEXDummyPastKeyValuesGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size",
      "sequence_length",
      "random_batch_size_range",
      "random_sequence_length_range"
    ],
    "generate": [
      "self",
      "input_name",
      "framework",
      "int_dtype",
      "float_dtype"
    ]
  },
  "IPEXDummyTextInputGenerator": {
    "__init__": [
      "self",
      "task",
      "normalized_config",
      "batch_size"
    ]
  },
  "LlamaIPEXConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": []
  },
  "FalconIPEXConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": []
  },
  "GPT2IPEXConfig": {
    "DUMMY_INPUT_GENERATOR_CLASSES": [],
    "DUMMY_PKV_GENERATOR_CLASS": []
  },
  "ipex_onnx_config": [],
  "__version__": [],
  "_import_structure": [],
  "TimmConfig": {
    "model_type": [],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir",
      "force_download",
      "local_files_only",
      "token",
      "revision"
    ]
  },
  "TimmOnnxConfig": {
    "DEFAULT_TIMM_ONNX_OPSET": [],
    "outputs": [],
    "NORMALIZED_CONFIG_CLASS": [],
    "MIN_TORCH_VERSION": [],
    "inputs": [
      "self"
    ]
  },
  "TimmForImageClassification": {
    "__init__": [
      "self",
      "config",
      "num_labels"
    ],
    "from_pretrained": [
      "cls",
      "model_name_or_path"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "TimmImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ],
    "resize": [
      "self",
      "image",
      "size",
      "resample",
      "data_format"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "return_tensors",
      "data_format"
    ]
  },
  "InputMode": {
    "LANGUAGE": [],
    "VISION": [],
    "SPEECH": [],
    "VISION_SPEECH": []
  },
  "OVModelWithEmbedForCausalLM": {
    "__init__": [
      "self",
      "model",
      "text_embeds_model",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "_ov_model_names": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "_compile_text_emb": [
      "self"
    ],
    "clear_requests": [
      "self"
    ],
    "embed_tokens": [
      "self",
      "input_ids"
    ],
    "prepare_inputs": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "position_ids",
      "inputs_embeds",
      "token_type_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "position_ids",
      "inputs_embeds"
    ]
  },
  "OVVisionEmbedding": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "OVResampler": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "image_feature",
      "pos_embed",
      "key_padding_mask"
    ]
  },
  "OVVisionProjection": {
    "_model_name": [],
    "forward": [
      "self",
      "img_features"
    ]
  },
  "OVVisionResampler": {
    "_model_name": []
  },
  "OVMultiModalProjector": {
    "_model_name": []
  },
  "OVAudioEmbeddings": {
    "_model_name": [],
    "forward": [
      "self",
      "audio_signal"
    ]
  },
  "OVAudioEncoder": {
    "_model_name": [],
    "forward": [
      "self",
      "audio_feature",
      "audio_mask"
    ]
  },
  "MODEL_PARTS_CLS_MAPPING": [],
  "OVModelForVisualCausalLM": {
    "export_feature": [],
    "additional_parts": [],
    "auto_model_class": [],
    "_all_ov_model_paths": [
      "cls"
    ],
    "__init__": [
      "self",
      "language_model",
      "text_embeddings",
      "vision_embeddings",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "clear_requests": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "_save_config": [
      "self",
      "save_directory"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "use_auth_token",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "local_files_only",
      "load_in_8bit",
      "quantization_config",
      "trust_remote_code"
    ],
    "_export": [
      "cls",
      "model_id",
      "config",
      "use_auth_token",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "use_cache",
      "trust_remote_code",
      "load_in_8bit",
      "quantization_config"
    ],
    "_component_names": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "ov_models": [
      "self"
    ],
    "reshape": [
      "self",
      "batch_size",
      "sequence_length"
    ],
    "half": [
      "self"
    ],
    "to": [
      "self",
      "device"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "past_key_values",
      "inputs_embeds",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "image_bound",
      "tgt_sizes",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas",
      "images",
      "second_per_grid_ts",
      "token_type_ids",
      "pixel_attention_mask",
      "input_image_embeds",
      "image_pixel_values",
      "image_attention_mask",
      "audio_input_features",
      "input_audio_embeds",
      "audio_embed_sizes",
      "audio_attention_mask",
      "input_mode"
    ],
    "_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ],
    "get_vision_embeddings": [
      "self",
      "pixel_values"
    ],
    "get_text_embeddings": [
      "self",
      "input_ids"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "image_sizes",
      "attention_mask"
    ],
    "can_generate": [
      "self"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ],
    "_prepare_cache_for_generation": [
      "self"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "_OVLlavaForCausalLM": {
    "__init__": [
      "self",
      "language_model",
      "text_embeddings",
      "vision_embeddings",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "position_ids",
      "legacy_processing"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values"
    ],
    "_filter_unattended_tokens": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVLlavaNextForCausalLM": {
    "pack_image_features": [
      "self",
      "image_features",
      "image_sizes",
      "image_newline"
    ],
    "add_image_features": [
      "self",
      "input_ids",
      "inputs_embeds",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "image_sizes",
      "legacy_processing"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "image_sizes"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "feature_lens",
      "input_ids",
      "attention_mask",
      "position_ids",
      "legacy_processing",
      "image_token_index"
    ],
    "get_text_embeddings": [
      "self",
      "input_ids"
    ]
  },
  "_OVLlavaNextVideoForCausalLM": {
    "additional_parts": [],
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "pack_image_features": [
      "self",
      "image_features",
      "image_sizes",
      "image_newline"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "image_sizes",
      "pixel_values_videos"
    ],
    "add_video_features": [
      "self",
      "input_ids",
      "inputs_embeds",
      "pixel_values_videos",
      "attention_mask",
      "position_ids",
      "legacy_processing"
    ],
    "get_video_features": [
      "self",
      "pixel_values",
      "input_ids"
    ]
  },
  "_OVInternVLForCausalLM": {
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "input_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ],
    "_prepare_generation_config": [
      "self",
      "generation_config",
      "use_model_defaults"
    ]
  },
  "_OVMiniCPMVForCausalLM": {
    "additional_parts": [],
    "__init__": [
      "self",
      "language_model",
      "text_embeddings",
      "vision_embeddings",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "resampling": [
      "self",
      "x",
      "tgt_sizes"
    ],
    "_set_2d_pos_cache": [
      "self",
      "max_size"
    ],
    "_adjust_pos_cache": [
      "self",
      "tgt_sizes"
    ],
    "_get_2d_sincos_pos_embed": [
      "self",
      "embed_dim",
      "image_size"
    ],
    "_get_2d_sincos_pos_embed_from_grid": [
      "self",
      "embed_dim",
      "grid"
    ],
    "_get_1d_sincos_pos_embed_from_grid_new": [
      "self",
      "embed_dim",
      "pos"
    ],
    "_prepare_vis_position_ids": [
      "self",
      "pixel_values",
      "patch_attention_mask",
      "tgt_sizes",
      "patch_size",
      "num_patches_per_side"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "input_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVMiniCPMOForCausalLM": {
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "inputs_embeds",
      "pixel_values",
      "image_sizes",
      "attention_mask",
      "audio_bounds",
      "spk_bounds",
      "audio_features",
      "audio_feature_lens"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVNanoLlavaForCausalLM": {
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVPhi3VisionForCausalLM": {
    "additional_parts": [],
    "__init__": [
      "self",
      "language_model",
      "text_embeddings",
      "vision_embeddings",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "image_sizes"
    ],
    "hd_feature_transform": [
      "self",
      "image_features",
      "image_sizes"
    ],
    "reshape_hd_patches_2x2merge": [
      "self",
      "image_features",
      "h_crop",
      "w_crop"
    ],
    "add_image_newline": [
      "self",
      "image_features_hd"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "image_sizes"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "QWen2VLModelOutputWithPast": {},
  "_OVQwen2VLForCausalLM": {
    "additional_parts": [],
    "__init__": [
      "self",
      "language_model",
      "text_embeddings",
      "vision_embeddings",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "grid_thw"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "past_key_values",
      "inputs_embeds",
      "image_sizes",
      "attention_mask",
      "position_ids",
      "image_bound",
      "tgt_sizes",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "rope_deltas"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVQwen2_5_VLForCausalLM": {
    "additional_parts": [],
    "__init__": [
      "self",
      "language_model",
      "text_embeddings",
      "vision_embeddings",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "second_per_grid_ts",
      "attention_mask"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "cache_position",
      "position_ids",
      "use_cache",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "second_per_grid_ts"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "cache_position",
      "second_per_grid_ts"
    ],
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "grid_thw"
    ],
    "get_window_index": [
      "self",
      "grid_thw"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ]
  },
  "_OVMaira2ForCausalLM": {
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVGemma3ForCausalLM": {
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ]
  },
  "_OVGotOCR2ForCausalLM": {
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVIdefics3ForCausalLM": {
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ]
  },
  "_OVSmolVLForCasualLM": {
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ]
  },
  "_OVPhi4MMForCausalLM": {
    "additional_parts": [],
    "__init__": [
      "self",
      "language_model",
      "text_embeddings",
      "vision_embeddings",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "image_embed": [
      "self",
      "input_ids",
      "image_pixel_values",
      "image_attention_mask",
      "inputs_embeds",
      "image_sizes"
    ],
    "audio_embed": [
      "self",
      "input_ids",
      "audio_input_embeds",
      "inputs_embeds",
      "audio_embed_sizes",
      "audio_projection_mode"
    ],
    "get_audio_features": [
      "self",
      "input_embeds",
      "audio_projection_mode"
    ],
    "_chunk_size_selection": [
      "self",
      "chunk_size",
      "left_chunk"
    ],
    "forward_embeddings": [
      "self",
      "xs_pad",
      "masks",
      "chunk_size_nc",
      "left_chunk_nc"
    ],
    "_streaming_mask": [
      "self",
      "seq_len",
      "batch_size",
      "chunk_size",
      "left_chunk"
    ],
    "compute_lens_change": [
      "self",
      "feature_lens"
    ],
    "calculate_hs_mask": [
      "self",
      "xs_pad",
      "mask"
    ],
    "unfold_tensor": [
      "xs_pad",
      "max_seq_len"
    ],
    "adaptive_enc_mask": [
      "x_len",
      "chunk_start_idx",
      "left_window",
      "right_window"
    ],
    "get_vision_position_ids": [
      "pixel_values",
      "patch_attention_mask",
      "patch_size",
      "num_patches_per_side"
    ],
    "embed_tokens_extend": [
      "self",
      "input_ids",
      "input_image_embeds",
      "input_audio_embeds",
      "image_sizes",
      "image_attention_mask",
      "audio_embed_sizes",
      "audio_projection_mode",
      "past_key_values"
    ],
    "get_multimodal_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "position_ids",
      "input_image_embeds",
      "image_sizes",
      "image_attention_mask",
      "input_audio_embeds",
      "audio_embed_sizes",
      "input_mode"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video",
      "audio"
    ],
    "get_img_features": [
      "self",
      "pixel_values",
      "image_attention_mask"
    ]
  },
  "_OVLlama4ForCausalLM": {
    "get_vision_embeddings": [
      "self",
      "pixel_values",
      "input_ids"
    ],
    "merge_vision_text_embeddings": [
      "self",
      "vision_embeds",
      "inputs_embeds",
      "input_ids",
      "attention_mask",
      "position_ids"
    ],
    "preprocess_inputs": [
      "text",
      "image",
      "processor",
      "tokenizer",
      "config",
      "video"
    ]
  },
  "MODEL_TYPE_TO_CLS_MAPPING": [],
  "DIFFUSION_MODEL_TRANSFORMER_SUBFOLDER": [],
  "DIFFUSION_MODEL_TEXT_ENCODER_3_SUBFOLDER": [],
  "OVDiffusionPipeline": {
    "auto_model_class": [],
    "config_name": [],
    "_library_name": [],
    "_all_ov_model_paths": [
      "cls"
    ],
    "__init__": [
      "self",
      "scheduler",
      "unet",
      "vae_decoder",
      "vae_encoder",
      "text_encoder",
      "text_encoder_2",
      "text_encoder_3",
      "transformer",
      "tokenizer",
      "tokenizer_2",
      "tokenizer_3",
      "feature_extractor",
      "force_zeros_for_empty_prompt",
      "requires_aesthetics_score",
      "add_watermarker",
      "device",
      "compile",
      "compile_only",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "_component_names": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "ov_models": [
      "self"
    ],
    "_save_pretrained": [
      "self",
      "save_directory"
    ],
    "_save_config": [
      "self",
      "save_directory"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "local_files_only",
      "cache_dir",
      "unet_file_name",
      "vae_decoder_file_name",
      "vae_encoder_file_name",
      "text_encoder_file_name",
      "text_encoder_2_file_name",
      "text_encoder_3_file_name",
      "transformer_file_name",
      "from_onnx",
      "load_in_8bit",
      "quantization_config",
      "model_save_dir",
      "trust_remote_code",
      "export_model_id"
    ],
    "_export": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "local_files_only",
      "load_in_8bit",
      "quantization_config",
      "compile_only"
    ],
    "to": [
      "self"
    ],
    "height": [
      "self"
    ],
    "width": [
      "self"
    ],
    "batch_size": [
      "self"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ],
    "_reshape_unet": [
      "self",
      "model",
      "batch_size",
      "height",
      "width",
      "num_images_per_prompt",
      "tokenizer_max_length"
    ],
    "_reshape_transformer": [
      "self",
      "model",
      "batch_size",
      "height",
      "width",
      "num_images_per_prompt",
      "tokenizer_max_length",
      "num_frames"
    ],
    "_reshape_text_encoder": [
      "self",
      "model",
      "batch_size",
      "tokenizer_max_length"
    ],
    "_reshape_vae_encoder": [
      "self",
      "model",
      "batch_size",
      "height",
      "width",
      "num_frames"
    ],
    "_reshape_vae_decoder": [
      "self",
      "model",
      "height",
      "width",
      "num_images_per_prompt",
      "num_frames"
    ],
    "reshape": [
      "self",
      "batch_size",
      "height",
      "width",
      "num_images_per_prompt",
      "num_frames"
    ],
    "half": [
      "self"
    ],
    "clear_requests": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "_load_config": [
      "cls",
      "config_name_or_path"
    ],
    "__call__": [
      "self"
    ]
  },
  "OVPipelinePart": {
    "__init__": [
      "self",
      "model",
      "parent_pipeline",
      "model_name"
    ],
    "_device": [
      "self"
    ],
    "device": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "clear_requests": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "to": [
      "self"
    ],
    "forward": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "modules": [
      "self"
    ],
    "named_modules": [
      "self"
    ]
  },
  "OVModelTextEncoder": {
    "__init__": [
      "self",
      "model",
      "parent_pipeline",
      "model_name"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "OVModelUnet": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "sample",
      "timestep",
      "encoder_hidden_states",
      "text_embeds",
      "time_ids",
      "timestep_cond",
      "cross_attention_kwargs",
      "added_cond_kwargs",
      "return_dict"
    ]
  },
  "OVModelTransformer": {
    "forward": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "pooled_projections",
      "timestep",
      "img_ids",
      "txt_ids",
      "guidance",
      "block_controlnet_hidden_states",
      "joint_attention_kwargs",
      "encoder_attention_mask",
      "num_frames",
      "height",
      "width",
      "rope_interpolation_scale",
      "video_coords",
      "attention_kwargs",
      "return_dict"
    ]
  },
  "OVModelVaeEncoder": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "sample",
      "generator",
      "return_dict"
    ]
  },
  "OVModelVaeDecoder": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "latent_sample",
      "timestep",
      "generator",
      "return_dict"
    ]
  },
  "OVModelVae": {
    "__init__": [
      "self",
      "decoder",
      "encoder"
    ],
    "_component_names": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "ov_models": [
      "self"
    ],
    "config": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "device": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "encode": [
      "self"
    ],
    "to": [
      "self"
    ]
  },
  "OVStableDiffusionPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVStableDiffusionImg2ImgPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVStableDiffusionInpaintPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVStableDiffusionXLPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": [],
    "_get_add_time_ids": [
      "self",
      "original_size",
      "crops_coords_top_left",
      "target_size",
      "dtype",
      "text_encoder_projection_dim"
    ]
  },
  "OVStableDiffusionXLImg2ImgPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": [],
    "_get_add_time_ids": [
      "self",
      "original_size",
      "crops_coords_top_left",
      "target_size",
      "aesthetic_score",
      "negative_aesthetic_score",
      "negative_original_size",
      "negative_crops_coords_top_left",
      "negative_target_size",
      "dtype",
      "text_encoder_projection_dim"
    ]
  },
  "OVStableDiffusionXLInpaintPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": [],
    "_get_add_time_ids": [
      "self",
      "original_size",
      "crops_coords_top_left",
      "target_size",
      "aesthetic_score",
      "negative_aesthetic_score",
      "negative_original_size",
      "negative_crops_coords_top_left",
      "negative_target_size",
      "dtype",
      "text_encoder_projection_dim"
    ]
  },
  "OVLatentConsistencyModelPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVLatentConsistencyModelImg2ImgPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVStableDiffusion3Pipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVStableDiffusion3Img2ImgPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVStableDiffusion3InpaintPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVFluxPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVFluxImg2ImgPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVFluxInpaintPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVFluxFillPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVSanaPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVSanaSprintPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "OVLTXPipeline": {
    "main_input_name": [],
    "export_feature": [],
    "auto_model_class": []
  },
  "SUPPORTED_OV_PIPELINES": [],
  "_get_ov_class": [
    "pipeline_class_name",
    "throw_error_if_not_exist"
  ],
  "OV_TEXT2IMAGE_PIPELINES_MAPPING": [],
  "OV_IMAGE2IMAGE_PIPELINES_MAPPING": [],
  "OV_INPAINT_PIPELINES_MAPPING": [],
  "OV_TEXT2VIDEO_PIPELINES_MAPPING": [],
  "SUPPORTED_OV_PIPELINES_MAPPINGS": [],
  "_get_task_ov_class": [
    "mapping",
    "pipeline_class_name"
  ],
  "OVPipelineForTask": {
    "auto_model_class": [],
    "config_name": [],
    "from_pretrained": [
      "cls",
      "pretrained_model_or_path"
    ]
  },
  "OVPipelineForText2Image": {
    "auto_model_class": [],
    "ov_pipelines_mapping": [],
    "export_feature": []
  },
  "OVPipelineForImage2Image": {
    "auto_model_class": [],
    "ov_pipelines_mapping": [],
    "export_feature": []
  },
  "OVPipelineForInpainting": {
    "auto_model_class": [],
    "ov_pipelines_mapping": [],
    "export_feature": []
  },
  "OVPipelineForText2Video": {
    "auto_model_class": [],
    "ov_pipelines_mapping": [],
    "export_feature": []
  },
  "OVCalibrationDataset": {
    "__init__": [
      "self",
      "calibration_dataset"
    ],
    "__getattr__": [
      "self",
      "item"
    ]
  },
  "InferRequestWrapper": {
    "__init__": [
      "self",
      "request",
      "collected_inputs",
      "apply_caching",
      "inference_result_mock"
    ],
    "collect_inputs": [
      "self",
      "inputs"
    ],
    "__call__": [
      "self"
    ],
    "infer": [
      "self",
      "inputs",
      "share_inputs"
    ],
    "start_async": [
      "self",
      "inputs",
      "userdata",
      "share_inputs"
    ],
    "wait": [
      "self"
    ],
    "get_tensor": [
      "self",
      "name"
    ],
    "reset_state": [
      "self"
    ],
    "__getattr__": [
      "self",
      "attr"
    ]
  },
  "OVCalibrationDatasetBuilder": {
    "__init__": [
      "self",
      "model",
      "seed",
      "trust_remote_code"
    ],
    "build_from_quantization_config": [
      "self",
      "config"
    ],
    "build_from_dataset_name": [
      "self",
      "quantization_config",
      "dataset_name",
      "num_samples",
      "dataset_config_name",
      "dataset_split",
      "preprocess_function",
      "preprocess_batch",
      "token",
      "cache_dir",
      "streaming",
      "batch_size",
      "data_collator",
      "remove_unused_columns"
    ],
    "build_from_dataset": [
      "self",
      "quantization_config",
      "dataset",
      "batch_size",
      "data_collator",
      "remove_unused_columns"
    ],
    "load_dataset": [
      "self",
      "dataset_name",
      "num_samples",
      "dataset_config_name",
      "dataset_split",
      "preprocess_function",
      "preprocess_batch",
      "token",
      "cache_dir",
      "streaming"
    ],
    "_get_calibration_dataloader": [
      "self",
      "dataset",
      "batch_size",
      "data_collator",
      "remove_unused_columns"
    ],
    "_prepare_decoder_calibration_data": [
      "self",
      "quantization_config",
      "dataloader"
    ],
    "_prepare_causal_lm_calibration_data": [
      "self",
      "config",
      "seqlen"
    ],
    "_prepare_visual_causal_lm_calibration_data": [
      "self",
      "config",
      "dataset",
      "max_image_size"
    ],
    "_prepare_speech_to_text_calibration_data": [
      "self",
      "config",
      "dataset"
    ],
    "_prepare_text_to_text_calibration_data": [
      "self",
      "config",
      "dataset",
      "seq_len"
    ],
    "_prepare_diffusion_calibration_data": [
      "self",
      "config",
      "dataset"
    ],
    "_remove_unused_columns": [
      "self",
      "dataset"
    ],
    "disable_progress_bar": [
      "self",
      "disable"
    ],
    "_prepare_text_encoder_model_calibration_data": [
      "self",
      "quantization_config",
      "dataset",
      "seq_len"
    ],
    "_prepare_text_image_encoder_model_calibration_data": [
      "self",
      "quantization_config",
      "dataset",
      "seq_len"
    ],
    "_prepare_sam_dataset": [
      "self",
      "config",
      "dataset"
    ],
    "_wrap_sample_as_array": [
      "sample",
      "add_batch_dim"
    ]
  },
  "OVQuantizer": {
    "__init__": [
      "self",
      "model",
      "task",
      "seed",
      "trust_remote_code"
    ],
    "task": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "model",
      "trust_remote_code"
    ],
    "quantize": [
      "self",
      "calibration_dataset",
      "save_directory",
      "ov_config",
      "file_name",
      "batch_size",
      "data_collator",
      "remove_unused_columns",
      "immediate_save"
    ],
    "_quantize_ovbasemodel": [
      "self",
      "ov_config",
      "save_directory",
      "calibration_dataset",
      "immediate_save"
    ],
    "_save_pretrained": [
      "model",
      "output_path"
    ],
    "get_calibration_dataset": [
      "self",
      "dataset_name",
      "num_samples",
      "dataset_config_name",
      "dataset_split",
      "preprocess_function",
      "preprocess_batch",
      "token",
      "cache_dir",
      "streaming"
    ],
    "_construct_pipeline_quantization_config": [
      "self",
      "quantization_config"
    ]
  },
  "_weight_only_quantization": [
    "model",
    "quantization_config",
    "calibration_dataset",
    "verify_not_optimized"
  ],
  "_full_quantization": [
    "model",
    "quantization_config",
    "calibration_dataset",
    "verify_not_optimized"
  ],
  "_get_operation_const_op": [
    "operation",
    "const_port_id"
  ],
  "_is_embedding": [
    "node"
  ],
  "_collect_ops_with_weights": [
    "model"
  ],
  "_get_hybrid_mixed_quantization_config": [
    "model",
    "quantization_config"
  ],
  "_mixed_quantization": [
    "model",
    "quantization_config",
    "dataset"
  ],
  "_verify_not_optimized": [
    "ov_model"
  ],
  "_remove_f16_kv_cache_precision_flag": [
    "model"
  ],
  "_add_nncf_version_flag": [
    "model"
  ],
  "numpy_seed": [
    "seed"
  ],
  "OV_ENCODER_NAME": [],
  "OV_DECODER_NAME": [],
  "OV_DECODER_WITH_PAST_NAME": [],
  "OV_TEXT_EMBEDDINGS_MODEL_NAME": [],
  "OV_LANGUAGE_MODEL_NAME": [],
  "OV_VISION_EMBEDDINGS_MODEL_NAME": [],
  "OV_VISION_ENCODER_MODEL_NAME": [],
  "ONNX_VISION_ENCODER_MODEL_NAME": [],
  "ONNX_PROMPT_ENCODER_MASK_DECODER_MODEL_NAME": [],
  "OV_PROMPT_ENCODER_MASK_DECODER_MODEL_NAME": [],
  "OV_TOKENIZER_NAME": [],
  "OV_DETOKENIZER_NAME": [],
  "ONNX_WEIGHTS_NAME": [],
  "ONNX_ENCODER_NAME": [],
  "ONNX_DECODER_NAME": [],
  "ONNX_DECODER_WITH_PAST_NAME": [],
  "MAX_ONNX_OPSET_2022_2_0": [],
  "MAX_ONNX_OPSET": [],
  "MIN_ONNX_QDQ_OPSET": [],
  "EXTERNAL_DATA_FORMAT_SIZE_LIMIT": [],
  "TEXTUAL_INVERSION_EMBEDDING_KEY": [],
  "TEXTUAL_INVERSION_EMBEDDING_KEYS": [],
  "OV_TO_NP_TYPE": [],
  "OV_TO_PT_TYPE": [],
  "STR_TO_OV_TYPE": [],
  "_HEAD_TO_AUTOMODELS": [],
  "PREDEFINED_CAUSAL_LANGUAGE_DATASETS": [],
  "PREDEFINED_LANGUAGE_DATASETS": [],
  "PREDEFINED_SD_DATASETS": [],
  "PREDEFINED_TEXT_IMAGE_ENCODER_DATASETS": [],
  "PREDEFINED_VISUAL_LM_DATASETS": [],
  "PREDEFINED_SPEECH_TO_TEXT_DATASETS": [],
  "PREDEFINED_SAM_DATASETS": [],
  "classproperty": {
    "__get__": [
      "self",
      "instance",
      "owner"
    ]
  },
  "maybe_convert_tokenizer_to_fast": [
    "hf_tokenizer",
    "tokenizer_path"
  ],
  "use_external_data_format": [
    "num_parameters"
  ],
  "_is_timm_ov_dir": [
    "model_dir"
  ],
  "_print_compiled_model_properties": [
    "compiled_model"
  ],
  "np_to_pt_generators": [
    "np_object",
    "device"
  ],
  "_raise_invalid_batch_size": [
    "expected_batch_size",
    "batch_size",
    "num_images_per_prompt",
    "guidance_scale"
  ],
  "get_export_transformers_version": [
    "model",
    "config"
  ],
  "model_has_dynamic_inputs": [
    "model"
  ],
  "_rmtree": [
    "path",
    "ignore_errors",
    "onerror"
  ],
  "TemporaryDirectory": {
    "__init__": [
      "self",
      "suffix",
      "prefix",
      "dir",
      "ignore_cleanup_errors"
    ],
    "_cleanup": [
      "cls",
      "name",
      "warn_message",
      "ignore_errors",
      "delete"
    ],
    "_rmtree": [
      "cls",
      "name",
      "ignore_errors",
      "repeated"
    ],
    "cleanup": [
      "self"
    ]
  },
  "check_scale_available": [
    "model"
  ],
  "_TOKENIZER_FOR_DOC": [],
  "INPUTS_DOCSTRING": [],
  "ENCODER_INPUTS_DOCSTRING": [],
  "DECODER_INPUTS_DOCSTRING": [],
  "SEQ2SEQ_MODEL_DOCSTRING": [],
  "TRANSLATION_EXAMPLE": [],
  "PIX2STRUCT_MODEL_DOCSTRING": [],
  "VISION_ENCODER_DECODER_SEQ2SEQ_MODEL_DOCSTRING": [],
  "_PROCESSOR_FOR_DOC": [],
  "PIX2STRUCT_EXAMPLE": [],
  "SPEECH_SEQ2SEQ_MODEL_DOCSTRING": [],
  "AUTOMATIC_SPEECH_RECOGNITION_EXAMPLE": [],
  "IMAGE_TO_TEXT_EXAMPLE": [],
  "OVModelForSeq2SeqLM": {
    "auto_model_class": [],
    "main_input_name": [],
    "export_feature": [],
    "_all_ov_model_paths": [
      "cls"
    ],
    "__init__": [
      "self",
      "encoder",
      "decoder",
      "decoder_with_past",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "dtype": [
      "self"
    ],
    "_component_names": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "ov_models": [
      "self"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "encoder_file_name",
      "decoder_file_name",
      "decoder_with_past_file_name",
      "local_files_only",
      "use_cache",
      "from_onnx",
      "load_in_8bit",
      "quantization_config",
      "trust_remote_code"
    ],
    "_export": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "use_cache",
      "trust_remote_code",
      "load_in_8bit",
      "quantization_config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "cache_position",
      "labels"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "head_mask",
      "decoder_head_mask",
      "cross_attn_head_mask",
      "use_cache",
      "encoder_outputs"
    ],
    "get_encoder": [
      "self"
    ],
    "_reorder_cache": [
      "self",
      "past",
      "beam_idx"
    ],
    "_reshape": [
      "self",
      "model",
      "batch_size",
      "sequence_length",
      "is_decoder"
    ],
    "reshape": [
      "self",
      "batch_size",
      "sequence_length"
    ],
    "half": [
      "self"
    ],
    "clear_requests": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "_shift_right": [
      "self",
      "input_ids"
    ],
    "_prepare_cache_for_generation": [
      "self"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "OVEncoder": {
    "__init__": [
      "self",
      "model",
      "parent_model",
      "ov_config",
      "model_name"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask"
    ]
  },
  "OVDecoder": {
    "__init__": [
      "self",
      "model",
      "parent_model",
      "ov_config",
      "model_name"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "past_key_values",
      "cache_position"
    ],
    "_get_past_length": [
      "self",
      "past_key_values"
    ],
    "compile": [
      "self"
    ],
    "_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ]
  },
  "OVModelForVision2Seq": {
    "auto_model_class": [],
    "main_input_name": [],
    "export_feature": [],
    "__init__": [
      "self",
      "encoder",
      "decoder",
      "decoder_with_past",
      "config"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask",
      "decoder_attention_mask",
      "past_key_values",
      "head_mask",
      "decoder_head_mask",
      "cross_attn_head_mask",
      "use_cache",
      "encoder_outputs"
    ],
    "forward": [
      "self",
      "pixel_values",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values"
    ],
    "_reshape": [
      "self",
      "model",
      "batch_size",
      "sequence_length",
      "is_decoder"
    ]
  },
  "OVModelForPix2Struct": {
    "auto_model_class": [],
    "main_input_name": [],
    "export_feature": [],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "flattened_patches",
      "attention_mask",
      "decoder_attention_mask",
      "past_key_values",
      "head_mask",
      "decoder_head_mask",
      "cross_attn_head_mask",
      "use_cache",
      "encoder_outputs"
    ],
    "forward": [
      "self",
      "flattened_patches",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values"
    ],
    "_reshape": [
      "self",
      "model",
      "batch_size",
      "sequence_length",
      "is_decoder"
    ]
  },
  "OVModelForSpeechSeq2Seq": {
    "auto_model_class": [],
    "main_input_name": [],
    "export_feature": [],
    "prepare_inputs_for_generation": [
      "self",
      "decoder_input_ids",
      "past_key_values",
      "attention_mask",
      "head_mask",
      "decoder_head_mask",
      "cross_attn_head_mask",
      "use_cache",
      "encoder_outputs",
      "decoder_attention_mask"
    ],
    "forward": [
      "self",
      "input_features",
      "attention_mask",
      "decoder_input_ids",
      "decoder_attention_mask",
      "encoder_outputs",
      "past_key_values",
      "cache_position"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config"
    ]
  },
  "_OVModelForWhisper": {
    "auto_model_class": [],
    "generate": [],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config"
    ],
    "model": [],
    "prepare_inputs_for_generation": [
      "self",
      "decoder_input_ids",
      "past_key_values",
      "use_cache",
      "encoder_outputs",
      "attention_mask",
      "decoder_attention_mask",
      "cache_position"
    ],
    "_get_logits_processor": [
      "self",
      "generation_config"
    ]
  },
  "TEXT_GENERATION_EXAMPLE": [],
  "OVBaseDecoderModel": {
    "__init__": [
      "self",
      "model",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "_has_cache_inputs": [
      "model"
    ],
    "_get_model_with_updated_pkv_precision": [
      "model",
      "pkv_precision"
    ],
    "update_pkv_precision": [
      "self",
      "force_fp32"
    ],
    "_save_pretrained": [
      "self",
      "save_directory"
    ],
    "_export": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "use_cache",
      "trust_remote_code",
      "load_in_8bit",
      "quantization_config"
    ],
    "_reshape": [
      "self",
      "model",
      "batch_size",
      "sequence_length",
      "height",
      "width"
    ],
    "reshape": [
      "self",
      "batch_size",
      "sequence_length"
    ],
    "normalized_config": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "_make_stateful": [
      "self"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "OVModelForCausalLM": {
    "export_feature": [],
    "auto_model_class": [],
    "prepare_inputs": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "position_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "position_ids",
      "token_type_ids"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder"
    ],
    "_expand_outputs_for_generation": [
      "self",
      "indicies",
      "logits",
      "past_key_values"
    ],
    "_deduplicate_inputs": [
      "self",
      "model_inputs"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus",
      "assistant_model",
      "streamer",
      "negative_prompt_ids",
      "negative_prompt_attention_mask"
    ],
    "_get_past_length": [
      "self",
      "past_key_values"
    ],
    "_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ],
    "_prepare_cache_for_generation": [
      "self"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "file_name",
      "subfolder",
      "from_onnx",
      "local_files_only",
      "load_in_8bit",
      "compile_only",
      "quantization_config",
      "trust_remote_code"
    ]
  },
  "OVBloomForCausalLM": {
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values"
    ],
    "_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ],
    "_convert_to_bloom_cache": [
      "past_key_value"
    ],
    "_convert_to_standard_cache": [
      "self",
      "past_key_value",
      "batch_size"
    ],
    "_expand_outputs_for_generation": [
      "self",
      "indicies",
      "logits",
      "past_key_values"
    ]
  },
  "OVGPTBigCodeForCausalLM": {
    "_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ]
  },
  "OVCacheWithMambaStates": {
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device",
      "max_batch_size",
      "conv_states",
      "ssm_states",
      "key_cache",
      "value_cache"
    ]
  },
  "OVOutputWithMambaStates": {},
  "OVModelWithMambaForCausalLM": {
    "__init__": [
      "self",
      "model",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "compile": [
      "self"
    ],
    "_has_cache_inputs": [
      "model"
    ],
    "prepare_inputs": [
      "self",
      "input_ids",
      "attention_mask",
      "cache_params",
      "use_cache",
      "cache_position"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "cache_params",
      "use_cache",
      "cache_position"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "num_new_tokens"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "inputs_embeds",
      "use_cache",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "OVMambaForCausalLM": {
    "__init__": [
      "self",
      "model",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ]
  },
  "OVSamVisionEncoder": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "OVSamPromptEncoder": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "input_points",
      "input_labels"
    ]
  },
  "OVSamModel": {
    "export_feature": [],
    "auto_model_class": [],
    "_all_ov_model_paths": [
      "cls"
    ],
    "__init__": [
      "self",
      "vision_encoder_model",
      "prompt_encoder_mask_decoder_model",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "clear_requests": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "_save_config": [
      "self",
      "save_directory"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "use_auth_token",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "vision_encoder_file_name",
      "prompt_encoder_mask_decoder_file_name",
      "local_files_only",
      "load_in_8bit",
      "quantization_config",
      "trust_remote_code"
    ],
    "_component_names": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "ov_models": [
      "self"
    ],
    "reshape": [
      "self",
      "batch_size",
      "point_batch_size",
      "num_points_per_image"
    ],
    "half": [
      "self"
    ],
    "forward": [
      "self",
      "pixel_values",
      "input_points",
      "input_labels",
      "image_embeddings"
    ],
    "get_image_wide_positional_embeddings": [
      "self"
    ],
    "get_image_features": [
      "self",
      "pixel_values"
    ],
    "is_dynamic": [
      "self"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "OVModelOpenCLIPBase": {
    "config_name": [],
    "_library_name": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "_load_config": [
      "cls",
      "config_name_or_path",
      "revision",
      "cache_dir",
      "token",
      "force_download",
      "subfolder",
      "trust_remote_code",
      "local_files_only"
    ],
    "from_pretrained": [
      "cls",
      "model_id",
      "export",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "trust_remote_code"
    ]
  },
  "OVModelOpenCLIPText": {
    "export_feature": [],
    "_all_ov_model_paths": [
      "cls"
    ],
    "__init__": [
      "self",
      "model",
      "config",
      "tokenize_cfg"
    ],
    "_export": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "trust_remote_code",
      "load_in_8bit",
      "quantization_config"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "file_name",
      "subfolder",
      "from_onnx",
      "local_files_only",
      "load_in_8bit",
      "quantization_config"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "OVModelOpenCLIPVisual": {
    "export_feature": [],
    "_all_ov_model_paths": [
      "cls"
    ],
    "__init__": [
      "self",
      "model",
      "config",
      "preprocess_cfg"
    ],
    "_export": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "trust_remote_code",
      "load_in_8bit",
      "quantization_config"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "file_name",
      "subfolder",
      "from_onnx",
      "local_files_only",
      "load_in_8bit",
      "quantization_config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "OVModelOpenCLIPForZeroShotImageClassification": {
    "export_feature": [],
    "__init__": [
      "self",
      "text_model",
      "visual_model",
      "config",
      "init_logit_scale",
      "init_logit_bias"
    ],
    "_component_names": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "ov_models": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "to": [
      "self",
      "device"
    ],
    "from_pretrained": [
      "cls",
      "model_id",
      "export",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "trust_remote_code",
      "init_logit_scale",
      "init_logit_bias"
    ],
    "forward": [
      "self",
      "input_ids",
      "pixel_values"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "push_to_hub"
    ],
    "push_to_hub": [
      "self",
      "save_directory",
      "repository_id",
      "private",
      "token"
    ],
    "compile": [
      "self"
    ],
    "reshape": [
      "self",
      "batch_size",
      "sequence_length",
      "height",
      "width"
    ],
    "half": [
      "self"
    ],
    "eval": [
      "self"
    ],
    "can_generate": [
      "self"
    ]
  },
  "OVSentenceTransformer": {
    "export_feature": [],
    "_library_name": [],
    "__init__": [
      "self",
      "model",
      "config",
      "tokenizer"
    ],
    "_save_pretrained": [
      "self",
      "save_directory"
    ],
    "forward": [
      "self",
      "inputs"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "file_name",
      "subfolder",
      "from_onnx",
      "local_files_only",
      "load_in_8bit",
      "quantization_config"
    ],
    "tokenize": [
      "self",
      "texts",
      "padding"
    ],
    "get_model_kwargs": [
      "self"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "_FEATURE_EXTRACTOR_FOR_DOC": [],
  "MODEL_START_DOCSTRING": [],
  "IMAGE_INPUTS_DOCSTRING": [],
  "AUDIO_INPUTS_DOCSTRING": [],
  "OVModel": {
    "base_model_prefix": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "SEQUENCE_CLASSIFICATION_EXAMPLE": [],
  "OVModelForSequenceClassification": {
    "export_feature": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids"
    ]
  },
  "QUESTION_ANSWERING_EXAMPLE": [],
  "OVModelForQuestionAnswering": {
    "export_feature": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids"
    ]
  },
  "TOKEN_CLASSIFICATION_EXAMPLE": [],
  "OVModelForTokenClassification": {
    "export_feature": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids"
    ]
  },
  "FEATURE_EXTRACTION_EXAMPLE": [],
  "OVModelForFeatureExtraction": {
    "export_feature": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "MASKED_LM_EXAMPLE": [],
  "OVModelForMaskedLM": {
    "export_feature": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "IMAGE_CLASSIFICATION_EXAMPLE": [],
  "OVModelForImageClassification": {
    "export_feature": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "from_pretrained": [
      "cls",
      "model_id",
      "export",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "trust_remote_code"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "AUDIO_CLASSIFICATION_EXAMPLE": [],
  "OVModelForAudioClassification": {
    "export_feature": [],
    "auto_model_class": [],
    "__init__": [
      "self",
      "model",
      "config"
    ],
    "forward": [
      "self",
      "input_values",
      "attention_mask"
    ]
  },
  "CTC_EXAMPLE": [],
  "OVModelForCTC": {
    "auto_model_class": [],
    "export_feature": [],
    "forward": [
      "self",
      "input_values",
      "attention_mask"
    ]
  },
  "AUDIO_XVECTOR_EXAMPLE": [],
  "OVModelForAudioXVector": {
    "auto_model_class": [],
    "export_feature": [],
    "forward": [
      "self",
      "input_values",
      "attention_mask"
    ]
  },
  "AUDIO_FRAME_CLASSIFICATION_EXAMPLE": [],
  "OVModelForAudioFrameClassification": {
    "auto_model_class": [],
    "export_feature": [],
    "forward": [
      "self",
      "input_values",
      "attention_mask"
    ]
  },
  "CUSTOM_TASKS_EXAMPLE": [],
  "OVModelForCustomTasks": {
    "forward": [
      "self"
    ]
  },
  "OVModelForZeroShotImageClassification": {
    "auto_model_class": [],
    "export_feature": [],
    "forward": [
      "self",
      "input_ids",
      "pixel_values",
      "attention_mask"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ]
  },
  "InsertTextEmbedding": {
    "__init__": [
      "self",
      "tokens_ids",
      "embeddings"
    ]
  },
  "OVTextualInversionLoaderMixin": {
    "load_textual_inversion": [
      "self",
      "pretrained_model_name_or_path",
      "token",
      "tokenizer",
      "text_encoder"
    ]
  },
  "OVTextToSpeechEncoder": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "input_ids"
    ]
  },
  "OVTextToSpeechDecoder": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "speaker_embeddings",
      "encoder_last_hidden_state",
      "encoder_attention_mask"
    ],
    "reset_state": [
      "self"
    ]
  },
  "OVTextToSpeechPostNet": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "spectrograms"
    ]
  },
  "OVTextToSpeechVocoder": {
    "_model_name": [],
    "__init__": [
      "self",
      "model",
      "parent_model"
    ],
    "forward": [
      "self",
      "spectrogram"
    ]
  },
  "OVModelForTextToSpeechSeq2Seq": {
    "auto_model_class": [],
    "export_feature": [],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config"
    ],
    "reshape": [
      "self"
    ]
  },
  "_OVModelForSpeechT5ForTextToSpeech": {
    "_all_ov_model_paths": [
      "cls"
    ],
    "main_input_name": [],
    "_supports_cache_class": [],
    "__init__": [
      "self",
      "encoder",
      "decoder",
      "postnet",
      "vocoder",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "clear_requests": [
      "self"
    ],
    "compile": [
      "self"
    ],
    "_component_names": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "ov_models": [
      "self"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "local_files_only",
      "load_in_8bit",
      "quantization_config",
      "trust_remote_code"
    ],
    "generate": [
      "self",
      "input_ids",
      "attention_mask",
      "speaker_embeddings",
      "threshold",
      "minlenratio",
      "maxlenratio",
      "vocoder",
      "output_cross_attentions",
      "return_output_lengths"
    ]
  },
  "OVModelHostMixin": {
    "_all_ov_model_paths": [
      "cls"
    ],
    "ov_models": [
      "self"
    ],
    "_ov_model_paths": [
      "self"
    ],
    "_ov_model_names": [
      "self"
    ],
    "_component_names": [
      "self"
    ],
    "components": [
      "self"
    ],
    "replace_ov_model": [
      "self",
      "current_model",
      "new_model"
    ],
    "_unload_ov_model": [
      "self",
      "ov_model"
    ],
    "clear_requests": [
      "self"
    ],
    "compile": [
      "self"
    ]
  },
  "OVBaseModel": {
    "auto_model_class": [],
    "export_feature": [],
    "_supports_cache_class": [],
    "_is_stateful": [],
    "_library_name": [],
    "_search_pattern": [],
    "__init__": [
      "self",
      "model",
      "config",
      "device",
      "dynamic_shapes",
      "ov_config",
      "model_save_dir",
      "quantization_config"
    ],
    "device": [
      "self"
    ],
    "to": [
      "self",
      "device"
    ],
    "clear_requests": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "load_model": [
      "file_name",
      "quantization_config"
    ],
    "_compile_model": [
      "model",
      "device",
      "ov_config",
      "model_save_dir"
    ],
    "_save_pretrained": [
      "self",
      "save_directory"
    ],
    "_save_openvino_config": [
      "self",
      "save_directory"
    ],
    "_from_pretrained": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "file_name",
      "subfolder",
      "from_onnx",
      "local_files_only",
      "load_in_8bit",
      "quantization_config",
      "trust_remote_code"
    ],
    "from_pretrained": [
      "cls",
      "model_id",
      "export",
      "force_download",
      "use_auth_token",
      "token",
      "cache_dir",
      "subfolder",
      "config",
      "local_files_only",
      "trust_remote_code",
      "revision"
    ],
    "_resolve_default_quantization_config": [
      "model_name_or_path",
      "quantization_config"
    ],
    "_preprocess_quantization_config": [
      "self",
      "quantization_config",
      "model_name_or_path"
    ],
    "_apply_quantization": [
      "self",
      "quantization_config",
      "compile_only",
      "compile_model",
      "model_name_or_path",
      "trust_remote_code"
    ],
    "_set_ov_config_parameters": [
      "self"
    ],
    "_cached_file": [
      "model_path",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "file_name",
      "subfolder",
      "local_files_only"
    ],
    "_export": [
      "cls",
      "model_id",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "task",
      "trust_remote_code",
      "load_in_8bit",
      "quantization_config"
    ],
    "_to_load": [
      "cls",
      "model",
      "config",
      "onnx_config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "local_files_only",
      "stateful"
    ],
    "compile": [
      "self"
    ],
    "_reshape": [
      "self",
      "model",
      "batch_size",
      "sequence_length",
      "height",
      "width"
    ],
    "reshape": [
      "self",
      "batch_size",
      "sequence_length",
      "height",
      "width"
    ],
    "half": [
      "self"
    ],
    "eval": [
      "self"
    ],
    "forward": [
      "self"
    ],
    "can_generate": [
      "self"
    ],
    "_inference": [
      "self",
      "inputs"
    ],
    "_incompatible_inputs_warning": [
      "self",
      "inputs"
    ]
  },
  "OVModelPart": {
    "__init__": [
      "self",
      "model",
      "parent_model",
      "ov_config",
      "model_name",
      "model_dir"
    ],
    "compile": [
      "self"
    ],
    "_device": [
      "self"
    ],
    "device": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "forward": [
      "self"
    ],
    "clear_requests": [
      "self"
    ]
  },
  "OVQuantizationMethod": {
    "DEFAULT": [],
    "HYBRID": [],
    "AWQ": []
  },
  "_DEFAULT_4BIT_WQ_CONFIGS": [],
  "model_id_aliases": [],
  "_DEFAULT_4BIT_WQ_CONFIG": [],
  "_DEFAULT_INT8_FQ_CONFIGS": [],
  "_DEFAULT_IGNORED_SCOPE_CONFIGS": [],
  "_get_model_id_candidates": [
    "model_id_or_path"
  ],
  "get_default_quantization_config": [
    "model_id_or_path",
    "weight_format",
    "quant_mode"
  ],
  "_merge_ignored_scopes": [
    "ignored_scope_1",
    "ignored_scope_2"
  ],
  "_apply_default_ignored_scope_config": [
    "model_id_or_path",
    "quantization_config"
  ],
  "OVQuantizationConfigBase": {
    "quant_method": [],
    "__init__": [
      "self",
      "ignored_scope",
      "num_samples",
      "dataset",
      "tokenizer",
      "processor"
    ],
    "post_init": [
      "self"
    ],
    "get_ignored_scope_instance": [
      "self"
    ],
    "clone": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "_dataclasses_to_dict": [
      "d"
    ]
  },
  "OVWeightQuantizationConfig": {
    "__init__": [
      "self",
      "bits",
      "sym",
      "group_size",
      "tokenizer",
      "dataset",
      "ratio",
      "all_layers",
      "sensitivity_metric",
      "ignored_scope",
      "num_samples",
      "quant_method",
      "scale_estimation",
      "dtype",
      "gptq",
      "processor",
      "lora_correction",
      "backup_precision",
      "statistics_path",
      "group_size_fallback"
    ],
    "post_init": [
      "self"
    ],
    "to_nncf_dict": [
      "self"
    ]
  },
  "OVDynamicQuantizationConfig": {
    "__init__": [
      "self",
      "bits",
      "sym",
      "weights_group_size",
      "activations_group_size"
    ]
  },
  "OVQuantizationConfig": {
    "__init__": [
      "self",
      "bits",
      "sym",
      "ignored_scope",
      "num_samples",
      "model_type",
      "fast_bias_correction",
      "overflow_fix",
      "dataset",
      "tokenizer",
      "processor",
      "smooth_quant_alpha",
      "dtype"
    ],
    "post_init": [
      "self"
    ],
    "to_nncf_dict": [
      "self"
    ]
  },
  "OVConfig": {
    "CONFIG_NAME": [],
    "FULL_CONFIGURATION_FILE": [],
    "__init__": [
      "self",
      "input_info",
      "save_onnx_model",
      "quantization_config",
      "dtype"
    ],
    "add_input_info": [
      "self",
      "model_inputs",
      "force_batch_one"
    ],
    "_to_dict_safe": [
      "self",
      "to_diff_dict"
    ],
    "_get_dtype": [
      "quantization_config"
    ],
    "to_dict": [
      "self"
    ],
    "to_diff_dict": [
      "self"
    ]
  },
  "OVMixedQuantizationConfig": {
    "__init__": [
      "self",
      "weight_quantization_config",
      "full_quantization_config",
      "ignored_scope",
      "num_samples",
      "dataset",
      "tokenizer",
      "processor"
    ],
    "post_init": [
      "self"
    ],
    "_initialize_quantization_config": [
      "config",
      "config_type"
    ],
    "to_dict": [
      "self"
    ]
  },
  "OVPipelineQuantizationConfig": {
    "__init__": [
      "self",
      "quantization_configs",
      "default_config",
      "num_samples",
      "dataset",
      "tokenizer",
      "processor"
    ],
    "to_dict": [
      "self"
    ],
    "post_init": [
      "self"
    ]
  },
  "_GPTOSSQuantizationConfig": {
    "__init__": [
      "self",
      "quantization_config1",
      "quantization_config2"
    ],
    "to_dict": [
      "self"
    ]
  },
  "_quantization_config_from_dict": [
    "config_dict"
  ],
  "INCStableDiffusionPipeline": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModel": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForCausalLM": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForMaskedLM": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForMultipleChoice": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForQuestionAnswering": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForSeq2SeqLM": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForSequenceClassification": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForTokenClassification": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCModelForVision2Seq": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCQuantizer": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCSeq2SeqTrainer": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCTrainer": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "INCConfig": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "STR_OPERATION_TO_FUNC": [],
  "_optimum_version": [],
  "_optimum_intel_version": [],
  "_transformers_available": [],
  "_transformers_version": [],
  "_tokenizers_available": [],
  "_tokenizers_version": [],
  "_torch_available": [],
  "_torch_version": [],
  "_neural_compressor_available": [],
  "_neural_compressor_version": [],
  "_ipex_available": [],
  "_ipex_version": [],
  "_openvino_available": [],
  "_openvino_version": [],
  "_nncf_available": [],
  "_nncf_version": [],
  "_diffusers_available": [],
  "_diffusers_version": [],
  "_open_clip_available": [],
  "_open_clip_version": [],
  "_safetensors_version": [],
  "_safetensors_available": [],
  "_timm_available": [],
  "_timm_version": [],
  "_datasets_available": [],
  "_datasets_version": [],
  "_pillow_available": [],
  "_accelerate_available": [],
  "_accelerate_version": [],
  "_numa_available": [],
  "_psutil_available": [],
  "_sentence_transformers_available": [],
  "_sentence_transformers_version": [],
  "_langchain_hf_available": [],
  "_langchain_hf_version": [],
  "is_transformers_available": [],
  "is_tokenizers_available": [],
  "is_neural_compressor_available": [],
  "is_ipex_available": [],
  "is_openvino_available": [],
  "is_openvino_tokenizers_available": [],
  "is_nncf_available": [],
  "is_diffusers_available": [],
  "is_open_clip_available": [],
  "is_safetensors_available": [],
  "is_timm_available": [],
  "is_datasets_available": [],
  "is_pillow_available": [],
  "is_accelerate_available": [],
  "is_sentence_transformers_available": [],
  "is_numa_available": [],
  "is_psutil_available": [],
  "compare_versions": [
    "library_or_version",
    "operation",
    "requirement_version"
  ],
  "is_transformers_version": [
    "operation",
    "version"
  ],
  "is_tokenizers_version": [
    "operation",
    "version"
  ],
  "is_optimum_version": [
    "operation",
    "version"
  ],
  "is_neural_compressor_version": [
    "operation",
    "version"
  ],
  "is_openvino_version": [
    "operation",
    "version"
  ],
  "is_nncf_version": [
    "operation",
    "version"
  ],
  "is_openvino_tokenizers_version": [
    "operation",
    "version"
  ],
  "is_diffusers_version": [
    "operation",
    "version"
  ],
  "is_torch_version": [
    "operation",
    "version"
  ],
  "is_ipex_version": [
    "operation",
    "version"
  ],
  "is_timm_version": [
    "operation",
    "version"
  ],
  "is_datasets_version": [
    "operation",
    "version"
  ],
  "is_sentence_transformers_version": [
    "operation",
    "version"
  ],
  "DIFFUSERS_IMPORT_ERROR": [],
  "IPEX_IMPORT_ERROR": [],
  "NNCF_IMPORT_ERROR": [],
  "OPENVINO_IMPORT_ERROR": [],
  "NEURAL_COMPRESSOR_IMPORT_ERROR": [],
  "DATASETS_IMPORT_ERROR": [],
  "PILLOW_IMPORT_ERROR": [],
  "ACCELERATE_IMPORT_ERROR": [],
  "SENTENCE_TRANSFORMERS_IMPORT_ERROR": [],
  "BACKENDS_MAPPING": [],
  "requires_backends": [
    "obj",
    "backends"
  ],
  "DummyObject": {
    "__getattr__": [
      "cls",
      "key"
    ]
  },
  "IPEXModel": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForSequenceClassification": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForTokenClassification": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForMaskedLM": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForCausalLM": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForSeq2SeqLM": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForQuestionAnswering": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForImageClassification": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXModelForAudioClassification": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "IPEXSentenceTransformer": {
    "_backends": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls"
    ]
  },
  "MULTI_QUERY_ATTN_MODELS": [],
  "get_model_device": [
    "model"
  ],
  "recursive_to_device": [
    "value",
    "device"
  ],
  "_setattr_from_module": [
    "new_module",
    "module"
  ],
  "_find_files_matching_pattern": [
    "model_name_or_path",
    "pattern",
    "subfolder",
    "use_auth_token",
    "revision"
  ],
  "replace_customized_linear_with_linear": [
    "model"
  ],
  "get_int_from_env": [
    "env_keys",
    "default"
  ],
  "bind_cores_for_best_perf": [],
  "_infer_library_from_model_name_or_path": [
    "model_name_or_path",
    "subfolder",
    "revision",
    "cache_dir",
    "token"
  ],
  "_infer_library_from_model_or_model_class": [
    "model",
    "library_name"
  ],
  "infer_library_from_model": [
    "model",
    "subfolder",
    "revision",
    "cache_dir",
    "token"
  ],
  "collect_open_clip_model_files": [
    "model_name_or_path"
  ],
  "_OpenClipForZeroShotImageClassification": {
    "__init__": [
      "self",
      "config"
    ],
    "load_config_from_file": [
      "config_path",
      "model_name"
    ],
    "find_config_by_hub_url": [
      "model_path"
    ],
    "create_open_clip_model": [
      "model_path",
      "open_clip_config",
      "dtype"
    ],
    "from_pretrained": [
      "cls",
      "model_name_or_path",
      "config",
      "token",
      "revision",
      "force_download",
      "cache_dir",
      "subfolder",
      "local_files_only",
      "dtype"
    ]
  },
  "_TASK_ALIASES": [],
  "_TASK_LEGACY": [],
  "WEIGHTS_NAME": [],
  "DIFFUSION_WEIGHTS_NAME": [],
  "TRAINING_ARGS_NAME": [],
  "MIN_QDQ_ONNX_OPSET": [],
  "IPEXTransformer": {
    "__init__": [
      "self"
    ],
    "_load_model": [
      "self",
      "model_name_or_path",
      "config",
      "cache_dir",
      "backend",
      "is_peft_model"
    ],
    "_load_ipex_model": [
      "self",
      "model_name_or_path",
      "config",
      "cache_dir"
    ]
  },
  "_IPEX_SUPPORT_MODEL_TYPES": [],
  "_IPEX_EXPORTED_GENERATION_METHODS": [],
  "_IPEX_MINIMUM_VERSION_FOR_COMPILE": [],
  "_is_patched_with_ipex": [
    "model",
    "task",
    "use_cache"
  ],
  "get_float_type": [
    "model_dtype"
  ],
  "prepare_jit_inputs": [
    "model",
    "task",
    "use_cache"
  ],
  "INCQuantizationMode": {
    "DYNAMIC": [],
    "STATIC": [],
    "AWARE_TRAINING": [],
    "WEIGHT_ONLY": []
  },
  "SUPPORTED_QUANT_MODE": [],
  "CONFIG_NAME": [],
  "QUANTIZATION_CONFIG_NAME": [],
  "IPEX_MINIMUM_VERSION": [],
  "NEURAL_COMPRESSOR_MINIMUM_VERSION": [],
  "NEURAL_COMPRESSOR_WEIGHT_ONLY_MINIMUM_VERSION": [],
  "INCDataLoader": {
    "use_label": [],
    "from_pytorch_dataloader": [
      "cls",
      "dataloader",
      "use_label"
    ],
    "__iter__": [
      "self"
    ]
  },
  "load_quantized_model": [
    "checkpoint_dir_or_file",
    "model"
  ],
  "_quantize": [
    "args"
  ],
  "_quantization_model": [],
  "NeuralCoderAdaptor": {
    "default_quant_dynamic": [],
    "default_quant_static": []
  },
  "pipeline": [
    "task",
    "model",
    "config",
    "tokenizer",
    "feature_extractor",
    "image_processor",
    "processor",
    "revision",
    "use_fast",
    "token",
    "device",
    "torch_dtype",
    "trust_remote_code",
    "model_kwargs",
    "pipeline_class",
    "accelerator"
  ],
  "get_openvino_model_class": [
    "task",
    "config",
    "model_id"
  ],
  "openvino_infer_framework_load_model": [
    "model",
    "config",
    "task"
  ],
  "get_ipex_model_class": [
    "task"
  ],
  "ipex_infer_framework_load_model": [
    "model",
    "config",
    "task"
  ],
  "patch_pipelines_to_load_accelerator_model": [
    "accelerator"
  ]
}