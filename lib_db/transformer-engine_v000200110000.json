{
  "__version__": [],
  "DeprecatedEnum": {
    "__init__": [
      "self",
      "enum_cls",
      "msg"
    ],
    "__iter__": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ]
  },
  "deprecate_wrapper": [
    "obj",
    "msg"
  ],
  "_is_package_installed": [
    "package"
  ],
  "_is_package_installed_from_wheel": [
    "package"
  ],
  "_find_shared_object_in_te_dir": [
    "te_path",
    "prefix"
  ],
  "_get_shared_object_file": [
    "library"
  ],
  "get_te_core_package_info": [],
  "load_framework_extension": [
    "framework"
  ],
  "sanity_checks_for_pypi_installation": [],
  "_get_sys_extension": [],
  "_nvidia_cudart_include_dir": [],
  "_load_cuda_library_from_python": [
    "lib_name",
    "strict"
  ],
  "_load_cuda_library_from_system": [
    "lib_name"
  ],
  "_load_cuda_library": [
    "lib_name"
  ],
  "_load_core_library": [],
  "online_softmax_kernel": [
    "X_ptr",
    "X_stride",
    "Y_ptr",
    "Y_stride",
    "m_d_X_y_ptr",
    "m_d_X_y_stride",
    "rank",
    "n_cols",
    "BLOCK_SIZE"
  ],
  "cross_entropy_kernel": [
    "X_ptr",
    "X_stride",
    "Y_ptr",
    "Y_stride",
    "loss_ptr",
    "loss_stride",
    "m_d_X_y_ptr",
    "m_d_X_y_stride",
    "rank",
    "world_size",
    "ignore_idx",
    "n_cols",
    "n_non_ignore",
    "reduce_loss",
    "label_smoothing",
    "BLOCK_SIZE"
  ],
  "element_mul_kernel": [
    "X_ptr",
    "X_stride",
    "grad_output_ptr",
    "grad_output_stride",
    "n_cols",
    "BLOCK_SIZE"
  ],
  "zero_pad_kernel": [
    "inp_ptr",
    "out_ptr",
    "in_dim0",
    "in_dim1",
    "out_dim0",
    "out_dim1",
    "in_s0",
    "in_s1",
    "out_s0",
    "out_s1",
    "BLOCK_M",
    "BLOCK_N"
  ],
  "get_int_dtype": [],
  "_compare_and_swap": [
    "x",
    "indices",
    "flip",
    "i",
    "n_dims"
  ],
  "_bitonic_merge": [
    "x",
    "indices",
    "stage",
    "order",
    "n_dims"
  ],
  "_argsort": [
    "x",
    "indices",
    "n_dims"
  ],
  "_row_id_map_pass_1_kernel": [
    "routing_map_ptr",
    "num_tokens",
    "stride_routing_map_token",
    "stride_routing_map_expert",
    "stride_row_id_map_token",
    "stride_row_id_map_expert",
    "row_id_map_ptr",
    "workspace_ptr",
    "BLOCK_SIZE"
  ],
  "_row_id_map_pass_2_kernel": [
    "row_id_map_ptr",
    "workspace_ptr",
    "num_tokens",
    "stride_row_id_map_token",
    "stride_row_id_map_expert",
    "WORKSPACE_LOAD_WIDTH",
    "BLOCK_SIZE"
  ],
  "_row_id_map_pass_3_kernel": [
    "row_id_map_ptr",
    "stride_row_id_map_token",
    "stride_row_id_map_expert",
    "num_experts",
    "LOAD_SIZE"
  ],
  "_permute_kernel": [
    "input_ptr",
    "row_id_map_ptr",
    "probs_ptr",
    "scale_ptr",
    "permuted_scale_ptr",
    "scale_hidden_dim",
    "stride_row_id_map_token",
    "stride_row_id_map_expert",
    "stride_input_token",
    "stride_input_hidden",
    "stride_output_token",
    "stride_output_hidden",
    "stride_probs_token",
    "stride_probs_expert",
    "stride_scale_token",
    "stride_scale_hidden",
    "stride_permuted_probs_token",
    "stride_permuted_scale_token",
    "stride_permuted_scale_hidden",
    "output_ptr",
    "permuted_probs_ptr",
    "num_experts",
    "hidden_size",
    "PERMUTE_PROBS",
    "PERMUTE_SCALE",
    "BLOCK_SIZE"
  ],
  "_unpermute_kernel": [
    "input_ptr",
    "row_id_map_ptr",
    "merging_probs_ptr",
    "permuted_probs_ptr",
    "stride_row_id_map_token",
    "stride_row_id_map_expert",
    "stride_input_token",
    "stride_input_hidden",
    "stride_output_token",
    "stride_output_hidden",
    "stride_merging_probs_token",
    "stride_merging_probs_expert",
    "stride_permuted_probs_token",
    "stride_unpermuted_probs_token",
    "stride_unpermuted_probs_expert",
    "output_ptr",
    "unpermuted_probs_ptr",
    "num_experts",
    "hidden_size",
    "PROBS_LOAD_WIDTH",
    "WITH_MERGING_PROBS",
    "PERMUTE_PROBS",
    "BLOCK_SIZE"
  ],
  "_unpermute_bwd_with_merging_probs_kernel": [
    "fwd_output_grad_ptr",
    "fwd_input_ptr",
    "merging_probs_ptr",
    "row_id_map_ptr",
    "stride_row_id_map_token",
    "stride_row_id_map_expert",
    "stride_fwd_output_grad_token",
    "stride_fwd_output_grad_hidden",
    "stride_fwd_input_grad_token",
    "stride_fwd_input_grad_hidden",
    "stride_fwd_input_token",
    "stride_fwd_input_hidden",
    "stride_merging_probs_token",
    "stride_merging_probs_expert",
    "stride_merging_probs_grad_token",
    "stride_merging_probs_grad_expert",
    "fwd_input_grad_ptr",
    "merging_probs_grad_ptr",
    "num_experts",
    "hidden_size",
    "PROBS_LOAD_WIDTH",
    "BLOCK_SIZE"
  ],
  "_make_chunk_sort_map_kernel": [
    "split_sizes_ptr",
    "sorted_indices_ptr",
    "dst_rows_ptr",
    "num_splits",
    "IDX_LOAD_WIDTH"
  ],
  "_sort_chunks_by_map_kernel": [
    "input_ptr",
    "row_id_map_ptr",
    "probs_ptr",
    "stride_input_token",
    "stride_input_hidden",
    "stride_output_token",
    "stride_output_hidden",
    "stride_probs_token",
    "stride_permuted_probs_token",
    "output_ptr",
    "permuted_probs_ptr",
    "hidden_size",
    "PERMUTE_PROBS",
    "BLOCK_SIZE",
    "FORWARD"
  ],
  "_FormatHelper": {},
  "Format": {
    "E2M1": [],
    "E4M3": [],
    "E5M2": [],
    "HYBRID": []
  },
  "MMParams": {},
  "QParams": {
    "__repr__": [
      "self"
    ]
  },
  "Recipe": {
    "nvfp4": [
      "self"
    ],
    "mxfp8": [
      "self"
    ],
    "delayed": [
      "self"
    ],
    "float8_current_scaling": [
      "self"
    ],
    "float8_per_tensor_scaling": [
      "self"
    ],
    "float8_block_scaling": [
      "self"
    ],
    "custom": [
      "self"
    ]
  },
  "DelayedScaling": {
    "__post_init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Float8CurrentScaling": {
    "fp8_quant_fwd_inp": [],
    "fp8_quant_fwd_weight": [],
    "fp8_quant_bwd_grad": [],
    "__post_init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "MXFP8BlockScaling": {
    "__post_init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Float8BlockScaling": {
    "fp8_quant_fwd_inp": [],
    "fp8_quant_fwd_weight": [],
    "fp8_quant_bwd_grad": [],
    "__post_init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "NVFP4BlockScaling": {
    "__post_init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "CustomRecipe": {
    "__repr__": [
      "self"
    ]
  },
  "aten": [],
  "_tensor_to_gemm_names_map": [],
  "API_CALL_MODIFY": [],
  "STANDARD_FP8_QUANTIZE": [],
  "HIGH_PRECISION": [],
  "DebugQuantizer": {
    "__init__": [
      "self",
      "layer_name",
      "tensor_name",
      "parent_quantizer",
      "tp_group"
    ],
    "get_plans_for_output_tensors": [
      "self"
    ],
    "get_enabled_look_at_tensors": [
      "self"
    ],
    "get_tensors_plan": [
      "self"
    ],
    "log_messages_about_plans": [
      "self"
    ],
    "_call_inspect_tensor_api": [
      "self",
      "tensor",
      "rowwise_gemm_tensor",
      "columnwise_gemm_tensor"
    ],
    "quantize": [
      "self",
      "tensor"
    ],
    "process_gemm_output": [
      "self",
      "tensor"
    ],
    "make_empty": [
      "self",
      "shape"
    ],
    "any_feature_enabled": [
      "self"
    ],
    "calibrate": [
      "self",
      "tensor"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "get_next_debug_iter": [
      "self"
    ],
    "_get_compatible_recipe": [
      "self"
    ],
    "process_enabled_api_call": [
      "self",
      "enabled_call_output"
    ],
    "supports_only_rowwise_all_gather": [
      "self"
    ],
    "_update_parent_quantizer_usage": [
      "self"
    ],
    "set_usage": [
      "self",
      "rowwise",
      "columnwise"
    ],
    "multi_tensor_quantize": [
      "cls",
      "tensor",
      "quantizers",
      "m_splits",
      "activation_dtype"
    ]
  },
  "DebugQuantizedTensor": {
    "__init__": [
      "self",
      "rowwise_gemm_tensor",
      "columnwise_gemm_tensor",
      "quantizer",
      "layer_name",
      "tensor_name"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "quantize_": [
      "self",
      "tensor"
    ],
    "dequantize": [
      "self"
    ],
    "get_tensor": [
      "self",
      "transpose"
    ],
    "size": [
      "self"
    ],
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ]
  },
  "TEDebugState": {
    "layer_count": [],
    "layers_initialized": [],
    "weight_tensor_tp_group_reduce": [],
    "debug_enabled": [],
    "initialize": [
      "cls"
    ],
    "_reset": [
      "cls"
    ],
    "get_layer_count": [
      "cls"
    ],
    "set_weight_tensor_tp_group_reduce": [
      "cls",
      "enabled"
    ],
    "get_iteration": [
      "cls"
    ]
  },
  "set_weight_tensor_tp_group_reduce": [
    "enabled"
  ],
  "next_iter_when_debug_should_be_run": [
    "quantizers"
  ],
  "any_feature_enabled": [
    "quantizers"
  ],
  "per_tensor_cast": [
    "tensor",
    "fp8_dtype",
    "out"
  ],
  "PerTensorScaling": {
    "fp8_gemm": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "iteration"
    ],
    "modify_tensor_enabled": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "gemm",
      "iteration"
    ],
    "modify_tensor": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "tensor_name",
      "tensor",
      "iteration",
      "default_quantizer",
      "out",
      "dtype"
    ]
  },
  "LogTensorStats": {
    "_is_supported_stat": [
      "self",
      "stat"
    ],
    "_parse_max_blockwise_dynamic_range_stats": [
      "self",
      "stats",
      "tensor_name"
    ],
    "_get_supported_stats_list": [
      "self"
    ],
    "inspect_tensor_enabled": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "iteration"
    ],
    "inspect_tensor": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "iteration",
      "tp_group",
      "tensor",
      "rowwise_quantized_tensor",
      "columnwise_quantized_tensor",
      "quantizer"
    ]
  },
  "ALL_RECIPE_NAMES": [],
  "_get_recipe_name": [
    "quantizer"
  ],
  "_get_new_quantizer": [
    "recipe_name",
    "fp8_dtype"
  ],
  "LogFp8TensorStats": {
    "check_if_stat_is_supported": [
      "self",
      "stat",
      "current_recipe"
    ],
    "get_recipe_from_stat": [
      "self",
      "stat",
      "default_recipe"
    ],
    "update_aux_dict": [
      "self",
      "aux_dict",
      "recipe_name",
      "quantized_tensor",
      "quantizer",
      "original_tensor",
      "recipes_in_stats"
    ],
    "inspect_tensor_enabled": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "iteration"
    ],
    "inspect_tensor": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "iteration",
      "tp_group",
      "tensor",
      "rowwise_quantized_tensor",
      "columnwise_quantized_tensor",
      "quantizer"
    ]
  },
  "fake_quantize": [
    "tensor",
    "fp8_format",
    "out"
  ],
  "FakeQuant": {
    "_supported_formats": [
      "self"
    ],
    "fp8_gemm_enabled": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "iteration"
    ],
    "modify_tensor_enabled": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "gemm",
      "iteration"
    ],
    "modify_tensor": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "tensor_name",
      "tensor",
      "iteration",
      "default_quantizer",
      "out",
      "dtype"
    ]
  },
  "TEConfigAPIMapper": {
    "parse_config_and_api": [
      "self",
      "config"
    ],
    "_validate_gemm": [
      "self",
      "gemm"
    ],
    "_process_transformer_engine_config": [
      "self",
      "config"
    ]
  },
  "required_kwargs": [],
  "TEDefaultFeatures": {
    "fp8_gemm_enabled": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "iteration"
    ],
    "modify_tensor_enabled": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "tensor_name",
      "iteration"
    ],
    "modify_tensor": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "tensor_name",
      "tensor",
      "default_quantizer",
      "iteration",
      "out"
    ],
    "inspect_tensor": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "tensor",
      "rowwise_quantized_tensor",
      "columnwise_quantized_tensor",
      "quantizer",
      "iteration",
      "tp_group"
    ],
    "inspect_tensor_postquantize": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "tensor",
      "iteration",
      "tp_group",
      "rowwise"
    ],
    "inspect_tensor_enabled": [
      "self",
      "config",
      "layer_name",
      "tensor_name",
      "iteration"
    ],
    "inspect_tensor_postquantize_enabled": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "tensor_name",
      "iteration"
    ]
  },
  "TransformerEngineAPI": {
    "__init__": [
      "self"
    ],
    "is_multiple_feature_invocation_allowed": [
      "self",
      "api_name"
    ],
    "input_assertions_hook": [
      "self",
      "api_name"
    ],
    "routing_condition": [
      "self",
      "api_name",
      "config",
      "_",
      "feature_obj"
    ],
    "output_assertions_hook": [
      "self",
      "api_name",
      "ret"
    ],
    "call_feature": [
      "self",
      "call",
      "feat_config",
      "layer_name"
    ],
    "handle_multi_feature_output": [
      "self",
      "api_name",
      "multi_feature_outputs",
      "features_to_invoke"
    ],
    "step": [
      "self"
    ],
    "end_debug": [
      "self"
    ]
  },
  "DisableFP8Layer": {
    "fp8_gemm_enabled": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "iteration"
    ],
    "parse_config_and_api": [
      "self",
      "config"
    ]
  },
  "DisableFP8GEMM": {
    "fp8_gemm_enabled": [
      "self",
      "config",
      "layer_name",
      "gemm",
      "iteration"
    ]
  },
  "_inspect_tensor_enabled_call_count": [],
  "_inspect_tensor_call_count": [],
  "TestDummyFeature": {
    "inspect_tensor_enabled": [
      "self",
      "config"
    ],
    "inspect_tensor": [
      "self",
      "_config"
    ]
  },
  "BlockwiseDynamicRangeStat": {
    "__str__": [
      "self"
    ]
  },
  "_compute_dynamic_range_top": [
    "tensor"
  ],
  "_compute_dynamic_range_bottom": [
    "tensor"
  ],
  "compute_max_blockwise_dynamic_range": [
    "tensor",
    "stat_config"
  ],
  "compute_variance": [
    "variances",
    "numels",
    "sums"
  ],
  "compute_std": [
    "variances",
    "numels",
    "sums"
  ],
  "compute_fp8_delayed_scaling_overflows_num": [
    "tensor",
    "quantized_tensor"
  ],
  "_get": [
    "buffers",
    "stat_name"
  ],
  "stats_to_num": [],
  "DEPENDENCIES": [],
  "STATS": [],
  "FP8_NEGATIVE_ZERO": [],
  "count_nonzero_fp8": [
    "fp8_data"
  ],
  "add_underflows_stats": [
    "recipe_name",
    "columnwise"
  ],
  "add_scale_inv_stats": [
    "recipe_name",
    "columnwise"
  ],
  "add_mse_stats": [
    "recipe_name",
    "columnwise"
  ],
  "add_max_blockwise_dynamic_range_stats": [
    "block_size",
    "dims",
    "max_over_orientations"
  ],
  "get_reduction_params": [
    "tensor_name",
    "tp_group"
  ],
  "next_enabled_iter": [
    "start_step",
    "end_step",
    "start_end_list",
    "freq",
    "iteration"
  ],
  "_Buffer": {
    "__init__": [
      "self",
      "layer_name",
      "tensor_name",
      "stats",
      "reduction_group",
      "reduce_within_microbatch"
    ],
    "_reset_before_next_step": [
      "self"
    ],
    "_gather_helper_stats": [
      "self"
    ],
    "feed": [
      "self",
      "tensor",
      "iteration",
      "aux_dict"
    ],
    "log": [
      "self"
    ]
  },
  "StatsBuffers": {
    "__init__": [
      "self"
    ],
    "_if_run_reduction": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "try_add_buffer": [
      "self",
      "layer_name",
      "tensor_name",
      "stats",
      "options",
      "reduction_group",
      "reduce_within_microbatch"
    ],
    "feed": [
      "self",
      "layer_name",
      "tensor_name",
      "options",
      "tensor",
      "iteration",
      "skip_reduction",
      "aux_dict"
    ],
    "log_stats": [
      "self"
    ]
  },
  "STATS_BUFFERS": [],
  "__all__": [],
  "CrossEntropyFunction": {
    "forward": [
      "ctx",
      "inp",
      "target",
      "label_smoothing",
      "reduce_loss",
      "dist_process_group",
      "ignore_idx",
      "is_cg_capturable"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "parallel_cross_entropy": [
    "inp",
    "target",
    "label_smoothing",
    "reduce_loss",
    "dist_process_group",
    "ignore_idx",
    "is_cg_capturable"
  ],
  "_quantized_tensor_cpu_supported_ops": [],
  "QuantizedTensorStorage": {
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ],
    "get_usages": [
      "self"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "_get_quantizer": [
      "self"
    ],
    "_build_default_quantizer": [
      "self"
    ],
    "quantize_": [
      "self",
      "tensor"
    ],
    "update_quantizer": [
      "self",
      "quantizer"
    ]
  },
  "prepare_for_saving": [],
  "restore_from_saved": [
    "tensors",
    "saved_tensors",
    "return_saved_tensors"
  ],
  "Quantizer": {
    "__init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "quantize": [
      "self",
      "tensor"
    ],
    "quantize_impl": [
      "self",
      "tensor"
    ],
    "multi_quantize": [
      "self",
      "list_of_tensors"
    ],
    "__call__": [
      "self",
      "tensor"
    ],
    "make_empty": [
      "self",
      "shape"
    ],
    "calibrate": [
      "self",
      "tensor"
    ],
    "set_usage": [
      "self"
    ],
    "onnx_quantize": [
      "self",
      "tensor"
    ],
    "onnx_dequantize": [
      "self",
      "tensor"
    ],
    "_get_compatible_recipe": [
      "self"
    ],
    "supports_only_rowwise_all_gather": [
      "self"
    ],
    "is_quantizable": [
      "self",
      "inp"
    ],
    "get_usages": [
      "self"
    ]
  },
  "QuantizedTensor": {
    "__new__": [
      "cls",
      "shape",
      "dtype"
    ],
    "dequantize": [
      "self"
    ],
    "quantize_": [
      "self",
      "tensor"
    ],
    "detach": [
      "self"
    ],
    "clear": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "float": [
      "self"
    ],
    "bfloat16": [
      "self"
    ],
    "half": [
      "self"
    ],
    "cpu": [
      "self",
      "memory_format"
    ],
    "expand_as": [
      "self",
      "other"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__torch_function__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "contiguous": [
      "self",
      "memory_format"
    ],
    "get_metadata": [
      "self"
    ],
    "make_like": [
      "cls",
      "tensor"
    ],
    "to_dtype": [
      "self",
      "dtype"
    ]
  },
  "_IN_ONNX_EXPORT_MODE": [],
  "TORCH_MAJOR": [],
  "TORCH_MINOR": [],
  "onnx_export": [
    "enabled"
  ],
  "is_in_onnx_export_mode": [],
  "assert_warmed_up": [
    "module"
  ],
  "_MODEL_PARALLEL_ATTRIBUTE_DEFAULTS": [],
  "_USE_REENTRANT_ACTIVATION_RECOMPUTE": [],
  "_FP8_ACTIVATION_RECOMPUTE_ENABLED": [],
  "_FP8_ACTIVATION_RECOMPUTE_PHASE": [],
  "_ALL_ACTIVE_RNG_STATES": [],
  "get_all_rng_states": [],
  "set_all_rng_states": [
    "states"
  ],
  "graph_safe_rng_available": [],
  "_get_cuda_rng_state": [
    "device",
    "clone",
    "graph_safe"
  ],
  "_set_cuda_rng_state": [
    "new_state",
    "device",
    "graph_safe"
  ],
  "set_tensor_model_parallel_attributes": [
    "tensor",
    "is_parallel",
    "dim",
    "stride"
  ],
  "get_distributed_world_size": [
    "group"
  ],
  "get_distributed_rank": [
    "group"
  ],
  "initialize_affine_weight_gpu": [
    "weight",
    "init_method",
    "get_rng_state_tracker",
    "partition_dim",
    "stride",
    "set_tp_attributes"
  ],
  "split_tensor_into_1d_equal_chunks": [
    "tensor",
    "tp_group",
    "new_buffer"
  ],
  "gather_split_1d_tensor": [
    "tensor",
    "tp_group"
  ],
  "activation_recompute_forward": {
    "__init__": [
      "self",
      "activation_recompute",
      "recompute_phase"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "is_fp8_activation_recompute_enabled": [],
  "in_fp8_activation_recompute_phase": [],
  "_get_active_autocast_contexts": [],
  "_CheckpointFunction": {
    "forward": [
      "ctx",
      "run_function",
      "distribute_saved_activations",
      "get_rng_state_tracker",
      "tp_group",
      "context_fn",
      "kwargs"
    ],
    "backward": [
      "ctx"
    ]
  },
  "_CheckpointFrame": {
    "__init__": [
      "self",
      "recompute_fn",
      "get_rng_state_tracker"
    ],
    "cache_rng_states": [
      "self",
      "forward"
    ],
    "restore_rng_states": [
      "self",
      "forward"
    ]
  },
  "_recomputation_hook": {
    "__init__": [
      "self",
      "frame"
    ]
  },
  "_checkpoint_hook": {
    "__init__": [
      "self",
      "frame",
      "args",
      "kwargs"
    ]
  },
  "use_reentrant_activation_recompute": [],
  "get_activation_recompute_contexts": [],
  "has_te_modules": [
    "network"
  ],
  "checkpoint": [
    "function"
  ],
  "CudaRNGStatesTracker": {
    "__init__": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "get_states": [
      "self"
    ],
    "set_states": [
      "self",
      "states"
    ],
    "add": [
      "self",
      "name",
      "seed"
    ],
    "fork": [
      "self",
      "name"
    ]
  },
  "reduce_scatter_along_first_dim": [
    "inp",
    "tp_group",
    "async_op"
  ],
  "_all_gather_fp8": [
    "inp",
    "process_group"
  ],
  "_get_quantizer_format": [
    "quantizer"
  ],
  "_set_quantizer_format": [
    "quantizer",
    "compact"
  ],
  "_post_process_fp8_blockwise_gather": [
    "out",
    "quantizer",
    "handle"
  ],
  "_FP8BlockwiseAllGatherAsyncHandle": {
    "wait": [
      "self"
    ]
  },
  "_all_gather_fp8_blockwise": [
    "inp",
    "process_group"
  ],
  "_swap_first_dims": [
    "tensor",
    "world_size"
  ],
  "_post_process_nvfp4_gather": [
    "out",
    "columnwise_data_interleaved",
    "columnwise_scale_inv_interleaved",
    "world_size",
    "handle"
  ],
  "_NVFP4AllGatherAsyncHandle": {
    "wait": [
      "self"
    ]
  },
  "_all_gather_nvfp4": [
    "inp",
    "process_group"
  ],
  "_all_gather_mxfp8": [
    "inp",
    "process_group"
  ],
  "gather_along_first_dim": [
    "inp",
    "process_group",
    "async_op",
    "quantizer"
  ],
  "symmetric_mem_cache": [],
  "get_symmetric_memory_tensor": [
    "tensor_numel",
    "tensor_dtype",
    "tensor_device",
    "tp_group",
    "tag"
  ],
  "symmetric_all_reduce": [
    "inp",
    "tp_group",
    "async_op",
    "all_reduce_type"
  ],
  "allreduce": [
    "inp",
    "tp_group",
    "async_op"
  ],
  "_get_module_fsdp_state": [
    "module"
  ],
  "_fsdp_scatter_tensors": [
    "fsdp_group"
  ],
  "_fsdp_gather_tensors": [
    "fsdp_group",
    "shapes"
  ],
  "_is_te_module": [
    "module"
  ],
  "prepare_te_modules_for_fsdp": [
    "fsdp_root"
  ],
  "FullyShardedDataParallel": {
    "__init__": [
      "self",
      "module"
    ]
  },
  "_IS_GRAPH_CAPTURING": [],
  "_T": [],
  "SingleOrTuple": [],
  "set_capture_start": [],
  "set_capture_end": [],
  "is_graph_capturing": [],
  "graph_pool_handle": [],
  "_graph_context_wrapper": [],
  "_make_graphed_callables": [
    "callables",
    "sample_args",
    "num_warmup_iters",
    "allow_unused_input",
    "cache_quantized_params",
    "sample_kwargs",
    "_order",
    "_num_layers_per_chunk",
    "pool",
    "retain_graph_in_backward",
    "_reuse_graph_input_output_buffers"
  ],
  "save_fp8_tensors": [
    "modules",
    "recipe"
  ],
  "restore_fp8_tensors": [
    "modules",
    "fp8_tensors"
  ],
  "make_graphed_callables": [
    "modules",
    "sample_args",
    "num_warmup_iters",
    "allow_unused_input",
    "sample_kwargs",
    "fp8_enabled",
    "fp8_calibrating",
    "fp8_recipe",
    "fp8_group",
    "fp8_weight_caching",
    "enabled",
    "calibrating",
    "recipe",
    "amax_reduction_group",
    "cache_quantized_params",
    "_order",
    "_num_layers_per_chunk",
    "pool",
    "retain_graph_in_backward",
    "_reuse_graph_input_output_buffers"
  ],
  "check_fp8_support": [],
  "check_mxfp8_support": [],
  "check_nvfp4_support": [],
  "check_fp8_block_scaling_support": [],
  "check_recipe_support": [
    "recipe"
  ],
  "get_default_fp8_recipe": [],
  "get_default_recipe": [],
  "get_align_size_for_quantization": [
    "recipe"
  ],
  "get_fp8_torch_dtype": [
    "fp8_recipe",
    "fprop_tensor"
  ],
  "get_fp8_te_dtype": [
    "fp8_recipe",
    "fprop_tensor"
  ],
  "get_fp4_te_dtype": [
    "fp4_recipe"
  ],
  "get_fp8_max": [
    "fp8_recipe",
    "fprop_tensor"
  ],
  "is_fp8_available": [
    "return_reason"
  ],
  "is_mxfp8_available": [
    "return_reason"
  ],
  "is_fp8_block_scaling_available": [
    "return_reason"
  ],
  "is_nvfp4_available": [
    "return_reason"
  ],
  "FP8GlobalStateManager": {
    "FP8_ENABLED": [],
    "FP8_CALIBRATION": [],
    "FP8_RECIPE": [],
    "FP8_DISTRIBUTED_GROUP": [],
    "FP8_PARAMETERS": [],
    "HIGH_PRECISION_INIT_VAL": [],
    "IS_FIRST_FP8_MODULE": [],
    "FP8_GRAPH_CAPTURING": [],
    "AUTOCAST_DEPTH": [],
    "global_amax_buffer": [],
    "global_amax_history_buffer": [],
    "global_scale_buffer": [],
    "fp8_tensors_recompute_buffer": [],
    "fp8_available": [],
    "reason_for_no_fp8": [],
    "autocast_arguments": [],
    "skip_fp8_weight_update_tensor": [],
    "mxfp8_available": [],
    "reason_for_no_mxfp8": [],
    "fp8_block_scaling_available": [],
    "reason_for_no_fp8_block_scaling": [],
    "nvfp4_available": [],
    "reason_for_no_nvfp4": [],
    "reset": [
      "cls"
    ],
    "set_skip_fp8_weight_update_tensor": [
      "cls",
      "skip"
    ],
    "get_skip_fp8_weight_update_tensor": [
      "cls"
    ],
    "is_fp8_available": [
      "cls"
    ],
    "is_mxfp8_available": [
      "cls"
    ],
    "is_fp8_block_scaling_available": [
      "cls"
    ],
    "is_nvfp4_available": [
      "cls"
    ],
    "get_meta_tensor_key": [
      "forward"
    ],
    "get_fwd_bwd_key": [
      "forward"
    ],
    "get_buffer_info": [
      "cls"
    ],
    "get_key_in_buffer": [
      "cls",
      "forward",
      "fp8_recipe",
      "fp8_group"
    ],
    "split_key_in_buffer": [
      "cls",
      "key"
    ],
    "add_fp8_tensors_to_global_buffer": [
      "cls",
      "fp8_meta"
    ],
    "is_fp8_enabled": [
      "cls"
    ],
    "is_fp8_calibration": [
      "cls"
    ],
    "with_fp8_parameters": [
      "cls"
    ],
    "with_high_precision_init_val": [
      "cls"
    ],
    "fp8_graph_capturing": [
      "cls"
    ],
    "is_first_fp8_module": [
      "cls"
    ],
    "get_fp8_recipe": [
      "cls"
    ],
    "get_fp8_group": [
      "cls"
    ],
    "get_autocast_state": [
      "cls"
    ],
    "set_autocast_state": [
      "cls",
      "fp8_state"
    ],
    "reduce_tensor_across_group_op_max": [
      "tensor",
      "group"
    ],
    "reduce_and_update_fp8_tensors": [
      "cls",
      "forward"
    ],
    "get_unique_autocast_key": [
      "cls",
      "recipe",
      "group"
    ],
    "autocast_enter": [
      "cls",
      "enabled",
      "calibrating",
      "fp8_recipe",
      "fp8_group",
      "_graph"
    ],
    "autocast_exit": [
      "cls",
      "enabled",
      "_graph"
    ],
    "copy_forward_fp8_meta_tensors_for_recompute": [
      "cls",
      "fp8_meta"
    ],
    "get_old_fp8_meta_tensors_for_recompute": [
      "cls",
      "fp8_meta"
    ],
    "restore_fp8_meta_tensors": [
      "fp8_meta"
    ]
  },
  "fp8_model_init": [
    "enabled",
    "recipe",
    "preserve_high_precision_init_val"
  ],
  "quantized_model_init": [
    "enabled",
    "recipe",
    "preserve_high_precision_init_val"
  ],
  "fp8_autocast": [
    "enabled",
    "calibrating",
    "fp8_recipe",
    "fp8_group",
    "_graph"
  ],
  "autocast": [
    "enabled",
    "calibrating",
    "recipe",
    "amax_reduction_group",
    "_graph"
  ],
  "_update_amax_history": [
    "amax_history"
  ],
  "_default_get_amax_and_update_history": [
    "amax_history",
    "amax_compute_algo"
  ],
  "_default_sf_compute": [
    "amax",
    "scale",
    "fp8_max",
    "margin",
    "_fp32_max"
  ],
  "_compute_amax_and_update_history": [
    "amax_history",
    "amax_compute_algo"
  ],
  "_compute_scaling_factor": [
    "amax",
    "scale",
    "fp8_max",
    "recipe"
  ],
  "_amax_and_scale_update": [
    "amax_history",
    "scale",
    "fp8_max",
    "recipe"
  ],
  "split_and_copy": [
    "buffer",
    "outputs",
    "chunk_sizes"
  ],
  "RecipeState": {
    "create": [
      "recipe"
    ],
    "make_quantizers": [
      "self"
    ]
  },
  "DelayedScalingRecipeState": {
    "__init__": [
      "self",
      "recipe"
    ],
    "make_quantizers": [
      "self"
    ]
  },
  "Float8CurrentScalingRecipeState": {
    "__init__": [
      "self",
      "recipe"
    ],
    "make_quantizers": [
      "self"
    ]
  },
  "MXFP8BlockScalingRecipeState": {
    "__init__": [
      "self",
      "recipe"
    ],
    "make_quantizers": [
      "self"
    ]
  },
  "Float8BlockScalingRecipeState": {
    "__init__": [
      "self",
      "recipe"
    ],
    "make_quantizers": [
      "self"
    ]
  },
  "NVFP4BlockScalingRecipeState": {
    "__init__": [
      "self",
      "recipe"
    ],
    "make_quantizers": [
      "self"
    ]
  },
  "CustomRecipeState": {
    "__init__": [
      "self",
      "recipe"
    ],
    "make_quantizers": [
      "self"
    ]
  },
  "lazy_compile": [
    "func"
  ],
  "jit_fuser": [],
  "dropout_fuser": [],
  "no_torch_dynamo": [],
  "set_jit_fusion_options": [],
  "bias_gelu_fused_": [
    "inp",
    "bias"
  ],
  "gelu_fused_": [
    "inp"
  ],
  "bgrad_dgelu_fused_": [
    "grad_output",
    "inp",
    "bias"
  ],
  "dgelu_fused_": [
    "grad_output",
    "inp"
  ],
  "l2normalization_fused_": [
    "x",
    "eps"
  ],
  "l2normalization_fwd_fused_": [
    "x",
    "eps"
  ],
  "l2normalization_backward_fused_": [
    "grad_output",
    "x",
    "rsqrt_norm",
    "eps"
  ],
  "bias_gelu_fused": [
    "inp",
    "bias"
  ],
  "bgrad_dgelu_fused": [
    "grad_output",
    "inp",
    "bias"
  ],
  "l2normalization_fused": [
    "x",
    "eps"
  ],
  "l2normalization_fwd_fused": [
    "x",
    "eps"
  ],
  "l2normalization_backward_fused": [
    "grad_output",
    "x",
    "rsqrt_norm",
    "eps"
  ],
  "bias_dropout_add": [
    "x",
    "bias",
    "residual",
    "prob",
    "training"
  ],
  "get_bias_dropout_add": [
    "training"
  ],
  "bias_dropout_add_fused_train_": [
    "x",
    "bias",
    "residual",
    "prob"
  ],
  "bias_dropout_add_fused_train": [
    "x",
    "bias",
    "residual",
    "prob"
  ],
  "bias_dropout_add_fused_inference_": [
    "x",
    "bias",
    "residual",
    "prob"
  ],
  "bias_dropout_add_fused_inference": [
    "x",
    "bias",
    "residual",
    "prob"
  ],
  "warmup_jit_bias_dropout_add": [
    "hidden_size",
    "dtype",
    "seq_length",
    "micro_batch_size"
  ],
  "warmup_jit_bias_dropout_add_all_dtypes": [
    "hidden_size",
    "seq_length",
    "micro_batch_size"
  ],
  "warmup_jit_bias_gelu": [
    "ffn_hidden_size_per_partition",
    "dtype",
    "seq_length",
    "micro_batch_size"
  ],
  "warmup_jit_bias_gelu_all_dtypes": [
    "ffn_hidden_size",
    "seq_length",
    "micro_batch_size"
  ],
  "warmup_jit_l2normalization": [
    "hidden_size",
    "dtype",
    "seq_length",
    "micro_batch_size"
  ],
  "warmup_jit_l2normalization_all_dtypes": [
    "hidden_size",
    "seq_length",
    "micro_batch_size"
  ],
  "requires_grad": [],
  "_empty_tensor": [],
  "clear_tensor_data": [],
  "_get_device_compute_capability": [
    "device"
  ],
  "get_device_compute_capability": [],
  "attention_mask_func": [
    "attention_scores",
    "attention_mask"
  ],
  "get_default_init_method": [],
  "init_method_constant": [
    "val"
  ],
  "init_method_normal": [
    "sigma"
  ],
  "scaled_init_method_normal": [
    "sigma",
    "num_layers"
  ],
  "all_close": [
    "a",
    "b"
  ],
  "print_rank_0": [],
  "compare_tensors": [
    "a",
    "b"
  ],
  "ensure_divisibility": [
    "numerator",
    "denominator"
  ],
  "divide": [
    "numerator",
    "denominator"
  ],
  "split_tensor_along_dim": [
    "tensor",
    "dim",
    "num_partitions",
    "contiguous_split_chunks"
  ],
  "combine_tensors": [
    "tensors",
    "dim"
  ],
  "SplitAlongDim": {
    "forward": [
      "ctx",
      "mixed_x_layer",
      "split_dim",
      "split_size_or_sections",
      "squeeze"
    ],
    "backward": [
      "ctx"
    ]
  },
  "validate_ctx_manager": [
    "ctx"
  ],
  "validate_rng_states_func": [
    "get_rng_tracker"
  ],
  "assert_viewless_tensor": [
    "tensor",
    "extra_msg"
  ],
  "safely_set_viewless_tensor_data": [
    "tensor",
    "new_data_tensor"
  ],
  "cast_if_needed": [
    "tensor",
    "dtype"
  ],
  "check_dim_for_fp8_exec": [
    "tensor"
  ],
  "assert_dim_for_fp8_exec": [],
  "assert_dim_for_all_gather": [
    "tensor",
    "with_all_gather",
    "quantizer"
  ],
  "is_bf16_compatible": [],
  "is_bf16_available": [
    "return_reason"
  ],
  "is_non_tn_fp8_gemm_supported": [],
  "get_cudnn_version": [],
  "canonicalize_device": [
    "device"
  ],
  "canonicalize_dtype": [
    "dtype"
  ],
  "devices_match": [
    "device1",
    "device2"
  ],
  "get_sm_count": [],
  "round_up_to_nearest_multiple": [
    "value",
    "multiple"
  ],
  "needs_quantized_gemm": [
    "obj",
    "rowwise"
  ],
  "_nvtx_enabled": [],
  "get_nvtx_range_context": [
    "msg"
  ],
  "nvtx_range_push": [
    "msg"
  ],
  "nvtx_range_pop": [
    "msg"
  ],
  "canonicalize_process_group": [
    "group"
  ],
  "torch_get_autocast_gpu_dtype": [],
  "_torch_dtype_to_np_typestr_dict": [],
  "_WeakRefTensor": {
    "__init__": [
      "self",
      "data_ptr",
      "dtype",
      "shape"
    ],
    "data_ptr": [
      "self"
    ],
    "dtype": [
      "self",
      "dtype"
    ],
    "shape": [
      "self",
      "shape"
    ],
    "numel": [
      "self"
    ],
    "__cuda_array_interface__": [
      "self"
    ],
    "torch_dtype_to_np_typestr": [
      "self"
    ]
  },
  "make_weak_ref": [
    "x"
  ],
  "FORCE_BUILD": [],
  "FORCE_CXX11_ABI": [],
  "SKIP_CUDA_BUILD": [],
  "PACKAGE_NAME": [],
  "BASE_WHEEL_URL": [],
  "current_file_path": [],
  "build_tools_dir": [],
  "CMakeBuildExtension": [],
  "get_platform": [],
  "get_wheel_url": [],
  "CachedWheelsCommand": {
    "run": [
      "self"
    ]
  },
  "torch_version": [],
  "DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "TransformerLayer": {
    "__init__": [
      "self",
      "hidden_size",
      "ffn_hidden_size",
      "num_attention_heads",
      "num_gqa_groups",
      "layernorm_epsilon",
      "hidden_dropout",
      "attention_dropout",
      "init_method",
      "output_layer_init_method",
      "layer_number",
      "kv_channels",
      "self_attn_mask_type",
      "window_size",
      "enc_dec_attn_mask_type",
      "enc_dec_window_size",
      "tp_group",
      "tp_size",
      "params_dtype",
      "get_rng_state_tracker",
      "fuse_wgrad_accumulation",
      "seq_length",
      "micro_batch_size",
      "sequence_parallel",
      "apply_residual_connection_post_layernorm",
      "output_layernorm",
      "parallel_attention_mlp",
      "layer_type",
      "drop_path_rate",
      "set_parallel_mode",
      "fuse_qkv_params",
      "rotary_pos_interleaved",
      "zero_centered_gamma",
      "qkv_weight_interleaved",
      "ub_tp_comm_overlap",
      "ub_overlap_ag",
      "ub_overlap_rs",
      "ub_overlap_rs_dgrad",
      "ub_bulk_dgrad",
      "ub_bulk_wgrad",
      "bias",
      "activation",
      "activation_params",
      "normalization",
      "device",
      "attn_input_format",
      "name",
      "qk_norm_type",
      "qk_norm_eps",
      "qk_norm_before_rope",
      "softmax_type"
    ],
    "set_tensor_parallel_group": [
      "self",
      "tp_group"
    ],
    "reset_fp8_meta_tensors": [
      "self"
    ],
    "set_context_parallel_group": [
      "self",
      "cp_group",
      "cp_global_ranks",
      "cp_stream",
      "cp_comm_type"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "self_attn_mask_type",
      "window_size",
      "encoder_output",
      "enc_dec_attn_mask",
      "enc_dec_attn_mask_type",
      "enc_dec_window_size",
      "is_first_microbatch",
      "checkpoint_core_attention",
      "inference_params",
      "rotary_pos_emb",
      "core_attention_bias_type",
      "core_attention_bias",
      "alibi_slopes",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "cu_seqlens_q_padded",
      "cu_seqlens_kv_padded",
      "max_seqlen_q",
      "max_seqlen_kv",
      "fast_zero_fill",
      "pad_between_seqs"
    ],
    "_bias_dropout_add": [
      "self",
      "hidden_state",
      "bias",
      "residual",
      "drop_path"
    ]
  },
  "CPUOffloadEnabled": [],
  "CPUOffloadedLayer": [],
  "mark_activation_offload": [],
  "is_cpu_offload_enabled": [],
  "is_current_layer_offloaded": [],
  "CpuOffloadSavedTensorHook": {
    "__init__": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ],
    "on_save_for_backward": [
      "self",
      "tensor"
    ],
    "on_get_saved_tensor": [
      "self",
      "saved_state"
    ]
  },
  "CpuOffloadHookWithOffloadHandler": {
    "__init__": [
      "self",
      "offload_handler",
      "handler_extra_kwargs",
      "debug"
    ],
    "on_save_for_backward": [
      "self",
      "tensor"
    ],
    "on_get_saved_tensor": [
      "self",
      "saved_state"
    ]
  },
  "OffloadHandler": {
    "__init__": [
      "self"
    ],
    "tensor_push": [
      "self",
      "tensor"
    ],
    "tensor_pop": [
      "self",
      "tensor_tag"
    ]
  },
  "GroupCommitFunction": {
    "forward": [
      "ctx",
      "tensor",
      "cpu_offload_handler"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "group_prefetch_offload_commit": [],
  "SynchronizedGroupOffloadHandler": {
    "__init__": [
      "self",
      "num_offload_group",
      "tensor_need_offloading_checker",
      "debug"
    ],
    "groupid_reset": [
      "self"
    ],
    "on_group_commit_forward": [
      "self"
    ],
    "on_group_commit_backward": [
      "self"
    ],
    "offload": [
      "src_tensor",
      "pin_memory"
    ],
    "reload": [
      "state",
      "non_blocking",
      "copy_buffer"
    ],
    "tensor_push": [
      "self",
      "tensor"
    ],
    "tensor_pop": [
      "self",
      "tensor_tag"
    ]
  },
  "AsyncDoubleBufferGroupOffloadHandler": {
    "__init__": [
      "self",
      "num_offload_group",
      "num_model_group",
      "tensor_need_offloading_checker",
      "double_buffering",
      "debug"
    ],
    "tensor_push": [
      "self",
      "tensor"
    ],
    "tensor_pop": [
      "self",
      "tensor_tag"
    ],
    "bulk_offload_group": [
      "self",
      "group_to_offload"
    ],
    "synchronize_on_group_commit_forward": [
      "self",
      "current_group"
    ],
    "on_group_commit_forward": [
      "self"
    ],
    "bulk_reload_group": [
      "self",
      "group_to_reload"
    ],
    "on_group_commit_backward": [
      "self"
    ]
  },
  "get_cpu_offload_context": [
    "enabled",
    "num_layers",
    "model_layers",
    "offload_activations",
    "offload_weights",
    "double_buffering"
  ],
  "trt_opset": [],
  "onnx_gemm": [
    "weight",
    "inp",
    "bias"
  ],
  "torch_onnx_gemm_inf_op": [
    "weight",
    "inp",
    "bias"
  ],
  "_": [
    "inp"
  ],
  "onnx_gemm_inf_symbolic": [
    "weight",
    "inp",
    "bias"
  ],
  "onnx_quantize_fp8_op": [
    "tensor",
    "scale"
  ],
  "onnx_quantize_fp8_symbolic": [
    "tensor",
    "scale"
  ],
  "schema": [],
  "TRT_FP8QuantizeLinear": [],
  "onnx_dequantize_fp8_op": [
    "tensor",
    "scale_inv"
  ],
  "onnx_dequantize_fp8_symbolic": [
    "tensor",
    "scale_inv"
  ],
  "TRT_FP8DequantizeLinear": [],
  "onnx_cs_quantize_fp8_op": [
    "tensor"
  ],
  "onnx_quantize_fp8_cs_symbolic": [
    "tensor"
  ],
  "onnx_quantize_mxfp8_op": [
    "tensor"
  ],
  "onnx_quantize_mxfp8_symbolic": [
    "tensor"
  ],
  "TRT_MXFP8DynamicQuantize": [],
  "onnx_dequantize_mxfp8_op": [
    "tensor",
    "scale_inv"
  ],
  "onnx_dequantize_mxfp8_symbolic": [
    "tensor",
    "scale_inv"
  ],
  "TRT_MXFP8DequantizeLinear": [],
  "onnx_layernorm_op": [
    "inp",
    "weight",
    "bias",
    "eps"
  ],
  "onnx_layernorm_symbolic": [
    "inp",
    "weight",
    "bias",
    "eps"
  ],
  "onnx_layernorm": [
    "inp",
    "layer_norm_weight",
    "layer_norm_bias",
    "eps",
    "normalization",
    "zero_centered_gamma",
    "output_dtype",
    "return_layernorm_output",
    "input_quantizer"
  ],
  "onnx_attention_mask_func": [
    "attention_scores",
    "attention_mask"
  ],
  "te_translation_table": [],
  "NVTE_CPU_OFFLOAD_V1": [],
  "OFFLOAD_SYNCHRONIZER": [],
  "mark_not_offload": [],
  "start_offload": [],
  "TensorGroup": {},
  "TensorGroupProcessor": {
    "tensor_group_process_before_offload": [
      "tensor_group"
    ],
    "tensor_group_process_after_reload": [
      "tensor_group"
    ],
    "_switch_to_base_tensors": [
      "aux",
      "tensor_group"
    ],
    "_deduplicate_tensors": [
      "aux",
      "tensor_group"
    ],
    "_restore_tensor_duplicates": [
      "tensor_group"
    ],
    "_switch_to_views": [
      "tensor_group"
    ]
  },
  "OffloadableLayerState": {
    "__init__": [
      "self",
      "offload_stream",
      "retain_pinned_cpu_buffers"
    ],
    "_validate_state": [
      "self",
      "func_name",
      "allowed_states"
    ],
    "start_offload": [
      "self"
    ],
    "release_activation_forward_gpu_memory": [
      "self"
    ],
    "start_reload": [
      "self"
    ],
    "push_tensor": [
      "self",
      "tensor"
    ],
    "pop_tensor": [
      "self",
      "tensor_or_tensor_id"
    ],
    "release_all_memory": [
      "self"
    ],
    "_check_if_offload": [
      "self",
      "t"
    ],
    "get_offloaded_total_size_mb": [
      "self"
    ]
  },
  "OffloadSynchronizer": {
    "__init__": [
      "self",
      "num_layers",
      "retain_pinned_cpu_buffers",
      "offload_stream"
    ],
    "fwd_step": [
      "self"
    ],
    "bwd_step": [
      "self",
      "layer_num"
    ],
    "push_tensor": [
      "self",
      "tensor"
    ],
    "pop_tensor": [
      "self",
      "tensor_or_tensor_id"
    ],
    "finish_part_of_bwd": [
      "self"
    ],
    "get_offloaded_total_size_mb": [
      "self"
    ]
  },
  "DefaultOffloadSynchronizer": {
    "__init__": [
      "self",
      "num_layers",
      "num_offloaded_layers",
      "retain_pinned_cpu_buffers",
      "offload_stream"
    ],
    "_init_offload_synchronization_dicts": [
      "self",
      "num_offloaded_layers"
    ],
    "fwd_step": [
      "self"
    ],
    "bwd_step": [
      "self",
      "layer_num"
    ]
  },
  "ManualOffloadSynchronizer": {
    "start_offload_layer": [
      "self",
      "layer_id"
    ],
    "release_activation_forward_gpu_memory": [
      "self",
      "layer_id"
    ],
    "start_reload_layer": [
      "self",
      "layer_id"
    ]
  },
  "TE_DType": [],
  "TE_DType_To_Torch": [],
  "AttnMaskTypes": [],
  "AttnTypes": [],
  "AttnBiasTypes": [],
  "QKVLayouts": [],
  "LayerTypes": [],
  "GemmParallelModes": [],
  "dist_group_type": [],
  "MXFP8_BLOCK_SCALING_SIZE": [],
  "NVFP4_BLOCK_SCALING_SIZE": [],
  "FusedTopkScoreFunction": {
    "forward": [
      "ctx",
      "logits",
      "topk",
      "use_pre_softmax",
      "num_groups",
      "group_topk",
      "scaling_factor",
      "score_function",
      "expert_bias"
    ],
    "backward": [
      "ctx",
      "grad_probs",
      "_"
    ]
  },
  "fused_topk_with_score_function": [
    "logits",
    "topk",
    "use_pre_softmax",
    "num_groups",
    "group_topk",
    "scaling_factor",
    "score_function",
    "expert_bias"
  ],
  "FusedComputeScoresForMoEAuxLoss": {
    "forward": [
      "ctx",
      "logits",
      "topk",
      "score_function"
    ],
    "backward": [
      "ctx",
      "_",
      "grad_scores"
    ]
  },
  "fused_compute_score_for_moe_aux_loss": [
    "logits",
    "topk",
    "score_function"
  ],
  "FusedAuxLoss": {
    "forward": [
      "ctx",
      "probs",
      "tokens_per_expert",
      "total_num_tokens",
      "num_experts",
      "topk",
      "coeff"
    ],
    "backward": [
      "ctx",
      "grad_aux_loss"
    ]
  },
  "fused_moe_aux_loss": [
    "probs",
    "tokens_per_expert",
    "total_num_tokens",
    "num_experts",
    "topk",
    "coeff"
  ],
  "_NUMERICS_DEBUG": [],
  "debug": [
    "enabled"
  ],
  "fp8_tensor_statistics": [
    "tensor",
    "fp8_format"
  ],
  "_moe_permute_index_map": {
    "workspace": [],
    "max_expanded_token_num": [],
    "forward": [
      "ctx",
      "inp",
      "index",
      "num_out_tokens",
      "max_token_num"
    ],
    "backward": [
      "ctx",
      "permuted_act_grad",
      "_"
    ]
  },
  "_moe_unpermute_index_map": {
    "forward": [
      "ctx",
      "inp",
      "row_id_map",
      "probs"
    ],
    "backward": [
      "ctx",
      "unpermuted_act_grad"
    ]
  },
  "_moe_permute_mask_map": {
    "forward": [
      "ctx",
      "inp",
      "routing_map",
      "num_out_tokens",
      "probs"
    ],
    "backward": [
      "ctx",
      "permuted_act_grad",
      "_",
      "permuted_probs_grad"
    ]
  },
  "_moe_unpermute_mask_map": {
    "forward": [
      "ctx",
      "inp",
      "row_id_map",
      "merging_probs",
      "restore_shape"
    ],
    "backward": [
      "ctx",
      "unpermuted_act_grad"
    ]
  },
  "moe_permute": [
    "inp",
    "routing_map",
    "num_out_tokens",
    "max_token_num",
    "map_type"
  ],
  "moe_permute_with_probs": [
    "inp",
    "probs",
    "routing_map",
    "num_out_tokens"
  ],
  "moe_unpermute": [
    "inp",
    "row_id_map",
    "merging_probs",
    "restore_shape",
    "map_type",
    "probs"
  ],
  "_moe_chunk_sort": {
    "forward": [
      "ctx",
      "inp",
      "split_sizes",
      "sorted_idxs",
      "probs"
    ],
    "backward": [
      "ctx",
      "permuted_act_grad",
      "permuted_probs_grad"
    ]
  },
  "moe_sort_chunks_by_index": [
    "inp",
    "split_sizes",
    "sorted_index"
  ],
  "moe_sort_chunks_by_index_with_probs": [
    "inp",
    "probs",
    "split_sizes",
    "sorted_index"
  ],
  "Sequential": {
    "__init__": [
      "self"
    ],
    "add_module": [
      "self",
      "name",
      "module"
    ],
    "_get_keys_by_idx": [
      "self",
      "idx"
    ],
    "_next_key": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__setitem__": [
      "self",
      "idx",
      "module"
    ],
    "__delitem__": [
      "self",
      "idx"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "append": [
      "self",
      "module"
    ],
    "extend": [
      "self",
      "modules"
    ],
    "insert": [
      "self",
      "idx",
      "module"
    ],
    "pop": [
      "self",
      "idx"
    ],
    "__iadd__": [
      "self",
      "modules"
    ],
    "__add__": [
      "self",
      "modules"
    ],
    "_make_module_groups": [
      "cls",
      "modules"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "_split_tuple": [
    "t",
    "idx"
  ],
  "_is_graph_capturing": [],
  "_OperationFuserAutogradFunction": {
    "forward": [
      "func_ctx",
      "input_",
      "fuser",
      "basic_op_kwargs"
    ],
    "backward": [
      "func_ctx",
      "grad_output"
    ]
  },
  "OperationFuser": {
    "__init__": [
      "self",
      "ops"
    ],
    "_fuse_forward_ops": [
      "cls",
      "ops",
      "recipe"
    ],
    "_fuse_backward_ops": [
      "cls",
      "ops",
      "recipe"
    ],
    "maybe_fuse_ops": [
      "self",
      "is_grad_enabled",
      "recipe",
      "input_",
      "extra_inputs"
    ],
    "__call__": [
      "self",
      "input"
    ]
  },
  "is_quantized_tensor": [
    "tensor"
  ],
  "maybe_dequantize": [
    "tensor",
    "dtype"
  ],
  "maybe_autocast_dtype": [],
  "get_fp8_meta_from_fp8_tensor": [
    "tensor"
  ],
  "OperationContext": {
    "save_for_backward": [
      "self"
    ]
  },
  "FusibleOperation": {
    "is_fused_op": [
      "self"
    ],
    "pre_first_fuser_forward": [
      "self"
    ],
    "pre_fuser_forward": [
      "self"
    ],
    "get_input_quantizer": [
      "self"
    ],
    "get_grad_output_quantizer": [
      "self"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "BasicOperation": {
    "__init__": [
      "self"
    ],
    "is_fused_op": [
      "self"
    ],
    "num_quantizers": [
      "self",
      "mode"
    ],
    "get_input_quantizer": [
      "self"
    ],
    "get_grad_output_quantizer": [
      "self"
    ],
    "reset_recipe_state": [
      "self"
    ],
    "get_quantizer": [
      "self",
      "mode",
      "index"
    ],
    "_save_fp8_metas": [
      "self"
    ],
    "_load_fp8_metas": [
      "self",
      "fp8_metas"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ],
    "forward": [
      "self",
      "input"
    ],
    "get_extra_state": [
      "self"
    ],
    "set_extra_state": [
      "self",
      "state"
    ],
    "_load_from_state_dict": [
      "self"
    ]
  },
  "FusedOperation": {
    "__init__": [
      "self",
      "basic_ops"
    ],
    "is_fused_op": [
      "self"
    ],
    "get_input_quantizer": [
      "self"
    ],
    "get_grad_output_quantizer": [
      "self"
    ],
    "pre_first_fuser_forward": [
      "self"
    ],
    "pre_fuser_forward": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "Linear": {
    "__init__": [
      "self",
      "in_features",
      "out_features"
    ],
    "register_parameter": [
      "self",
      "name",
      "param"
    ],
    "state_dict": [
      "self"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix"
    ]
  },
  "MakeExtraOutput": {
    "__init__": [
      "self"
    ],
    "op_forward": [
      "self"
    ],
    "op_backward": [
      "self"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "_ActivationOperation": {
    "__init__": [
      "self"
    ],
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "GELU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "GEGLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "QGELU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "QGEGLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "ReLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "ReGLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "SReLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "SReGLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "SiLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "SwiGLU": {
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "ClampedSwiGLU": {
    "__init__": [
      "self"
    ],
    "_activation_forward_impl": [
      "self"
    ],
    "_activation_backward_impl": [
      "self"
    ]
  },
  "Identity": {
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "Dropout": {
    "__init__": [
      "self",
      "p"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "L2Normalization": {
    "__init__": [
      "self"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "LayerNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "reset_parameters": [
      "self"
    ],
    "pre_first_fuser_forward": [
      "self"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ],
    "op_onnx_forward": [
      "self",
      "input_"
    ]
  },
  "Quantize": {
    "__init__": [
      "self",
      "forward",
      "backward"
    ],
    "num_quantizers": [
      "self",
      "mode"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "ConstantScale": {
    "__init__": [
      "self",
      "scale"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "AllGather": {
    "__init__": [
      "self",
      "process_group"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "_wait_async": [
    "handle"
  ],
  "BasicLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features"
    ],
    "_canonicalize_tensor_parallelism": [
      "cls"
    ],
    "num_quantizers": [
      "self",
      "mode"
    ],
    "reset_parameters": [
      "self"
    ],
    "pre_first_fuser_forward": [
      "self"
    ],
    "pre_fuser_forward": [
      "self"
    ],
    "reset_recipe_state": [
      "self"
    ],
    "_functional_forward": [
      "input",
      "weight"
    ],
    "_functional_backward": [
      "grad_output",
      "input",
      "weight"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "Reshape": {
    "__init__": [
      "self",
      "shape"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "Bias": {
    "__init__": [
      "self",
      "size"
    ],
    "reset_parameters": [
      "self"
    ],
    "pre_first_fuser_forward": [
      "self"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "AllReduce": {
    "__init__": [
      "self",
      "process_group",
      "reduce_in_backward"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "ReduceScatter": {
    "__init__": [
      "self",
      "process_group"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ]
  },
  "RMSNorm": {
    "__init__": [
      "self",
      "normalized_shape"
    ],
    "reset_parameters": [
      "self"
    ],
    "pre_first_fuser_forward": [
      "self"
    ],
    "op_forward": [
      "self",
      "ctx",
      "input_",
      "prev_op_grad_output_quantizer",
      "next_op_input_quantizer"
    ],
    "op_backward": [
      "self",
      "ctx",
      "grad_output"
    ],
    "op_onnx_forward": [
      "self",
      "input_"
    ]
  },
  "AddExtraInput": {
    "__init__": [
      "self"
    ],
    "op_forward": [
      "self"
    ],
    "op_backward": [
      "self"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "ForwardLinearScaleAdd": {
    "__init__": [
      "self"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ]
  },
  "fuse_forward_linear_scale_add": [
    "ops"
  ],
  "BackwardLinearScale": {
    "__init__": [
      "self"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "fuse_backward_linear_scale": [
    "ops"
  ],
  "UserbuffersForwardLinear": {
    "__init__": [
      "self"
    ],
    "_functional_forward": [
      "input",
      "weight"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ]
  },
  "fuse_userbuffers_forward_linear": [
    "ops"
  ],
  "BackwardLinearAdd": {
    "__init__": [
      "self"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "fuse_backward_linear_add": [
    "ops"
  ],
  "_fused_activations": [],
  "_fusible_activations": [],
  "BackwardActivationBias": {
    "__init__": [
      "self"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "fuse_backward_activation_bias": [
    "ops",
    "recipe"
  ],
  "UserbuffersBackwardLinear": {
    "__init__": [
      "self"
    ],
    "_functional_backward": [
      "grad_output",
      "input",
      "weight"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "fuse_userbuffers_backward_linear": [
    "ops"
  ],
  "ForwardLinearBiasAdd": {
    "__init__": [
      "self"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ]
  },
  "fuse_forward_linear_bias_add": [
    "ops"
  ],
  "ForwardLinearBiasActivation": {
    "__init__": [
      "self"
    ],
    "fuser_forward": [
      "self",
      "basic_op_ctxs",
      "input_"
    ]
  },
  "fuse_forward_linear_bias_activation": [
    "ops"
  ],
  "BackwardAddRMSNorm": {
    "__init__": [
      "self"
    ],
    "fuser_backward": [
      "self",
      "basic_op_ctxs",
      "grad_output"
    ]
  },
  "fuse_backward_add_rmsnorm": [
    "ops"
  ],
  "_NUM_MAX_UB_STREAMS": [],
  "get_cublas_workspace_size_bytes": [],
  "get_cublas_workspace": [
    "device",
    "ub",
    "grouped_gemm"
  ],
  "validate_gemm_scale": [
    "scale",
    "required"
  ],
  "get_tensor_device": [
    "tensor"
  ],
  "general_gemm": [
    "A",
    "B",
    "out_dtype",
    "quantization_params",
    "gelu",
    "gelu_in",
    "alpha",
    "beta",
    "accumulate",
    "layout",
    "out",
    "bias",
    "use_split_accumulator",
    "grad",
    "ub",
    "ub_type",
    "extra_output",
    "bulk_overlap"
  ],
  "general_grouped_gemm": [
    "A",
    "B",
    "out",
    "quantization_params",
    "out_dtype",
    "layout",
    "m_splits",
    "gelu",
    "grad",
    "accumulate",
    "bias",
    "use_bias",
    "use_split_accumulator",
    "D_dtype",
    "single_output"
  ],
  "TORCH_DType": [],
  "QKVFormat": [],
  "QKVLayout": [],
  "AttnBiasType": [],
  "AttnMaskType": [],
  "SoftmaxType": [],
  "FusedAttnBackend": [],
  "BACKEND_F16m512_FP8_THREADS_PER_CTA": [],
  "BACKEND_F16arb_ELTS_PER_THREADS": [],
  "META_QKV": [],
  "META_DQKV": [],
  "META_O": [],
  "META_DO": [],
  "META_S": [],
  "META_DP": [],
  "fused_attn_fwd": [
    "is_training",
    "max_seqlen_q",
    "max_seqlen_kv",
    "cu_seqlens_q",
    "cu_seqlens_kv",
    "q",
    "k",
    "v",
    "fake_dtype",
    "fused_attention_backend",
    "attn_bias",
    "cu_seqlens_q_padded",
    "cu_seqlens_kv_padded",
    "page_table_k",
    "page_table_v",
    "s_quantizer",
    "o_quantizer",
    "attn_scale",
    "dropout",
    "fast_zero_fill",
    "qkv_layout",
    "attn_bias_type",
    "attn_mask_type",
    "softmax_type",
    "window_size",
    "rng_gen",
    "softmax_offset",
    "return_max_logit",
    "cuda_graph"
  ],
  "fused_attn_bwd": [
    "max_seqlen_q",
    "max_seqlen_kv",
    "cu_seqlens_q",
    "cu_seqlens_kv",
    "q",
    "k",
    "v",
    "o",
    "d_o",
    "fake_dtype",
    "dqkv_dtype",
    "aux_ctx_tensors",
    "fused_attention_backend",
    "cu_seqlens_q_padded",
    "cu_seqlens_kv_padded",
    "s_quantizer",
    "dp_quantizer",
    "dqkv_quantizer",
    "attn_scale",
    "dropout",
    "fast_zero_fill",
    "qkv_layout",
    "attn_bias_type",
    "attn_mask_type",
    "softmax_type",
    "window_size",
    "deterministic",
    "cuda_graph"
  ],
  "MAX_FUSED_SIZE": [],
  "cross_entropy_forward": [
    "_input",
    "target",
    "label_smoothing",
    "reduce_loss",
    "dist_process_group",
    "ignore_idx"
  ],
  "cross_entropy_backward": [
    "_input",
    "grad_output",
    "is_cg_capturable"
  ],
  "pad_columnwise_scale_inv": [
    "inp"
  ],
  "make_row_id_map": [
    "routing_map",
    "num_tokens",
    "num_experts"
  ],
  "permute_with_mask_map": [
    "inp",
    "row_id_map",
    "probs",
    "scale",
    "num_tokens",
    "num_experts",
    "num_out_tokens",
    "hidden_size",
    "scale_hidden_dim"
  ],
  "unpermute_with_mask_map": [
    "inp",
    "row_id_map",
    "merging_probs",
    "permuted_probs",
    "num_tokens",
    "num_experts",
    "hidden_size"
  ],
  "unpermute_with_mask_map_bwd_with_merging_probs": [
    "fwd_output_grad",
    "row_id_map",
    "fwd_input",
    "merging_probs",
    "num_tokens",
    "num_experts",
    "num_out_tokens",
    "hidden_size"
  ],
  "make_chunk_sort_map": [
    "split_sizes",
    "sorted_indices",
    "num_tokens",
    "num_splits"
  ],
  "sort_chunks_by_map": [
    "inp",
    "row_id_map",
    "probs",
    "num_tokens",
    "hidden_size",
    "is_forward"
  ],
  "get_no_random_sign_vector": [
    "device"
  ],
  "get_sign_from_vector": [
    "vector"
  ],
  "get_wgrad_sign_vector": [
    "device"
  ],
  "get_hadamard_matrix": [
    "hadamard_dimension",
    "device"
  ],
  "get_rht_matrix": [
    "with_random_sign_mask",
    "device"
  ],
  "get_random_sign_mask_for_rht": [
    "with_random_sign_mask",
    "device"
  ],
  "NVFP4Quantizer": {
    "__init__": [
      "self",
      "fp4_dtype",
      "rowwise",
      "columnwise",
      "with_amax_reduction",
      "amax_reduction_group",
      "with_rht",
      "with_post_rht_amax",
      "with_2d_quantization",
      "stochastic_rounding",
      "with_random_sign_mask"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "copy": [
      "self"
    ],
    "quantize_impl": [
      "self",
      "tensor"
    ],
    "is_quantizable": [
      "self",
      "inp"
    ],
    "get_scale_shape": [
      "self",
      "shape",
      "columnwise"
    ],
    "get_columnwise_shape": [
      "shape"
    ],
    "convert_shape_for_fp4": [
      "shape"
    ],
    "make_empty": [
      "self",
      "shape"
    ],
    "calibrate": [
      "self",
      "tensor"
    ],
    "_canonicalized_amax_reduction_group": [
      "self"
    ],
    "_get_compatible_recipe": [
      "self"
    ]
  },
  "NVFP4Tensor": {
    "__new__": [
      "cls"
    ],
    "__repr__": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "_get_quantizer": [
      "self"
    ],
    "quantize_": [
      "self",
      "tensor"
    ],
    "detach": [
      "self"
    ],
    "clone": [
      "self"
    ],
    "view": [
      "self"
    ],
    "reshape": [
      "self"
    ],
    "contiguous": [
      "self",
      "memory_format"
    ],
    "get_usages": [
      "self"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "_make_in_reduce_ex": [
      "cls",
      "shape",
      "rowwise_data",
      "rowwise_scale_inv",
      "columnwise_data",
      "columnwise_scale_inv",
      "amax_rowwise",
      "amax_columnwise",
      "fp4_dtype",
      "dtype",
      "quantizer"
    ],
    "__reduce_ex__": [
      "self",
      "protocol"
    ],
    "_get_data": [
      "self"
    ],
    "_set_data": [
      "self",
      "tensor"
    ],
    "data": []
  },
  "_ViewFunc": {
    "forward": [
      "ctx",
      "tensor",
      "shape"
    ],
    "backward": [
      "ctx",
      "grad"
    ]
  },
  "_ReshapeFunc": {
    "forward": [
      "ctx",
      "tensor",
      "shape"
    ],
    "backward": [
      "ctx",
      "grad"
    ]
  },
  "MXFP8Quantizer": {
    "__init__": [
      "self",
      "fp8_dtype"
    ],
    "copy": [
      "self"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "quantize_impl": [
      "self",
      "tensor"
    ],
    "is_quantizable": [
      "self",
      "inp"
    ],
    "make_empty": [
      "self",
      "shape"
    ],
    "calibrate": [
      "self",
      "tensor"
    ],
    "create_tensor_from_data": [
      "self",
      "data",
      "scale_inv",
      "fake_dtype",
      "fp8_dtype"
    ],
    "onnx_quantize": [
      "self",
      "tensor"
    ],
    "onnx_dequantize": [
      "self",
      "tensor"
    ],
    "_get_compatible_recipe": [
      "self"
    ]
  },
  "MXFP8Tensor": {
    "__new__": [
      "cls"
    ],
    "__repr__": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "_build_default_quantizer": [
      "self"
    ],
    "quantize_": [
      "self",
      "tensor"
    ],
    "detach": [
      "self"
    ],
    "clone": [
      "self"
    ],
    "view": [
      "self"
    ],
    "reshape": [
      "self"
    ],
    "contiguous": [
      "self",
      "memory_format"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh",
      "orig_size",
      "contiguous_orig_stride",
      "module",
      "mp_policy"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ],
    "_make_in_reduce_ex": [
      "cls",
      "rowwise_data",
      "rowwise_scale_inv",
      "columnwise_data",
      "columnwise_scale_inv",
      "fp8_dtype",
      "dtype",
      "shape",
      "quantizer"
    ],
    "__reduce_ex__": [
      "self",
      "protocol"
    ],
    "_get_data": [
      "self"
    ],
    "_set_data": [
      "self",
      "tensor"
    ],
    "data": []
  },
  "_ops_to_preserve_subclass_in_fsdp2": [],
  "Float8Quantizer": {
    "__init__": [
      "self",
      "scale",
      "amax",
      "fp8_dtype"
    ],
    "copy": [
      "self"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "quantize_impl": [
      "self",
      "tensor"
    ],
    "make_empty": [
      "self",
      "shape"
    ],
    "calibrate": [
      "self",
      "tensor"
    ],
    "create_tensor_from_data": [
      "self",
      "data",
      "fake_dtype",
      "requires_grad",
      "internal"
    ],
    "onnx_quantize": [
      "self",
      "tensor"
    ],
    "onnx_dequantize": [
      "self",
      "tensor"
    ],
    "_get_compatible_recipe": [
      "self"
    ],
    "supports_only_rowwise_all_gather": [
      "self"
    ]
  },
  "Float8CurrentScalingQuantizer": {
    "__init__": [
      "self",
      "fp8_dtype",
      "device"
    ],
    "copy": [
      "self"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "quantize_impl": [
      "self",
      "tensor"
    ],
    "make_empty": [
      "self",
      "shape"
    ],
    "calibrate": [
      "self",
      "tensor"
    ],
    "create_tensor_from_data": [
      "self",
      "data",
      "fake_dtype",
      "requires_grad",
      "internal"
    ],
    "onnx_quantize": [
      "self",
      "tensor"
    ],
    "onnx_dequantize": [
      "self",
      "tensor"
    ],
    "_canonicalized_amax_reduction_group": [
      "self"
    ],
    "_get_compatible_recipe": [
      "self"
    ],
    "supports_only_rowwise_all_gather": [
      "self"
    ]
  },
  "Float8Tensor": {
    "__repr__": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "quantize_": [
      "self",
      "tensor"
    ],
    "detach": [
      "self"
    ],
    "clone": [
      "self"
    ],
    "view": [
      "self"
    ],
    "reshape": [
      "self"
    ],
    "contiguous": [
      "self",
      "memory_format"
    ],
    "_reset_caches": [
      "self"
    ],
    "remove_caches": [
      "self"
    ],
    "make_like": [
      "cls",
      "tensor"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "fsdp_pre_all_gather": [
      "self",
      "mesh",
      "orig_size",
      "contiguous_orig_stride",
      "module",
      "mp_policy"
    ],
    "fsdp_post_all_gather": [
      "self",
      "all_gather_outputs",
      "metadata",
      "param_dtype"
    ],
    "_make_in_reduce_ex": [
      "cls",
      "data",
      "fp8_dtype",
      "fp8_scale_inv",
      "dtype",
      "shape"
    ],
    "__reduce_ex__": [
      "self",
      "protocol"
    ],
    "_get_data": [
      "self"
    ],
    "_set_data": [
      "self",
      "tensor"
    ],
    "data": []
  },
  "replace_raw_data": [
    "tensor",
    "new_raw_data"
  ],
  "cast_master_weights_to_fp8": [
    "model_weights",
    "master_weights",
    "start_offsets",
    "group",
    "fsdp_shard_model_weights",
    "manual_post_all_gather_processing"
  ],
  "_cast_master_weights_to_fp8_delayed_scaling": [
    "params",
    "group",
    "use_fsdp_shard_model_weights",
    "manual_post_all_gather_processing"
  ],
  "_cast_master_weights_to_fp8_current_scaling": [
    "params",
    "group",
    "use_fsdp_shard_model_weights",
    "manual_post_all_gather_processing"
  ],
  "_cast_master_weights_to_fp8_blockwise_scaling": [
    "params",
    "group",
    "use_fsdp_shard_model_weights",
    "manual_post_all_gather_processing"
  ],
  "_cast_master_weights_to_fp8_mxfp8_scaling": [
    "params",
    "group",
    "use_fsdp_shard_model_weights",
    "manual_post_all_gather_processing"
  ],
  "post_all_gather_processing": [
    "model_weights"
  ],
  "is_custom": [
    "x"
  ],
  "_make_module_cast_func": [
    "dtype"
  ],
  "get_all_tensor_types": [],
  "Float8BlockQuantizer": {
    "__init__": [
      "self",
      "fp8_dtype"
    ],
    "copy": [
      "self"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "quantize_impl": [
      "self",
      "tensor"
    ],
    "get_scale_shape": [
      "self",
      "shape",
      "columnwise"
    ],
    "get_columnwise_shape": [
      "self",
      "shape"
    ],
    "is_quantizable": [
      "self",
      "inp"
    ],
    "make_empty": [
      "self",
      "shape"
    ],
    "calibrate": [
      "self",
      "tensor"
    ],
    "_get_compatible_recipe": [
      "self"
    ]
  },
  "Float8BlockwiseQTensor": {
    "__new__": [
      "cls"
    ],
    "__repr__": [
      "self"
    ],
    "quantize_": [
      "self",
      "tensor"
    ],
    "dequantize": [
      "self"
    ],
    "detach": [
      "self"
    ],
    "clone": [
      "self"
    ],
    "view": [
      "self"
    ],
    "reshape": [
      "self"
    ],
    "untyped_storage": [
      "self"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "contiguous": [
      "self",
      "memory_format"
    ],
    "_make_in_reduce_ex": [
      "cls",
      "shape",
      "rowwise_data",
      "rowwise_scale_inv",
      "columnwise_data",
      "columnwise_scale_inv",
      "fp8_dtype",
      "dtype",
      "quantizer",
      "is_2D_scaled",
      "data_format"
    ],
    "__reduce_ex__": [
      "self",
      "protocol"
    ],
    "_get_data": [
      "self"
    ],
    "_set_data": [
      "self",
      "tensor"
    ],
    "data": []
  },
  "_QuantizeFunc": {
    "forward": [
      "_ctx",
      "tensor",
      "quantize_impl"
    ],
    "backward": [
      "_ctx",
      "grad"
    ]
  },
  "_IdentityFunc": {
    "forward": [
      "ctx",
      "tensor",
      "init_kwargs"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_stride_from_shape": [
    "shape"
  ],
  "Float8BlockwiseQTensorStorage": {
    "__new__": [
      "cls",
      "rowwise_data",
      "rowwise_scale_inv",
      "columnwise_data",
      "columnwise_scale_inv",
      "fp8_dtype",
      "quantizer",
      "is_2D_scaled",
      "data_format"
    ],
    "clear": [
      "self"
    ],
    "get_metadata": [
      "self"
    ],
    "_is_gemm_ready_format": [
      "self"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "get_data_tensors": [
      "self",
      "rowwise_data",
      "columnwise_data"
    ],
    "_transpose_dq_columnwise_output": [
      "self",
      "columnwise_dq"
    ],
    "_dequantize_vectorwise": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "size": [
      "self"
    ],
    "_create_columnwise": [
      "self"
    ],
    "_transpose_columnwise_data": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ],
    "get_usages": [
      "self"
    ]
  },
  "_FromFloat8Func": {
    "forward": [
      "_ctx",
      "tensor",
      "dtype"
    ],
    "backward": [
      "_ctx",
      "grad"
    ]
  },
  "Float8TensorStorage": {
    "__new__": [
      "cls"
    ],
    "clear": [
      "self"
    ],
    "get_metadata": [
      "self"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "get_data_tensors": [
      "self",
      "rowwise_data",
      "columnwise_data"
    ],
    "dequantize": [
      "self"
    ],
    "size": [
      "self"
    ],
    "view": [
      "self",
      "shape"
    ],
    "__repr__": [
      "self"
    ],
    "_create_transpose": [
      "self"
    ],
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ],
    "get_usages": [
      "self"
    ]
  },
  "_FromMXFP8Func": {
    "forward": [
      "_ctx",
      "tensor",
      "dtype"
    ],
    "backward": [
      "_ctx",
      "grad"
    ]
  },
  "MXFP8TensorStorage": {
    "__new__": [
      "cls",
      "rowwise_data",
      "rowwise_scale_inv",
      "columnwise_data",
      "columnwise_scale_inv",
      "fp8_dtype",
      "quantizer"
    ],
    "clear": [
      "self"
    ],
    "get_metadata": [
      "self"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "get_data_tensors": [
      "self",
      "rowwise_data",
      "columnwise_data"
    ],
    "dequantize": [
      "self"
    ],
    "size": [
      "self"
    ],
    "view": [
      "self",
      "shape"
    ],
    "__repr__": [
      "self"
    ],
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ],
    "get_usages": [
      "self"
    ]
  },
  "_fp4_e2m1_vals": [
    "device",
    "dtype"
  ],
  "_FromNVFP4Func": {
    "forward": [
      "_ctx",
      "tensor",
      "dtype"
    ],
    "backward": [
      "_ctx",
      "grad"
    ]
  },
  "NVFP4TensorStorage": {
    "__new__": [
      "cls",
      "rowwise_data",
      "rowwise_scale_inv",
      "columnwise_data",
      "columnwise_scale_inv",
      "amax_rowwise",
      "amax_columnwise",
      "fp4_dtype",
      "quantizer"
    ],
    "clear": [
      "self"
    ],
    "get_metadata": [
      "self"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "get_data_tensors": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "size": [
      "self",
      "dim"
    ],
    "view": [
      "self",
      "shape"
    ],
    "__repr__": [
      "self"
    ],
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ]
  },
  "GEMMType": {
    "FPROP": [],
    "DGRAD": [],
    "WGRAD": []
  },
  "HIGH_PRECISION_FLOAT_DTYPES": [],
  "Fp4Formats": {
    "E2M1": []
  },
  "roundup_div": [
    "x",
    "y"
  ],
  "nvfp4_ref_rht_2d_quantizer_factory": [
    "role"
  ],
  "cast_to_fp4x2": [
    "x"
  ],
  "cast_from_fp4x2": [
    "x",
    "dq_dtype"
  ],
  "cast_to_e8": [
    "decode_scale"
  ],
  "cast_to_e4m3": [
    "decode_scale",
    "global_amax"
  ],
  "high_precision_gemm_ref": [
    "a",
    "b",
    "out_dtype",
    "accumulate",
    "is_a_transposed",
    "is_b_transposed",
    "out",
    "bias",
    "scale_alpha"
  ],
  "NVFP4TensorRef": {
    "custom": [
      "self"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "_data": [
      "self",
      "value"
    ],
    "_scale_inv": [
      "self",
      "value"
    ],
    "__repr__": [
      "self"
    ],
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ],
    "_create_transpose": [
      "self"
    ],
    "size": [
      "self"
    ]
  },
  "NVFP4QuantizerRef": {
    "__init__": [
      "self",
      "dtype",
      "rowwise",
      "columnwise",
      "pow_2_scales",
      "eps",
      "quant_tile_shape",
      "with_rht",
      "with_random_sign_mask"
    ],
    "custom": [
      "self"
    ],
    "_build_hadamard_matrix": [
      "size",
      "device",
      "dtype",
      "with_random_sign_mask"
    ],
    "_apply_rht": [
      "self",
      "x"
    ],
    "_recover_swizzled_scales": [
      "swizzled_scale",
      "scale",
      "m",
      "n",
      "block_length"
    ],
    "_quantize_blockwise_reference": [
      "cls",
      "x",
      "global_amax",
      "tile_len_x",
      "tile_len_y"
    ],
    "_pad_tensor": [
      "tensor",
      "row_divisor",
      "col_divisor"
    ],
    "_rm_pad_tensor": [
      "tensor",
      "original_size"
    ],
    "_quantize": [
      "self",
      "tensor"
    ],
    "quantize": [
      "self",
      "tensor"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "supports_allgather_fp8": [
      "self"
    ],
    "transpose_qresult": [
      "self",
      "qresult"
    ],
    "supports_dequantize": [
      "self"
    ],
    "is_data_t_transposed_in_memory": [
      "self"
    ],
    "qgemm": [
      "self",
      "qx",
      "qw",
      "m_params",
      "out_dtype",
      "sx",
      "sw",
      "bias",
      "out",
      "accumulate",
      "gemm_type",
      "qresult_x",
      "qresult_w"
    ]
  },
  "custom_gemm": [
    "A",
    "B",
    "workspace",
    "out_dtype",
    "quantization_params",
    "gelu",
    "gelu_in",
    "accumulate",
    "layout",
    "out",
    "bias",
    "use_split_accumulator",
    "grad"
  ],
  "current_scaling_ref_quantizer_factory": [
    "role"
  ],
  "CurrentScalingTensorRef": {
    "custom": [
      "self"
    ],
    "prepare_for_saving": [
      "self"
    ],
    "restore_from_saved": [
      "self",
      "tensors"
    ],
    "_data": [
      "self",
      "value"
    ],
    "_scale_inv": [
      "self",
      "value"
    ],
    "__repr__": [
      "self"
    ],
    "update_usage": [
      "self",
      "rowwise_usage",
      "columnwise_usage"
    ],
    "_create_transpose": [
      "self"
    ],
    "size": [
      "self"
    ]
  },
  "_scale_from_amax_tensor": [
    "x_dtype",
    "amax",
    "quant_dtype"
  ],
  "CurrentScalingQuantizerRef": {
    "__init__": [
      "self",
      "dtype",
      "rowwise",
      "columnwise",
      "pow_2_scales",
      "eps"
    ],
    "custom": [
      "self"
    ],
    "supports_allgather_fp8": [
      "self"
    ],
    "compute_scale": [
      "cls",
      "x",
      "quant_dtype",
      "eps",
      "pow_2_scales"
    ],
    "_quantize": [
      "self",
      "tensor"
    ],
    "quantize": [
      "self",
      "tensor"
    ],
    "dequantize": [
      "self",
      "tensor",
      "scale",
      "dtype"
    ],
    "qgemm": [
      "self",
      "qx",
      "qw",
      "m_params",
      "out_dtype",
      "sx",
      "sw",
      "bias",
      "out",
      "accumulate",
      "gemm_type",
      "qresult_x",
      "qresult_w"
    ],
    "transpose_qresult": [
      "self",
      "qresult"
    ],
    "update_quantized": [
      "self",
      "src",
      "dst"
    ],
    "make_empty": [
      "self",
      "shape"
    ]
  },
  "get_fp8_meta": [
    "fp8_tensor"
  ],
  "FusedAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "amsgrad"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "_apply_scale": [
      "self",
      "state_name",
      "unscaled_state",
      "scaled_state",
      "scale"
    ],
    "get_unscaled_state": [
      "self",
      "param",
      "state_name"
    ],
    "set_scaled_state": [
      "self",
      "param",
      "state_name",
      "unscaled_state"
    ],
    "_initialize_state": [
      "self",
      "param",
      "state_name",
      "zero_buffer",
      "store_param_remainders"
    ],
    "initialize_state": [
      "self",
      "param",
      "store_param_remainders"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "step": [
      "self",
      "closure",
      "grad_scaler"
    ]
  },
  "FusedSGD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "dampening",
      "weight_decay",
      "nesterov"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "get_momentums": [
      "self",
      "params"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "MultiTensorApply": {
    "__init__": [
      "self",
      "chunk_size"
    ],
    "__call__": [
      "self",
      "op",
      "noop_flag_buffer",
      "tensor_lists"
    ]
  },
  "multi_tensor_applier": [],
  "_Fp8Unpadding": {
    "forward": [
      "ctx",
      "inp",
      "non_tensor_args"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "Fp8Unpadding": {
    "__init__": [
      "self",
      "num_gemms",
      "align_size"
    ],
    "forward": [
      "self",
      "inp",
      "m_splits"
    ]
  },
  "_get_act_func_supported_list": [
    "recipe"
  ],
  "_act_func": [
    "activation",
    "recipe"
  ],
  "_LayerNormMLP": {
    "_forward": [
      "ctx",
      "inp",
      "ln_weight",
      "ln_bias",
      "fc1_weight",
      "fc1_bias",
      "fc2_weight",
      "fc2_bias",
      "non_tensor_args"
    ],
    "forward": [
      "ctx",
      "inp",
      "ln_weight",
      "ln_bias",
      "fc1_weight",
      "fc1_bias",
      "fc2_weight",
      "fc2_bias",
      "non_tensor_args"
    ],
    "_recompute": [
      "ctx"
    ],
    "backward": [
      "ctx"
    ]
  },
  "LayerNormMLP": {
    "__init__": [
      "self",
      "hidden_size",
      "ffn_hidden_size",
      "eps",
      "sequence_parallel",
      "return_bias",
      "get_rng_state_tracker",
      "tp_group",
      "tp_size",
      "init_method",
      "bias",
      "normalization",
      "activation",
      "activation_params",
      "output_layer_init_method",
      "fuse_wgrad_accumulation",
      "params_dtype",
      "return_layernorm_output",
      "return_layernorm_output_gathered",
      "seq_length",
      "micro_batch_size",
      "set_parallel_mode",
      "zero_centered_gamma",
      "device",
      "ub_overlap_ag",
      "name",
      "ub_overlap_rs",
      "ub_overlap_rs_dgrad",
      "ub_bulk_dgrad",
      "ub_bulk_wgrad",
      "delay_wgrad_compute",
      "symmetric_ar_type",
      "checkpoint"
    ],
    "set_meta_tensor": [
      "self",
      "fwd",
      "recipe"
    ],
    "reset_layer_norm_parameters": [
      "self"
    ],
    "reset_parameters": [
      "self",
      "defer_init"
    ],
    "forward": [
      "self",
      "inp",
      "is_first_microbatch"
    ],
    "_get_quantizers": [
      "self",
      "fp8_output",
      "is_grad_enabled"
    ],
    "onnx_forward": [
      "self",
      "inp",
      "is_grad_enabled"
    ],
    "_get_debug_quantizers": [
      "self",
      "fp8_output",
      "is_grad_enabled"
    ],
    "_customize_quantizers_float8_current_scaling": [
      "self",
      "fwd",
      "recipe"
    ],
    "_customize_quantizers_nvfp4": [
      "self",
      "fwd",
      "recipe"
    ],
    "_get_weight_tensors": [
      "self"
    ],
    "_get_weight_quantizers": [
      "self"
    ],
    "_customize_quantizers_float8_blockwise_scaling": [
      "self",
      "fwd",
      "recipe"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "_2X_ACC_FPROP": [],
  "_2X_ACC_DGRAD": [],
  "_2X_ACC_WGRAD": [],
  "_dummy_wgrads": [],
  "_ub_communicators": [],
  "layers_atomic_ring_exchange": [],
  "UserBufferQuantizationMode": {
    "NONE": [],
    "FP8": []
  },
  "get_dummy_wgrad": [
    "shape",
    "dtype",
    "zero"
  ],
  "initialize_ub": [
    "shape",
    "tp_size",
    "use_fp8",
    "quantization_modes",
    "dtype",
    "ub_cfgs",
    "bootstrap_backend"
  ],
  "get_ub": [
    "name",
    "use_fp8"
  ],
  "destroy_ub": [],
  "fill_userbuffers_buffer_for_all_gather": [
    "comm",
    "local_tensor",
    "quantizer",
    "process_group"
  ],
  "TransformerEngineBaseModule": {
    "__init__": [
      "self"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "adjust_amax_history_length": [
      "self",
      "length",
      "fwd"
    ],
    "set_meta_tensor": [
      "self",
      "fwd",
      "recipe"
    ],
    "_update_weight_quantizers": [
      "self"
    ],
    "_get_weight_tensors": [
      "self"
    ],
    "_get_weight_quantizers": [
      "self"
    ],
    "init_fp8_meta_tensors": [
      "self",
      "recipe"
    ],
    "get_fp8_meta_tensors": [
      "self"
    ],
    "reset_fp8_meta_tensors": [
      "self",
      "fp8_meta_tensors"
    ],
    "get_extra_state": [
      "self"
    ],
    "set_extra_state": [
      "self",
      "state"
    ],
    "set_activation_dtype": [
      "self",
      "inp"
    ],
    "set_tensor_parallel_group": [
      "self",
      "tp_group"
    ],
    "_get_fp8_params": [
      "self"
    ],
    "init_fp8_metadata": [
      "self",
      "num_gemms"
    ],
    "prepare_forward": [
      "self",
      "inp",
      "num_gemms",
      "allow_non_contiguous",
      "allow_different_data_and_param_types"
    ],
    "set_nccl_overlap_warning_if_tp": [
      "self"
    ],
    "grad_output_preprocess": [
      "ctx",
      "grad_output",
      "row_parallel_mode",
      "quantizer"
    ],
    "register_parameter": [
      "self",
      "name",
      "param"
    ],
    "reset_parameters": [
      "self",
      "defer_init"
    ],
    "forward": [
      "self"
    ],
    "get_weight_workspace": [
      "self"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "register_wgrad_accumulation_and_reduce_hooks": [
      "self",
      "wgrad_accumulation_and_reduce_hook"
    ],
    "need_backward_dw": [
      "self"
    ],
    "backward_dw": [
      "self"
    ],
    "is_debug_iter": [
      "self"
    ],
    "no_debug_features_active": [
      "self",
      "quantizers"
    ],
    "_validate_name": [
      "self"
    ],
    "_check_weight_tensor_recipe_correspondence": [
      "self"
    ]
  },
  "_Fp8Padding": {
    "forward": [
      "ctx",
      "inp",
      "non_tensor_args"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "Fp8Padding": {
    "__init__": [
      "self",
      "num_gemms",
      "align_size"
    ],
    "forward": [
      "self",
      "inp",
      "m_splits"
    ]
  },
  "_get_normalization_func": [
    "normalization",
    "forward"
  ],
  "apply_normalization": [
    "inputmat",
    "ln_out",
    "ln_weight",
    "ln_bias",
    "eps",
    "output_quantizer",
    "output_dtype",
    "normalization",
    "fwd_ln_sm_margin",
    "zero_centered_gamma"
  ],
  "_NoopCatFunc": {
    "forward": [
      "ctx",
      "dim"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "noop_cat": [
    "tensors",
    "dim"
  ],
  "_ParameterInitMeta": {
    "__post_init__": [
      "self"
    ]
  },
  "WeightGradStore": {
    "__init__": [
      "self",
      "delay_wgrad_compute",
      "ub_bulk_wgrad"
    ],
    "delay_wgrad_compute": [
      "self"
    ],
    "enable_delay_wgrad_compute": [
      "self"
    ],
    "disable_delay_wgrad_compute": [
      "self"
    ],
    "put": [
      "self",
      "tensor_list",
      "func"
    ],
    "pop": [
      "self"
    ],
    "assert_empty": [
      "self"
    ]
  },
  "_GroupedLinear": {
    "forward": [
      "ctx",
      "inp",
      "non_tensor_args"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "GroupedLinear": {
    "__init__": [
      "self",
      "num_gemms",
      "in_features",
      "out_features",
      "sequence_parallel",
      "fuse_wgrad_accumulation",
      "tp_group",
      "tp_size",
      "get_rng_state_tracker",
      "rng_tracker_name",
      "init_method",
      "bias",
      "return_bias",
      "params_dtype",
      "parallel_mode",
      "device",
      "ub_overlap_rs",
      "ub_overlap_ag",
      "ub_name",
      "delay_wgrad_compute",
      "save_original_input",
      "name"
    ],
    "set_meta_tensor": [
      "self",
      "fwd",
      "recipe"
    ],
    "reset_parameters": [
      "self",
      "defer_init"
    ],
    "forward": [
      "self",
      "inp",
      "m_splits",
      "is_first_microbatch"
    ],
    "backward_dw": [
      "self"
    ],
    "_customize_quantizers_float8_current_scaling": [
      "self",
      "fwd",
      "recipe"
    ],
    "_get_weight_tensors": [
      "self"
    ],
    "_get_weight_quantizers": [
      "self"
    ],
    "_get_quantizers": [
      "self"
    ],
    "_get_debug_quantizers": [
      "self"
    ]
  },
  "_Linear": {
    "forward": [
      "ctx",
      "weight",
      "inp",
      "bias",
      "non_tensor_args"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_LayerNormLinear": {
    "forward": [
      "ctx",
      "inp",
      "ln_weight",
      "ln_bias",
      "weight",
      "bias",
      "non_tensor_args"
    ],
    "backward": [
      "ctx"
    ]
  },
  "LayerNormLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "eps",
      "sequence_parallel",
      "fuse_wgrad_accumulation",
      "tp_group",
      "tp_size",
      "get_rng_state_tracker",
      "init_method",
      "bias",
      "normalization",
      "return_bias",
      "params_dtype",
      "parallel_mode",
      "return_layernorm_output",
      "return_layernorm_output_gathered",
      "parameters_split",
      "zero_centered_gamma",
      "device",
      "ub_overlap_ag",
      "ub_overlap_rs",
      "ub_overlap_rs_dgrad",
      "ub_bulk_wgrad",
      "ub_bulk_dgrad",
      "ub_name",
      "delay_wgrad_compute",
      "symmetric_ar_type",
      "name"
    ],
    "set_meta_tensor": [
      "self",
      "fwd",
      "recipe"
    ],
    "reset_layer_norm_parameters": [
      "self"
    ],
    "reset_parameters": [
      "self",
      "defer_init"
    ],
    "forward": [
      "self",
      "inp",
      "is_first_microbatch",
      "fp8_output",
      "fp8_grad"
    ],
    "_get_quantizers": [
      "self",
      "fp8_output",
      "fp8_grad",
      "is_grad_enabled"
    ],
    "_get_debug_quantizers": [
      "self",
      "fp8_output",
      "fp8_grad",
      "is_grad_enabled"
    ],
    "_get_weight_and_bias_tensors": [
      "self"
    ],
    "onnx_forward": [
      "self",
      "inp",
      "fp8_output",
      "is_grad_enabled"
    ],
    "_customize_quantizers_float8_current_scaling": [
      "self",
      "fwd",
      "recipe"
    ],
    "_customize_quantizers_nvfp4": [
      "self",
      "fwd",
      "recipe"
    ],
    "_get_weight_tensors": [
      "self"
    ],
    "_get_weight_quantizers": [
      "self"
    ],
    "_customize_quantizers_float8_blockwise_scaling": [
      "self",
      "fwd",
      "recipe"
    ]
  },
  "_dpa_fp8_recipe": [],
  "_dpa_fp8_recipe_dpa": [],
  "_dpa_fp8_recipe_mha": [],
  "MultiheadAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "kv_channels",
      "attention_dropout",
      "layernorm_epsilon",
      "init_method",
      "output_layer_init_method",
      "layer_number",
      "attn_mask_type",
      "window_size",
      "tp_group",
      "tp_size",
      "num_gqa_groups",
      "fuse_wgrad_accumulation",
      "get_rng_state_tracker",
      "sequence_parallel",
      "params_dtype",
      "return_bias",
      "return_layernorm_output",
      "input_layernorm",
      "attention_type",
      "set_parallel_mode",
      "fuse_qkv_params",
      "zero_centered_gamma",
      "qkv_weight_interleaved",
      "rotary_pos_interleaved",
      "ub_overlap_ag",
      "ub_overlap_rs",
      "ub_overlap_rs_dgrad",
      "ub_bulk_dgrad",
      "ub_bulk_wgrad",
      "bias",
      "normalization",
      "device",
      "qkv_format",
      "name",
      "qk_norm_type",
      "qk_norm_eps",
      "qk_norm_before_rope",
      "seq_length",
      "micro_batch_size",
      "softmax_type"
    ],
    "_create_qk_norm_modules": [
      "self",
      "qk_norm_type",
      "qk_norm_eps",
      "device",
      "seq_length",
      "micro_batch_size"
    ],
    "set_tensor_parallel_group": [
      "self",
      "tp_group"
    ],
    "set_context_parallel_group": [
      "self",
      "cp_group",
      "cp_global_ranks",
      "cp_stream",
      "cp_comm_type"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_output",
      "attn_mask_type",
      "window_size",
      "is_first_microbatch",
      "checkpoint_core_attention",
      "inference_params",
      "rotary_pos_emb",
      "core_attention_bias_type",
      "core_attention_bias",
      "alibi_slopes",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "cu_seqlens_q_padded",
      "cu_seqlens_kv_padded",
      "max_seqlen_q",
      "max_seqlen_kv",
      "fast_zero_fill",
      "pad_between_seqs"
    ]
  },
  "RotaryPositionEmbedding": {
    "__init__": [
      "self",
      "dim",
      "rotary_percent",
      "seq_len_interpolation_factor",
      "pretrained_max_position_embeddings",
      "rotary_base",
      "interleaved"
    ],
    "forward": [
      "self",
      "max_seq_len",
      "offset"
    ]
  },
  "FusedRoPEFunc": {
    "forward": [
      "ctx",
      "t",
      "freqs",
      "start_positions",
      "tensor_format",
      "interleaved",
      "cu_seqlens",
      "cp_size",
      "cp_rank"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "FusedQKVRoPEFunc": {
    "forward": [
      "ctx",
      "qkv",
      "q_freqs",
      "k_freqs",
      "qkv_split_arg_list",
      "start_positions",
      "tensor_format",
      "interleaved",
      "cp_size",
      "cp_rank"
    ],
    "backward": [
      "ctx",
      "grad_output_q",
      "grad_output_k",
      "grad_output_v"
    ]
  },
  "_rotate_half": [
    "x",
    "interleaved"
  ],
  "_apply_rotary_pos_emb_base": [
    "t",
    "freqs",
    "tensor_format",
    "interleaved"
  ],
  "_get_freqs_on_this_cp_rank": [
    "freqs",
    "seqlen",
    "cp_size",
    "cp_rank"
  ],
  "apply_rotary_pos_emb": [
    "t",
    "freqs",
    "tensor_format",
    "start_positions",
    "interleaved",
    "fused",
    "cu_seqlens",
    "cp_size",
    "cp_rank"
  ],
  "apply_fused_qkv_rotary_pos_emb": [
    "qkv",
    "q_freqs",
    "k_freqs",
    "qkv_split_arg_list",
    "tensor_format",
    "start_positions",
    "interleaved",
    "cu_seqlens",
    "cp_size",
    "cp_rank"
  ],
  "KVCacheManager": {
    "__init__": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "allocate_memory": [
      "self",
      "layer_number"
    ],
    "pre_step": [
      "self",
      "step_dict"
    ],
    "step": [
      "self",
      "layer_number",
      "new_k",
      "new_v",
      "cu_new_seqlens",
      "cu_cached_seqlens",
      "qkv_format"
    ]
  },
  "InferenceParams": {
    "__init__": [
      "self",
      "max_batch_size",
      "max_sequence_length",
      "num_heads_kv",
      "head_dim_k",
      "dtype",
      "head_dim_v",
      "is_paged",
      "total_num_pages",
      "page_size",
      "max_ctx_len",
      "qkv_format",
      "custom_cache_manager"
    ],
    "reset": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "allocate_memory": [
      "self",
      "layer_number"
    ],
    "pre_step": [
      "self",
      "step_dict"
    ],
    "get_seqlens_pre_step": [
      "self"
    ],
    "convert_paged_to_nonpaged": [
      "self",
      "layer_number"
    ],
    "step": [
      "self",
      "layer_number",
      "new_k",
      "new_v",
      "qkv_format"
    ]
  },
  "NonPagedKVCacheManager": {
    "__init__": [
      "self",
      "max_batch_size",
      "max_seqlen",
      "num_heads",
      "head_dim_k",
      "dtype",
      "head_dim_v"
    ],
    "allocate_memory": [
      "self",
      "layer_number"
    ],
    "pre_step": [
      "self",
      "step_dict"
    ],
    "step": [
      "self",
      "layer_number",
      "new_k",
      "new_v",
      "cu_new_seqlens",
      "cu_cached_seqlens",
      "qkv_format"
    ]
  },
  "Page": {
    "__init__": [
      "self",
      "page_id"
    ],
    "allocate_page": [
      "self"
    ],
    "deallocate_page": [
      "self"
    ]
  },
  "PagedKVCacheManager": {
    "__init__": [
      "self",
      "total_num_pages",
      "page_size",
      "num_heads",
      "head_dim_k",
      "dtype",
      "max_batch_size",
      "max_seqlen",
      "head_dim_v"
    ],
    "reset": [
      "self"
    ],
    "allocate_memory": [
      "self",
      "layer_number"
    ],
    "print_cache": [
      "self"
    ],
    "get_sequence_count": [
      "self"
    ],
    "get_sequence_lengths": [
      "self"
    ],
    "has_free_page": [
      "self"
    ],
    "get_page_count": [
      "self",
      "seq"
    ],
    "get_page_list": [
      "self",
      "seq"
    ],
    "get_page_table": [
      "self",
      "sequences"
    ],
    "allocate_page": [
      "self",
      "seq"
    ],
    "allocate_sequence": [
      "self",
      "seq",
      "context_len"
    ],
    "deallocate_sequence": [
      "self",
      "seq"
    ],
    "pre_step": [
      "self",
      "step_dict"
    ],
    "step": [
      "self",
      "layer_number",
      "new_k",
      "new_v",
      "cu_new_seqlens",
      "cu_cached_seqlens",
      "qkv_format"
    ]
  },
  "_NVTE_DEBUG": [],
  "_NVTE_DEBUG_LEVEL": [],
  "_NVTE_FLASH_ATTN": [],
  "_print_layer": [],
  "_print_rank": [],
  "_cu_seqlens_cache": [],
  "AttentionLogging": {
    "_log_level": [],
    "_formatter": [],
    "_stream_handler": [],
    "fa_logger": [],
    "_is_logging_setup": [],
    "setup_logging": []
  },
  "_get_supported_versions": [
    "version_min",
    "version_max"
  ],
  "maybe_contiguous": [
    "tensor"
  ],
  "FlashAttentionUtils": {
    "is_installed": [],
    "version": [],
    "version_required": [],
    "version_required_blackwell": [],
    "max_version": [],
    "v2_plus": [],
    "v2_1_plus": [],
    "v2_3_plus": [],
    "v2_4_plus": [],
    "v2_4_1_plus": [],
    "v2_5_plus": [],
    "v2_5_7_plus": [],
    "v2_6_0_plus": [],
    "v2_7_0_plus": [],
    "warning_printed": [],
    "v3_is_installed": [],
    "fa3_version": [],
    "v3_0_0_beta": [],
    "use_v3": [],
    "v3_installation_steps": [],
    "v3_warning_printed": [],
    "set_flash_attention_version": [],
    "set_flash_attention_3_params": []
  },
  "AttentionParams": {
    "__eq__": [
      "self",
      "other"
    ]
  },
  "get_attention_backend": [
    "attention_params"
  ],
  "get_padding_mask": [
    "batch_size",
    "cu_seqlens_q",
    "cu_seqlens_kv",
    "max_seqlen_q",
    "max_seqlen_kv",
    "attention_type"
  ],
  "get_full_mask": [
    "max_seqlen_q",
    "max_seqlen_kv",
    "attn_mask_type",
    "attention_mask",
    "window_size",
    "attention_type",
    "bottom_right_alignment"
  ],
  "get_alibi": [
    "_alibi_cache",
    "num_heads",
    "max_seqlen_q",
    "max_seqlen_kv",
    "actual_seqlens_q",
    "actual_seqlens_kv",
    "alibi_slopes",
    "bias_dtype",
    "bottom_right_alignment"
  ],
  "get_cu_seqlens": [
    "mask"
  ],
  "get_cu_seqlens_and_indices": [
    "mask"
  ],
  "get_indices": [
    "max_seqlen",
    "cu_seqlens"
  ],
  "get_full_cu_seqlens": [
    "batch_size",
    "max_seqlen",
    "device"
  ],
  "_pack_tensor": [
    "indices",
    "tensor"
  ],
  "_pack_2_tensors": [
    "indices",
    "t1",
    "t2"
  ],
  "_pack_3_tensors": [
    "indices",
    "t1",
    "t2",
    "t3"
  ],
  "_unpack_tensor": [
    "indices",
    "dim0",
    "tensor"
  ],
  "_unpack_2_tensors": [
    "indices",
    "dim0",
    "t1",
    "t2"
  ],
  "_unpack_3_tensors": [
    "indices",
    "dim0",
    "t1",
    "t2",
    "t3"
  ],
  "PackTensors": {
    "forward": [
      "ctx",
      "indices"
    ],
    "backward": [
      "ctx"
    ]
  },
  "UnpackTensor": {
    "forward": [
      "ctx",
      "indices",
      "dim0",
      "tensor"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "ConvertTHDtoBSHD": {
    "forward": [
      "ctx",
      "thd_tensor",
      "cu_seqlens",
      "max_seqlen"
    ],
    "backward": [
      "ctx",
      "bshd_tensor"
    ]
  },
  "ConvertBSHDtoTHD": {
    "forward": [
      "ctx",
      "bshd_tensor",
      "cu_seqlens"
    ],
    "backward": [
      "ctx",
      "thd_tensor"
    ]
  },
  "get_qkv_format": [
    "qkv_layout",
    "inference_params"
  ],
  "get_qkv_layout": [
    "q",
    "k",
    "v",
    "qkv_format",
    "inference_params"
  ],
  "check_set_window_size": [
    "attn_mask_type",
    "window_size"
  ],
  "get_attention_quantizers": [
    "fp8",
    "quantizers"
  ],
  "print_quantizers": [
    "label",
    "layer_number",
    "QKV_quantizer",
    "O_quantizer",
    "S_quantizer",
    "dQKV_quantizer",
    "dO_quantizer",
    "dP_quantizer"
  ],
  "combine_and_quantize": [
    "qkv_layout",
    "q",
    "k",
    "v",
    "qkv_quantizer"
  ],
  "combine_and_dequantize": [
    "qkv_layout",
    "q_fp8",
    "k_fp8",
    "v_fp8",
    "src_nominal_dtype",
    "des_nominal_dtype"
  ],
  "flash_attn_cuda_bwd": [],
  "flash_attn_func": [],
  "flash_attn_varlen_func": [],
  "_flash_attn_fwd": [],
  "_flash_attn_bwd": [],
  "_flash_attn_varlen_fwd": [],
  "_flash_attn_varlen_bwd": [],
  "_dpa_fp8_cs_o_in_f16": [],
  "FP8EmulationFunc": {
    "forward": [
      "ctx",
      "tensor1",
      "tensor2",
      "tensor3",
      "quantizer",
      "quantizer_name",
      "qkv_layout"
    ],
    "backward": [
      "ctx",
      "grad1",
      "grad2",
      "grad3"
    ]
  },
  "UnfusedDotProductAttention": {
    "__init__": [
      "self",
      "softmax_scale",
      "attention_type",
      "attention_dropout",
      "attention_dropout_ctx",
      "layer_number",
      "softmax_type",
      "return_max_logit"
    ],
    "forward": [
      "self",
      "_alibi_cache",
      "query_layer",
      "key_layer",
      "value_layer",
      "qkv_layout",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "max_seqlen_q",
      "max_seqlen_kv",
      "attn_mask_type",
      "attention_mask",
      "window_size",
      "core_attention_bias_type",
      "core_attention_bias",
      "alibi_slopes",
      "inference_params",
      "softmax_offset",
      "fp8",
      "fp8_meta",
      "quantizers",
      "fp8_output"
    ]
  },
  "_PrepareQKVForFA": {
    "forward": [
      "_ctx",
      "query_layer",
      "key_layer",
      "value_layer"
    ],
    "backward": [
      "_ctx",
      "dq",
      "dk",
      "dv"
    ]
  },
  "FlashAttention": {
    "__init__": [
      "self",
      "softmax_scale",
      "attention_dropout",
      "attention_dropout_ctx",
      "attention_type",
      "layer_number",
      "deterministic"
    ],
    "forward": [
      "self",
      "query_layer",
      "key_layer",
      "value_layer",
      "attention_mask",
      "qkv_layout",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "max_seqlen_q",
      "max_seqlen_kv",
      "attn_mask_type",
      "window_size",
      "alibi_slopes",
      "cp_group",
      "cp_global_ranks",
      "cp_stream",
      "cp_comm_type",
      "fp8",
      "fp8_meta",
      "quantizers",
      "inference_params",
      "flash_attention_backend",
      "fp8_output",
      "num_splits"
    ]
  },
  "FusedAttnFunc": {
    "forward": [
      "ctx",
      "is_training",
      "max_seqlen_q",
      "max_seqlen_kv",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "cu_seqlens_q_padded",
      "cu_seqlens_kv_padded",
      "page_table_k",
      "page_table_v",
      "q",
      "k",
      "v",
      "attn_bias",
      "attn_scale",
      "dropout_p",
      "fast_zero_fill",
      "qkv_layout",
      "attn_bias_type",
      "attn_mask_type",
      "softmax_type",
      "window_size",
      "rng_gen",
      "fused_attention_backend",
      "use_FAv2_bwd",
      "fp8",
      "fp8_meta",
      "quantizers",
      "deterministic",
      "softmax_offset",
      "fp8_output",
      "layer_number",
      "return_max_logit"
    ],
    "backward": [
      "ctx",
      "d_out"
    ]
  },
  "FusedAttention": {
    "__init__": [
      "self",
      "softmax_scale",
      "attention_dropout",
      "attention_dropout_ctx",
      "attention_type",
      "layer_number",
      "deterministic",
      "softmax_type",
      "return_max_logit"
    ],
    "forward": [
      "self",
      "query_layer",
      "key_layer",
      "value_layer",
      "qkv_layout",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "cu_seqlens_q_padded",
      "cu_seqlens_kv_padded",
      "max_seqlen_q",
      "max_seqlen_kv",
      "attn_mask_type",
      "attention_mask",
      "window_size",
      "fused_attention_backend",
      "core_attention_bias_type",
      "core_attention_bias",
      "fast_zero_fill",
      "cp_group",
      "cp_global_ranks",
      "cp_stream",
      "cp_comm_type",
      "fp8",
      "fp8_meta",
      "quantizers",
      "pad_between_seqs",
      "inference_params",
      "softmax_offset",
      "fp8_output"
    ]
  },
  "THREADS_PER_WARP": [],
  "THREADS_PER_BLOCK": [],
  "_default_causal_mask": [],
  "_get_default_causal_mask": [
    "mask_type",
    "sq",
    "sk"
  ],
  "ScaledUpperTriangMaskedSoftmax": {
    "forward": [
      "ctx",
      "inputs",
      "scale"
    ],
    "backward": [
      "ctx",
      "output_grads"
    ]
  },
  "ScaledAlignedCausalMaskedSoftmax": {
    "forward": [
      "ctx",
      "inputs",
      "scale"
    ],
    "backward": [
      "ctx",
      "output_grads"
    ]
  },
  "ScaledMaskedSoftmax": {
    "forward": [
      "ctx",
      "inputs",
      "mask",
      "scale"
    ],
    "backward": [
      "ctx",
      "output_grads"
    ]
  },
  "ScaledSoftmax": {
    "forward": [
      "ctx",
      "inputs",
      "scale"
    ],
    "backward": [
      "ctx",
      "output_grads"
    ]
  },
  "FusedScaleMaskSoftmax": {
    "__init__": [
      "self",
      "mask_func",
      "softmax_in_fp32"
    ],
    "forward": [
      "self",
      "inp",
      "mask",
      "attn_mask_type",
      "scale"
    ],
    "is_kernel_available": [
      "self",
      "mask",
      "b",
      "np",
      "sq",
      "sk"
    ],
    "forward_fused_softmax": [
      "self",
      "inp",
      "mask",
      "scale"
    ],
    "forward_torch_softmax": [
      "self",
      "inp",
      "mask",
      "scale"
    ],
    "get_batch_per_block": [
      "key_seq_len"
    ]
  },
  "_attention_backends": [],
  "_alibi_cache": [],
  "formats": [],
  "_dpa_fp8_format": [],
  "_dpa_fp8ds_amax_algo": [],
  "_dpa_fp8ds_amax_histlen": [],
  "_dpa_fp8ds_reduce_amax": [],
  "DotProductAttention": {
    "__init__": [
      "self",
      "num_attention_heads",
      "kv_channels",
      "num_gqa_groups",
      "attention_dropout",
      "qkv_format",
      "attn_mask_type",
      "window_size",
      "sequence_parallel",
      "tp_size",
      "get_rng_state_tracker",
      "tp_group",
      "layer_number",
      "attention_type",
      "cp_group",
      "cp_global_ranks",
      "cp_stream",
      "cp_comm_type",
      "softmax_scale",
      "softmax_type",
      "return_max_logit"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ],
    "_checkpointed_attention_forward": [
      "self",
      "attention_func"
    ],
    "set_context_parallel_group": [
      "self",
      "cp_group",
      "cp_global_ranks",
      "cp_stream",
      "cp_comm_type"
    ],
    "init_fp8_metadata": [
      "self",
      "num_gemms"
    ],
    "set_meta_tensor": [
      "self",
      "fwd",
      "recipe"
    ],
    "forward": [
      "self",
      "query_layer",
      "key_layer",
      "value_layer",
      "attention_mask",
      "qkv_format",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "cu_seqlens_q_padded",
      "cu_seqlens_kv_padded",
      "max_seqlen_q",
      "max_seqlen_kv",
      "attn_mask_type",
      "window_size",
      "checkpoint_core_attention",
      "core_attention_bias_type",
      "core_attention_bias",
      "alibi_slopes",
      "fast_zero_fill",
      "inference_params",
      "pad_between_seqs",
      "fp8_output",
      "num_splits"
    ]
  },
  "_cu_seqlens_info_with_cp_cache": [],
  "_seq_chunk_ids_cache_for_reordering_before_attn": [],
  "_seq_chunk_ids_cache_for_reordering_after_attn": [],
  "_softmax_offset_chunk_ids_cache": [],
  "flash_attn_p2p_communicate": [
    "rank",
    "send_tensor",
    "send_dst",
    "recv_tensor",
    "recv_src",
    "cp_group",
    "batch_p2p_comm"
  ],
  "flash_attn_fwd_out_correction_init": [
    "out_init_step",
    "softmax_lse",
    "softmax_lse_init_step",
    "seq_dim"
  ],
  "flash_attn_fwd_out_correction": [
    "out",
    "out_per_step",
    "softmax_lse",
    "softmax_lse_per_step",
    "seq_dim"
  ],
  "flash_attn_fwd_second_half_out_correction": [
    "out",
    "out_per_step",
    "softmax_lse",
    "softmax_lse_per_step",
    "seq_dim"
  ],
  "flash_attn_fwd_softmax_lse_correction": [
    "softmax_lse",
    "softmax_lse_per_step"
  ],
  "flash_attn_fwd_second_half_softmax_lse_correction": [
    "softmax_lse",
    "softmax_lse_per_step"
  ],
  "get_cu_seqlens_on_cp_rank": [
    "cu_seqlens",
    "cu_seqlens_padded_on_cp_rank",
    "cp_size",
    "cp_rank",
    "first_half",
    "second_half"
  ],
  "get_seq_chunk_ids_for_reordering_before_attn": [
    "cp_size",
    "device"
  ],
  "get_seq_chunk_ids_for_reordering_after_attn": [
    "cp_size",
    "device"
  ],
  "reorder_seq_chunks_for_a2a_before_attn": [
    "x",
    "chunk_ids_for_a2a",
    "seq_dim",
    "cp_size"
  ],
  "reorder_seq_chunks_for_a2a_after_attn": [
    "x",
    "chunk_ids_for_a2a",
    "seq_dim",
    "cp_size"
  ],
  "reorder_seq_chunks_before_a2a_after_attn_thd": [
    "x",
    "cu_seqlens",
    "cp_size",
    "seq_dim"
  ],
  "reorder_seq_chunks_after_a2a_before_attn_thd": [
    "x",
    "cu_seqlens",
    "seq_chunk_ids",
    "cp_size",
    "seq_dim"
  ],
  "flash_attn_a2a_communicate": [
    "a2a_inputs",
    "chunk_ids_for_a2a",
    "seq_dim",
    "cp_size",
    "cp_group",
    "cp_stream",
    "before_attn",
    "qkv_format",
    "cu_seqlens_padded"
  ],
  "flash_attn_a2a_communicate_softmax_offset": [
    "tensor",
    "h_dim",
    "cp_size",
    "cp_group",
    "cp_stream",
    "before_attn"
  ],
  "_get_cu_seqlens_info_with_cp": [
    "batch_size",
    "max_seqlen",
    "cp_size",
    "cu_seqlens"
  ],
  "get_fa_args": [
    "forward",
    "use_flash_attn_3",
    "qkv_format",
    "cu_seqlens_q",
    "cu_seqlens_kv",
    "max_seqlen_q",
    "max_seqlen_kv",
    "dq",
    "dk",
    "dv"
  ],
  "cp_p2p_fwd_prepare_qkv": [
    "q_part",
    "k_part",
    "v_part",
    "qkv_format",
    "pad_between_seqs",
    "cu_seqlens_q",
    "cu_seqlens_kv",
    "cu_seqlens_q_padded",
    "cu_seqlens_kv_padded",
    "cu_seqlens_q_half",
    "cu_seqlens_kv_half",
    "rank",
    "step",
    "cp_size",
    "section"
  ],
  "cp_p2p_fwd_fused_attn": [
    "attn_bias",
    "attn_bias_",
    "is_training",
    "max_seqlen_q",
    "max_seqlen_kv",
    "cu_seqlens_q_padded",
    "cu_seqlens_kv_padded",
    "fused_attn_backend",
    "softmax_scale",
    "dropout_p",
    "qkv_layout",
    "attn_mask_type",
    "attn_bias_type",
    "fp8",
    "q_fp8",
    "k_fp8",
    "v_fp8",
    "fwd_nominal_dtype",
    "S_quantizer_per_step",
    "O_quantizer_per_step",
    "rank",
    "step",
    "cp_size",
    "return_max_logit",
    "q_part",
    "k_part",
    "v_part",
    "cu_seqlens_q_per_step",
    "cu_seqlens_kv_per_step",
    "section"
  ],
  "cp_p2p_fwd_flash_attn": [
    "use_flash_attn_3",
    "qkv_format",
    "fa_forward_kwargs",
    "flash_attn_fwd",
    "max_seqlen_q",
    "max_seqlen_kv",
    "q_part",
    "k_part",
    "v_part",
    "cu_seqlens_q_per_step",
    "cu_seqlens_kv_per_step",
    "section"
  ],
  "cp_p2p_bwd_prepare_qkv": [
    "q_part",
    "k_part",
    "v_part",
    "out_part",
    "dout_part",
    "qkv_format",
    "cu_seqlens_q_padded",
    "cu_seqlens_kv_padded",
    "section"
  ],
  "cp_p2p_bwd_fused_attn": [
    "fp8",
    "fp8_recipe",
    "q_fp8",
    "kv_fp8",
    "out_fp8",
    "dout_fp8",
    "softmax_lse",
    "softmax_lse_",
    "rng_states",
    "attn_dbias",
    "attn_biases",
    "max_seqlen_q",
    "max_seqlen_kv",
    "step",
    "cp_size",
    "cu_seqlens_q_per_step",
    "cu_seqlens_kv_per_step",
    "cu_seqlens_q_padded",
    "cu_seqlens_kv_padded",
    "fused_attn_backend",
    "softmax_scale",
    "dropout_p",
    "qkv_layout",
    "attn_mask_type",
    "attn_bias_type",
    "deterministic",
    "fwd_nominal_dtype",
    "bwd_nominal_dtype",
    "bwd_output_te_dtype",
    "S_quantizer",
    "dP_quantizer_per_step",
    "dQKV_quantizer_per_step",
    "q_part",
    "k_part",
    "v_part",
    "out_part",
    "dout_part",
    "section"
  ],
  "cp_p2p_bwd_flash_attn": [
    "use_flash_attn_3",
    "qkv_format",
    "max_seqlen_q",
    "max_seqlen_kv",
    "cu_seqlens_q_per_step",
    "cu_seqlens_kv_per_step",
    "step",
    "cp_size",
    "fa_backward_kwargs",
    "flash_attn_bwd",
    "rng_states",
    "softmax_lse",
    "softmax_lse_",
    "q_part",
    "k_part",
    "v_part",
    "out_part",
    "dout_part",
    "section"
  ],
  "AttnFuncWithCPAndKVP2P": {
    "forward": [
      "ctx",
      "is_training",
      "q",
      "k",
      "v",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "max_seqlen_q",
      "max_seqlen_kv",
      "cu_seqlens_q_padded",
      "cu_seqlens_kv_padded",
      "dropout_p",
      "softmax_scale",
      "qkv_format",
      "attn_mask_type",
      "attn_bias_type",
      "attn_bias",
      "deterministic",
      "use_fused_attention",
      "return_max_logit",
      "fp8",
      "fp8_meta",
      "cp_group",
      "cp_global_ranks",
      "cp_stream",
      "quantizers",
      "pad_between_seqs",
      "use_flash_attn_3",
      "fp8_output",
      "layer_number"
    ],
    "backward": [
      "ctx",
      "dout"
    ]
  },
  "get_kv_seq_info_after_all_gather": [
    "local_chunk_id",
    "cp_size",
    "max_seqlen_q",
    "max_seqlen_kv",
    "window_size",
    "causal"
  ],
  "AttnFuncWithCPAndKVAllGather": {
    "forward": [
      "ctx",
      "is_training",
      "q",
      "k",
      "v",
      "cu_seqlens_q",
      "max_seqlen_q",
      "max_seqlen_kv",
      "cu_seqlens_q_padded",
      "dropout_p",
      "softmax_scale",
      "qkv_format",
      "attn_mask_type",
      "attn_bias_type",
      "attn_bias",
      "deterministic",
      "use_fused_attention",
      "return_max_logit",
      "window_size",
      "cp_group",
      "cp_stream",
      "use_flash_attn_3"
    ],
    "backward": [
      "ctx",
      "dout"
    ]
  },
  "AttnFuncWithCPAndQKVOA2A": {
    "forward": [
      "ctx",
      "is_training",
      "q",
      "k",
      "v",
      "cu_seqlens_q",
      "cu_seqlens_kv",
      "max_seqlen_q",
      "max_seqlen_kv",
      "cu_seqlens_q_padded",
      "cu_seqlens_kv_padded",
      "dropout_p",
      "softmax_scale",
      "qkv_format",
      "attn_mask_type",
      "attn_bias_type",
      "attn_bias",
      "deterministic",
      "use_fused_attention",
      "return_max_logit",
      "window_size",
      "fp8",
      "fp8_meta",
      "cp_group",
      "cp_stream",
      "quantizers",
      "use_flash_attn_3",
      "softmax_type",
      "softmax_offset",
      "fp8_output"
    ],
    "backward": [
      "ctx",
      "dout"
    ]
  },
  "attn_forward_func_with_cp": [
    "is_training",
    "q",
    "k",
    "v",
    "cu_seqlens_q",
    "cu_seqlens_kv",
    "max_seqlen_q",
    "max_seqlen_kv",
    "cu_seqlens_q_padded",
    "cu_seqlens_kv_padded",
    "dropout_p",
    "cp_group",
    "cp_global_ranks",
    "cp_stream",
    "cp_comm_type",
    "softmax_scale",
    "qkv_format",
    "attn_mask_type",
    "attn_bias_type",
    "attn_bias",
    "deterministic",
    "use_fused_attention",
    "window_size",
    "fp8",
    "fp8_meta",
    "quantizers",
    "pad_between_seqs",
    "use_flash_attn_3",
    "softmax_type",
    "softmax_offset",
    "fp8_output",
    "layer_number",
    "return_max_logit"
  ],
  "pad_thd_sequences_for_cp": [
    "input_ids",
    "labels",
    "cu_seqlens",
    "divisibility_factor",
    "padding_token_id",
    "padding_label_id"
  ],
  "generate_positional_ids_for_cp": [
    "cu_seqlens",
    "divisibility_factor",
    "dtype"
  ],
  "get_batch_on_this_cp_rank": [
    "cu_seqlens_padded",
    "input_ids_padded",
    "labels_padded",
    "position_ids_padded",
    "cp_group",
    "qvk_format"
  ],
  "layernorm_mlp": [
    "x",
    "gamma",
    "beta",
    "kernels",
    "biases",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "norm_input_axes",
    "dot_1_input_axes",
    "dot_2_input_axes",
    "kernel_1_axes",
    "kernel_2_axes",
    "ffn1_ckpt_name",
    "ffn2_ckpt_name",
    "activation_type",
    "activation_params",
    "collective_op_sets",
    "quantizer_sets"
  ],
  "_layernorm_mlp": [
    "x",
    "gamma",
    "beta",
    "kernel_1",
    "kernel_2",
    "bias_1",
    "bias_2",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "norm_input_axes",
    "dot_1_input_axes",
    "dot_2_input_axes",
    "kernel_1_axes",
    "kernel_2_axes",
    "ffn1_ckpt_name",
    "ffn2_ckpt_name",
    "activation_type",
    "activation_params",
    "collective_op_sets",
    "quantizer_sets"
  ],
  "_layernorm_mlp_fwd_rule": [
    "x",
    "gamma",
    "beta",
    "kernel_1",
    "kernel_2",
    "bias_1",
    "bias_2",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "norm_input_axes",
    "dot_1_input_axes",
    "dot_2_input_axes",
    "kernel_1_axes",
    "kernel_2_axes",
    "ffn1_ckpt_name",
    "ffn2_ckpt_name",
    "activation_type",
    "activation_params",
    "collective_op_sets",
    "quantizer_sets"
  ],
  "_layernorm_mlp_bwd_rule": [
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "norm_input_axes",
    "dot_1_input_axes",
    "dot_2_input_axes",
    "kernel_1_axes",
    "kernel_2_axes",
    "ffn1_ckpt_name",
    "ffn2_ckpt_name",
    "activation_type",
    "activation_params",
    "collective_op_sets",
    "ctx",
    "grad"
  ],
  "_all_gather_kernel": [
    "kernel",
    "mesh_axis",
    "axis_idx"
  ],
  "_psum_scatter_kernel": [
    "kernel",
    "scattered_kernel_shape",
    "mesh_axis",
    "axis_idx"
  ],
  "dense": [
    "x",
    "kernel",
    "bias",
    "contracting_dims",
    "transpose_batch_sequence",
    "input_axes",
    "kernel_axes",
    "output_axes",
    "collective_op_set",
    "quantizer_set"
  ],
  "_dense": [
    "x",
    "kernel",
    "bias",
    "contracting_dims",
    "transpose_batch_sequence",
    "input_axes",
    "kernel_axes",
    "output_axes",
    "collective_op_set",
    "quantizer_set"
  ],
  "_dense_fwd_rule": [
    "x",
    "kernel",
    "bias",
    "contracting_dims",
    "transpose_batch_sequence",
    "input_axes",
    "kernel_axes",
    "output_axes",
    "collective_op_set",
    "quantizer_set"
  ],
  "_dense_bwd_rule": [
    "contracting_dims",
    "transpose_batch_sequence",
    "input_axes",
    "kernel_axes",
    "output_axes",
    "collective_op_set",
    "ctx",
    "grad"
  ],
  "grouped_dense": [
    "x",
    "kernel",
    "group_sizes",
    "contracting_dims",
    "bias",
    "kernel_amax",
    "precision",
    "preferred_element_type",
    "group_offset",
    "quantizer_set",
    "kernel_fsdp_info"
  ],
  "_grouped_dense": [
    "x",
    "kernel",
    "group_sizes",
    "contracting_dims",
    "bias",
    "kernel_amax",
    "precision",
    "preferred_element_type",
    "group_offset",
    "quantizer_set",
    "kernel_fsdp_info"
  ],
  "_grouped_dense_fwd_rule": [
    "x",
    "kernel",
    "group_sizes",
    "contracting_dims",
    "bias",
    "kernel_amax",
    "precision",
    "preferred_element_type",
    "group_offset",
    "quantizer_set",
    "kernel_fsdp_info"
  ],
  "_grouped_dense_bwd_rule": [
    "contracting_dims",
    "precision",
    "preferred_element_type",
    "group_offset",
    "kernel_fsdp_info",
    "ctx",
    "grad"
  ],
  "_PXLA_THREAD_RESOURCES": [],
  "BATCH_AXES": [],
  "SEQLEN_AXES": [],
  "SEQLEN_TP_AXES": [],
  "SEQLEN_CP_AXES": [],
  "HEAD_AXES": [],
  "HIDDEN_AXES": [],
  "HIDDEN_TP_AXES": [],
  "JOINED_AXES": [],
  "W_NO_SHARD_AXES": [],
  "W_FSDP_AXES": [],
  "W_TP_AXES": [],
  "W_JOINED_AXES": [],
  "_get_mesh_info": [
    "resource",
    "mesh"
  ],
  "_validate_mesh_resource_configuration": [
    "mesh_resource"
  ],
  "get_sharding_map_logic_axis_to_mesh_axis": [],
  "_generate_pspec": [
    "logical_axis_names"
  ],
  "with_sharding_constraint": [
    "x",
    "pspec"
  ],
  "with_sharding_constraint_by_logical_axes": [
    "x",
    "logical_axis_names"
  ],
  "get_all_mesh_axes": [],
  "get_padded_spec": [
    "spec",
    "ndim"
  ],
  "lax_paral_op": [
    "x",
    "ops",
    "mesh_resource",
    "mesh"
  ],
  "num_of_devices": [],
  "get_num_devices_in_mesh": [
    "mesh"
  ],
  "get_mesh_axis_size": [
    "axis",
    "mesh"
  ],
  "get_mesh_axis_rank": [
    "axis",
    "mesh"
  ],
  "get_mesh_axis_rank_host": [
    "axis",
    "mesh"
  ],
  "MeshResource": {},
  "_GLOBAL_MESH_RESOURCE": [],
  "global_shard_guard": [
    "resource"
  ],
  "global_mesh_resource": [],
  "all_reduce_sum_along_dp_fsdp": [
    "x",
    "mesh"
  ],
  "all_reduce_sum_along_dp_fsdp_tpsp": [
    "x",
    "mesh"
  ],
  "all_reduce_max_along_all_axes_except_PP": [
    "x",
    "mesh"
  ],
  "tpsp_axis_size": [],
  "dp_or_fsdp_axis_size": [],
  "activation": [
    "x",
    "activation_type",
    "quantizer",
    "act_params"
  ],
  "_activation": [
    "x",
    "activation_type",
    "quantizer",
    "act_params"
  ],
  "_activation_fwd_rule": [
    "x",
    "activation_type",
    "quantizer",
    "act_params"
  ],
  "_activation_bwd_rule": [
    "activation_type",
    "act_params",
    "ctx",
    "g"
  ],
  "get_cuda_major_version": [],
  "AttnSoftmaxType": {
    "VANILLA_SOFTMAX": [],
    "OFF_BY_ONE_SOFTMAX": [],
    "LEARNABLE_SOFTMAX": [],
    "from_str": [
      "cls",
      "softmax_type"
    ]
  },
  "CPStrategy": {
    "DEFAULT": [],
    "ALL_GATHER": [],
    "RING": []
  },
  "ReorderStrategy": {
    "DualChunkSwap": [],
    "Striped": []
  },
  "make_swa_mask": [
    "segment_pos_q",
    "segment_pos_kv",
    "window_size",
    "dtype",
    "segment_ids_q",
    "segment_ids_kv"
  ],
  "canonicalize_attn_mask_type": [
    "attn_mask_type"
  ],
  "is_fused_attn_kernel_available": [
    "is_training",
    "q_dtype",
    "kv_dtype",
    "qkv_layout",
    "attn_bias_type",
    "attn_mask_type",
    "softmax_type",
    "dropout_probability",
    "q_num_heads",
    "kv_num_heads",
    "q_max_seqlen",
    "kv_max_seqlen",
    "head_dim_qk",
    "head_dim_v",
    "window_size"
  ],
  "_obtain_batch_and_max_seqlen": [
    "qkv",
    "qkv_layout"
  ],
  "reorder_causal_load_balancing": [
    "tensor",
    "strategy",
    "cp_size",
    "seq_dim",
    "stripe_size"
  ],
  "inverse_reorder_causal_load_balancing": [
    "tensor",
    "strategy",
    "cp_size",
    "seq_dim",
    "stripe_size"
  ],
  "_get_seqlens_and_offsets": [
    "segment_ids",
    "max_segments_per_seq"
  ],
  "_mask_to_seqlens_offset": [
    "mask",
    "max_segments_per_seq"
  ],
  "_fast_causal_adjust_seqlen_and_offsets": [
    "segment_pos_q",
    "q_len",
    "q_offset",
    "segment_pos_kv",
    "kv_len",
    "kv_offset"
  ],
  "_segment_ids_pos_to_seqlens_offsets_fast_causal_path": [
    "segment_ids_q",
    "segment_ids_kv",
    "segment_pos_q",
    "segment_pos_kv",
    "max_segments_per_seq"
  ],
  "run_length_fill_flattened": [
    "segment_ids_flattened"
  ],
  "run_length_fill": [
    "segment_ids"
  ],
  "_segment_ids_pos_to_seqlens_offsets": [
    "segment_ids_q",
    "segment_ids_kv",
    "segment_pos_q",
    "segment_pos_kv",
    "attn_mask_type",
    "window_size",
    "max_segments_per_seq"
  ],
  "_segment_ids_to_seqlens": [
    "segment_ids_q",
    "segment_ids_kv",
    "attn_mask_type"
  ],
  "SequenceDescriptor": {
    "__init__": [
      "self",
      "seqlens",
      "seq_offsets",
      "segment_ids",
      "segment_pos"
    ],
    "tree_flatten": [
      "self"
    ],
    "tree_unflatten": [
      "cls",
      "aux_data",
      "children"
    ],
    "get_seqlens_and_offsets": [
      "self",
      "attn_mask_type",
      "qkv_layout",
      "window_size",
      "max_segments_per_seq"
    ],
    "_expand_to_pair": [
      "cls",
      "value"
    ],
    "from_seqlens": [
      "cls",
      "seqlens"
    ],
    "from_seqlens_and_offsets": [
      "cls",
      "seqlens",
      "seq_offsets"
    ],
    "from_segment_ids_and_pos": [
      "cls",
      "segment_ids",
      "segment_pos"
    ]
  },
  "_legacy_fused_attn": [
    "qkv",
    "bias",
    "mask",
    "seed",
    "attn_bias_type",
    "attn_mask_type",
    "qkv_layout",
    "softmax_type",
    "scaling_factor",
    "dropout_probability",
    "is_training",
    "window_size",
    "context_parallel_strategy",
    "context_parallel_causal_load_balanced",
    "context_parallel_axis",
    "softmax_offset"
  ],
  "fused_attn_thd": [
    "qkv",
    "bias",
    "q_seq_lens",
    "kv_seq_lens",
    "q_seq_offsets",
    "kv_seq_offsets",
    "seed",
    "attn_bias_type",
    "attn_mask_type",
    "qkv_layout",
    "scaling_factor",
    "dropout_probability",
    "is_training",
    "max_segments_per_seq",
    "window_size",
    "context_parallel_strategy",
    "context_parallel_causal_load_balanced",
    "context_parallel_axis",
    "softmax_offset"
  ],
  "_fused_attn": [
    "qkv",
    "bias",
    "softmax_offset",
    "sequence_descriptor",
    "seed",
    "attn_bias_type",
    "attn_mask_type",
    "qkv_layout",
    "softmax_type",
    "scaling_factor",
    "dropout_probability",
    "is_training",
    "max_segments_per_seq",
    "window_size",
    "context_parallel_strategy",
    "context_parallel_causal_load_balanced",
    "context_parallel_axis",
    "context_checkpoint_name",
    "stripe_size"
  ],
  "_fused_attn_fwd_rule": [
    "qkv",
    "bias",
    "softmax_offset",
    "sequence_descriptor",
    "seed",
    "attn_bias_type",
    "attn_mask_type",
    "qkv_layout",
    "softmax_type",
    "scaling_factor",
    "dropout_probability",
    "is_training",
    "max_segments_per_seq",
    "window_size",
    "context_parallel_strategy",
    "context_parallel_causal_load_balanced",
    "context_parallel_axis",
    "context_checkpoint_name",
    "stripe_size"
  ],
  "_fused_attn_bwd_rule": [
    "attn_bias_type",
    "attn_mask_type",
    "qkv_layout",
    "softmax_type",
    "scaling_factor",
    "dropout_probability",
    "is_training",
    "max_segments_per_seq",
    "window_size",
    "context_parallel_strategy",
    "context_parallel_causal_load_balanced",
    "context_parallel_axis",
    "context_checkpoint_name",
    "stripe_size",
    "ctx",
    "dz"
  ],
  "fused_attn": [
    "qkv",
    "bias",
    "sequence_descriptor",
    "seed",
    "attn_bias_type",
    "attn_mask_type",
    "qkv_layout",
    "softmax_type",
    "scaling_factor",
    "dropout_probability",
    "is_training",
    "max_segments_per_seq",
    "window_size",
    "context_parallel_strategy",
    "context_parallel_causal_load_balanced",
    "context_parallel_axis",
    "context_checkpoint_name",
    "softmax_offset",
    "stripe_size"
  ],
  "SoftmaxFusionType": {
    "SCALED": [],
    "SCALED_MASKED": [],
    "SCALED_UPPER_TRIANG_MASKED": []
  },
  "softmax": [
    "logits",
    "mask",
    "scale_factor",
    "softmax_fusion_type"
  ],
  "_softmax": [
    "logits",
    "mask",
    "scale_factor",
    "softmax_fusion_type"
  ],
  "_softmax_fwd_rule": [
    "logits",
    "mask",
    "scale_factor",
    "softmax_fusion_type"
  ],
  "_softmax_bwd_rule": [
    "scale_factor",
    "softmax_fusion_type",
    "ctx",
    "dz"
  ],
  "layernorm_dense": [
    "x",
    "kernel",
    "gamma",
    "beta",
    "bias",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "layernorm_input_axes",
    "dot_input_axes",
    "kernel_axes",
    "quantizer_set"
  ],
  "_layernorm_dense": [
    "x",
    "kernel",
    "gamma",
    "beta",
    "bias",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "layernorm_input_axes",
    "dot_input_axes",
    "kernel_axes",
    "quantizer_set"
  ],
  "_layernorm_dense_fwd_rule": [
    "x",
    "kernel",
    "gamma",
    "beta",
    "bias",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "layernorm_input_axes",
    "dot_input_axes",
    "kernel_axes",
    "quantizer_set"
  ],
  "_layernorm_dense_bwd_rule": [
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "transpose_batch_sequence",
    "layernorm_input_axes",
    "dot_input_axes",
    "kernel_axes",
    "ctx",
    "grad"
  ],
  "canonicalize_norm_type": [
    "x"
  ],
  "layernorm": [
    "x",
    "gamma",
    "beta",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "quantizer"
  ],
  "_layernorm": [
    "x",
    "gamma",
    "beta",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "quantizer"
  ],
  "_layernorm_fwd_rule": [
    "x",
    "gamma",
    "beta",
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "quantizer"
  ],
  "_layernorm_bwd_rule": [
    "norm_type",
    "zero_centered_gamma",
    "epsilon",
    "ctx",
    "dz"
  ],
  "te_gemms_saveable": [
    "prim"
  ],
  "dots_and_te_gemms_with_no_batch_dims": [],
  "checkpoint_dots_and_te_gemms": [],
  "token_dispatch": [
    "inp",
    "routing_map",
    "num_out_tokens",
    "probs"
  ],
  "_token_dispatch": [
    "inp",
    "routing_map",
    "probs",
    "num_out_tokens"
  ],
  "_token_dispatch_fwd_rule": [
    "inp",
    "routing_map",
    "probs",
    "num_out_tokens"
  ],
  "_token_dispatch_bwd_rule": [
    "_routing_map",
    "_num_out_tokens",
    "residuals",
    "g"
  ],
  "token_combine": [
    "inp",
    "row_id_map",
    "merging_probs"
  ],
  "_token_combine": [
    "inp",
    "row_id_map",
    "merging_probs"
  ],
  "_token_combine_fwd_rule": [
    "inp",
    "row_id_map",
    "merging_probs"
  ],
  "_token_combine_bwd_rule": [
    "row_id_map",
    "residuals",
    "g"
  ],
  "sort_chunks_by_index": [
    "inp",
    "split_sizes",
    "sorted_indices"
  ],
  "_sort_chunks_by_index": [
    "inp",
    "split_sizes",
    "sorted_indices"
  ],
  "_sort_chunks_by_index_fwd_rule": [
    "inp",
    "split_sizes",
    "sorted_indices"
  ],
  "_sort_chunks_by_index_bwd_rule": [
    "_split_sizes",
    "_sorted_indices",
    "residuals",
    "g"
  ],
  "BasePrimitive": {
    "name": [],
    "_is_enabled": [],
    "_default_disable_names": [],
    "enabled": [
      "cls"
    ],
    "set_enabled": [
      "cls",
      "enabled"
    ],
    "abstract": [],
    "outer_abstract": [
      "cls"
    ],
    "lowering": [],
    "impl": [],
    "outer_impl": [
      "cls"
    ],
    "batcher": [],
    "infer_sharding_from_operands": [],
    "partition": [],
    "shardy_sharding_rule": []
  },
  "_primitive_registry": [],
  "register_primitive": [
    "cls",
    "outer_only"
  ],
  "manage_primitives": [
    "enable_names",
    "disable_names",
    "disable_all_first"
  ],
  "BaseDBiasQuantizePrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "x_aval",
      "scale_aval",
      "amax_aval",
      "sr_rng_state_aval",
      "post_rht_amax_aval",
      "rht_matrix_aval"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "x",
      "scale",
      "amax",
      "sr_rng_state",
      "post_rht_amax",
      "rht_matrix"
    ],
    "impl": [
      "x",
      "scale",
      "amax",
      "sr_rng_state",
      "post_rht_amax",
      "rht_matrix",
      "out_dtype",
      "scaling_mode",
      "q_layout",
      "flatten_axis",
      "scale_dtype",
      "is_dbias",
      "is_outer",
      "stochastic_rounding",
      "use_rht"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "out_dtype",
      "scaling_mode",
      "q_layout",
      "flatten_axis",
      "scale_dtype",
      "is_dbias",
      "is_outer",
      "stochastic_rounding",
      "use_rht",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "out_dtype",
      "scaling_mode",
      "q_layout",
      "flatten_axis",
      "scale_dtype",
      "is_dbias",
      "is_outer",
      "stochastic_rounding",
      "use_rht",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "out_dtype",
      "scaling_mode",
      "q_layout",
      "flatten_axis",
      "scale_dtype",
      "is_dbias",
      "is_outer",
      "stochastic_rounding",
      "use_rht",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "DBiasQuantizePrimitive": {},
  "QuantizePrimitive": {},
  "_jax_quantize": [
    "x",
    "quantizer",
    "dq_dtype",
    "flatten_axis"
  ],
  "_jax_dbias": [
    "dx",
    "dtype",
    "flatten_axis"
  ],
  "_jax_quantize_dbias": [
    "x",
    "quantizer",
    "dq_dtype",
    "flatten_axis"
  ],
  "_quantize_dbias_impl": [
    "x",
    "quantizer",
    "is_dbias",
    "dq_dtype",
    "flatten_axis",
    "amax_scope",
    "transpose_batch_sequence"
  ],
  "quantize": [
    "x",
    "quantizer",
    "flatten_axis",
    "amax_scope",
    "transpose_batch_sequence"
  ],
  "quantize_dbias": [
    "dz",
    "quantizer",
    "is_dbias",
    "flatten_axis",
    "amax_scope",
    "transpose_batch_sequence"
  ],
  "GroupedQuantizePrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "x_aval",
      "scale_aval",
      "group_sizes_aval"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "x",
      "scale",
      "group_sizes"
    ],
    "impl": [
      "x",
      "scale",
      "group_sizes",
      "out_dtype",
      "scaling_mode",
      "q_layout",
      "flatten_axis",
      "group_axis",
      "scale_dtype"
    ]
  },
  "grouped_quantize": [
    "x",
    "quantizer",
    "group_sizes",
    "amax",
    "flatten_axis"
  ],
  "grouped_dbias": [
    "grad",
    "group_sizes"
  ],
  "ActivationEnum": [],
  "ClampedSwigluParams": {
    "__hash__": [
      "self"
    ],
    "to_ffi_lowering_dict": [
      "self"
    ]
  },
  "ActivationParams": {
    "create": [
      "activation_type"
    ],
    "__hash__": [
      "self"
    ],
    "to_ffi_lowering_dict": [
      "self"
    ]
  },
  "_convert_to_activation_function": [
    "fn_or_string",
    "act_params"
  ],
  "ActLuPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "x_aval",
      "scale_aval",
      "amax_aval"
    ],
    "lowering": [
      "ctx",
      "x",
      "scale",
      "amax"
    ],
    "impl": [
      "x",
      "scale",
      "amax",
      "out_dtype",
      "act_enum",
      "act_len",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "out_dtype",
      "act_enum",
      "act_len",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "out_dtype",
      "act_enum",
      "act_len",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "out_dtype",
      "act_enum",
      "act_len",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "BaseDActLuDBiasQuantizePrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "dz_aval",
      "x_aval",
      "scale_aval",
      "amax_aval"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "dz",
      "x",
      "scale",
      "amax"
    ],
    "impl": [
      "dz",
      "x",
      "scale",
      "amax",
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "is_dbias",
      "act_enum",
      "act_len",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "is_dbias",
      "act_enum",
      "act_len",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "is_dbias",
      "act_enum",
      "act_len",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "is_dbias",
      "act_enum",
      "act_len",
      "act_params",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "DActLuDBiasQuantizePrimitive": {},
  "DActLuQuantizePrimitive": {},
  "_jax_act_lu": [
    "inputs",
    "activation_type",
    "quantizer",
    "act_params"
  ],
  "_jax_quantize_dact_dbias": [
    "dz",
    "x",
    "activation_type",
    "is_dbias",
    "quantizer",
    "act_params"
  ],
  "act_lu": [
    "x",
    "activation_type",
    "quantizer",
    "act_params",
    "amax_scope",
    "transpose_batch_sequence",
    "output_amax_when_no_scaling"
  ],
  "quantize_dact_dbias": [
    "dz",
    "x",
    "activation_type",
    "is_dbias",
    "quantizer",
    "act_params",
    "amax_scope",
    "transpose_batch_sequence",
    "output_amax_when_no_scaling"
  ],
  "dact_lu": [
    "dz",
    "x",
    "activation_type",
    "quantizer",
    "act_params",
    "amax_scope",
    "transpose_batch_sequence",
    "output_amax_when_no_scaling"
  ],
  "get_forward_sm_margin": [],
  "get_backward_sm_margin": [],
  "is_norm_fwd_cudnn_enabled": [
    "scaling_mode"
  ],
  "is_norm_zero_centered_gamma_in_weight_dtype": [
    "scaling_mode"
  ],
  "FUSED_MXFP8_NORM_CUDNN_MIN_VERSION": [],
  "NormFwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "x_aval",
      "scale_aval",
      "amax_aval",
      "gamma_aval",
      "beta_aval"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "x",
      "scale",
      "amax",
      "gamma",
      "beta"
    ],
    "impl": [
      "x",
      "scale",
      "amax",
      "gamma",
      "beta",
      "norm_type",
      "zero_centered_gamma",
      "epsilon",
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "norm_type",
      "zero_centered_gamma",
      "epsilon",
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "norm_type",
      "zero_centered_gamma",
      "epsilon",
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "norm_type",
      "zero_centered_gamma",
      "epsilon",
      "out_dtype",
      "scaling_mode",
      "quantize_layout",
      "scale_dtype",
      "amax_scope",
      "transpose_batch_sequence",
      "output_amax_when_no_scaling",
      "is_outer",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "NormBwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "dz_aval",
      "x_aval",
      "mu_aval",
      "rsigma_aval",
      "gamma_aval",
      "norm_type",
      "zero_centered_gamma"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "dz",
      "x",
      "mu",
      "rsigma",
      "gamma"
    ],
    "impl": [
      "dz",
      "x",
      "mu",
      "rsigma",
      "gamma",
      "norm_type",
      "zero_centered_gamma"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "norm_type",
      "zero_centered_gamma",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "norm_type",
      "zero_centered_gamma",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": []
  },
  "_jax_layernorm": [
    "x",
    "gamma",
    "beta",
    "zero_centered_gamma",
    "epsilon",
    "quantizer"
  ],
  "_jax_rmsnorm": [
    "x",
    "gamma",
    "zero_centered_gamma",
    "epsilon",
    "quantizer"
  ],
  "layernorm_fwd": [
    "x",
    "gamma",
    "beta",
    "zero_centered_gamma",
    "epsilon",
    "quantizer",
    "amax_scope",
    "transpose_batch_sequence",
    "output_amax_when_no_scaling"
  ],
  "layernorm_bwd": [
    "dz",
    "x",
    "mu",
    "rsigma",
    "gamma",
    "beta",
    "zero_centered_gamma",
    "epsilon"
  ],
  "rmsnorm_fwd": [
    "x",
    "gamma",
    "zero_centered_gamma",
    "epsilon",
    "quantizer",
    "amax_scope",
    "transpose_batch_sequence",
    "output_amax_when_no_scaling"
  ],
  "rmsnorm_bwd": [
    "dz",
    "x",
    "rsigma",
    "gamma",
    "zero_centered_gamma",
    "epsilon"
  ],
  "normalization_fwd": [
    "x",
    "gamma",
    "beta",
    "zero_centered_gamma",
    "epsilon",
    "norm_type",
    "quantizer",
    "amax_scope",
    "transpose_batch_sequence"
  ],
  "normalization_bwd": [
    "dz",
    "x",
    "mu",
    "rsigma",
    "gamma",
    "beta",
    "zero_centered_gamma",
    "epsilon",
    "norm_type"
  ],
  "AmaxScope": {
    "LOCAL": [],
    "TPSP": [],
    "FSDP": [],
    "all_reduce_amax_along_TPSP_and_FSDP": [
      "self",
      "amax",
      "data_spec",
      "transpose_batch_sequence",
      "mesh"
    ]
  },
  "AmaxCalculationPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "x_aval"
    ],
    "impl": [
      "x",
      "amax_scope",
      "transpose_batch_sequence"
    ],
    "infer_sharding_from_operands": [
      "amax_scope",
      "transpose_batch_sequence",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "amax_scope",
      "transpose_batch_sequence",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "amax_scope",
      "transpose_batch_sequence",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "RHTAmaxCalculationPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "x_aval"
    ],
    "lowering": [
      "ctx",
      "x"
    ],
    "impl": [
      "x",
      "amax_scope",
      "transpose_batch_sequence",
      "rht_matrix_random_sign_mask_t",
      "produce_regular_amax",
      "flatten_axis"
    ],
    "infer_sharding_from_operands": [
      "amax_scope",
      "transpose_batch_sequence",
      "rht_matrix_random_sign_mask_t",
      "produce_regular_amax",
      "flatten_axis",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "amax_scope",
      "transpose_batch_sequence",
      "rht_matrix_random_sign_mask_t",
      "produce_regular_amax",
      "flatten_axis",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "amax_scope",
      "transpose_batch_sequence",
      "rht_matrix_random_sign_mask_t",
      "produce_regular_amax",
      "flatten_axis",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "calculate_amax": [
    "x",
    "amax_scope",
    "transpose_batch_sequence"
  ],
  "calculate_post_rht_amax": [
    "x",
    "amax_scope",
    "transpose_batch_sequence",
    "produce_regular_amax",
    "flatten_axis"
  ],
  "_FusedAttnConfig": {},
  "FusedAttnHelper": {
    "is_fused_attn_kernel_available": [
      "self"
    ],
    "get_fused_attn_backend": [
      "self"
    ],
    "is_non_deterministic_allowed": [],
    "parse_qkv_aval": [
      "q_aval",
      "k_aval",
      "v_aval",
      "qkv_layout"
    ]
  },
  "_FusedAttnRNGStateChecker": {
    "check_seed": [
      "self",
      "seed",
      "dropout_probability",
      "is_training"
    ]
  },
  "generate_cu_seqlen": [
    "actual_seqlen"
  ],
  "FusedAttnFwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "q_aval",
      "k_aval",
      "v_aval",
      "bias_aval",
      "softmax_offset_aval",
      "seed_aval",
      "q_seqlen_or_cu_seqlen_aval",
      "kv_seqlen_or_cu_seqlen_aval",
      "_q_seq_offsets",
      "_k_seq_offsets",
      "_q_segment_ids",
      "_kv_segment_ids",
      "_q_segment_pos",
      "_kv_segment_pos"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "q",
      "k",
      "v",
      "bias",
      "softmax_offset",
      "seed",
      "q_cu_seqlen",
      "kv_cu_seqlen",
      "q_seq_offsets",
      "k_seq_offsets",
      "_q_segment_ids",
      "_kv_segment_ids",
      "_q_segment_pos",
      "_kv_segment_pos"
    ],
    "impl": [
      "q",
      "k",
      "v",
      "bias",
      "softmax_offset",
      "seed",
      "q_seqlen",
      "kv_seqlen",
      "q_seq_offsets",
      "k_seq_offsets",
      "_q_segment_ids",
      "_kv_segment_ids",
      "_q_segment_pos",
      "_kv_segment_pos",
      "config"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "config",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "FusedAttnBwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "q_aval",
      "k_aval",
      "v_aval",
      "bias_aval",
      "softmax_offset_aval",
      "softmax_aux_aval",
      "rng_state_aval",
      "output_aval",
      "doutput_aval",
      "q_seqlen_or_cu_seqlen_aval",
      "kv_seqlen_or_cu_seqlen_aval",
      "_q_seq_offsets",
      "_k_seq_offsets",
      "_q_segment_ids",
      "_kv_segment_ids",
      "_q_segment_pos",
      "_kv_segment_pos"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "q",
      "k",
      "v",
      "bias",
      "softmax_offset",
      "softmax_aux",
      "rng_state",
      "output",
      "doutput",
      "q_cu_seqlen",
      "kv_cu_seqlen",
      "q_seq_offsets",
      "k_seq_offsets",
      "q_segment_ids",
      "kv_segment_ids",
      "q_segment_pos",
      "kv_segment_pos"
    ],
    "impl": [
      "q",
      "k",
      "v",
      "bias",
      "softmax_offset",
      "softmax_aux",
      "rng_state",
      "output",
      "doutput",
      "q_seqlen",
      "kv_seqlen",
      "q_seq_offsets",
      "k_seq_offsets",
      "_q_segment_ids",
      "_kv_segment_ids",
      "_q_segment_pos",
      "_kv_segment_pos",
      "config"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "config",
      "mesh",
      "value_types",
      "result_types"
    ]
  },
  "reorder_causal_dual_chunk_swap": [
    "tensor",
    "cp_size",
    "seq_dim",
    "to_contiguous"
  ],
  "reorder_causal_striped": [
    "tensor",
    "cp_size",
    "seq_dim",
    "is_inverse",
    "stripe_size"
  ],
  "_FusedAttnCPWithAllGatherHelper": {
    "check_supported": [
      "self"
    ],
    "get_adjusted_mask": [
      "self"
    ],
    "get_adjusted_max_segments_per_seq": [
      "self",
      "max_seqlen",
      "cp_size"
    ],
    "get_step_config": [
      "self"
    ],
    "get_step_config_for_striped": [
      "self",
      "max_seqlen",
      "cp_size"
    ],
    "all_gather_kv": [
      "self",
      "k",
      "v"
    ],
    "all_gather_segment_ids_and_pos": [
      "self",
      "kv_segment_ids",
      "kv_segment_pos"
    ],
    "reduce_scatter_dkv": [
      "self",
      "dk",
      "dv"
    ],
    "kv_seqlens_for_rank": [
      "self",
      "cp_rank",
      "kv_max_seqlen",
      "kv_seqlen_per_subrank"
    ],
    "slice_kv": [
      "self",
      "k",
      "v",
      "slice_seq_len"
    ],
    "pad_kv": [
      "self",
      "dk",
      "dv",
      "pad_seq_len"
    ],
    "q_seqlens_for_striped_for_rank": [
      "self",
      "q_segment_ids",
      "q_segment_pos",
      "max_segments_per_seq"
    ],
    "q_seqoffsets_for_striped_for_rank": [
      "self",
      "q_segment_ids",
      "q_segment_pos",
      "max_segments_per_seq"
    ],
    "kv_seqlens_for_striped_for_rank": [
      "self",
      "kv_segment_ids",
      "kv_segment_pos",
      "max_segments_per_seq"
    ],
    "kv_seqoffsets_for_striped_for_rank": [
      "self",
      "kv_segment_pos",
      "kv_segment_ids",
      "kv_segment_pos_ag",
      "kv_segment_ids_ag",
      "max_segments_per_seq"
    ]
  },
  "FusedAttnCPWithAllGatherFwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "FusedAttnCPWithAllGatherBwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "FusedAttnCPStripedWithAllGatherFwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "FusedAttnCPStripedWithAllGatherBwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "_FusedAttnCPWithP2PHelper": {
    "use_scanloop": [],
    "check_supported": [
      "self"
    ],
    "get_step_config": [
      "self",
      "attn_mask_type"
    ],
    "stack_kv": [
      "self",
      "k",
      "v"
    ],
    "unstack_kv": [
      "self",
      "kv"
    ],
    "permute_kv": [
      "self",
      "kv",
      "cp_perm"
    ],
    "correct_output_and_softmax_aux": [
      "output",
      "softmax_aux",
      "partial_output",
      "partial_softmax_aux"
    ],
    "adjust_seqlen": [
      "self",
      "seqlen",
      "max_seqlen",
      "idx"
    ]
  },
  "FusedRingAttnFwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "FusedRingAttnBwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "adjust_cp_striped_window_size": [
    "q_pos0",
    "kv_pos0",
    "cp_size",
    "window_size"
  ],
  "FusedRingAttnStripedFwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "FusedRingAttnStripedBwdPrimitive": {
    "partition": [
      "config",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "_maybe_context_parallel_axis": [
    "cp_axis"
  ],
  "num_cublas_streams": [],
  "sanitize_dims": [
    "ndim",
    "dims"
  ],
  "get_non_contracting_dims": [
    "ndim",
    "contracting_dims"
  ],
  "transpose_dims": [
    "ndim",
    "dims_to_transpose",
    "flatten_axis"
  ],
  "_compatible_fp8_gemm_dtypes": [
    "lhs_dtype",
    "rhs_dtype"
  ],
  "_get_gemm_layout": [
    "operand_ndims",
    "contracting_dims"
  ],
  "_quantize_gemm_operands": [
    "lhs",
    "rhs",
    "lhs_quantizer",
    "rhs_quantizer",
    "contracting_dims"
  ],
  "_get_nvfp4_tensor_scale_inv": [
    "amax"
  ],
  "collective_gemm_bootstrap": [
    "num_total_devices",
    "num_devices_per_process",
    "process_id",
    "tensor_parallel_size",
    "num_max_streams",
    "compute_stream_priority",
    "communication_stream_priority",
    "num_sm_for_communication",
    "use_ce",
    "aggregate_all_gather"
  ],
  "CollectiveOp": {
    "NONE": [],
    "ALL_GATHER": [],
    "REDUCE_SCATTER": [],
    "is_all_gather": [
      "self"
    ],
    "is_reduce_scatter": [
      "self"
    ],
    "is_none": [
      "self"
    ]
  },
  "CollectiveOpSet": {
    "create": [
      "forward_collective_op"
    ]
  },
  "noop_collective_op_set": [],
  "swizzled_scale": [
    "scale_inv",
    "flatten_axis",
    "is_colwise"
  ],
  "get_lhs_axis_boundary": [
    "lhs_cdims",
    "is_transposed"
  ],
  "get_rhs_axis_boundary": [
    "rhs_cdims",
    "is_transposed"
  ],
  "assert_cublas_requirements": [
    "scaling_mode",
    "contracting_size",
    "tensor_name"
  ],
  "GemmPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "lhs",
      "lhs_scale_inv",
      "rhs",
      "rhs_scale_inv",
      "bias",
      "gelu_input",
      "alpha",
      "beta",
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer",
      "collective_op"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "lhs",
      "lhs_scale_inv",
      "rhs",
      "rhs_scale_inv",
      "bias",
      "gelu_input",
      "alpha",
      "beta",
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer",
      "collective_op"
    ],
    "impl": [
      "lhs",
      "lhs_scale_inv",
      "rhs",
      "rhs_scale_inv",
      "bias",
      "gelu_input",
      "alpha",
      "beta",
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer",
      "collective_op"
    ],
    "outer_impl": [
      "lhs",
      "lhs_scale_inv",
      "rhs",
      "rhs_scale_inv",
      "bias",
      "gelu_input",
      "alpha",
      "beta",
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer",
      "collective_op"
    ],
    "batcher": [
      "batched_args",
      "batch_dims",
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "collective_op",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer"
    ],
    "_parse_operand_output_specs": [
      "arg_infos",
      "contracting_dims",
      "transpose_batch_sequence",
      "collective_op"
    ],
    "infer_sharding_from_operands": [
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer",
      "collective_op",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer",
      "collective_op",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": [
      "out_dtype",
      "contracting_dims",
      "scaling_mode",
      "fuse_bias",
      "fuse_gelu",
      "grad",
      "use_split_accumulator",
      "transpose_batch_sequence",
      "sequence_dim",
      "is_outer",
      "collective_op",
      "mesh",
      "operand_types",
      "result_types"
    ]
  },
  "gemm_uses_jax_dot": [],
  "_te_gemm": [
    "lhs",
    "rhs",
    "bias",
    "gelu_input",
    "lhs_quantizer",
    "rhs_quantizer",
    "contracting_dims",
    "fuse_bias",
    "fuse_gelu",
    "grad",
    "use_split_accumulator",
    "transpose_batch_sequence",
    "collective_op"
  ],
  "GroupedGemmCopySizesPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "group_sizes_aval"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx",
      "group_sizes",
      "num_gemms"
    ],
    "impl": [
      "group_sizes",
      "num_gemms"
    ]
  },
  "GroupedGemmPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "lhs_data_aval",
      "lhs_scale_inv_aval",
      "rhs_data_aval",
      "rhs_scale_inv_aval",
      "bias_aval",
      "group_sizes_aval",
      "group_offset_aval"
    ],
    "outer_abstract": [],
    "lowering": [
      "ctx"
    ],
    "impl": [
      "lhs_data",
      "lhs_scale_inv",
      "rhs_data",
      "rhs_scale_inv",
      "bias",
      "group_sizes",
      "group_offset",
      "M",
      "N",
      "K",
      "lhs_is_trans",
      "rhs_is_trans",
      "scaling_mode",
      "out_dtype",
      "has_bias",
      "is_grouped_dense_wgrad",
      "use_async_d2h_group_sizes"
    ]
  },
  "_shape_normalization": [
    "x",
    "dimension_numbers",
    "already_transposed"
  ],
  "_calculate_remaining_shape": [
    "shape",
    "contracting_dims"
  ],
  "_jax_gemm_tensor_scaling_fp8": [
    "lhs",
    "rhs",
    "dim_nums",
    "precision"
  ],
  "_jax_scaled_matmul": [
    "lhs",
    "rhs",
    "dim_nums"
  ],
  "_jax_gemm": [
    "lhs",
    "rhs",
    "contracting_dims",
    "lhs_quantizer",
    "rhs_quantizer"
  ],
  "gemm": [
    "lhs",
    "rhs",
    "contracting_dims",
    "lhs_quantizer",
    "rhs_quantizer",
    "transpose_batch_sequence",
    "collective_op"
  ],
  "grouped_gemm_copy_group_sizes": [
    "group_sizes",
    "num_gemms"
  ],
  "grouped_gemm": [
    "lhs",
    "rhs",
    "group_sizes",
    "contracting_dims",
    "bias",
    "precision",
    "preferred_element_type",
    "group_offset",
    "quantizer_set",
    "use_async_d2h_group_sizes"
  ],
  "is_softmax_kernel_available": [
    "softmax_fusion_type",
    "softmax_type",
    "batch",
    "heads",
    "q_seqlen",
    "k_seqlen",
    "dtype"
  ],
  "SoftmaxPrimitive": {
    "max_k_seqlen_supported": [],
    "name": [],
    "is_kernel_available": [
      "batch",
      "heads",
      "q_seqlen",
      "k_seqlen",
      "dtype"
    ],
    "get_batch_per_block": [
      "k_seqlen"
    ],
    "forward_abstract": [
      "logits_aval",
      "scale_factor"
    ],
    "forward_lowering": [
      "name",
      "ctx",
      "logits"
    ],
    "forward_impl": [
      "primitive",
      "logits",
      "scale_factor"
    ],
    "forward_batcher": [
      "primitive",
      "batched_args",
      "batch_dims"
    ],
    "forward_infer_sharding_from_operands": [
      "cls",
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "forward_partition": [
      "cls",
      "impl",
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "backward_abstract": [
      "dz_aval",
      "softmax_out_aval",
      "scale_factor"
    ],
    "backward_lowering": [
      "name",
      "ctx",
      "dz",
      "softmax_out"
    ],
    "backward_impl": [
      "primitive",
      "dz",
      "softmax_out",
      "scale_factor"
    ],
    "backward_batcher": [
      "primitive",
      "batched_args",
      "batch_dims"
    ],
    "backward_infer_sharding_from_operands": [
      "cls",
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "backward_partition": [
      "cls",
      "impl",
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ]
  },
  "ScaledSoftmaxFwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "is_kernel_available": [
      "batch",
      "heads",
      "q_seqlen",
      "k_seqlen",
      "dtype"
    ],
    "abstract": [
      "logits_aval",
      "scale_factor"
    ],
    "lowering": [
      "ctx",
      "logits"
    ],
    "impl": [
      "logits",
      "scale_factor"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": []
  },
  "ScaledSoftmaxBwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "is_kernel_available": [
      "batch",
      "heads",
      "q_seqlen",
      "k_seqlen",
      "dtype"
    ],
    "abstract": [
      "dz_aval",
      "softmax_out_aval",
      "scale_factor"
    ],
    "lowering": [
      "ctx",
      "dz",
      "softmax_out"
    ],
    "impl": [
      "dz",
      "softmax_out",
      "scale_factor"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": []
  },
  "scaled_softmax_bwd": [
    "dz",
    "softmax_out",
    "logits",
    "scale_factor"
  ],
  "ScaledMaskedSoftmaxFwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "is_kernel_available": [
      "batch",
      "heads",
      "q_seqlen",
      "k_seqlen",
      "dtype"
    ],
    "abstract": [
      "logits_aval",
      "mask_aval",
      "scale_factor"
    ],
    "lowering": [
      "ctx",
      "logits",
      "mask"
    ],
    "impl": [
      "logits",
      "mask",
      "scale_factor"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": []
  },
  "ScaledMaskedSoftmaxBwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "is_kernel_available": [
      "batch",
      "heads",
      "q_seqlen",
      "k_seqlen",
      "dtype"
    ],
    "abstract": [
      "dz_aval",
      "softmax_out_aval"
    ],
    "lowering": [
      "ctx",
      "dz",
      "softmax_out"
    ],
    "impl": [
      "dz",
      "softmax_out",
      "scale_factor"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": []
  },
  "ScaledUpperTriangMaskedSoftmaxFwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "is_kernel_available": [
      "batch",
      "heads",
      "q_seqlen",
      "k_seqlen",
      "dtype"
    ],
    "abstract": [
      "logits_aval",
      "scale_factor"
    ],
    "lowering": [
      "ctx",
      "logits"
    ],
    "impl": [
      "logits",
      "scale_factor"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": []
  },
  "ScaledUpperTriangMaskedSoftmaxBwdPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "is_kernel_available": [
      "batch",
      "heads",
      "q_seqlen",
      "k_seqlen",
      "dtype"
    ],
    "abstract": [
      "dz_aval",
      "softmax_out_aval"
    ],
    "lowering": [
      "ctx",
      "dz",
      "softmax_out"
    ],
    "impl": [
      "dz",
      "softmax_out",
      "scale_factor"
    ],
    "batcher": [
      "batched_args",
      "batch_dims"
    ],
    "infer_sharding_from_operands": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "partition": [
      "scale_factor",
      "mesh",
      "arg_infos",
      "result_infos"
    ],
    "shardy_sharding_rule": []
  },
  "jax_scaled_softmax": [
    "logits",
    "scale_factor",
    "softmax_offset"
  ],
  "jax_scaled_masked_softmax": [
    "logits",
    "mask",
    "scale_factor",
    "softmax_offset"
  ],
  "jax_scaled_upper_triang_masked_softmax": [
    "logits",
    "scale_factor",
    "softmax_offset"
  ],
  "jax_general_softmax": [
    "x",
    "axis",
    "where",
    "initial",
    "offset"
  ],
  "scaled_softmax_fwd": [
    "logits",
    "scale_factor"
  ],
  "scaled_masked_softmax_fwd": [
    "logits",
    "mask",
    "scale_factor"
  ],
  "scaled_masked_softmax_bwd": [
    "dz",
    "softmax_out",
    "logits",
    "mask",
    "scale_factor"
  ],
  "scaled_upper_triang_masked_softmax_fwd": [
    "logits",
    "scale_factor"
  ],
  "scaled_upper_triang_masked_softmax_bwd": [
    "dz",
    "softmax_out",
    "logits",
    "scale_factor"
  ],
  "TEDType": [],
  "te_dtype_to_jax_dtype": [
    "te_dtype"
  ],
  "te_dtype_to_ir_dtype": [
    "te_dtype"
  ],
  "jax_dtype_to_ir_dtype": [
    "jax_dtype"
  ],
  "jax_dtype_to_te_dtype": [
    "jax_dtype"
  ],
  "check_valid_batch_dims": [
    "bdims"
  ],
  "normalize_axis_boundary": [
    "axis",
    "ndim"
  ],
  "multidim_transpose": [
    "shape",
    "static_axis_boundary",
    "transpose_axis"
  ],
  "get_xla_flag": [
    "flag",
    "default",
    "cast"
  ],
  "get_min_device_compute_capability": [],
  "get_all_device_compute_capability": [],
  "should_apply_1x_fused_dbias_war_for_arch_l_100": [
    "is_dbias",
    "quantizer"
  ],
  "try_apply_delayed_scaling_2x_war": [
    "f"
  ],
  "NamedSharding": {
    "__init__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "duplicate_with_new_description": [
      "self",
      "desc"
    ]
  },
  "is_all_reduce_in_float32": [],
  "apply_rht": [
    "x",
    "inverse"
  ],
  "QuantizeMeta": {
    "merge": [
      "a",
      "b"
    ],
    "__init__": [
      "self"
    ],
    "get_kwargs_dictionary": [
      "self"
    ]
  },
  "QuantizeMetaSet": {},
  "TensorUsage": {
    "LHS": [],
    "LHS_TRANS": [],
    "RHS": [],
    "RHS_TRANS": [],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "DIVUP": [
    "a",
    "b"
  ],
  "QuantizeShardyRules": {},
  "ScalingModeMetadataImpl": {
    "get_scale_dtype": [
      "self"
    ],
    "get_data_layout": [
      "self"
    ],
    "get_scale_shape": [
      "self",
      "data_shape",
      "data_layout",
      "is_colwise",
      "is_padded",
      "flatten_axis"
    ],
    "get_grouped_scale_shape": [
      "self",
      "data_shape",
      "n_groups",
      "group_axis",
      "is_colwise",
      "is_padded",
      "flatten_axis"
    ],
    "get_quantize_layout": [
      "self",
      "usage"
    ],
    "get_shardy_sharding_rules": [
      "self",
      "input_shape",
      "unique_var",
      "flatten_axis",
      "q_layout",
      "broadcast_2d_scale_shape_to_1d",
      "is_colwise_transposed"
    ]
  },
  "NoScalingModeMetadataImpl": {
    "get_scale_dtype": [
      "self"
    ],
    "get_data_layout": [
      "self"
    ],
    "get_scale_shape": [
      "self",
      "data_shape",
      "data_layout",
      "is_colwise",
      "is_padded",
      "flatten_axis",
      "broadcast_2d_scale_shape_to_1d"
    ],
    "get_quantize_layout": [
      "self",
      "usage"
    ],
    "get_grouped_scale_shape": [
      "self",
      "data_shape",
      "n_groups",
      "group_axis",
      "is_colwise",
      "is_padded",
      "flatten_axis"
    ],
    "get_shardy_sharding_rules": [
      "self",
      "input_shape",
      "unique_var",
      "flatten_axis",
      "q_layout",
      "broadcast_2d_scale_shape_to_1d",
      "is_colwise_transposed"
    ]
  },
  "CurrentScalingModeMetadataImpl": {
    "get_scale_dtype": [
      "self"
    ],
    "get_data_layout": [
      "self"
    ],
    "get_scale_shape": [
      "self",
      "data_shape",
      "data_layout",
      "is_colwise",
      "is_padded",
      "flatten_axis",
      "broadcast_2d_scale_shape_to_1d"
    ],
    "get_quantize_layout": [
      "self",
      "usage"
    ],
    "get_grouped_scale_shape": [
      "self",
      "data_shape",
      "n_groups",
      "group_axis",
      "is_colwise",
      "is_padded",
      "flatten_axis"
    ],
    "get_shardy_sharding_rules": [
      "self",
      "input_shape",
      "unique_var",
      "flatten_axis",
      "q_layout",
      "broadcast_2d_scale_shape_to_1d",
      "is_colwise_transposed"
    ]
  },
  "DelayedScalingModeMetadataImpl": {},
  "BlockScalingModeMetadataImpl": {
    "__init__": [
      "self",
      "block_dims",
      "scale_dtype",
      "data_layout"
    ],
    "get_scale_dtype": [
      "self"
    ],
    "get_data_layout": [
      "self"
    ],
    "_apply_scale_shape_correction": [
      "self",
      "data_shape",
      "n_scale_blocks",
      "scale_block_dim"
    ],
    "get_scale_shape": [
      "self",
      "data_shape",
      "data_layout",
      "is_colwise",
      "is_padded",
      "flatten_axis",
      "broadcast_2d_scale_shape_to_1d"
    ],
    "get_quantize_layout": [
      "self",
      "usage"
    ],
    "get_grouped_scale_shape": [
      "self",
      "data_shape",
      "n_groups",
      "group_axis",
      "is_colwise",
      "is_padded",
      "flatten_axis"
    ],
    "get_shardy_sharding_rules": [
      "self",
      "input_shape",
      "unique_var",
      "flatten_axis",
      "q_layout",
      "broadcast_2d_scale_shape_to_1d",
      "is_colwise_transposed"
    ]
  },
  "ScalingMode": {
    "NO_SCALING": [],
    "DELAYED_TENSOR_SCALING": [],
    "MXFP8_1D_SCALING": [],
    "CURRENT_TENSOR_SCALING": [],
    "NVFP4_1D_SCALING": [],
    "NVFP4_2D_SCALING": [],
    "_get_impl": [
      "self"
    ],
    "get_scale_dtype": [
      "self"
    ],
    "get_scale_shape_2x": [
      "self",
      "data_shape",
      "is_padded",
      "flatten_axis",
      "broadcast_2d_scale_shape_to_1d"
    ],
    "get_scale_shape": [
      "self",
      "data_shape",
      "data_layout",
      "is_colwise",
      "is_padded",
      "flatten_axis",
      "broadcast_2d_scale_shape_to_1d"
    ],
    "get_quantize_layout": [
      "self",
      "usage"
    ],
    "get_shardy_sharding_rules": [
      "self",
      "input_shape",
      "unique_var",
      "flatten_axis",
      "q_layout",
      "broadcast_2d_scale_shape_to_1d"
    ],
    "get_grouped_scale_shape_2x": [
      "self",
      "data_shape",
      "n_groups",
      "group_axis",
      "is_padded",
      "flatten_axis"
    ],
    "get_grouped_scale_shape": [
      "self",
      "data_shape",
      "n_groups",
      "group_axis",
      "is_colwise",
      "is_padded",
      "flatten_axis"
    ],
    "is_tensor_scaling": [
      "self"
    ],
    "is_1d_block_scaling": [
      "self"
    ],
    "is_block_scaling": [
      "self"
    ],
    "get_compatible_q_dtypes": [
      "self"
    ],
    "is_nvfp4_scaling": [
      "self"
    ],
    "is_mxfp8_scaling": [
      "self"
    ],
    "is_colwise_transposed": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "tree_flatten": [
      "self"
    ],
    "tree_unflatten": [
      "cls",
      "aux_data",
      "_children"
    ]
  },
  "compute_scale_from_amax": [
    "amax",
    "q_dtype",
    "margin",
    "scale"
  ],
  "CurrentScaleQuantizer": {
    "_quantize_func": [
      "self",
      "x",
      "is_colwise",
      "dq_dtype",
      "flatten_axis"
    ],
    "quantize": [
      "self",
      "x",
      "is_rowwise",
      "is_colwise",
      "dq_dtype",
      "flatten_axis"
    ]
  },
  "DelayedScaleQuantizer": {
    "__post_init__": [
      "self"
    ],
    "tree_flatten": [
      "self"
    ],
    "_quantize_func": [
      "self",
      "x",
      "is_colwise",
      "dq_dtype",
      "flatten_axis"
    ],
    "_update_amax_history": [
      "amax_history",
      "new_amax"
    ],
    "_compute_scale": [
      "amax_history",
      "scale",
      "q_dtype",
      "amax_compute_algo",
      "margin"
    ],
    "_roll_and_reset_amax_history": [
      "amax_history"
    ],
    "update": [
      "self",
      "new_amax"
    ]
  },
  "BlockScaleQuantizer": {
    "_quantize_func": [
      "self",
      "x",
      "is_colwise",
      "dq_dtype",
      "flatten_axis"
    ],
    "_cast_to_e8m0_with_rounding_up": [
      "self",
      "scales"
    ],
    "_e8m0_to_dtype": [
      "self",
      "x",
      "dtype"
    ]
  },
  "QuantizerSet": {
    "tree_flatten": [
      "self"
    ],
    "tree_unflatten": [
      "cls",
      "aux_data",
      "children"
    ]
  },
  "GroupedQuantizer": {
    "tree_flatten": [
      "self"
    ],
    "__post_init__": [
      "self"
    ],
    "_create_grouped_tensor_from_tensor_list": [
      "self",
      "tensor_list",
      "group_sizes",
      "original_shape",
      "group_axis",
      "mode"
    ],
    "_quantize_func": [
      "self"
    ],
    "quantize": [
      "self",
      "x",
      "is_rowwise",
      "is_colwise",
      "dq_dtype",
      "flatten_axis",
      "group_sizes",
      "group_axis"
    ],
    "get_scale_shapes": [
      "self",
      "data_shape",
      "is_padded",
      "flatten_axis",
      "group_sizes"
    ]
  },
  "QuantizerFactory": {
    "quantizer_type_map": [],
    "create": [
      "n_quantizers",
      "scaling_mode",
      "q_dtype",
      "q_layout",
      "n_groups",
      "checkpoint_name"
    ],
    "_create_set": [
      "x_scaling_mode",
      "kernel_scaling_mode",
      "grad_scaling_mode",
      "fwd_dtype",
      "bwd_dtype",
      "is_2x2x",
      "n_groups",
      "is_inference_mode",
      "checkpoint_name"
    ],
    "create_set": [
      "n_quantizer_sets",
      "scaling_mode",
      "fwd_dtype",
      "bwd_dtype",
      "is_2x2x",
      "n_groups",
      "checkpoint_name",
      "fp8_recipe"
    ]
  },
  "noop_quantizer_set": [],
  "Dequantizer": {
    "_dequantize_func": [
      "data",
      "scale_inv",
      "dq_dtype"
    ],
    "dequantize": [
      "scaled_tensor"
    ]
  },
  "NoopDequantizer": {
    "_dequantize_func": [
      "data"
    ],
    "dequantize": [
      "scaled_tensor"
    ]
  },
  "TensorScaleDequantizer": {
    "_dequantize_func": [
      "data",
      "scale_inv",
      "dq_dtype"
    ],
    "dequantize": [
      "scaled_tensor"
    ]
  },
  "BlockScaleDequantizer": {
    "_dequantize_func": [
      "data",
      "scale_inv",
      "dq_dtype",
      "scaling_mode",
      "is_colwise",
      "flatten_axis"
    ],
    "dequantize": [
      "scaled_tensor"
    ]
  },
  "NVFP4Dequantizer": {
    "_dequantize_func": [
      "data",
      "scale_inv",
      "amax",
      "dq_dtype",
      "scaling_mode",
      "is_colwise",
      "flatten_axis",
      "has_rht_applied"
    ],
    "dequantize": [
      "scaled_tensor"
    ]
  },
  "ScalingModeToDequantizerMap": [],
  "_grouped_dequantize": [
    "grouped_scaled_tensor"
  ],
  "QuantizeLayout": {
    "ROWWISE": [],
    "COLWISE": [],
    "ROWWISE_COLWISE": [],
    "has_rowwise": [
      "self"
    ],
    "has_colwise": [
      "self"
    ],
    "is_rowwise_colwise": [
      "self"
    ],
    "is_rowwise_only": [
      "self"
    ],
    "is_colwise_only": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ]
  },
  "AbstractBaseTensor": {
    "tree_unflatten": [
      "cls",
      "aux_data",
      "children"
    ],
    "ndim": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "get_tensor": [
      "self",
      "usage"
    ],
    "apply_sharding_constraint_by_logical_axes": [
      "self",
      "logical_axis_names"
    ],
    "checkpoint": [
      "self",
      "quantizer"
    ]
  },
  "AbstractBaseTensor1x": {},
  "NoScaleTensor": {
    "__post_init__": [
      "self"
    ],
    "tree_flatten": [
      "self"
    ],
    "ndim": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "get_tensor": [
      "self",
      "usage"
    ],
    "apply_sharding_constraint_by_logical_axes": [
      "self",
      "logical_axis_names"
    ],
    "checkpoint": [
      "self",
      "quantizer"
    ]
  },
  "ScaledTensor": {},
  "ScaledTensor1x": {
    "__post_init__": [
      "self"
    ],
    "tree_flatten": [
      "self"
    ],
    "ndim": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "get_tensor": [
      "self",
      "usage"
    ],
    "apply_sharding_constraint_by_logical_axes": [
      "self",
      "logical_axis_names"
    ],
    "checkpoint": [
      "self",
      "quantizer"
    ]
  },
  "GroupedScaledTensor1x": {
    "__init__": [
      "self",
      "data",
      "scale_inv",
      "amax",
      "group_sizes",
      "scaling_mode",
      "dq_dtype",
      "_dq_func",
      "is_colwise",
      "data_layout",
      "flatten_axis",
      "original_shape",
      "group_axis"
    ],
    "__post_init__": [
      "self"
    ],
    "tree_flatten": [
      "self"
    ],
    "apply_sharding_constraint_by_logical_axes": [
      "self",
      "logical_axis_names"
    ],
    "checkpoint": [
      "self",
      "quantizer"
    ]
  },
  "ScaledTensor2x": {
    "tree_flatten": [
      "self"
    ],
    "ndim": [
      "self"
    ],
    "dequantize": [
      "self"
    ],
    "get_tensor": [
      "self",
      "usage"
    ],
    "apply_sharding_constraint_by_logical_axes": [
      "self",
      "logical_axis_names"
    ],
    "checkpoint": [
      "self",
      "quantizer"
    ]
  },
  "ScaledTensorFactory": {
    "create_1x": [
      "data",
      "scale_inv",
      "amax",
      "scaling_mode",
      "dq_dtype",
      "is_colwise",
      "data_layout",
      "flatten_axis",
      "group_sizes",
      "original_shape",
      "group_axis",
      "has_rht_applied"
    ],
    "create_2x": [
      "data",
      "scale_inv",
      "colwise_data",
      "colwise_scale_inv",
      "amax",
      "colwise_amax",
      "scaling_mode",
      "dq_dtype",
      "data_layout",
      "flatten_axis",
      "group_sizes",
      "original_shape",
      "group_axis",
      "rowwise_has_rht_applied",
      "colwise_has_rht_applied"
    ],
    "create": [
      "data",
      "scale_inv",
      "colwise_data",
      "colwise_scale_inv",
      "amax",
      "colwise_amax",
      "scaling_mode",
      "dq_dtype",
      "data_layout",
      "q_layout",
      "flatten_axis",
      "group_sizes",
      "original_shape",
      "group_axis",
      "rowwise_has_rht_applied",
      "colwise_has_rht_applied"
    ]
  },
  "_is_scaling_mode_supported": [],
  "_reason_for_no_scaling_mode": [],
  "Collection": [],
  "NVTE_FP8_COLLECTION_NAME": [],
  "_jax_version_meet_requirement": [
    "version"
  ],
  "_check_delayed_scaling_fp8_support": [
    "gpu_arch"
  ],
  "_check_block_scaling_fp8_support": [
    "gpu_arch"
  ],
  "_check_fp4_support": [
    "gpu_arch"
  ],
  "_check_scaling_support": [
    "scaling_mode",
    "gpu_id"
  ],
  "is_scaling_mode_supported": [
    "scaling_mode",
    "gpu_id"
  ],
  "get_supported_scaling_modes": [],
  "get_supported_quantization_recipes": [],
  "_format2dtypes": [
    "format_"
  ],
  "TensorSource": {
    "X": [],
    "KERNEL": [],
    "DGRAD": []
  },
  "AmaxComputeAlgo": {
    "MAX": [],
    "MOST_RECENT": []
  },
  "BaseQuantizeConfig": {
    "INITIALIZED": [],
    "initialize_from_recipe": [
      "self",
      "fp8_recipe"
    ],
    "is_fp8_enabled": [
      "self"
    ],
    "get_scaling_mode": [
      "self",
      "tensor_source"
    ],
    "get_quantize_flax_meta": [
      "self",
      "module",
      "collection_name",
      "postfix",
      "tensor_source",
      "quantizer_name"
    ],
    "is_supported": [
      "self"
    ]
  },
  "NoOpQuantizeConfig": {
    "initialize_from_recipe": [
      "self",
      "fp8_recipe"
    ],
    "get_scaling_mode": [
      "self",
      "tensor_source"
    ],
    "get_quantize_flax_meta": [
      "self",
      "module",
      "collection_name",
      "postfix",
      "tensor_source",
      "quantizer_name"
    ]
  },
  "DelayedScalingQuantizeConfig": {
    "initialize_from_recipe": [
      "self",
      "fp8_recipe"
    ],
    "get_scaling_mode": [
      "self",
      "tensor_source"
    ],
    "get_quantize_flax_meta": [
      "self",
      "module",
      "collection_name",
      "postfix",
      "tensor_source",
      "quantizer_name"
    ]
  },
  "CurrentScalingQuantizeConfig": {
    "initialize_from_recipe": [
      "self",
      "fp8_recipe"
    ],
    "get_scaling_mode": [
      "self",
      "tensor_source"
    ],
    "get_quantize_flax_meta": [
      "self",
      "module",
      "collection_name",
      "postfix",
      "tensor_source",
      "quantizer_name"
    ]
  },
  "BlockScalingQuantizeConfig": {
    "initialize_from_recipe": [
      "self",
      "fp8_recipe"
    ],
    "get_scaling_mode": [
      "self",
      "tensor_source"
    ],
    "get_quantize_flax_meta": [
      "self",
      "module",
      "collection_name",
      "postfix",
      "tensor_source",
      "quantizer_name"
    ]
  },
  "NVFP4ScalingQuantizeConfig": {
    "initialize_from_recipe": [
      "self",
      "fp8_recipe"
    ],
    "get_scaling_mode": [
      "self",
      "tensor_source"
    ],
    "_make_rht_quantize_meta": [
      "self",
      "q_layout",
      "tensor_source"
    ],
    "_make_stochastic_rounding_rng_state": [
      "self",
      "module",
      "tensor_source",
      "quantizer_name"
    ],
    "get_quantize_flax_meta": [
      "self",
      "module",
      "collection_name",
      "postfix",
      "tensor_source",
      "quantizer_name"
    ]
  },
  "get_quantize_config_class": [
    "fp8_recipe"
  ],
  "get_quantize_config_with_recipe": [
    "fp8_recipe"
  ],
  "get_global_quantize_recipe": [],
  "update_collections": [
    "new",
    "original"
  ],
  "remove_padding_from_scale_inv": [
    "scale_inv",
    "scaling_mode",
    "data_shape",
    "is_colwise",
    "flatten_axis"
  ],
  "apply_padding_to_scale_inv": [
    "scale_inv",
    "scaling_mode",
    "data_shape",
    "is_colwise",
    "flatten_axis"
  ],
  "is_fp8_gemm_with_all_layouts_supported": [],
  "_TRITON_KERNEL_CACHE": [],
  "get_triton_dtype": [
    "aval"
  ],
  "compile_triton": [
    "kernel_fn",
    "signature",
    "constants",
    "num_warps",
    "num_stages",
    "num_ctas",
    "compute_capability",
    "enable_fp_fusion"
  ],
  "triton_call_lowering": [
    "ctx",
    "kernel_fn"
  ],
  "DEFAULT_BLOCK_SIZE": [],
  "_get_min_block_size": [
    "kernel",
    "default"
  ],
  "RowIdMapPass1Primitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "routing_map_aval"
    ],
    "impl": [
      "routing_map",
      "num_tokens",
      "num_experts",
      "block_size"
    ],
    "lowering": [
      "ctx",
      "routing_map"
    ]
  },
  "RowIdMapPass2Primitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "row_id_map_aval",
      "workspace_aval"
    ],
    "impl": [
      "row_id_map",
      "workspace",
      "num_tokens",
      "num_experts",
      "block_size"
    ],
    "lowering": [
      "ctx",
      "row_id_map",
      "workspace"
    ]
  },
  "RowIdMapPass3Primitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "row_id_map_aval"
    ],
    "impl": [
      "row_id_map",
      "num_tokens",
      "num_experts"
    ],
    "lowering": [
      "ctx",
      "row_id_map"
    ]
  },
  "PermuteWithMaskMapPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "inp_aval",
      "row_id_map_aval",
      "probs_aval",
      "scale_aval",
      "permuted_scale_aval"
    ],
    "impl": [
      "inp",
      "row_id_map",
      "probs",
      "scale",
      "permuted_scale",
      "num_tokens",
      "num_experts",
      "num_out_tokens",
      "hidden_size",
      "with_probs"
    ],
    "lowering": [
      "ctx",
      "inp",
      "row_id_map",
      "probs",
      "scale",
      "permuted_scale"
    ]
  },
  "UnpermuteWithMaskMapPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "inp_aval",
      "row_id_map_aval",
      "merging_probs_aval",
      "permuted_probs_aval"
    ],
    "impl": [
      "inp",
      "row_id_map",
      "merging_probs",
      "permuted_probs",
      "num_tokens",
      "num_experts",
      "hidden_size",
      "with_merging_probs",
      "with_probs"
    ],
    "lowering": [
      "ctx",
      "inp",
      "row_id_map",
      "merging_probs",
      "permuted_probs"
    ]
  },
  "UnpermuteBwdWithMergingProbsPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "fwd_output_grad_aval",
      "fwd_input_aval",
      "merging_probs_aval",
      "row_id_map_aval"
    ],
    "impl": [
      "fwd_output_grad",
      "fwd_input",
      "merging_probs",
      "row_id_map",
      "num_tokens",
      "num_experts",
      "num_out_tokens",
      "hidden_size"
    ],
    "lowering": [
      "ctx",
      "fwd_output_grad",
      "fwd_input",
      "merging_probs",
      "row_id_map"
    ]
  },
  "unpermute_bwd_with_merging_probs": [
    "fwd_output_grad",
    "row_id_map",
    "fwd_input",
    "merging_probs",
    "num_tokens",
    "num_experts",
    "num_out_tokens",
    "hidden_size"
  ],
  "MakeChunkSortMapPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "split_sizes_aval",
      "sorted_indices_aval"
    ],
    "impl": [
      "split_sizes",
      "sorted_indices",
      "num_tokens",
      "num_splits"
    ],
    "lowering": [
      "ctx",
      "split_sizes",
      "sorted_indices"
    ]
  },
  "SortChunksByMapPrimitive": {
    "name": [],
    "multiple_results": [],
    "impl_static_args": [],
    "inner_primitive": [],
    "outer_primitive": [],
    "abstract": [
      "inp_aval",
      "row_id_map_aval",
      "probs_aval"
    ],
    "impl": [
      "inp",
      "row_id_map",
      "probs",
      "num_tokens",
      "hidden_size",
      "is_forward",
      "with_probs"
    ],
    "lowering": [
      "ctx",
      "inp",
      "row_id_map",
      "probs"
    ]
  },
  "PRNGKey": [],
  "Shape": [],
  "DType": [],
  "Array": [],
  "PrecisionLike": [],
  "Initializer": [],
  "_normalize_axes": [
    "axes",
    "ndim"
  ],
  "_canonicalize_tuple": [
    "x"
  ],
  "_obtain_default_layernorm_scale_init_if_need": [
    "original_init",
    "zero_centered_gamma"
  ],
  "_create_layernorm_parameters": [
    "module",
    "norm_type",
    "shape",
    "scale_init",
    "scale_axes",
    "bias_init",
    "bias_axes",
    "input_dtype",
    "dtype"
  ],
  "_combine_biases": [],
  "_apply_low_rank_adaptation": [
    "x",
    "axis",
    "features",
    "lora_a_kernel",
    "lora_b_kernel",
    "alpha"
  ],
  "Softmax": {
    "__call__": [
      "self",
      "inputs",
      "mask",
      "bias",
      "softmax_offset"
    ]
  },
  "TransformerEngineBase": {
    "generate_quantizer_set": [
      "self",
      "postfix",
      "variable_collection",
      "quantization_checkpoint_name",
      "fp8_recipe"
    ]
  },
  "DenseGeneral": {
    "__post_init__": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "LayerNormDenseGeneral": {
    "__post_init__": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "LogicalRules": [],
  "_generate_drop_path_shape": [
    "shape",
    "batch_dim"
  ],
  "extend_logical_axis_rules": [
    "rules"
  ],
  "_UnfusedDotProductAttention": {
    "__call__": [
      "self",
      "query",
      "key",
      "value",
      "mask",
      "bias"
    ]
  },
  "_FusedDotProductAttention": {
    "__call__": [
      "self",
      "query",
      "key",
      "value",
      "sequence_descriptor",
      "bias"
    ]
  },
  "rotary_pos_emb": [
    "x",
    "windows",
    "transpose_batch_sequence",
    "group_method"
  ],
  "LoRAScope": {
    "__init__": [
      "self",
      "qkv_proj",
      "output_proj",
      "mlp"
    ],
    "__eq__": [
      "self",
      "other"
    ]
  },
  "_canonicalize_lora_scope": [
    "scope"
  ],
  "MultiHeadAttention": {
    "__post_init__": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs_q",
      "inputs_kv",
      "mask",
      "bias"
    ]
  },
  "RelativePositionBiases": {
    "__call__": [
      "self",
      "q_seqlen",
      "k_seqlen",
      "bidirectional"
    ]
  },
  "TransformerLayerType": {
    "ENCODER": [],
    "DECODER": []
  }
}