{
  "logger": [],
  "align_multiple": [
    "value",
    "multiple_size"
  ],
  "autocast_to_fp16": [
    "x"
  ],
  "RodimusAttention": {
    "__init__": [
      "self",
      "block_type",
      "mode",
      "hidden_size",
      "input_gate_low_rank",
      "expand_ratio",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "norm_eps",
      "k_norm_eps",
      "residual_in_fp32",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "SlidingWindowSharedKeyAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "qkv_bias",
      "qk_norm",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "PaTHAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "num_kv_heads",
      "use_forget_gate",
      "use_qk_norm",
      "layer_idx",
      "use_low_rank_w",
      "use_w_shortconv",
      "conv_size",
      "conv_bias"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "SimpleGatedLinearAttention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "num_kv_heads",
      "feature_map",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "gate_fn",
      "elementwise_affine",
      "norm_eps",
      "gate_logit_normalizer",
      "fuse_norm",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ],
    "state_size": [
      "self"
    ]
  },
  "MesaNet": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "head_dim",
      "mode",
      "use_output_gate",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "layer_idx",
      "norm_eps",
      "lambda_lower_bound",
      "max_cg_step_training",
      "max_cg_step_decoding"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "DeltaFormerAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "num_kv_heads",
      "qkv_bias",
      "qk_norm",
      "rope_theta",
      "max_position_embeddings",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "LinearAttention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "num_kv_heads",
      "feature_map",
      "tie_feature_map_qk",
      "output_norm",
      "norm_q",
      "norm_k",
      "do_feature_map_norm",
      "elementwise_affine",
      "norm_eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "NativeSparseAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "num_kv_heads",
      "head_dim",
      "qkv_bias",
      "block_size",
      "block_counts",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "GatedLinearAttention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "num_kv_heads",
      "feature_map",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "use_output_gate",
      "gate_fn",
      "elementwise_affine",
      "norm_eps",
      "gate_logit_normalizer",
      "gate_low_rank_dim",
      "clamp_min",
      "fuse_norm",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "HGRN2Attention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "num_heads",
      "expand_ratio",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "elementwise_affine",
      "norm_eps",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "lower_bound"
    ]
  },
  "elu_p1": [
    "x"
  ],
  "sum_norm": [
    "x"
  ],
  "DeltaNet": {
    "__init__": [
      "self",
      "mode",
      "d_model",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "use_beta",
      "use_gate",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "allow_neg_eigval",
      "layer_idx",
      "qk_activation",
      "qk_norm",
      "norm_eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "LightNetAttention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "num_heads",
      "expand_ratio",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "gate_low_rank_dim",
      "elementwise_affine",
      "norm_eps",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ],
    "state_size": [
      "self"
    ]
  },
  "BasedLinearAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "feature_dim",
      "num_key_value_heads",
      "num_heads",
      "feature_name",
      "eps",
      "causal",
      "mode"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "forward_reference": [
      "self",
      "hidden_states"
    ]
  },
  "KimiDeltaAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "expand_v",
      "head_dim",
      "num_heads",
      "num_v_heads",
      "mode",
      "use_short_conv",
      "allow_neg_eigval",
      "conv_size",
      "conv_bias",
      "layer_idx",
      "norm_eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "IndexFirstAxis": {
    "forward": [
      "ctx",
      "x",
      "indices"
    ],
    "backward": [
      "ctx",
      "do"
    ]
  },
  "index_first_axis": [],
  "IndexPutFirstAxis": {
    "forward": [
      "ctx",
      "x",
      "indices",
      "first_axis_dim"
    ],
    "backward": [
      "ctx",
      "do"
    ]
  },
  "index_put_first_axis": [],
  "get_unpad_data": [
    "attention_mask"
  ],
  "unpad_input": [
    "q",
    "states",
    "attention_mask",
    "q_len",
    "keepdim"
  ],
  "pad_input": [
    "hidden_states",
    "indices",
    "batch_size",
    "seq_len"
  ],
  "ceil_log": [
    "x",
    "b"
  ],
  "get_num_levels": [
    "length",
    "base"
  ],
  "MAX_SEQUENCE_LENGTH": [],
  "LAMBDA_LEVEL_BASE": [],
  "MAX_NUM_LEVELS": [],
  "hmamba_chunk_scan_combined": [
    "x",
    "dt",
    "A",
    "B",
    "C",
    "dl",
    "L",
    "chunk_size",
    "D",
    "z",
    "dt_bias",
    "initial_states",
    "seq_idx",
    "cu_seqlens",
    "dt_softplus",
    "dt_limit",
    "return_final_states"
  ],
  "hmamba_split_conv1d_scan_combined": [
    "zxbcdtdl",
    "conv1d_weight",
    "conv1d_bias",
    "dt_bias",
    "A",
    "L",
    "D",
    "chunk_size",
    "initial_states",
    "seq_idx",
    "dt_limit",
    "return_final_states",
    "activation",
    "rmsnorm_weight",
    "rmsnorm_eps",
    "outproj_weight",
    "outproj_bias",
    "headdim",
    "ngroups",
    "norm_before_gate"
  ],
  "LogLinearMamba2": {
    "__init__": [
      "self",
      "num_heads",
      "head_dim",
      "hidden_size",
      "state_size",
      "expand",
      "n_groups",
      "conv_kernel",
      "use_conv_bias",
      "hidden_act",
      "rms_norm",
      "chunk_size",
      "time_step_rank",
      "time_step_limit",
      "time_step_min",
      "time_step_max",
      "use_bias",
      "norm_eps",
      "layer_idx"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "apply_mask_to_padding_states": [
    "hidden_states",
    "attention_mask"
  ],
  "pad_tensor_by_size": [
    "input_tensor",
    "pad_size"
  ],
  "reshape_into_chunks": [
    "input_tensor",
    "pad_size",
    "chunk_size"
  ],
  "segment_sum": [
    "input_tensor"
  ],
  "Mamba2": {
    "__init__": [
      "self",
      "num_heads",
      "head_dim",
      "hidden_size",
      "state_size",
      "expand",
      "n_groups",
      "conv_kernel",
      "use_conv_bias",
      "hidden_act",
      "rms_norm",
      "chunk_size",
      "time_step_rank",
      "time_step_limit",
      "time_step_min",
      "time_step_max",
      "use_bias",
      "norm_eps",
      "layer_idx",
      "backend"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "torch_forward": [
      "self",
      "input_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "Mamba": {
    "__init__": [
      "self",
      "hidden_size",
      "state_size",
      "conv_kernel",
      "use_conv_bias",
      "intermediate_size",
      "time_step_rank",
      "use_bias",
      "hidden_act",
      "layer_idx",
      "backend"
    ],
    "cuda_kernels_forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "slow_forward": [
      "self",
      "input_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "RWKV7Attention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "head_dim",
      "num_heads",
      "decay_low_rank_dim",
      "gate_low_rank_dim",
      "a_low_rank_dim",
      "v_low_rank_dim",
      "elementwise_affine",
      "norm_eps",
      "layer_idx",
      "fuse_norm",
      "value_dim",
      "num_hidden_layers"
    ],
    "_initialize_weights": [
      "self",
      "module"
    ],
    "_orthogonal_init": [
      "weight",
      "gain"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "v_first",
      "cu_seqlens"
    ]
  },
  "__all__": [],
  "GatedDeltaNet": {
    "__init__": [
      "self",
      "hidden_size",
      "expand_v",
      "head_dim",
      "num_heads",
      "num_v_heads",
      "mode",
      "use_gate",
      "use_short_conv",
      "allow_neg_eigval",
      "conv_size",
      "conv_bias",
      "layer_idx",
      "norm_eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "Attention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "num_kv_heads",
      "qkv_bias",
      "qk_norm",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "GatedDeltaProduct": {
    "__init__": [
      "self",
      "hidden_size",
      "expand_v",
      "head_dim",
      "num_heads",
      "num_v_heads",
      "mode",
      "use_output_gate",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "layer_idx",
      "norm_eps",
      "use_forget_gate",
      "allow_neg_eigval",
      "num_householder"
    ],
    "_initialize_weights": [
      "self",
      "module"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "ForgettingAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "num_kv_heads",
      "qkv_bias",
      "qk_norm",
      "window_size",
      "use_output_gate",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "yarn_get_mscale": [
    "scale",
    "mscale"
  ],
  "MultiheadLatentAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "q_lora_rank",
      "qk_rope_head_dim",
      "kv_lora_rank",
      "v_head_dim",
      "qk_nope_head_dim",
      "qk_head_dim",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "rope_scaling",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "Comba": {
    "__init__": [
      "self",
      "hidden_size",
      "expand_v",
      "head_dim",
      "num_heads",
      "num_v_heads",
      "mode",
      "use_short_conv",
      "use_output_gate",
      "use_output_correction",
      "use_inner_decay",
      "correction_factor",
      "conv_size",
      "conv_bias",
      "layer_idx",
      "norm_eps"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "ReBasedLinearAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "l_max",
      "feature_dim",
      "num_key_value_heads",
      "num_heads",
      "use_gamma",
      "use_beta",
      "normalize",
      "causal",
      "eps",
      "mode",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "forward_reference": [
      "self",
      "hidden_states",
      "filters"
    ]
  },
  "GatedSlotAttention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "num_kv_heads",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "num_slots",
      "elementwise_affine",
      "norm_eps",
      "gate_logit_normalizer",
      "feature_map",
      "use_output_gate",
      "use_norm",
      "layer_idx",
      "scale"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "BitAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "num_kv_heads",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "norm_eps",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "HGRNAttention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "expand_ratio",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "elementwise_affine",
      "norm_eps",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "lower_bound"
    ],
    "state_size": [
      "self"
    ]
  },
  "MultiScaleRetention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "num_kv_heads",
      "feature_map",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "use_output_gate",
      "gate_fn",
      "elementwise_affine",
      "norm_eps",
      "fuse_norm",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "_upad_input": [
    "query_layer",
    "key_layer",
    "value_layer",
    "gate_layer",
    "beta_layer",
    "attention_mask"
  ],
  "transform": [
    "x",
    "routing_mask",
    "num_memories",
    "selected_memories",
    "attention_mask"
  ],
  "reconstruct": [
    "transformed_x",
    "indices",
    "sorted_indices",
    "batch_size",
    "seq_len",
    "topk",
    "routing_weights",
    "mask"
  ],
  "MomAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "head_dim",
      "num_heads",
      "expand_v",
      "mode",
      "use_output_gate",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "layer_idx",
      "norm_eps",
      "num_memories",
      "topk",
      "capacity",
      "shared_mem",
      "single_kv_proj"
    ],
    "_initialize_weights": [
      "self",
      "module"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ],
    "shared_o": [
      "self",
      "hidden_states",
      "attention_mask",
      "recurrent_state",
      "use_cache",
      "conv_state_q",
      "conv_state_k",
      "conv_state_v"
    ],
    "cu2pad": [
      "self",
      "x",
      "cu_seqlens"
    ],
    "pad_for_conv": [
      "self",
      "cu_seqlens",
      "cu_q",
      "cu_k",
      "cu_v"
    ],
    "unpad_after_conv": [
      "self",
      "conv_cu_seqlens",
      "cu_seqlens",
      "cu_q",
      "cu_k",
      "cu_v",
      "pad_lengths"
    ],
    "prepare_recurrent_state": [
      "self",
      "recurrent_state",
      "cu_seqlens",
      "cu_seqlen_all",
      "reverse_indices",
      "batch_size"
    ],
    "handle_recurrent_state": [
      "self",
      "recurrent_state",
      "recurrent_state_new",
      "cu_seqlens",
      "cu_seqlen_all",
      "reverse_indices"
    ]
  },
  "RWKV6Attention": {
    "__init__": [
      "self",
      "mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "gate_fn",
      "proj_low_rank_dim",
      "gate_low_rank_dim",
      "fuse_norm",
      "elementwise_affine",
      "norm_eps",
      "layer_idx"
    ],
    "_initialize_weights": [
      "self",
      "module"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cu_seqlens"
    ]
  },
  "LoRA": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "low_rank_dim",
      "bias",
      "activation"
    ],
    "__repr__": [
      "self"
    ],
    "_initialize_weights": [
      "self",
      "module"
    ],
    "set_bias_value": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LerpLinear": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "low_rank_dim"
    ],
    "__repr__": [
      "self"
    ],
    "forward": [
      "self",
      "x",
      "delta",
      "cu_seqlens"
    ]
  },
  "DDLerpLinear": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "low_rank_dim"
    ],
    "__repr__": [
      "self"
    ],
    "forward": [
      "self",
      "x",
      "mu",
      "delta",
      "cu_seqlens"
    ]
  },
  "ABCAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "expand_k",
      "expand_v",
      "num_heads",
      "use_short_conv",
      "conv_size",
      "conv_bias",
      "num_slots",
      "elementwise_affine",
      "norm_eps",
      "gate_low_rank_dim",
      "gate_logit_normalizer",
      "use_rope",
      "use_input_gate",
      "use_output_gate",
      "use_norm",
      "clamp_min",
      "clamp_max",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ],
    "state_size": [
      "self",
      "seq_len"
    ]
  },
  "_TF_VERSION": [],
  "_NEED_NEW": [],
  "_IS_TRANSFORMERS_4_56_PLUS": [],
  "FLALayer": {
    "is_compileable": [],
    "is_sliding": [],
    "__init__": [
      "self"
    ],
    "lazy_initialization": [
      "self",
      "key_states"
    ],
    "update": [
      "self"
    ],
    "get_seq_length": [
      "self",
      "cache_position"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "offload": [
      "self"
    ],
    "prefetch": [
      "self"
    ],
    "reset": [
      "self"
    ]
  },
  "LegacyFLACache": {
    "is_compileable": [],
    "__init__": [
      "self",
      "seen_tokens"
    ],
    "__getitem__": [
      "self",
      "layer_idx"
    ],
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "update": [
      "self",
      "recurrent_state",
      "attn_state",
      "conv_state",
      "ffn_state",
      "layer_idx",
      "offset",
      "cache_kwargs"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "to_legacy_cache": [
      "self"
    ],
    "from_legacy_cache": [
      "cls",
      "past_key_values",
      "seen_tokens"
    ]
  },
  "FLACache": {
    "is_compileable": [],
    "__init__": [
      "self",
      "seen_tokens"
    ],
    "update": [
      "self",
      "recurrent_state",
      "attn_state",
      "conv_state",
      "ffn_state",
      "layer_idx",
      "offset",
      "cache_kwargs"
    ],
    "__getitem__": [
      "self",
      "layer_idx"
    ],
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "get_seq_length": [
      "self",
      "layer_idx",
      "cache_position"
    ],
    "get_max_cache_shape": [
      "self",
      "layer_idx"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position",
      "layer_idx"
    ],
    "to_legacy_cache": [
      "self"
    ],
    "from_legacy_cache": [
      "cls",
      "past_key_values",
      "seen_tokens"
    ]
  },
  "FLAGenerationMixin": {
    "__init__": [
      "self"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds",
      "use_cache",
      "logits_to_keep",
      "cache_position"
    ]
  },
  "GradientCheckpointingLayer": {
    "gradient_checkpointing": [],
    "__call__": [
      "self"
    ]
  },
  "MLABlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "MLAPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "MLAModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MLAForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "MLAConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_heads",
      "q_lora_rank",
      "qk_rope_head_dim",
      "kv_lora_rank",
      "v_head_dim",
      "qk_nope_head_dim",
      "qk_head_dim",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "rope_scaling",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "initializer_range",
      "elementwise_affine",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "SambaBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params"
    ]
  },
  "SambaPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "SambaOutput": {},
  "SambaCausalLMOutput": {},
  "SambaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "use_cache",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "SambaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "cache_params",
      "labels",
      "output_hidden_states",
      "return_dict",
      "use_cache",
      "logits_to_keep"
    ]
  },
  "SambaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "hidden_size",
      "state_size",
      "num_hidden_layers",
      "norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "expand",
      "conv_kernel",
      "use_bias",
      "use_conv_bias",
      "hidden_act",
      "initializer_range",
      "residual_in_fp32",
      "time_step_rank",
      "time_step_scale",
      "time_step_min",
      "time_step_max",
      "time_step_init_scheme",
      "time_step_floor",
      "max_position_embeddings",
      "attn",
      "hidden_ratio",
      "rescale_prenorm_residual",
      "use_cache",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size",
      "tie_word_embeddings"
    ]
  },
  "load_balancing_loss_func": [
    "gate_logits",
    "num_experts",
    "top_k",
    "attention_mask"
  ],
  "MomBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "MomPreTrainedModel": {
    "config_class": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "rescale_prenorm_residual",
      "num_residuals_per_layer"
    ]
  },
  "MomOutputWithPast": {},
  "MomModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MomCausalLMOutputWithPast": {},
  "MomForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "num_logits_to_keep"
    ]
  },
  "MomConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "conv_size",
      "num_heads",
      "head_dim",
      "expand_v",
      "use_output_gate",
      "use_short_conv",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "num_hidden_layers",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "num_memories",
      "topk",
      "capacity",
      "use_layer_wise_balance",
      "aux_loss_scale",
      "shared_mem",
      "single_kv_proj",
      "mom_backend",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "vocab_size"
    ]
  },
  "LightNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "attn_mode",
      "num_heads",
      "expand_ratio",
      "use_short_conv",
      "conv_size",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "max_position_embeddings",
      "gate_low_rank_dim",
      "elementwise_affine",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "LightNetBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "LightNetPreTrainedModel": {
    "config_class": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "LightNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LightNetForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "ForgettingTransformerBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "ForgettingTransformerPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "rescale_prenorm_residual",
      "num_residuals_per_layer"
    ]
  },
  "ForgettingTransformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ForgettingTransformerForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "ForgettingTransformerConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "qkv_bias",
      "qk_norm",
      "window_size",
      "use_output_gate",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "initializer_range",
      "elementwise_affine",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "MambaCache": {
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device",
      "max_batch_size"
    ],
    "update_conv_state": [
      "self",
      "layer_idx",
      "new_conv_state",
      "cache_position"
    ],
    "update_ssm_state": [
      "self",
      "layer_idx",
      "new_ssm_state"
    ],
    "reset": [
      "self"
    ]
  },
  "MambaBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "MambaPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module"
    ]
  },
  "MambaOutput": {},
  "MambaCausalLMOutput": {},
  "MambaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "load_hook": [
      "self",
      "state_dict",
      "prefix"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "use_cache",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "attention_mask"
    ]
  },
  "MambaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "num_new_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "cache_params",
      "labels",
      "output_hidden_states",
      "return_dict",
      "use_cache",
      "cache_position",
      "logits_to_keep"
    ]
  },
  "MambaConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "state_size",
      "num_hidden_layers",
      "norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "expand",
      "conv_kernel",
      "use_bias",
      "use_conv_bias",
      "hidden_act",
      "initializer_range",
      "residual_in_fp32",
      "time_step_rank",
      "time_step_scale",
      "time_step_min",
      "time_step_max",
      "time_step_init_scheme",
      "time_step_floor",
      "rescale_prenorm_residual",
      "use_cache",
      "fuse_norm",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "tie_word_embeddings"
    ]
  },
  "LinearAttentionConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "feature_map",
      "tie_feature_map_qk",
      "norm_q",
      "norm_k",
      "norm_feature_map",
      "hidden_act",
      "max_position_embeddings",
      "elementwise_affine",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "LinearAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "LinearAttentionPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "LinearAttentionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LinearAttentionForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "DeltaFormerBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "DeltaFormerPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "DeltaFormerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DeltaFormerForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "tie_weights": [
      "self"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "labels",
      "logits_to_keep"
    ]
  },
  "DeltaFormerConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "attn_mode",
      "hidden_act",
      "max_position_embeddings",
      "elementwise_affine",
      "norm_eps",
      "qkv_bias",
      "qk_norm",
      "rope_theta",
      "rope_max_position_embeddings",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size",
      "output_attentions",
      "output_hidden_states"
    ]
  },
  "GLAConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "expand_k",
      "expand_v",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "feature_map",
      "attn_mode",
      "use_short_conv",
      "conv_size",
      "use_output_gate",
      "clamp_min",
      "hidden_act",
      "max_position_embeddings",
      "elementwise_affine",
      "norm_eps",
      "use_gk",
      "use_gv",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "GLABlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "GLAPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "GLAModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GLAForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "ABCBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "ABCPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "ABCModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "ABCForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "ABCConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "gate_low_rank_dim",
      "clamp_min",
      "clamp_max",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "num_heads",
      "num_slots",
      "use_short_conv",
      "conv_size",
      "exapnd_k",
      "exapnd_v",
      "hidden_act",
      "max_position_embeddings",
      "elementwise_affine",
      "norm_eps",
      "use_rope",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "RWKV6Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "num_heads",
      "proj_low_rank_dim",
      "gate_low_rank_dim",
      "hidden_act",
      "max_position_embeddings",
      "norm_first",
      "norm_bias",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "RWKV6FeedForward": {
    "__init__": [
      "self",
      "hidden_size",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "layer_idx"
    ],
    "forward": [
      "self",
      "x",
      "attention_mask",
      "state",
      "cu_seqlens"
    ]
  },
  "RWKV6Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "cu_seqlens"
    ]
  },
  "RWKV6PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "rescale_prenorm_residual",
      "num_residuals_per_layer"
    ]
  },
  "RWKV6Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cu_seqlens"
    ]
  },
  "RWKV6ForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "BitNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "initializer_range",
      "elementwise_affine",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "BitNetMLP": {
    "__init__": [
      "self",
      "hidden_size",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "fuse_swiglu"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BitNetBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "BitNetPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "rescale_prenorm_residual",
      "num_residuals_per_layer"
    ]
  },
  "BitNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BitNetForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "HGRN2Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "lower_bound"
    ]
  },
  "HGRN2PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "HGRN2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "HGRN2ForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "HGRN2Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "attn_mode",
      "num_heads",
      "expand_ratio",
      "use_short_conv",
      "conv_size",
      "use_lower_bound",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "max_position_embeddings",
      "elementwise_affine",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "RodimusBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "residual"
    ]
  },
  "RodimusPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy"
    ]
  },
  "RodimusModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RodimusForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "RodimusConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "block_type",
      "hidden_size",
      "num_hidden_layers",
      "attn_mode",
      "residual_in_fp32",
      "block_residual_in_fp32",
      "expand_ratio",
      "input_gate_low_rank",
      "use_short_conv",
      "conv_size",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "max_position_embeddings",
      "norm_eps",
      "k_norm_eps",
      "attn",
      "ska_attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "RetNetBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "RetNetPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "RetNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RetNetForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "RetNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "feature_map",
      "hidden_act",
      "use_short_conv",
      "conv_size",
      "use_output_gate",
      "max_position_embeddings",
      "elementwise_affine",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "GatedDeltaNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "expand_v",
      "use_gate",
      "use_short_conv",
      "allow_neg_eigval",
      "conv_size",
      "head_dim",
      "num_heads",
      "num_v_heads",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "num_hidden_layers",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "GatedDeltaNetBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "GatedDeltaNetPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "GatedDeltaNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GatedDeltaNetForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "DeltaNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "expand_k",
      "expand_v",
      "use_gate",
      "use_short_conv",
      "conv_size",
      "use_beta",
      "use_output_norm",
      "num_heads",
      "qk_norm",
      "qk_activation",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "num_hidden_layers",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "DeltaNetBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "DeltaNetPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "DeltaNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "DeltaNetForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "HGRNConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "num_hidden_layers",
      "expand_ratio",
      "use_short_conv",
      "conv_size",
      "use_lower_bound",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "elementwise_affine",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "HGRNBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "lower_bound"
    ]
  },
  "HGRNPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "HGRNModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "HGRNForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "TransformerBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "TransformerPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "rescale_prenorm_residual",
      "num_residuals_per_layer"
    ]
  },
  "TransformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "TransformerForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "TransformerConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "qkv_bias",
      "qk_norm",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "initializer_range",
      "elementwise_affine",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "LogLinearMamba2Cache": {
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device"
    ],
    "update_conv_state": [
      "self",
      "layer_idx",
      "new_conv_state",
      "cache_init"
    ],
    "update_hssm_state": [
      "self",
      "layer_idx",
      "new_hssm_state"
    ],
    "reset": [
      "self"
    ]
  },
  "LogLinearMamba2Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "LogLinearMamba2PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module",
      "num_residuals_per_layer"
    ]
  },
  "LogLinearMamba2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "load_hook": [
      "self",
      "state_dict",
      "prefix"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "use_cache",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "attention_mask"
    ]
  },
  "LogLinearMamba2ForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "labels",
      "output_hidden_states",
      "return_dict",
      "use_cache",
      "cache_position",
      "attention_mask",
      "logits_to_keep"
    ]
  },
  "LogLinearMamba2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "residual_in_fp32",
      "chunk_size"
    ]
  },
  "KDAConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "expand_v",
      "use_short_conv",
      "allow_neg_eigval",
      "conv_size",
      "head_dim",
      "num_heads",
      "num_v_heads",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "num_hidden_layers",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "KDABlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "KDAPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "KDAModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "KDAForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "RWKV7FeedForward": {
    "__init__": [
      "self",
      "hidden_size",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "layer_idx",
      "num_hidden_layers"
    ],
    "_initialize_weights": [
      "self",
      "module"
    ],
    "forward": [
      "self",
      "x",
      "attention_mask",
      "state",
      "cu_seqlens"
    ]
  },
  "RWKV7Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "v_first",
      "cu_seqlens"
    ]
  },
  "RWKV7PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "_skip_keys_device_placement": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "rescale_prenorm_residual",
      "num_residuals_per_layer"
    ]
  },
  "RWKV7Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "load_state_dict": [
      "self",
      "state_dict",
      "strict",
      "assign"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "cu_seqlens"
    ]
  },
  "RWKV7ForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "shift_labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "RWKV7Config": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "head_dim",
      "num_heads",
      "decay_low_rank_dim",
      "gate_low_rank_dim",
      "a_low_rank_dim",
      "v_low_rank_dim",
      "hidden_act",
      "max_position_embeddings",
      "norm_first",
      "norm_bias",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size",
      "value_dim"
    ]
  },
  "PaTHAttentionBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "output_attentions",
      "use_cache"
    ]
  },
  "PaTHAttentionPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "rescale_prenorm_residual",
      "num_residuals_per_layer"
    ]
  },
  "PaTHAttentionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PaTHAttentionForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "PaTHAttentionConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "initializer_range",
      "elementwise_affine",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size",
      "use_forget_gate",
      "use_w_shortconv",
      "use_low_rank_w"
    ]
  },
  "Mamba2Config": {
    "model_type": [],
    "__init__": [
      "self",
      "head_dim",
      "vocab_size",
      "hidden_size",
      "state_size",
      "num_hidden_layers",
      "norm_eps",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "expand",
      "conv_kernel",
      "n_groups",
      "use_bias",
      "use_conv_bias",
      "hidden_act",
      "initializer_range",
      "residual_in_fp32",
      "time_step_rank",
      "time_step_min",
      "time_step_max",
      "time_step_floor",
      "time_step_limit",
      "rescale_prenorm_residual",
      "use_cache",
      "rms_norm",
      "chunk_size",
      "fuse_norm",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "tie_word_embeddings"
    ]
  },
  "Mamba2Cache": {
    "__init__": [
      "self",
      "config",
      "batch_size",
      "dtype",
      "device"
    ],
    "update_conv_state": [
      "self",
      "layer_idx",
      "new_conv_state",
      "cache_init"
    ],
    "update_ssm_state": [
      "self",
      "layer_idx",
      "new_ssm_state"
    ],
    "reset": [
      "self"
    ]
  },
  "Mamba2Block": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "cache_params",
      "cache_position",
      "attention_mask"
    ]
  },
  "Mamba2PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "_no_split_modules": [],
    "supports_gradient_checkpointing": [],
    "_is_stateful": [],
    "_init_weights": [
      "self",
      "module",
      "num_residuals_per_layer"
    ]
  },
  "Mamba2Output": {},
  "Mamba2CausalLMOutput": {},
  "Mamba2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "load_hook": [
      "self",
      "state_dict",
      "prefix"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "use_cache",
      "output_hidden_states",
      "return_dict",
      "cache_position",
      "attention_mask"
    ]
  },
  "Mamba2ForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "new_embeddings"
    ],
    "forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "cache_params",
      "labels",
      "output_hidden_states",
      "return_dict",
      "use_cache",
      "cache_position",
      "attention_mask",
      "logits_to_keep"
    ]
  },
  "GSABlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "GSAPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "GSAModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GSAForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "GSAConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "gate_logit_normalizer",
      "clamp_min",
      "clamp_max",
      "hidden_ratio",
      "intermediate_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "num_slots",
      "use_short_conv",
      "conv_size",
      "exapnd_k",
      "exapnd_v",
      "feature_map",
      "use_output_gate",
      "use_norm",
      "max_position_embeddings",
      "hidden_act",
      "elementwise_affine",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "initializer_range",
      "tie_word_embeddings",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "NSABlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "NSAPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "NSAModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "NSAForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "NSAConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "hidden_size",
      "num_hidden_layers",
      "num_heads",
      "num_kv_heads",
      "head_dim",
      "qkv_bias",
      "block_size",
      "block_counts",
      "window_size",
      "rope_theta",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "initializer_range",
      "elementwise_affine",
      "norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "GatedDeltaProductConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "conv_size",
      "head_dim",
      "num_heads",
      "hidden_size",
      "expand_v",
      "use_output_gate",
      "use_short_conv",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "num_hidden_layers",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size",
      "use_forget_gate",
      "allow_neg_eigval",
      "num_householder"
    ]
  },
  "GatedDeltaProductBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "GatedDeltaProductPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "GatedDeltaProductModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "GatedDeltaProductForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "MesaNetBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "MesaNetPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "MesaNetModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "MesaNetForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  },
  "MesaNetConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "use_output_gate",
      "use_short_conv",
      "conv_size",
      "num_heads",
      "head_dim",
      "lambda_lower_bound",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "num_hidden_layers",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size",
      "max_cg_step_training",
      "max_cg_step_decoding"
    ]
  },
  "CombaConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "attn_mode",
      "hidden_size",
      "conv_size",
      "head_dim",
      "num_heads",
      "num_v_heads",
      "expand_v",
      "use_output_gate",
      "use_short_conv",
      "use_output_correction",
      "use_inner_decay",
      "correction_factor",
      "max_position_embeddings",
      "hidden_ratio",
      "intermediate_size",
      "hidden_act",
      "num_hidden_layers",
      "norm_eps",
      "attn",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings",
      "initializer_range",
      "fuse_norm",
      "fuse_swiglu",
      "fuse_cross_entropy",
      "fuse_linear_cross_entropy",
      "use_l2warp",
      "vocab_size"
    ]
  },
  "CombaBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "past_key_values",
      "use_cache",
      "output_attentions"
    ]
  },
  "CombaPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_supports_cache_class": [],
    "__init__": [
      "self"
    ],
    "_init_weights": [
      "self",
      "module",
      "prenorm_residual_strategy",
      "num_residuals_per_layer"
    ]
  },
  "CombaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "CombaForCausalLM": {
    "_tied_weights_keys": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "past_key_values",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "logits_to_keep"
    ]
  }
}