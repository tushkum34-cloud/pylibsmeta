{
  "__version__": [],
  "__id__": [],
  "_is_windows": [],
  "_is_linux": [],
  "_is_macos": [],
  "add_onnxruntime_dependency": [
    "package_id"
  ],
  "add_cuda_dependency": [],
  "GGUFTensorModule": {
    "__init__": [
      "self"
    ]
  },
  "GGUFAttention": {
    "__init__": [
      "self"
    ]
  },
  "GGUFMLP": {
    "__init__": [
      "self"
    ]
  },
  "GGUFDecoderLayer": {
    "__init__": [
      "self",
      "layer_id"
    ]
  },
  "GGUFModel": {
    "__init__": [
      "self",
      "input_path",
      "head_size",
      "hidden_size",
      "intermediate_size",
      "num_attn_heads",
      "num_kv_heads",
      "vocab_size"
    ],
    "modules": [
      "self"
    ],
    "undo_permute": [
      "self",
      "head_size",
      "hidden_size",
      "num_attn_heads",
      "num_kv_heads"
    ],
    "swap_mlp_types": [
      "self"
    ],
    "swap_norm_types": [
      "self"
    ],
    "from_pretrained": [
      "model_type",
      "input_path",
      "head_size",
      "hidden_size",
      "intermediate_size",
      "num_attn_heads",
      "num_kv_heads",
      "vocab_size"
    ]
  },
  "check_extra_options": [
    "kv_pairs",
    "execution_provider"
  ],
  "parse_extra_options": [
    "kv_items",
    "execution_provider"
  ],
  "parse_hf_token": [
    "hf_token"
  ],
  "set_io_dtype": [
    "precision",
    "execution_provider",
    "extra_options"
  ],
  "set_onnx_dtype": [
    "precision",
    "extra_options"
  ],
  "create_model": [
    "model_name",
    "input_path",
    "output_dir",
    "precision",
    "execution_provider",
    "cache_dir"
  ],
  "get_args": [],
  "QuantizedTensorModule": {
    "__init__": [
      "self"
    ],
    "group_size": [
      "self",
      "value"
    ],
    "__str__": [
      "self"
    ]
  },
  "TensorModule": {
    "__init__": [
      "self"
    ]
  },
  "QuantizedAttention": {
    "__init__": [
      "self"
    ]
  },
  "QuantizedExpert": {
    "__init__": [
      "self",
      "expert_id"
    ]
  },
  "QuantizedExperts": {
    "__init__": [
      "self"
    ],
    "add_expert": [
      "self",
      "expert_id"
    ],
    "get_expert": [
      "self",
      "expert_id"
    ],
    "items": [
      "self"
    ],
    "values": [
      "self"
    ],
    "keys": [
      "self"
    ],
    "__getitem__": [
      "self",
      "expert_id"
    ],
    "num_experts": [
      "self"
    ],
    "set_weight_data": [
      "self",
      "expert_id",
      "proj_type",
      "param_type",
      "tensor",
      "bits",
      "group_size"
    ],
    "__str__": [
      "self"
    ]
  },
  "QuantizedMLP": {
    "__init__": [
      "self"
    ]
  },
  "QuantizedDecoderLayer": {
    "__init__": [
      "self",
      "layer_id"
    ],
    "is_empty": [
      "self"
    ]
  },
  "QuantizedModel": {
    "__init__": [
      "self",
      "quant_type",
      "input_path",
      "quant_attrs",
      "q_size",
      "kv_size",
      "intermediate_size",
      "num_layers"
    ],
    "_load_quant_config": [
      "self",
      "quant_attrs"
    ],
    "get_layer_bits": [
      "self",
      "layer_name"
    ],
    "get_layer_group_size": [
      "self",
      "layer_name"
    ],
    "_initialize_quantized_lm_head": [
      "self",
      "bits",
      "group_size"
    ],
    "set_g_idx": [
      "self",
      "module"
    ],
    "set_properties": [
      "self"
    ],
    "modules": [
      "self"
    ],
    "unpack": [
      "self",
      "module"
    ],
    "repack": [
      "self",
      "module"
    ],
    "unpack_qzeros": [
      "self",
      "module"
    ],
    "unpack_qweight": [
      "self",
      "module"
    ],
    "pack_qzeros": [
      "self",
      "module"
    ],
    "unpack_on_row_for_2_4_8_bits": [
      "self",
      "tensor",
      "bits",
      "transpose"
    ],
    "unpack_on_row": [
      "self",
      "tensor",
      "bits",
      "transpose"
    ],
    "pack_on_row_for_2_4_8_bits": [
      "self",
      "tensor",
      "bits",
      "transpose",
      "packed_dtype"
    ],
    "pack_on_row": [
      "self",
      "tensor",
      "bits",
      "transpose",
      "packed_dtype"
    ],
    "dequant_weight": [
      "self",
      "module"
    ],
    "quant_weight": [
      "self",
      "module"
    ],
    "pack_ort_format": [
      "self",
      "module",
      "intweight"
    ],
    "pack_zeros_ort_format": [
      "self",
      "module",
      "reshape"
    ]
  },
  "AWQModel": {
    "__init__": [
      "self",
      "quant_type",
      "input_path",
      "quant_attrs",
      "q_size",
      "kv_size",
      "intermediate_size",
      "num_layers"
    ],
    "unpack_qweight": [
      "self",
      "module"
    ],
    "unpack_qzeros": [
      "self",
      "module"
    ],
    "reverse_reorder_tensor": [
      "self",
      "tensor",
      "bits"
    ]
  },
  "GPTQModel": {
    "__init__": [
      "self",
      "quant_type",
      "input_path",
      "quant_attrs",
      "q_size",
      "kv_size",
      "intermediate_size",
      "num_layers"
    ],
    "handle_qzeros": [
      "self",
      "module"
    ],
    "_load_quant_config": [
      "self",
      "quant_attrs"
    ],
    "get_overrides": [
      "self",
      "layer_name"
    ],
    "get_layer_bits": [
      "self",
      "layer_name"
    ],
    "get_layer_group_size": [
      "self",
      "layer_name"
    ]
  },
  "QuarkModel": {
    "__init__": [
      "self",
      "quant_type",
      "input_path",
      "quant_attrs",
      "q_size",
      "kv_size",
      "intermediate_size",
      "num_layers"
    ],
    "unpack_repack_experts": [
      "self",
      "experts"
    ],
    "repack_qmoe_weights": [
      "self",
      "experts"
    ],
    "repack_projections": [
      "self",
      "experts",
      "projection_types"
    ],
    "combine_and_repack_gate_up": [
      "self",
      "experts"
    ],
    "repack_qweight": [
      "self",
      "weights",
      "bits"
    ],
    "_load_quant_config": [
      "self",
      "quant_attrs"
    ],
    "get_layer_bits": [
      "self",
      "layer_name"
    ],
    "get_layer_group_size": [
      "self",
      "layer_name"
    ],
    "unpack_qweight": [
      "self",
      "module"
    ],
    "unpack_qzeros": [
      "self",
      "module"
    ],
    "reverse_reorder_tensor": [
      "self",
      "tensor",
      "bits"
    ],
    "unpack_qweight_quark": [
      "self",
      "module",
      "reorder",
      "dtype"
    ]
  },
  "OliveModel": {
    "_load_quant_config": [
      "self",
      "quant_attrs"
    ],
    "get_layer_bits": [
      "self",
      "layer_name"
    ],
    "get_layer_group_size": [
      "self",
      "layer_name"
    ],
    "handle_qzeros": [
      "self",
      "module"
    ],
    "unpack": [
      "self",
      "module"
    ],
    "repack": [
      "self",
      "module"
    ]
  },
  "QuantModel": {
    "from_pretrained": [
      "quant_type"
    ]
  },
  "MistralModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "NemotronModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_mlp_proj": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ]
  },
  "GemmaModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "Gemma2Model": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "is_local": [
      "self",
      "layer_id"
    ],
    "make_layernorm": [
      "self",
      "layer_id",
      "layernorm",
      "skip",
      "simple",
      "location"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ],
    "make_attention": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ]
  },
  "Gemma3Model": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "is_local": [
      "self",
      "layer_id"
    ],
    "make_attention_init": [
      "self"
    ],
    "make_rotary_embedding_multi_cache": [
      "self"
    ],
    "make_rotary_embedding_caches": [
      "self"
    ]
  },
  "GraniteModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ]
  },
  "OLMoModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_layernorm": [
      "self",
      "layer_id",
      "layernorm",
      "skip",
      "simple",
      "location"
    ]
  },
  "Model": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "to_str_dtype": [
      "self",
      "dtype"
    ],
    "make_outputs_init": [
      "self"
    ],
    "make_rope_init": [
      "self",
      "config"
    ],
    "is_gqa_supported": [
      "self"
    ],
    "make_attention_init": [
      "self"
    ],
    "make_genai_config": [
      "self",
      "model_name_or_path",
      "extra_kwargs",
      "out_dir"
    ],
    "make_key_value_cache_shape": [
      "self",
      "layer_id",
      "shape"
    ],
    "save_processing": [
      "self",
      "model_name_or_path",
      "extra_kwargs",
      "out_dir"
    ],
    "make_int4_algo_config": [
      "self",
      "quant_method"
    ],
    "to_int4": [
      "self"
    ],
    "save_model": [
      "self",
      "out_dir"
    ],
    "make_initializer": [
      "name",
      "to"
    ],
    "make_node": [
      "self",
      "op_type",
      "inputs",
      "outputs"
    ],
    "make_value": [
      "self",
      "name",
      "dtype",
      "shape"
    ],
    "make_inputs_and_outputs": [
      "self"
    ],
    "make_constant": [
      "self",
      "name"
    ],
    "make_gather": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape",
      "axis"
    ],
    "make_reshape": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_shape": [
      "self",
      "name",
      "root_input",
      "shape"
    ],
    "make_constant_of_shape": [
      "self",
      "name",
      "root_input",
      "value",
      "dtype",
      "shape"
    ],
    "make_unsqueeze": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_squeeze": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_concat": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape",
      "axis"
    ],
    "make_tile": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_equal": [
      "self",
      "name",
      "inputs",
      "shape"
    ],
    "make_greater": [
      "self",
      "name",
      "inputs",
      "shape"
    ],
    "make_greater_or_equal": [
      "self",
      "name",
      "inputs",
      "shape"
    ],
    "make_isinf": [
      "self",
      "name",
      "root_input",
      "shape"
    ],
    "make_clip": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_where": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_expand": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_reduce_sum": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape",
      "keepdims"
    ],
    "make_reduce_max": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape",
      "keepdims"
    ],
    "make_reduce_mean": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape",
      "keepdims"
    ],
    "make_sqrt": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_cast": [
      "self",
      "name",
      "root_input",
      "dtype",
      "shape"
    ],
    "make_add": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_sub": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_less": [
      "self",
      "name",
      "inputs"
    ],
    "make_range": [
      "self",
      "name",
      "inputs"
    ],
    "make_slice": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_mul": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_transpose": [
      "self",
      "name",
      "root_input",
      "dtype",
      "shape",
      "perm"
    ],
    "make_div": [
      "self",
      "name",
      "inputs",
      "dtype",
      "shape"
    ],
    "make_tanh": [
      "self",
      "name",
      "root_input",
      "dtype",
      "shape"
    ],
    "make_softmax": [
      "self",
      "name",
      "root_input",
      "dtype",
      "shape",
      "axis"
    ],
    "make_sigmoid": [
      "self",
      "name",
      "root_input",
      "dtype",
      "shape"
    ],
    "make_matmul": [
      "self",
      "matmul",
      "basename",
      "root_input"
    ],
    "make_matmul_op": [
      "self",
      "matmul",
      "basename",
      "root_input"
    ],
    "make_matmul_float": [
      "self",
      "matmul",
      "name",
      "root_input"
    ],
    "make_matmul_int4": [
      "self",
      "matmul",
      "basename",
      "root_input"
    ],
    "make_dequantize_linear": [
      "self",
      "dequantize_name",
      "quantized_op"
    ],
    "make_matmul_int4_qdq": [
      "self",
      "matmul",
      "matmul_name",
      "root_input"
    ],
    "make_matmul_lora": [
      "self",
      "matmul",
      "basename",
      "root_input"
    ],
    "make_packed_matmul": [
      "self",
      "q_matmul",
      "k_matmul",
      "v_matmul",
      "basename",
      "root_input"
    ],
    "make_packed_matmul_float": [
      "self",
      "q_matmul",
      "k_matmul",
      "v_matmul",
      "basename",
      "root_input"
    ],
    "make_packed_matmul_int4": [
      "self",
      "q_matmul",
      "k_matmul",
      "v_matmul",
      "basename",
      "root_input"
    ],
    "make_add_bias": [
      "self",
      "add",
      "name",
      "root_input"
    ],
    "make_packed_add": [
      "self",
      "q_add",
      "k_add",
      "v_add",
      "name",
      "root_input"
    ],
    "make_embedding": [
      "self",
      "embedding"
    ],
    "make_layernorm": [
      "self",
      "layer_id",
      "layernorm",
      "skip",
      "simple",
      "location"
    ],
    "make_layernorm_op": [
      "self",
      "layer_id",
      "layernorm",
      "skip",
      "simple",
      "location"
    ],
    "_make_layernorm_op": [
      "self",
      "layer_id",
      "layernorm",
      "skip",
      "simple",
      "location"
    ],
    "make_layernorm_casts": [
      "self",
      "name",
      "inputs",
      "outputs",
      "old_dtype",
      "new_dtype"
    ],
    "make_mscale_su": [
      "self",
      "mscale"
    ],
    "make_mscale_yarn": [
      "self",
      "mscale"
    ],
    "make_mscale": [
      "self",
      "mscale"
    ],
    "make_inv_freq_rescaled": [
      "self",
      "inv_freq"
    ],
    "make_inv_freq_rescaled_with_freq_factors": [
      "self",
      "inv_freq"
    ],
    "make_inv_freq_rescaled_with_ntk": [
      "self",
      "inv_freq"
    ],
    "make_rotary_embedding_caches_from_scratch": [
      "self"
    ],
    "make_rotary_embedding_caches": [
      "self"
    ],
    "make_padded_cache": [
      "self",
      "small_cache",
      "large_cache",
      "pad_value"
    ],
    "_make_split_if_nodes_for_trt_rtx": [
      "self",
      "basename",
      "greater_name",
      "cos_cache_name",
      "sin_cache_name",
      "cos_cache_large",
      "sin_cache_large",
      "cos_cache_small",
      "sin_cache_small",
      "cos_cache_large_name",
      "sin_cache_large_name",
      "cos_cache_small_name",
      "sin_cache_small_name",
      "small_cache_shape"
    ],
    "make_rotary_embedding": [
      "self",
      "name",
      "root_input"
    ],
    "make_rotary_embedding_multi_cache": [
      "self"
    ],
    "_make_skip_simplified_layer_norm": [
      "self",
      "basename",
      "root_input",
      "skip_input",
      "weight_name",
      "output_0",
      "output_3",
      "io_dtype",
      "shape"
    ],
    "_make_skip_layer_norm": [
      "self",
      "basename",
      "root_input",
      "skip_input",
      "weight_name",
      "bias_name",
      "output_0",
      "output_3",
      "io_dtype",
      "shape"
    ],
    "_make_simplified_layer_norm": [
      "self",
      "basename",
      "root_input",
      "weight_name",
      "output_0",
      "io_dtype",
      "shape"
    ],
    "make_qk_norm": [
      "self",
      "layer_id",
      "attention"
    ],
    "make_repeat_kv": [
      "self",
      "layer_id",
      "root_input",
      "past_kv",
      "present_kv"
    ],
    "make_attention_op": [
      "self",
      "name"
    ],
    "make_multi_head_attention": [
      "self",
      "name"
    ],
    "make_group_query_attention": [
      "self",
      "name"
    ],
    "make_sparse_attention": [
      "self",
      "name"
    ],
    "make_attention": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "make_attention_input_proj": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "make_attention_qk_subgraph": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "make_attention_output_proj": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "make_attention_unpacked": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "make_attention_unpacked_lora": [
      "self",
      "layer_id",
      "attention",
      "qkv_linear",
      "root_input"
    ],
    "make_attention_unpacked_regular": [
      "self",
      "layer_id",
      "attention",
      "qkv_linear",
      "root_input"
    ],
    "make_mlp": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "make_mlp_unpacked": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "make_mlp_unpacked_lora": [
      "self",
      "layer_id",
      "mlp",
      "gate_up_linear",
      "root_input"
    ],
    "make_mlp_unpacked_regular": [
      "self",
      "layer_id",
      "mlp",
      "gate_up_linear",
      "root_input"
    ],
    "make_mlp_proj": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "make_mlp_fc": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "make_moe_op": [
      "self",
      "name"
    ],
    "make_base_moe_op": [
      "self",
      "name"
    ],
    "make_qmoe_op": [
      "self",
      "name"
    ],
    "make_qmoe_weights": [
      "self",
      "weights"
    ],
    "_symmetric_blockwise_quantize": [
      "self",
      "weights",
      "block_size"
    ],
    "make_block_sparse_moe": [
      "self",
      "layer_id",
      "bsm",
      "root_input"
    ],
    "make_activation_with_mul": [
      "self",
      "layer_id",
      "root_input",
      "activation",
      "domain"
    ],
    "make_gelu": [
      "self",
      "layer_id",
      "root_input",
      "activation"
    ],
    "make_relu": [
      "self",
      "layer_id",
      "root_input",
      "activation"
    ],
    "make_relu_squared": [
      "self",
      "layer_id",
      "root_input",
      "activation"
    ],
    "make_activation": [
      "self",
      "layer_id",
      "root_input"
    ],
    "make_lm_head": [
      "self",
      "lm_head"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ],
    "load_weights": [
      "self",
      "input_path"
    ],
    "make_model": [
      "self",
      "input_path"
    ],
    "has_final_norm": [
      "self",
      "module",
      "orig_model"
    ],
    "make_preprocessing_nodes": [
      "self"
    ],
    "make_attention_mask_reformatting": [
      "self"
    ],
    "make_attention_mask_reformatting_for_mha": [
      "self"
    ],
    "make_past_key_subgraph": [
      "self",
      "basename"
    ],
    "make_input_ids_subgraph": [
      "self",
      "basename",
      "past_key_gather_name"
    ],
    "make_attention_mask_subgraph": [
      "self",
      "basename",
      "unsqueeze_for_concat"
    ],
    "make_common_mask_reformat_subgraph": [
      "self",
      "basename",
      "root_input",
      "unsqueeze_for_concat",
      "unsqueeze_for_expand",
      "input_ids_subgraph"
    ],
    "make_attention_mask_graph_capture_reformatting_for_gqa": [
      "self",
      "attn_mask_basename"
    ],
    "make_attention_mask_standard_reformatting_for_gqa": [
      "self",
      "attn_mask_basename"
    ],
    "make_attention_mask_reformatting_for_gqa": [
      "self"
    ],
    "make_attention_mask_reformatting_for_sparse_attn": [
      "self"
    ],
    "make_position_ids_reformatting": [
      "self"
    ]
  },
  "PhiModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ]
  },
  "Phi3MiniModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "Phi3MiniLongRoPEModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_position_ids_reformatting": [
      "self"
    ],
    "make_attention": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ]
  },
  "Phi3SmallModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "calculate_cdiv": [
      "self",
      "a",
      "b"
    ],
    "calculate_block_mask": [
      "self"
    ],
    "make_attention": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "make_mlp_proj": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ]
  },
  "Phi3SmallLongRoPEModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "Phi3VModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "Phi3MoELongRoPEModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ]
  },
  "Phi4MMModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ]
  },
  "__all__": [],
  "QwenModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "Qwen3Model": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_attention_init": [
      "self"
    ]
  },
  "Qwen25VLTextModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_inv_freq_tensor": [
      "self"
    ],
    "make_inputs_and_outputs": [
      "self"
    ],
    "make_dynamic_rope_caches": [
      "self",
      "layer_id",
      "basename"
    ],
    "make_mrope_flattened_caches": [
      "self",
      "layer_id",
      "dyn_cos",
      "dyn_sin"
    ],
    "apply_mrope_rotation": [
      "self",
      "layer_id",
      "q_or_k_path",
      "q_or_k_shape",
      "dyn_cos",
      "dyn_sin",
      "num_heads",
      "basename"
    ],
    "make_attention_qk_subgraph": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "load_weights": [
      "self",
      "input_path"
    ]
  },
  "LlamaModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "ErnieModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ]
  },
  "InternLM2Model": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "load_weights": [
      "self",
      "input_path"
    ]
  },
  "ChatGLMModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_mlp": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ]
  },
  "SmolLM3Model": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_attention": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ]
  },
  "GPTOSSModel": {
    "__init__": [
      "self",
      "config",
      "io_dtype",
      "onnx_dtype",
      "ep",
      "cache_dir",
      "extra_options"
    ],
    "make_layer": [
      "self",
      "layer_id",
      "layer"
    ],
    "make_layernorm": [
      "self",
      "layer_id",
      "layernorm",
      "skip",
      "simple",
      "location"
    ],
    "make_rotary_embedding_caches_from_scratch": [
      "self"
    ],
    "make_attention": [
      "self",
      "layer_id",
      "attention",
      "root_input"
    ],
    "make_moe": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "make_moe_decomposed": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "make_moe_fused": [
      "self",
      "layer_id",
      "mlp",
      "root_input"
    ],
    "has_quark_experts": [
      "self",
      "experts"
    ],
    "combine_quark_gate_up_biases_from_experts": [
      "self",
      "experts"
    ],
    "combine_quark_down_biases_from_experts": [
      "self",
      "experts"
    ]
  }
}