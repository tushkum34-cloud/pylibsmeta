{
  "PLAT_TO_CMAKE": [],
  "CMakeExtension": {
    "__init__": [
      "self",
      "name",
      "sourcedir"
    ]
  },
  "CMakeBuild": {
    "build_extension": [
      "self",
      "ext"
    ]
  },
  "CompileGrpc": {
    "run": [
      "self"
    ]
  },
  "ext_modules": [],
  "ARCTIC_INFERENCE_SKIP_SPEC_MODEL_CHECK": [],
  "__getattr__": [
    "name"
  ],
  "get_compatible_vllm_version": [],
  "print0": [],
  "logger": [],
  "Patchable": [],
  "ArcticPatch": {
    "__init_subclass__": [
      "cls"
    ],
    "__class_getitem__": [
      "cls",
      "target"
    ],
    "apply_patch": [
      "cls"
    ]
  },
  "try_load_torch_library": [],
  "try_load_jit_library": [],
  "reshape_and_cache_flash_bulk": [
    "keys",
    "values",
    "key_caches",
    "value_caches",
    "slot_mapping",
    "kv_cache_dtype",
    "k_scales",
    "v_scales",
    "num_heads",
    "head_size"
  ],
  "reshape_and_cache_flash_fp4": [
    "keys",
    "values",
    "key_cache",
    "value_cache",
    "key_cache_scales",
    "value_cache_scales",
    "slot_mapping",
    "kv_cache_dtype",
    "k_scale",
    "v_scale"
  ],
  "speculator_ln": [
    "input",
    "weight",
    "bias",
    "eps"
  ],
  "sum_lstm": [
    "states_4d",
    "z4_4d",
    "prev_cell_d",
    "w_cell",
    "b_cell",
    "w_state",
    "b_state",
    "alpha",
    "eps_cell",
    "eps_state",
    "use_fast_gelu"
  ],
  "LlamaSwiftKVConfig": {
    "model_type": [],
    "__init__": [
      "self",
      "num_key_value_layers"
    ]
  },
  "__all__": [],
  "generate_grpc_code": [],
  "InferenceServicer": {
    "__init__": [
      "self",
      "engine_args"
    ],
    "start": [
      "self"
    ],
    "stop": [
      "self"
    ],
    "_encode_one_prompt": [
      "self",
      "prompt",
      "pooling_params",
      "request_id",
      "priority"
    ],
    "Encode": [
      "self",
      "request",
      "context"
    ],
    "Abort": [
      "self",
      "request",
      "context"
    ],
    "GetReplicaInfo": [
      "self",
      "request",
      "context"
    ],
    "HealthCheck": [
      "self",
      "request",
      "context"
    ]
  },
  "InferenceServer": {
    "__init__": [
      "self",
      "args"
    ],
    "start": [
      "self"
    ],
    "stop": [
      "self"
    ]
  },
  "serve": [
    "args"
  ],
  "patch_embedding_performance": [],
  "patch_model_config_hash": [],
  "ROOT_DIR": [],
  "LoadingerType": {
    "ROUND_ROBIN": [],
    "LEAST_LOADED": [],
    "RANDOM": []
  },
  "ReplicaInfo": {
    "close": [
      "self"
    ]
  },
  "ReplicaManager": {
    "__init__": [
      "self",
      "args",
      "args_list",
      "base_port",
      "num_replicas",
      "lb",
      "health_interval"
    ],
    "start": [
      "self"
    ],
    "stop": [
      "self"
    ],
    "route_request": [
      "self",
      "method_name",
      "request",
      "context"
    ],
    "_check_port_available": [
      "self",
      "port"
    ],
    "_build_replica_cmd": [
      "self",
      "host",
      "port"
    ],
    "_launch_replica_process": [
      "self",
      "host",
      "port"
    ],
    "_stream_subprocess_output": [
      "self",
      "proc",
      "prefix"
    ],
    "_health_loop": [
      "self"
    ],
    "_periodic_health": [
      "self",
      "replica"
    ],
    "_check_health": [
      "self",
      "replica"
    ],
    "_mark_unhealthy": [
      "self",
      "replica"
    ],
    "_select_replica": [
      "self"
    ],
    "_response_type_name": [
      "method_name"
    ]
  },
  "ManagerServicer": {
    "__init__": [
      "self",
      "replica_manager"
    ],
    "Encode": [
      "self",
      "request",
      "context"
    ],
    "HealthCheck": [
      "self",
      "request",
      "context"
    ],
    "GetReplicaInfo": [
      "self",
      "request",
      "context"
    ]
  },
  "parent_dir": [],
  "InferenceClient": {
    "__init__": [
      "self",
      "host",
      "port"
    ],
    "connect": [
      "self"
    ],
    "close": [
      "self"
    ],
    "embed": [
      "self",
      "prompts",
      "model_name",
      "request_id"
    ],
    "generate": [
      "self",
      "prompt",
      "temperature",
      "top_p",
      "top_k",
      "max_tokens",
      "stop",
      "stream",
      "request_id",
      "include_logprobs",
      "lora_name",
      "priority"
    ],
    "abort": [
      "self",
      "request_id"
    ],
    "get_replica_info": [
      "self"
    ],
    "health_check": [
      "self"
    ]
  },
  "main": [],
  "GRPC_GENERATED_VERSION": [],
  "GRPC_VERSION": [],
  "_version_not_supported": [],
  "InferenceServiceStub": {
    "__init__": [
      "self",
      "channel"
    ]
  },
  "InferenceServiceServicer": {
    "Encode": [
      "self",
      "request",
      "context"
    ],
    "Abort": [
      "self",
      "request",
      "context"
    ],
    "GetReplicaInfo": [
      "self",
      "request",
      "context"
    ],
    "HealthCheck": [
      "self",
      "request",
      "context"
    ]
  },
  "add_InferenceServiceServicer_to_server": [
    "servicer",
    "server"
  ],
  "InferenceService": {
    "Encode": [
      "request",
      "target",
      "options",
      "channel_credentials",
      "call_credentials",
      "insecure",
      "compression",
      "wait_for_ready",
      "timeout",
      "metadata"
    ],
    "Abort": [
      "request",
      "target",
      "options",
      "channel_credentials",
      "call_credentials",
      "insecure",
      "compression",
      "wait_for_ready",
      "timeout",
      "metadata"
    ],
    "GetReplicaInfo": [
      "request",
      "target",
      "options",
      "channel_credentials",
      "call_credentials",
      "insecure",
      "compression",
      "wait_for_ready",
      "timeout",
      "metadata"
    ],
    "HealthCheck": [
      "request",
      "target",
      "options",
      "channel_credentials",
      "call_credentials",
      "insecure",
      "compression",
      "wait_for_ready",
      "timeout",
      "metadata"
    ]
  },
  "_sym_db": [],
  "DESCRIPTOR": [],
  "_globals": [],
  "SwiftKVOpsBuilder": {
    "__init__": [
      "self"
    ],
    "absolute_name": [
      "self"
    ],
    "get_prefix": [
      "self"
    ],
    "sources": [
      "self"
    ],
    "include_paths": [
      "self"
    ]
  },
  "YELLOW": [],
  "END": [],
  "WARNING": [],
  "DEFAULT_TORCH_EXTENSION_PATH": [],
  "DEFAULT_COMPUTE_CAPABILITIES": [],
  "MissingCUDAException": {},
  "CUDAMismatchException": {},
  "installed_cuda_version": [
    "name"
  ],
  "get_default_compute_capabilities": [],
  "cuda_minor_mismatch_ok": [],
  "assert_no_cuda_mismatch": [
    "name"
  ],
  "OpBuilder": {
    "_loaded_ops": [],
    "__init__": [
      "self",
      "name"
    ],
    "absolute_name": [
      "self"
    ],
    "sources": [
      "self"
    ],
    "validate_torch_version": [
      "torch_info"
    ],
    "validate_torch_op_version": [
      "torch_info"
    ],
    "include_paths": [
      "self"
    ],
    "nvcc_args": [
      "self"
    ],
    "cxx_args": [
      "self"
    ],
    "is_compatible": [
      "self",
      "verbose"
    ],
    "extra_ldflags": [
      "self"
    ],
    "has_function": [
      "self",
      "funcname",
      "libraries",
      "library_dirs",
      "verbose"
    ],
    "strip_empty_entries": [
      "self",
      "args"
    ],
    "get_cuda_compile_flag": [
      "self"
    ],
    "command_exists": [
      "self",
      "cmd"
    ],
    "warning": [
      "self",
      "msg"
    ],
    "_src_path": [
      "self",
      "code_path"
    ],
    "builder": [
      "self"
    ],
    "load": [
      "self",
      "verbose"
    ],
    "jit_load": [
      "self",
      "verbose"
    ]
  },
  "CUDAOpBuilder": {
    "compute_capability_args": [
      "self",
      "cross_compile_archs"
    ],
    "filter_ccs": [
      "self",
      "ccs"
    ],
    "version_dependent_macros": [
      "self"
    ],
    "is_compatible": [
      "self",
      "verbose"
    ],
    "builder": [
      "self"
    ],
    "cxx_args": [
      "self"
    ],
    "nvcc_args": [
      "self"
    ],
    "libraries_args": [
      "self"
    ]
  },
  "suffix_decode": [
    "suffix_cache",
    "request_id",
    "prompt",
    "ground_truth_response",
    "max_spec_tokens",
    "max_spec_factor",
    "min_token_prob",
    "use_tree_spec",
    "use_cached_prompt"
  ],
  "sample_data": [
    "dataset",
    "train_dataset",
    "num_eval",
    "num_train",
    "seed"
  ],
  "process_task": [
    "dataset",
    "train_dataset",
    "task_id",
    "num_eval",
    "num_train",
    "seed",
    "max_depth",
    "max_spec_tokens",
    "max_spec_factor",
    "min_token_prob",
    "use_tree_spec",
    "use_cached_prompt",
    "evict_fraction",
    "evict_strategy",
    "max_cached_requests"
  ],
  "results_summary": [
    "df",
    "config_cols"
  ],
  "read_data_file": [
    "path",
    "prompt_column",
    "response_column",
    "format"
  ],
  "tokenize_data": [
    "dataset",
    "tokenizer_name"
  ],
  "ensure_tokenized": [
    "dataset"
  ],
  "get_data": [
    "args"
  ],
  "bool_arg": [
    "v"
  ],
  "get_parser": [],
  "SuffixDecodingDraft": {
    "from_native": [
      "draft"
    ]
  },
  "SuffixDecodingCache": {
    "__init__": [
      "self",
      "max_tree_depth",
      "max_cached_requests"
    ],
    "max_tree_depth": [
      "self"
    ],
    "max_cached_requests": [
      "self"
    ],
    "active_requests": [
      "self"
    ],
    "cached_requests": [
      "self"
    ],
    "start_request": [
      "self",
      "req_id",
      "prompt_token_ids"
    ],
    "stop_request": [
      "self",
      "req_id"
    ],
    "add_active_response": [
      "self",
      "req_id",
      "token_ids"
    ],
    "evict_cached_response": [
      "self",
      "req_id"
    ],
    "speculate": [
      "self",
      "req_id",
      "context",
      "max_spec_tokens",
      "max_spec_factor",
      "max_spec_offset",
      "min_token_prob",
      "use_tree_spec"
    ],
    "_generate_seq_id": [
      "self",
      "req_id"
    ],
    "_maybe_evict_requests": [
      "self",
      "new_seq_id"
    ],
    "_validate_ndarray": [
      "self",
      "arr"
    ]
  },
  "SpecDecodingStatsPatch": {
    "_orig_observe_draft": [],
    "__init__": [
      "self",
      "num_spec_tokens",
      "num_accepted_tokens_per_pos"
    ],
    "observe_draft": [
      "self",
      "num_draft_tokens",
      "num_accepted_tokens"
    ]
  },
  "SpecDecodingLoggingPatch": {
    "_orig_log": [],
    "log": [
      "self",
      "log_fn"
    ]
  },
  "arctic_inference_plugin": [],
  "SP_TP_MODE": [],
  "set_shift_parallel_mode": [
    "mode"
  ],
  "is_shift_parallel_mode": [],
  "GPUModelRunnerPatch": {
    "_orig_initialize_kv_cache": [],
    "_orig_capture_cudagraphs": [],
    "_orig_prepare_inputs": [],
    "_orig_profile_run": [],
    "_orig_load_model": [],
    "_orig_propose_draft_token_ids": [],
    "_orig_dummy_run": [],
    "_orig_init": [],
    "__init__": [
      "self",
      "vllm_config",
      "device"
    ],
    "profile_run": [
      "self"
    ],
    "_prepare_inputs": [
      "self"
    ],
    "monkeypatch_forward": [
      "self"
    ],
    "_dummy_run": [
      "self",
      "num_tokens",
      "cudagraph_runtime_mode",
      "force_attention",
      "uniform_decode",
      "allow_microbatching",
      "skip_eplb",
      "is_profile",
      "create_mixed_batch",
      "remove_lora"
    ],
    "execute_model": [
      "self",
      "scheduler_output",
      "intermediate_tensors"
    ],
    "propose_draft_token_ids": [
      "self",
      "scheduler_output",
      "sampled_token_ids",
      "original_sampled_token_ids",
      "sampling_metadata",
      "hidden_states",
      "sample_hidden_states",
      "aux_hidden_states",
      "spec_decode_metadata",
      "common_attn_metadata"
    ],
    "propose_arctic_draft_token_ids": [
      "self",
      "scheduler_output",
      "sampled_token_ids",
      "previous_hidden_states"
    ],
    "_update_suffix_cache": [
      "self",
      "sampled_token_ids"
    ],
    "propose_suffix_draft_token_ids": [
      "self",
      "sampled_token_ids"
    ],
    "load_model": [
      "self",
      "eep_scale_up"
    ],
    "_capture_cudagraphs": [
      "self",
      "compilation_cases",
      "cudagraph_runtime_mode",
      "uniform_decode"
    ],
    "initialize_kv_cache": [
      "self",
      "kv_cache_config"
    ]
  },
  "EngineCoreProcPatch": {
    "_orig_run_engine_core": [],
    "run_engine_core": []
  },
  "WorkerBasePatch": {
    "_orig_init": [],
    "__init__": [
      "self"
    ]
  },
  "apply_arctic_patches": [],
  "ArcticParallelConfig": {
    "__post_init__": [
      "self"
    ],
    "world_size": [
      "self",
      "value"
    ]
  },
  "ArcticSpeculativeConfig": {},
  "ParallelConfigPatch": {
    "__new__": [
      "cls"
    ]
  },
  "SpeculativeConfigPatch": {
    "_orig_post_init": [],
    "__new__": [
      "cls"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "VllmConfigPatch": {
    "_orig_str": [],
    "__str__": [
      "self"
    ]
  },
  "MLPSpeculatorConfigPatch": {
    "_orig_init": [],
    "__init__": [
      "self"
    ]
  },
  "XgrammarBackendPatch": {
    "_orig_post_init": [],
    "__post_init__": [
      "self"
    ]
  },
  "ArcticArgs": {},
  "ArcticEngineArgs": {},
  "ArcticAsyncEngineArgs": {},
  "EngineArgsPatch": {
    "_orig_post_init": [],
    "_orig_add_cli_args": [],
    "_orig_from_cli_args": [],
    "_orig_create_engine_config": [],
    "_orig_is_v1_supported_oracle": [],
    "__new__": [
      "cls"
    ],
    "__post_init__": [
      "self"
    ],
    "add_cli_args": [
      "parser"
    ],
    "from_cli_args": [
      "cls",
      "args"
    ],
    "create_engine_config": [
      "self"
    ],
    "_is_v1_supported_oracle": [
      "self"
    ]
  },
  "AsyncEngineArgsPatch": {
    "__new__": [
      "cls"
    ]
  },
  "apply_shift_parallel_patches": [],
  "UlyssesModelConfig": {
    "_orig_get_num_kv_heads": [],
    "_orig_get_num_attention_heads": [],
    "get_num_kv_heads": [
      "self",
      "parallel_config"
    ],
    "get_num_attention_heads": [
      "self",
      "parallel_config"
    ],
    "get_layers_start_end_indices": [
      "self",
      "parallel_config"
    ]
  },
  "UlyssesParallelState": {
    "_SP": [],
    "_SP_TP": [],
    "initialize_model_parallel": [
      "tensor_model_parallel_size",
      "pipeline_model_parallel_size",
      "decode_context_model_parallel_size",
      "backend"
    ],
    "graph_capture": [
      "device"
    ]
  },
  "UlyssesWorkerProc": {
    "destroy_model_parallel": [
      "self"
    ],
    "shutdown": [
      "self"
    ]
  },
  "UlyssesMultiprocExecutor": {
    "_init_executor": [
      "self"
    ]
  },
  "UlyssesAttention": {
    "_orig_init": [],
    "_orig_forward": [],
    "__init__": [
      "self",
      "num_heads"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value"
    ]
  },
  "UlyssesCudagraphDispatcher": {
    "_orig_initialize_cudagraph_keys": [],
    "initialize_cudagraph_keys": [
      "self",
      "cudagraph_mode",
      "uniform_decode_query_len"
    ]
  },
  "DEFAULT_VOCAB_PADDING_SIZE": [],
  "SpeculatorTPInit": {
    "__init__": [
      "self"
    ],
    "init_tensor_parallelism": [
      "self"
    ]
  },
  "UnquantizedEmbeddingMethod": {
    "create_weights": [
      "self",
      "layer",
      "input_size_per_partition",
      "output_partition_sizes",
      "input_size",
      "output_size",
      "params_dtype"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "embedding": [
      "self",
      "layer",
      "input_"
    ]
  },
  "pad_vocab_size": [
    "vocab_size",
    "pad_to"
  ],
  "vocab_range_from_per_partition_vocab_size": [
    "per_partition_vocab_size",
    "rank",
    "offset"
  ],
  "vocab_range_from_global_vocab_size": [
    "global_vocab_size",
    "rank",
    "world_size",
    "offset"
  ],
  "VocabParallelEmbeddingShardIndices": {
    "num_org_elements": [
      "self"
    ],
    "num_added_elements": [
      "self"
    ],
    "num_org_elements_padded": [
      "self"
    ],
    "num_added_elements_padded": [
      "self"
    ],
    "num_org_vocab_padding": [
      "self"
    ],
    "num_added_vocab_padding": [
      "self"
    ],
    "num_elements_padded": [
      "self"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "get_masked_input_and_mask": [
    "input_",
    "org_vocab_start_index",
    "org_vocab_end_index",
    "num_org_vocab_padding",
    "added_vocab_start_index",
    "added_vocab_end_index"
  ],
  "VocabParallelEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "params_dtype",
      "org_num_embeddings",
      "padding_size",
      "quant_config",
      "prefix",
      "skip_quantization"
    ],
    "_get_indices": [
      "cls",
      "vocab_size_padded",
      "org_vocab_size_padded",
      "vocab_size",
      "org_vocab_size",
      "tp_rank",
      "tp_size"
    ],
    "get_sharded_to_full_mapping": [
      "self"
    ],
    "weight_loader": [
      "self",
      "param",
      "loaded_weight"
    ],
    "forward": [
      "self",
      "input_"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "ParallelLMHead": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "bias",
      "params_dtype",
      "org_num_embeddings",
      "padding_size",
      "quant_config",
      "prefix",
      "skip_quantization"
    ],
    "tie_weights": [
      "self",
      "embed_tokens"
    ],
    "forward": [
      "self",
      "input_"
    ]
  },
  "LogitsProcessorOpt": {
    "__init__": [
      "self",
      "vocab_size",
      "org_vocab_size",
      "scale",
      "logits_as_input",
      "soft_cap",
      "skip_last_gather"
    ],
    "_get_logits_and_post_processing": [
      "self",
      "lm_head",
      "hidden_states",
      "embedding_bias"
    ],
    "forward": [
      "self",
      "lm_head",
      "hidden_states"
    ],
    "_get_logits": [
      "self",
      "hidden_states",
      "lm_head",
      "embedding_bias"
    ]
  },
  "OriginalFp8LinearMethod": {
    "__init__": [
      "self",
      "quant_config"
    ],
    "create_weights": [
      "self",
      "layer",
      "input_size_per_partition",
      "output_partition_sizes",
      "input_size",
      "output_size",
      "params_dtype"
    ],
    "_maybe_pad_weight": [
      "self",
      "weight"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "bias"
    ]
  },
  "Fp8LinearMethodEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "embedding": [
      "self",
      "layer",
      "input_"
    ]
  },
  "Fp8ConfigWithEmbedding": {
    "get_quant_method_patch": [
      "self",
      "layer",
      "prefix"
    ]
  },
  "SQRT2": [],
  "USE_CUSTOM_OP": [],
  "padding_size": [
    "size"
  ],
  "graph_capture": [
    "device"
  ],
  "MLPSpeculatorLayerNorm": {
    "__init__": [
      "self",
      "normalized_shape",
      "eps",
      "elementwise_scale_and_shift"
    ],
    "forward_fallback": [
      "self",
      "x"
    ],
    "forward_opt": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_generate_cg_key": [
    "padding_size",
    "head_index"
  ],
  "ArcticMLPSpeculator": {
    "__init__": [
      "self"
    ],
    "_prepare_cuda_graph_ios": [
      "self",
      "size",
      "last_tokens",
      "previous_hidden_states"
    ],
    "generate_states": [
      "self",
      "last_tokens",
      "previous_hidden_states",
      "head_index"
    ],
    "generate_token_ids": [
      "self",
      "batch_size",
      "num_predict_tokens",
      "last_tokens",
      "previous_hidden_states",
      "next_tokens_tensors"
    ],
    "generate_proposals": [
      "self",
      "input_ids",
      "previous_hidden_states",
      "num_predict_tokens"
    ],
    "maybe_load_weight": [
      "self",
      "param",
      "loaded_weight"
    ],
    "load_weights": [
      "self",
      "weights"
    ]
  },
  "ArcticLSTMSpeculator": {
    "__init__": [
      "self"
    ],
    "_prepare_cuda_graph_ios": [
      "self",
      "size",
      "last_tokens",
      "previous_hidden_states",
      "hidden_state_buffers",
      "cell_states",
      "use_lstm"
    ],
    "generate_states": [
      "self",
      "last_tokens",
      "previous_hidden_states",
      "head_index",
      "cell_states"
    ],
    "generate_token_ids": [
      "self",
      "batch_size",
      "num_predict_tokens",
      "last_tokens",
      "previous_hidden_states",
      "next_tokens_tensors",
      "cell_states"
    ],
    "generate_proposals": [
      "self",
      "input_ids",
      "previous_hidden_states",
      "num_predict_tokens"
    ],
    "maybe_load_weight": [
      "self",
      "param",
      "loaded_weight"
    ],
    "load_weights": [
      "self",
      "weights"
    ]
  },
  "ArcticProposer": {
    "__init__": [
      "self",
      "vllm_config"
    ],
    "load_model": [
      "self",
      "model"
    ],
    "prepare_hidden_states": [
      "self",
      "sample_hidden_states",
      "sampled_token_ids",
      "spec_decode_metadata"
    ],
    "propose": [
      "self",
      "context_token_ids",
      "previous_hidden_states",
      "num_predict_tokens"
    ]
  },
  "SuffixProposer": {
    "__init__": [
      "self"
    ],
    "load_model": [
      "self",
      "model"
    ]
  },
  "get_attn_metadata_for_swiftkv": [],
  "LlamaSwiftKVAttention": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "num_heads",
      "num_kv_heads",
      "rope_theta",
      "rope_scaling",
      "max_position_embeddings",
      "quant_config",
      "bias",
      "bias_o_proj",
      "cache_config",
      "prefix",
      "attn_type"
    ],
    "forward": [
      "self",
      "positions",
      "hidden_states",
      "k",
      "v"
    ]
  },
  "LlamaSwiftKVDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "cache_config",
      "quant_config",
      "prefix"
    ],
    "forward": [
      "self",
      "positions",
      "hidden_states",
      "k_states",
      "v_states",
      "residual"
    ]
  },
  "LlamaSwiftKVPrefillRunner": {
    "__init__": [
      "self"
    ],
    "model": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "positions"
    ]
  },
  "LlamaSwiftKVDecodeRunner": {
    "__init__": [
      "self"
    ],
    "model": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "residual",
      "positions",
      "k_states",
      "v_states"
    ]
  },
  "LlamaSwiftKVModel": {
    "__init__": [
      "self"
    ],
    "get_input_embeddings": [
      "self",
      "input_ids"
    ],
    "_init_prefill_runner": [
      "self",
      "vllm_config"
    ],
    "_init_decode_runner": [
      "self",
      "vllm_config"
    ],
    "_fix_flash_attention_metadata": [
      "self",
      "attn_metadata",
      "logits_indices",
      "num_surviving_tokens"
    ],
    "_fix_flashinfer_metadata": [
      "self",
      "attn_metadata",
      "logits_indices",
      "num_surviving_tokens"
    ],
    "swiftkv_select": [
      "self",
      "hidden_states",
      "residual",
      "positions",
      "k_states",
      "v_states"
    ],
    "forward": [
      "self",
      "input_ids",
      "positions"
    ],
    "load_weights": [
      "self",
      "weights"
    ]
  },
  "LlamaSwiftKVForCausalLM": {
    "packed_modules_mapping": [],
    "__init__": [
      "self"
    ],
    "_init_model": [
      "self",
      "vllm_config",
      "prefix"
    ],
    "get_input_embeddings": [
      "self",
      "input_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "positions",
      "intermediate_tensors",
      "inputs_embeds"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ],
    "load_weights": [
      "self",
      "weights"
    ]
  },
  "check_health": [
    "port",
    "retry_interval"
  ],
  "run_vllm_server": [
    "args"
  ],
  "run_openai_server": [
    "config"
  ],
  "register_signal_handlers": [],
  "_fix_fracs": [
    "string"
  ],
  "_fix_a_slash_b": [
    "string"
  ],
  "_fix_sqrt": [
    "string"
  ],
  "convert_word_number": [
    "text"
  ],
  "unit_texts": [],
  "strip_string": [
    "string",
    "skip_unit"
  ],
  "extract_answer": [
    "pred_str",
    "data_name",
    "use_last_number"
  ],
  "extract_first_boxed_answer": [
    "pred_str",
    "data_name"
  ],
  "extract_boxed_answer": [
    "pred_str",
    "data_name"
  ],
  "choice_answer_clean": [
    "pred"
  ],
  "parse_digits": [
    "num"
  ],
  "is_digit": [
    "num"
  ],
  "str_to_pmatrix": [
    "input_str"
  ],
  "math_equal": [
    "prediction",
    "reference",
    "include_percentage",
    "is_close",
    "timeout"
  ],
  "count_not_empty": [
    "answers"
  ],
  "equal_group": [
    "answers"
  ],
  "math_equal_process": [
    "param"
  ],
  "numeric_equal": [
    "prediction",
    "reference"
  ],
  "symbolic_equal": [
    "a",
    "b"
  ],
  "symbolic_equal_process": [
    "a",
    "b",
    "output_queue"
  ],
  "call_with_timeout": [
    "func"
  ],
  "uncertain_words": [],
  "sys": [],
  "default_probing_suffix": [],
  "format_prompt_for_completions": [
    "prompt",
    "generated"
  ],
  "formalize_final_response": [
    "generated_text",
    "answer"
  ],
  "obtain_answer": [
    "s"
  ],
  "openai_chat_completion_stream": [
    "client",
    "model",
    "prompt",
    "temperature",
    "max_tokens",
    "dynasor_saving_effort",
    "probing_suffix"
  ],
  "listen_for_disconnect": [
    "request"
  ],
  "with_cancellation": [
    "handler_func"
  ],
  "init_logger": [],
  "ProxyConfig": {},
  "make_parser": [],
  "parse_args": [
    "args_"
  ],
  "app": [],
  "set_config": [
    "c"
  ],
  "execute_single_probe": [
    "client",
    "model_id",
    "prompt",
    "generated",
    "probe_in_progress_event",
    "max_tokens"
  ],
  "handle_chat_completion_request": [
    "request",
    "path"
  ],
  "chat_completions_endpoint": [
    "request"
  ],
  "completions_endpoint": [
    "request"
  ],
  "proxy_request": [
    "request",
    "path"
  ],
  "proxy": [
    "request",
    "path"
  ],
  "start_server": [
    "config"
  ],
  "entropy": [
    "Plist"
  ],
  "norm": [
    "Olist"
  ],
  "count": [
    "Olist"
  ],
  "item_entropy": [
    "answers"
  ],
  "majority_voting": [
    "answers"
  ],
  "is_certain_answer": [
    "probe_response_text",
    "uncertain_words"
  ],
  "has_value": [
    "x"
  ],
  "should_early_exit": [
    "answers",
    "probe_response_text",
    "uncertain_words",
    "continue_certain_bar",
    "is_certains"
  ]
}