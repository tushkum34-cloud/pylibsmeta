{
  "Sentence": {},
  "POSTag": [],
  "SenseId": [],
  "PretokenizedToken": {},
  "PretokenizedTokenList": [],
  "SimilarMorpheme": {
    "form_tag": [
      "self"
    ],
    "form_tag_sense": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "SimilarContext": {
    "repr_form": [
      "self"
    ],
    "repr_analyses": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "NgramCandidate": {},
  "NgramExtractor": {
    "__init__": [
      "self",
      "kiwi",
      "gather_lm_score"
    ],
    "add": [
      "self",
      "text"
    ],
    "extract": [
      "self",
      "max_candidates",
      "min_cnt",
      "max_length",
      "min_score",
      "num_workers"
    ]
  },
  "TypoDefinition": {
    "__post_init__": [
      "self"
    ]
  },
  "_c_to_onset": [],
  "_c_to_coda": [],
  "_convert_consonant": [
    "s"
  ],
  "_convert_dialect": [
    "dialect"
  ],
  "TypoTransformer": {
    "__init__": [
      "self",
      "defs",
      "continual_typo_cost",
      "lengthening_typo_cost"
    ],
    "generate": [
      "self",
      "text",
      "cost_threshold"
    ],
    "copy": [
      "self"
    ],
    "update": [
      "self",
      "other"
    ],
    "scale_cost": [
      "self",
      "scale"
    ],
    "__or__": [
      "self",
      "other"
    ],
    "__ior__": [
      "self",
      "other"
    ],
    "__mul__": [
      "self",
      "scale"
    ],
    "__imul__": [
      "self",
      "scale"
    ],
    "defs": [
      "self"
    ],
    "continual_typo_cost": [
      "self"
    ],
    "lengthening_typo_cost": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "HSDataset": {},
  "MorphemeSet": {
    "__init__": [
      "self",
      "kiwi",
      "morphs"
    ],
    "__repr__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "_normalize": [
      "self",
      "tagged_form"
    ]
  },
  "KiwiConfig": {},
  "Kiwi": {
    "__init__": [
      "self",
      "num_workers",
      "model_path",
      "integrate_allomorph",
      "load_default_dict",
      "load_typo_dict",
      "load_multi_dict",
      "model_type",
      "typos",
      "typo_cost_threshold",
      "enabled_dialects"
    ],
    "__repr__": [
      "self"
    ],
    "add_user_word": [
      "self",
      "word",
      "tag",
      "score",
      "orig_word",
      "user_value"
    ],
    "add_pre_analyzed_word": [
      "self",
      "form",
      "analyzed",
      "score",
      "dialect"
    ],
    "add_re_word": [
      "self",
      "pattern",
      "pretokenized",
      "user_value"
    ],
    "clear_re_words": [
      "self"
    ],
    "add_rule": [
      "self",
      "tag",
      "replacer",
      "score",
      "user_value"
    ],
    "add_re_rule": [
      "self",
      "tag",
      "pattern",
      "repl",
      "score",
      "user_value"
    ],
    "load_user_dictionary": [
      "self",
      "dict_path"
    ],
    "extract_words": [
      "self",
      "texts",
      "min_cnt",
      "max_word_len",
      "min_score",
      "pos_score",
      "lm_filter"
    ],
    "extract_add_words": [
      "self",
      "texts",
      "min_cnt",
      "max_word_len",
      "min_score",
      "pos_score",
      "lm_filter"
    ],
    "_make_pretokenized_spans": [
      "self",
      "override_pretokenized",
      "text"
    ],
    "analyze": [
      "self",
      "text",
      "top_n",
      "match_options",
      "normalize_coda",
      "z_coda",
      "split_complex",
      "compatible_jamo",
      "saisiot",
      "blocklist",
      "open_ending",
      "allowed_dialects",
      "dialect_cost",
      "pretokenized",
      "override_config"
    ],
    "morpheme": [
      "self",
      "idx"
    ],
    "global_config": [
      "self"
    ],
    "cutoff_threshold": [
      "self",
      "v"
    ],
    "integrate_allomorph": [
      "self",
      "v"
    ],
    "space_penalty": [
      "self",
      "v"
    ],
    "space_tolerance": [
      "self",
      "v"
    ],
    "max_unk_form_size": [
      "self",
      "v"
    ],
    "typo_cost_weight": [
      "self",
      "v"
    ],
    "num_workers": [
      "self"
    ],
    "model_type": [
      "self"
    ],
    "typo_cost_threshold": [
      "self",
      "v"
    ],
    "_tokenize": [
      "self",
      "text",
      "match_options",
      "normalize_coda",
      "z_coda",
      "split_complex",
      "compatible_jamo",
      "saisiot",
      "split_sents",
      "stopwords",
      "echo",
      "blocklist",
      "open_ending",
      "allowed_dialects",
      "dialect_cost",
      "pretokenized",
      "override_config"
    ],
    "tokenize": [
      "self",
      "text",
      "match_options",
      "normalize_coda",
      "z_coda",
      "split_complex",
      "compatible_jamo",
      "saisiot",
      "split_sents",
      "stopwords",
      "echo",
      "blocklist",
      "open_ending",
      "allowed_dialects",
      "dialect_cost",
      "pretokenized",
      "override_config"
    ],
    "split_into_sents": [
      "self",
      "text",
      "match_options",
      "normalize_coda",
      "z_coda",
      "split_complex",
      "compatible_jamo",
      "saisiot",
      "stopwords",
      "blocklist",
      "allowed_dialects",
      "dialect_cost",
      "override_config",
      "return_tokens",
      "return_sub_sents"
    ],
    "glue": [
      "self",
      "text_chunks",
      "insert_new_lines",
      "return_space_insertions"
    ],
    "space": [
      "self",
      "text",
      "reset_whitespace"
    ],
    "join": [
      "self",
      "morphs",
      "lm_search",
      "return_positions"
    ],
    "evaluate": [
      "self",
      "sequences",
      "prefix",
      "suffix"
    ],
    "predict_next": [
      "self",
      "prefix"
    ],
    "template": [
      "self",
      "format_str",
      "cache"
    ],
    "list_senses": [
      "self",
      "form"
    ],
    "list_all_scripts": [
      "self"
    ],
    "_convert_input_to_token_list": [
      "self",
      "inp",
      "name"
    ],
    "most_similar_morphemes": [
      "self",
      "target",
      "top_n"
    ],
    "most_similar_contexts": [
      "self",
      "target",
      "context_id",
      "top_n"
    ],
    "predict_next_morpheme": [
      "self",
      "prefix",
      "bg_prefix",
      "bg_weight",
      "top_n"
    ],
    "morpheme_similarity": [
      "self",
      "morpheme1",
      "morpheme2"
    ],
    "context_similarity": [
      "self",
      "context1",
      "context2"
    ],
    "convert_hsdata": [
      "self",
      "input_path",
      "output_path",
      "morpheme_def_path",
      "morpheme_def_min_cnt",
      "generate_oov_dict",
      "transform"
    ],
    "make_hsdataset": [
      "self",
      "inputs",
      "batch_size",
      "causal_context_size",
      "window_size",
      "num_workers",
      "dropout",
      "dropout_on_history",
      "noun_augmenting_prob",
      "emoji_augmenting_prob",
      "sb_augmenting_prob",
      "token_filter",
      "window_filter",
      "split_ratio",
      "separate_default_morpheme",
      "morpheme_def_path",
      "morpheme_def_min_cnt",
      "contextual_mapper",
      "transform",
      "seed",
      "generate_unlikelihoods"
    ]
  },
  "extract_substrings": [
    "text",
    "min_cnt",
    "min_length",
    "max_length",
    "longest_only",
    "stop_chr"
  ],
  "MultipleFileLoader": {
    "_count_lines": [
      "self",
      "path",
      "chunk_size"
    ],
    "__init__": [
      "self",
      "pathes"
    ],
    "__iter__": [
      "self"
    ],
    "__len__": [
      "self"
    ]
  },
  "main": [
    "args"
  ],
  "_group_by_two": [
    "iterator"
  ],
  "KiwiTokenizer": {
    "vocab_files_names": [],
    "__init__": [
      "self",
      "tokenizer_file"
    ],
    "unk_token": [
      "self",
      "s"
    ],
    "cls_token": [
      "self",
      "s"
    ],
    "sep_token": [
      "self",
      "s"
    ],
    "pad_token": [
      "self",
      "s"
    ],
    "mask_token": [
      "self",
      "s"
    ],
    "bos_token": [
      "self",
      "s"
    ],
    "eos_token": [
      "self",
      "s"
    ],
    "unk_token_id": [
      "self"
    ],
    "cls_token_id": [
      "self"
    ],
    "sep_token_id": [
      "self"
    ],
    "pad_token_id": [
      "self"
    ],
    "mask_token_id": [
      "self"
    ],
    "bos_token_id": [
      "self"
    ],
    "eos_token_id": [
      "self"
    ],
    "_batch_encode_plus": [
      "self",
      "batch_text_or_text_pairs",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_encode_plus": [
      "self",
      "text",
      "text_pair",
      "add_special_tokens",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "stride",
      "is_split_into_words",
      "pad_to_multiple_of",
      "return_tensors",
      "return_token_type_ids",
      "return_attention_mask",
      "return_overflowing_tokens",
      "return_special_tokens_mask",
      "return_offsets_mapping",
      "return_length",
      "verbose"
    ],
    "_make_encoded": [
      "self",
      "batch_text_or_text_pairs",
      "add_special_tokens",
      "return_token_type_ids",
      "return_attention_mask",
      "return_offsets_mapping",
      "return_as_list",
      "padding_strategy",
      "truncation_strategy",
      "max_length",
      "pad_to_multiple_of"
    ],
    "_decode": [
      "self",
      "token_ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "get_added_vocab": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "vocab": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "is_fast": [
      "self"
    ],
    "convert_tokens_to_ids": [
      "self",
      "tokens"
    ],
    "num_special_tokens_to_add": [
      "self",
      "pair"
    ],
    "_add_tokens": [
      "self",
      "new_tokens",
      "special_tokens"
    ],
    "tokenize": [
      "self",
      "text",
      "pair",
      "add_special_tokens"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "_save_pretrained": [
      "self",
      "save_directory",
      "file_names",
      "legacy_format",
      "filename_prefix"
    ],
    "added_tokens_decoder": [
      "self"
    ]
  },
  "_tag_set": [],
  "Stopwords": {
    "_load_stopwords": [
      "self",
      "filename"
    ],
    "_save_stopwords": [
      "self",
      "filename",
      "stopwords",
      "stoptags"
    ],
    "__init__": [
      "self",
      "filename"
    ],
    "save": [
      "self",
      "filename"
    ],
    "__contains__": [
      "self",
      "word"
    ],
    "_tag_exists": [
      "self",
      "tag"
    ],
    "_token_exists": [
      "self",
      "token"
    ],
    "_is_not_stopword": [
      "self",
      "token"
    ],
    "add": [
      "self",
      "tokens"
    ],
    "remove": [
      "self",
      "tokens"
    ],
    "filter": [
      "self",
      "tokens"
    ]
  },
  "KNLangModel": {
    "__init__": [
      "self"
    ],
    "load": [
      "cls",
      "path",
      "num_workers"
    ],
    "from_arrays": [
      "cls",
      "token_arrays",
      "ngram_size",
      "min_cf",
      "bos_token_id",
      "eos_token_id",
      "unk_token_id",
      "num_workers",
      "token_clusters"
    ],
    "ngram_size": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "num_nodes": [
      "self"
    ],
    "num_workers": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "save": [
      "self",
      "path"
    ],
    "next_tokens": [
      "self",
      "token_ids",
      "top_n",
      "deferred"
    ],
    "evaluate": [
      "self",
      "token_ids",
      "deferred"
    ]
  },
  "SwTokenizerConfig": {},
  "TrainerCallback": {
    "begin_tokenization": [
      "self",
      "num_processed_lines"
    ],
    "proc_tokenization": [
      "self",
      "num_processed_lines"
    ],
    "end_tokenization": [
      "self",
      "num_processed_lines"
    ],
    "begin_reduction": [
      "self",
      "n_tkn",
      "iteration",
      "cur_vocab_size",
      "unigram_loss"
    ],
    "proc_reduction": [
      "self",
      "n_tkn",
      "iteration",
      "cur_vocab_size",
      "unigram_loss"
    ],
    "end_reduction": [
      "self",
      "n_tkn",
      "iteration",
      "cur_vocab_size",
      "unigram_loss"
    ]
  },
  "_ProgressShower": {
    "__init__": [
      "self",
      "file",
      "total",
      "iterations"
    ],
    "__del__": [
      "self"
    ],
    "begin_tokenization": [
      "self",
      "num_processed_lines"
    ],
    "proc_tokenization": [
      "self",
      "num_processed_lines"
    ],
    "end_tokenization": [
      "self",
      "num_processed_lines"
    ],
    "begin_reduction": [
      "self",
      "n_tkn",
      "iteration",
      "cur_vocab_size",
      "unigram_loss"
    ],
    "proc_reduction": [
      "self",
      "n_tkn",
      "iteration",
      "cur_vocab_size",
      "unigram_loss"
    ],
    "end_reduction": [
      "self",
      "n_tkn",
      "iteration",
      "cur_vocab_size",
      "unigram_loss"
    ]
  },
  "SPECIAL_TOKEN_NAMES": [],
  "SwTokenizer": {
    "__init__": [
      "self",
      "path",
      "kiwi",
      "num_workers"
    ],
    "encode": [
      "self",
      "text",
      "return_offsets"
    ],
    "encode_from_morphs": [
      "self",
      "morphs",
      "return_offsets"
    ],
    "tokenize_encode": [
      "self",
      "text",
      "return_offsets"
    ],
    "decode": [
      "self",
      "ids",
      "ignore_errors"
    ],
    "save": [
      "self",
      "path"
    ],
    "vocab": [
      "self"
    ],
    "id2vocab": [
      "self"
    ],
    "config": [
      "self"
    ],
    "kiwi": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "unk_token": [
      "self"
    ],
    "pad_token": [
      "self"
    ],
    "mask_token": [
      "self"
    ],
    "cls_token": [
      "self"
    ],
    "sep_token": [
      "self"
    ],
    "bos_token": [
      "self"
    ],
    "eos_token": [
      "self"
    ],
    "unk_token_id": [
      "self"
    ],
    "pad_token_id": [
      "self"
    ],
    "mask_token_id": [
      "self"
    ],
    "cls_token_id": [
      "self"
    ],
    "sep_token_id": [
      "self"
    ],
    "bos_token_id": [
      "self"
    ],
    "eos_token_id": [
      "self"
    ],
    "all_special_tokens": [
      "self"
    ],
    "all_special_ids": [
      "self"
    ],
    "train": [
      "save_path",
      "texts",
      "config",
      "vocab_size",
      "chr_coverage",
      "prefix_min_cnt",
      "prefix_max_length",
      "strict_reduction",
      "remove_repetitive",
      "prevent_mixed_digit_tokens",
      "iterations",
      "reduction_ratio",
      "kiwi",
      "num_workers",
      "show_progress",
      "total_texts",
      "callback"
    ]
  },
  "_basic_defs": [],
  "_continual_defs": [],
  "basic_typos": [],
  "continual_typos": [],
  "basic_typos_with_continual": [],
  "lengthening_typos": [],
  "basic_typos_with_continual_and_lengthening": [],
  "format_pattern": [],
  "_to_kiwi_tokens": [
    "token"
  ],
  "Template": {
    "__init__": [
      "self",
      "kiwi",
      "format_str"
    ],
    "format": [
      "self"
    ]
  },
  "Match": {
    "URL": [],
    "EMAIL": [],
    "HASHTAG": [],
    "MENTION": [],
    "SERIAL": [],
    "EMOJI": [],
    "ALL": [],
    "NORMALIZING_CODA": [],
    "JOIN_NOUN_PREFIX": [],
    "JOIN_NOUN_SUFFIX": [],
    "JOIN_VERB_SUFFIX": [],
    "JOIN_ADJ_SUFFIX": [],
    "JOIN_ADV_SUFFIX": [],
    "SPLIT_COMPLEX": [],
    "Z_CODA": [],
    "COMPATIBLE_JAMO": [],
    "SPLIT_SAISIOT": [],
    "MERGE_SAISIOT": [],
    "JOIN_V_SUFFIX": [],
    "JOIN_AFFIX": []
  },
  "Dialect": {
    "STANDARD": [],
    "\ud45c\uc900": [],
    "GYEONGGI": [],
    "\uacbd\uae30": [],
    "CHUNGCHEONG": [],
    "\ucda9\uccad": [],
    "GANGWON": [],
    "\uac15\uc6d0": [],
    "GYEONGSANG": [],
    "\uacbd\uc0c1": [],
    "JEOLLA": [],
    "\uc804\ub77c": [],
    "JEJU": [],
    "\uc81c\uc8fc": [],
    "HWANGHAE": [],
    "\ud669\ud574": [],
    "HAMGYEONG": [],
    "\ud568\uacbd": [],
    "PYEONGAN": [],
    "\ud3c9\uc548": [],
    "ARCHAIC": [],
    "\uc61b\ub9d0": [],
    "ALL": []
  },
  "tokenize": [
    "args",
    "kiwi"
  ],
  "space": [
    "args",
    "kiwi"
  ],
  "join": [
    "args",
    "kiwi"
  ],
  "split": [
    "args",
    "kiwi"
  ],
  "__version__": []
}