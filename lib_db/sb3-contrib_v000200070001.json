{
  "version_file": [],
  "__all__": [],
  "quantile_huber_loss": [
    "current_quantiles",
    "target_quantiles",
    "cum_prob",
    "sum_over_quantiles"
  ],
  "conjugate_gradient_solver": [
    "matrix_vector_dot_fn",
    "b",
    "max_iter",
    "residual_tol"
  ],
  "flat_grad": [
    "output",
    "parameters",
    "create_graph",
    "retain_graph"
  ],
  "BatchRenorm": {
    "__init__": [
      "self",
      "num_features",
      "eps",
      "momentum",
      "affine",
      "warmup_steps"
    ],
    "_check_input_dim": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "BatchRenorm1d": {
    "_check_input_dim": [
      "self",
      "x"
    ]
  },
  "_worker": [
    "remote",
    "parent_remote",
    "worker_env_wrapper",
    "train_policy_wrapper",
    "n_eval_episodes"
  ],
  "AsyncEval": {
    "__init__": [
      "self",
      "envs_fn",
      "train_policy",
      "start_method",
      "n_eval_episodes"
    ],
    "send_jobs": [
      "self",
      "candidate_weights",
      "pop_size"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "set_options": [
      "self",
      "options"
    ],
    "get_results": [
      "self"
    ],
    "get_obs_rms": [
      "self"
    ],
    "sync_obs_rms": [
      "self",
      "obs_rms"
    ],
    "close": [
      "self"
    ]
  },
  "TimeFeatureObs": [],
  "TimeFeatureWrapper": {
    "__init__": [
      "self",
      "env",
      "max_steps",
      "test_mode"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "_get_obs": [
      "self",
      "obs"
    ]
  },
  "ActionMasker": {
    "__init__": [
      "self",
      "env",
      "action_mask_fn"
    ],
    "action_masks": [
      "self"
    ]
  },
  "RecurrentActorCriticPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "squash_output",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs",
      "lstm_hidden_size",
      "n_lstm_layers",
      "shared_lstm",
      "enable_critic_lstm",
      "lstm_kwargs"
    ],
    "_build_mlp_extractor": [
      "self"
    ],
    "_process_sequence": [
      "features",
      "lstm_states",
      "episode_starts",
      "lstm"
    ],
    "forward": [
      "self",
      "obs",
      "lstm_states",
      "episode_starts",
      "deterministic"
    ],
    "get_distribution": [
      "self",
      "obs",
      "lstm_states",
      "episode_starts"
    ],
    "predict_values": [
      "self",
      "obs",
      "lstm_states",
      "episode_starts"
    ],
    "evaluate_actions": [
      "self",
      "obs",
      "actions",
      "lstm_states",
      "episode_starts"
    ],
    "_predict": [
      "self",
      "observation",
      "lstm_states",
      "episode_starts",
      "deterministic"
    ],
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic"
    ]
  },
  "RecurrentActorCriticCnnPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "squash_output",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs",
      "lstm_hidden_size",
      "n_lstm_layers",
      "shared_lstm",
      "enable_critic_lstm",
      "lstm_kwargs"
    ]
  },
  "RecurrentMultiInputActorCriticPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "squash_output",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs",
      "lstm_hidden_size",
      "n_lstm_layers",
      "shared_lstm",
      "enable_critic_lstm",
      "lstm_kwargs"
    ]
  },
  "RNNStates": {},
  "RecurrentRolloutBufferSamples": {},
  "RecurrentDictRolloutBufferSamples": {},
  "pad": [
    "seq_start_indices",
    "seq_end_indices",
    "device",
    "tensor",
    "padding_value"
  ],
  "pad_and_flatten": [
    "seq_start_indices",
    "seq_end_indices",
    "device",
    "tensor",
    "padding_value"
  ],
  "create_sequencers": [
    "episode_starts",
    "env_change",
    "device"
  ],
  "RecurrentRolloutBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "hidden_state_shape",
      "device",
      "gae_lambda",
      "gamma",
      "n_envs"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self"
    ],
    "get": [
      "self",
      "batch_size"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env_change",
      "env"
    ]
  },
  "RecurrentDictRolloutBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "hidden_state_shape",
      "device",
      "gae_lambda",
      "gamma",
      "n_envs"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self"
    ],
    "get": [
      "self",
      "batch_size"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env_change",
      "env"
    ]
  },
  "MaskableEvalCallback": {
    "__init__": [
      "self"
    ],
    "_on_step": [
      "self"
    ]
  },
  "evaluate_policy": [
    "model",
    "env",
    "n_eval_episodes",
    "deterministic",
    "render",
    "callback",
    "reward_threshold",
    "return_episode_rewards",
    "warn",
    "use_masking"
  ],
  "SelfMaskableCategoricalDistribution": [],
  "SelfMaskableMultiCategoricalDistribution": [],
  "MaybeMasks": [],
  "MaskableCategorical": {
    "__init__": [
      "self",
      "probs",
      "logits",
      "validate_args",
      "masks"
    ],
    "apply_masking": [
      "self",
      "masks"
    ],
    "entropy": [
      "self"
    ]
  },
  "MaskableDistribution": {
    "apply_masking": [
      "self",
      "masks"
    ],
    "proba_distribution_net": [
      "self"
    ]
  },
  "MaskableCategoricalDistribution": {
    "__init__": [
      "self",
      "action_dim"
    ],
    "proba_distribution_net": [
      "self",
      "latent_dim"
    ],
    "proba_distribution": [
      "self",
      "action_logits"
    ],
    "log_prob": [
      "self",
      "actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "actions_from_params": [
      "self",
      "action_logits",
      "deterministic"
    ],
    "log_prob_from_params": [
      "self",
      "action_logits"
    ],
    "apply_masking": [
      "self",
      "masks"
    ]
  },
  "MaskableMultiCategoricalDistribution": {
    "__init__": [
      "self",
      "action_dims"
    ],
    "proba_distribution_net": [
      "self",
      "latent_dim"
    ],
    "proba_distribution": [
      "self",
      "action_logits"
    ],
    "log_prob": [
      "self",
      "actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "actions_from_params": [
      "self",
      "action_logits",
      "deterministic"
    ],
    "log_prob_from_params": [
      "self",
      "action_logits"
    ],
    "apply_masking": [
      "self",
      "masks"
    ]
  },
  "MaskableBernoulliDistribution": {
    "__init__": [
      "self",
      "action_dim"
    ]
  },
  "make_masked_proba_distribution": [
    "action_space"
  ],
  "EXPECTED_METHOD_NAME": [],
  "get_action_masks": [
    "env"
  ],
  "is_masking_supported": [
    "env"
  ],
  "MaskableActorCriticPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic",
      "action_masks"
    ],
    "extract_features": [
      "self",
      "obs",
      "features_extractor"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "_build_mlp_extractor": [
      "self"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "_get_action_dist_from_latent": [
      "self",
      "latent_pi"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic",
      "action_masks"
    ],
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic",
      "action_masks"
    ],
    "evaluate_actions": [
      "self",
      "obs",
      "actions",
      "action_masks"
    ],
    "get_distribution": [
      "self",
      "obs",
      "action_masks"
    ],
    "predict_values": [
      "self",
      "obs"
    ]
  },
  "MaskableActorCriticCnnPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ]
  },
  "MaskableMultiInputActorCriticPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ]
  },
  "MaskableRolloutBufferSamples": {},
  "MaskableDictRolloutBufferSamples": {},
  "MaskableRolloutBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "device",
      "gae_lambda",
      "gamma",
      "n_envs"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self"
    ],
    "get": [
      "self",
      "batch_size"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ]
  },
  "MaskableDictRolloutBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "device",
      "gae_lambda",
      "gamma",
      "n_envs"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self"
    ],
    "get": [
      "self",
      "batch_size"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ]
  },
  "InvalidActionEnvDiscrete": {
    "__init__": [
      "self",
      "dim",
      "ep_length",
      "n_invalid_actions"
    ],
    "_choose_next_state": [
      "self"
    ],
    "action_masks": [
      "self"
    ]
  },
  "InvalidActionEnvMultiDiscrete": {
    "__init__": [
      "self",
      "dims",
      "ep_length",
      "n_invalid_actions"
    ],
    "_choose_next_state": [
      "self"
    ],
    "action_masks": [
      "self"
    ]
  },
  "InvalidActionEnvMultiBinary": {
    "__init__": [
      "self",
      "dims",
      "ep_length",
      "n_invalid_actions"
    ],
    "_choose_next_state": [
      "self"
    ],
    "action_masks": [
      "self"
    ]
  },
  "SelfMaskablePPO": [],
  "MaskablePPO": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "n_steps",
      "batch_size",
      "n_epochs",
      "gamma",
      "gae_lambda",
      "clip_range",
      "clip_range_vf",
      "normalize_advantage",
      "ent_coef",
      "vf_coef",
      "max_grad_norm",
      "rollout_buffer_class",
      "rollout_buffer_kwargs",
      "target_kl",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "collect_rollouts": [
      "self",
      "env",
      "callback",
      "rollout_buffer",
      "n_rollout_steps",
      "use_masking"
    ],
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic",
      "action_masks"
    ],
    "train": [
      "self"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "use_masking",
      "progress_bar"
    ]
  },
  "MlpPolicy": [],
  "CnnPolicy": [],
  "MultiInputPolicy": [],
  "SelfTRPO": [],
  "TRPO": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "n_steps",
      "batch_size",
      "gamma",
      "cg_max_steps",
      "cg_damping",
      "line_search_shrinking_factor",
      "line_search_max_iter",
      "n_critic_updates",
      "gae_lambda",
      "use_sde",
      "sde_sample_freq",
      "rollout_buffer_class",
      "rollout_buffer_kwargs",
      "normalize_advantage",
      "target_kl",
      "sub_sampling_factor",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_compute_actor_grad": [
      "self",
      "kl_div",
      "policy_objective"
    ],
    "train": [
      "self"
    ],
    "hessian_vector_product": [
      "self",
      "params",
      "grad_kl",
      "vector",
      "retain_graph"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ]
  },
  "LOG_STD_MAX": [],
  "LOG_STD_MIN": [],
  "Actor": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "net_arch",
      "features_extractor",
      "features_dim",
      "activation_fn",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "clip_mean",
      "normalize_images",
      "batch_norm",
      "batch_norm_momentum",
      "batch_norm_eps",
      "renorm_warmup_steps"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "get_std": [
      "self"
    ],
    "reset_noise": [
      "self",
      "batch_size"
    ],
    "get_action_dist_params": [
      "self",
      "obs"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "action_log_prob": [
      "self",
      "obs"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "set_bn_training_mode": [
      "self",
      "mode"
    ]
  },
  "CrossQCritic": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "net_arch",
      "features_extractor",
      "features_dim",
      "activation_fn",
      "normalize_images",
      "n_critics",
      "share_features_extractor",
      "batch_norm",
      "batch_norm_momentum",
      "batch_norm_eps",
      "renorm_warmup_steps"
    ],
    "forward": [
      "self",
      "obs",
      "actions"
    ],
    "set_bn_training_mode": [
      "self",
      "mode"
    ]
  },
  "CrossQPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "batch_norm",
      "batch_norm_momentum",
      "batch_norm_eps",
      "renorm_warmup_steps",
      "use_sde",
      "log_std_init",
      "use_expln",
      "clip_mean",
      "features_extractor_class",
      "features_extractor_kwargs",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs",
      "n_critics",
      "share_features_extractor"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "reset_noise": [
      "self",
      "batch_size"
    ],
    "make_actor": [
      "self",
      "features_extractor"
    ],
    "make_critic": [
      "self",
      "features_extractor"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "set_training_mode": [
      "self",
      "mode"
    ]
  },
  "SelfCrossQ": [],
  "CrossQ": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "gamma",
      "train_freq",
      "gradient_steps",
      "action_noise",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "ent_coef",
      "target_entropy",
      "policy_delay",
      "use_sde",
      "sde_sample_freq",
      "use_sde_at_warmup",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "_create_aliases": [
      "self"
    ],
    "train": [
      "self",
      "gradient_steps",
      "batch_size"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_excluded_save_params": [
      "self"
    ],
    "_get_torch_save_params": [
      "self"
    ]
  },
  "SelfRecurrentPPO": [],
  "RecurrentPPO": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "n_steps",
      "batch_size",
      "n_epochs",
      "gamma",
      "gae_lambda",
      "clip_range",
      "clip_range_vf",
      "normalize_advantage",
      "ent_coef",
      "vf_coef",
      "max_grad_norm",
      "use_sde",
      "sde_sample_freq",
      "target_kl",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "collect_rollouts": [
      "self",
      "env",
      "callback",
      "rollout_buffer",
      "n_rollout_steps"
    ],
    "train": [
      "self"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_excluded_save_params": [
      "self"
    ]
  },
  "MlpLstmPolicy": [],
  "CnnLstmPolicy": [],
  "MultiInputLstmPolicy": [],
  "QuantileNetwork": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "features_extractor",
      "features_dim",
      "n_quantiles",
      "net_arch",
      "activation_fn",
      "normalize_images"
    ],
    "forward": [
      "self",
      "obs"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "_get_constructor_parameters": [
      "self"
    ]
  },
  "QRDQNPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "n_quantiles",
      "net_arch",
      "activation_fn",
      "features_extractor_class",
      "features_extractor_kwargs",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "make_quantile_net": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "_predict": [
      "self",
      "obs",
      "deterministic"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "set_training_mode": [
      "self",
      "mode"
    ]
  },
  "SelfQRDQN": [],
  "QRDQN": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "tau",
      "gamma",
      "train_freq",
      "gradient_steps",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "target_update_interval",
      "exploration_fraction",
      "exploration_initial_eps",
      "exploration_final_eps",
      "max_grad_norm",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "_create_aliases": [
      "self"
    ],
    "_on_step": [
      "self"
    ],
    "train": [
      "self",
      "gradient_steps",
      "batch_size"
    ],
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_excluded_save_params": [
      "self"
    ],
    "_get_torch_save_params": [
      "self"
    ]
  },
  "Critic": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "net_arch",
      "features_extractor",
      "features_dim",
      "activation_fn",
      "normalize_images",
      "n_quantiles",
      "n_critics",
      "share_features_extractor"
    ],
    "forward": [
      "self",
      "obs",
      "action"
    ]
  },
  "TQCPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "use_sde",
      "log_std_init",
      "use_expln",
      "clip_mean",
      "features_extractor_class",
      "features_extractor_kwargs",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs",
      "n_quantiles",
      "n_critics",
      "share_features_extractor"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "reset_noise": [
      "self",
      "batch_size"
    ],
    "make_actor": [
      "self",
      "features_extractor"
    ],
    "make_critic": [
      "self",
      "features_extractor"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "set_training_mode": [
      "self",
      "mode"
    ]
  },
  "SelfTQC": [],
  "TQC": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "tau",
      "gamma",
      "train_freq",
      "gradient_steps",
      "action_noise",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "ent_coef",
      "target_update_interval",
      "target_entropy",
      "top_quantiles_to_drop_per_net",
      "use_sde",
      "sde_sample_freq",
      "use_sde_at_warmup",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "_create_aliases": [
      "self"
    ],
    "train": [
      "self",
      "gradient_steps",
      "batch_size"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_excluded_save_params": [
      "self"
    ],
    "_get_torch_save_params": [
      "self"
    ]
  },
  "SelfARS": [],
  "ARS": {
    "__init__": [
      "self",
      "policy",
      "env",
      "n_delta",
      "n_top",
      "learning_rate",
      "delta_std",
      "zero_policy",
      "alive_bonus_offset",
      "n_eval_episodes",
      "policy_kwargs",
      "stats_window_size",
      "tensorboard_log",
      "seed",
      "verbose",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "_mimic_monitor_wrapper": [
      "self",
      "episode_rewards",
      "episode_lengths"
    ],
    "_trigger_callback": [
      "self",
      "_locals",
      "_globals",
      "callback",
      "n_envs"
    ],
    "evaluate_candidates": [
      "self",
      "candidate_weights",
      "callback",
      "async_eval"
    ],
    "dump_logs": [
      "self"
    ],
    "_do_one_update": [
      "self",
      "callback",
      "async_eval"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "async_eval",
      "progress_bar"
    ],
    "set_parameters": [
      "self",
      "load_path_or_dict",
      "exact_match",
      "device"
    ]
  },
  "ARSPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "net_arch",
      "activation_fn",
      "with_bias",
      "squash_output"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "obs"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ]
  },
  "ARSLinearPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "with_bias",
      "squash_output"
    ]
  },
  "LinearPolicy": []
}