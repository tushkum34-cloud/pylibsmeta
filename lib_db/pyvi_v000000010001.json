{
  "python_version": [],
  "remove_accents": [
    "s"
  ],
  "add_accents": [
    "s"
  ],
  "ViTokenizer": {
    "bi_grams": [],
    "tri_grams": [],
    "model_file": [],
    "word2features": [
      "sent",
      "i",
      "is_training"
    ],
    "sent2features": [
      "sent",
      "is_training"
    ],
    "sylabelize": [
      "text"
    ],
    "tokenize": [
      "str"
    ],
    "spacy_tokenize": [
      "str"
    ]
  },
  "spacy_tokenize": [
    "str"
  ],
  "tokenize": [
    "str"
  ],
  "__author__": [],
  "ViPosTagger": {
    "filtered_tags": [],
    "model_file": [],
    "word2features": [
      "sent",
      "i",
      "is_training"
    ],
    "sent2features": [
      "sent",
      "is_training"
    ],
    "postagging": [
      "str"
    ],
    "postagging_tokens": [
      "tokens"
    ]
  },
  "postagging": [
    "str"
  ],
  "postagging_tokens": [
    "tokens"
  ],
  "ViDiac": {
    "maAciiTexlex": [],
    "telex": [],
    "mapping": [],
    "reversed_mapping": [],
    "crf": [],
    "data": [],
    "model": [],
    "prepare_data": [
      "str_line"
    ],
    "word2features": [
      "i",
      "feature_generator"
    ],
    "sent2features": [
      "tokens"
    ],
    "doit": [
      "str_sentence"
    ]
  },
  "tokenized_X_train": [],
  "tokenized_X_test": [],
  "tokenized_y_train": [],
  "tokenized_y_test": [],
  "X_train": [],
  "X_test": [],
  "y_train": [],
  "y_test": [],
  "labels": [],
  "crf": [],
  "params_space": [],
  "f1_scorer": [],
  "rs": [],
  "tokenizer_model": [],
  "tokenizer_model_py3": []
}