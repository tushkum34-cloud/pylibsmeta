{
  "__version__": [],
  "_top_k_renorm_probs_internal": [
    "probs",
    "maybe_top_k_arr",
    "top_k_val"
  ],
  "top_k_renorm_probs": [
    "probs",
    "top_k"
  ],
  "top_k_renorm_prob": [],
  "_top_p_renorm_probs_internal": [
    "probs",
    "maybe_top_p_arr",
    "top_p_val"
  ],
  "top_p_renorm_probs": [
    "probs",
    "top_p"
  ],
  "top_p_renorm_prob": [],
  "_top_p_sampling_from_probs_internal": [
    "probs",
    "indices",
    "maybe_top_p_arr",
    "top_p_val",
    "deterministic",
    "generator"
  ],
  "top_p_sampling_from_probs": [
    "probs",
    "top_p",
    "indices",
    "deterministic",
    "generator",
    "check_nan"
  ],
  "_top_k_top_p_sampling_from_probs_internal": [
    "probs",
    "indices",
    "maybe_top_k_arr",
    "top_k_val",
    "maybe_top_p_arr",
    "top_p_val",
    "deterministic",
    "generator"
  ],
  "top_k_top_p_sampling_from_probs": [
    "probs",
    "top_k",
    "top_p",
    "indices",
    "filter_apply_order",
    "deterministic",
    "generator",
    "check_nan"
  ],
  "_min_p_sampling_from_probs_internal": [
    "probs",
    "indices",
    "maybe_min_p_arr",
    "min_p_val",
    "deterministic",
    "generator"
  ],
  "min_p_sampling_from_probs": [
    "probs",
    "min_p",
    "indices",
    "deterministic",
    "generator",
    "check_nan"
  ],
  "_top_k_mask_logits_internal": [
    "logits",
    "maybe_top_k_arr",
    "top_k_val"
  ],
  "top_k_mask_logits": [
    "logits",
    "top_k"
  ],
  "top_k_top_p_sampling_from_logits": [
    "logits",
    "top_k",
    "top_p",
    "indices",
    "filter_apply_order",
    "deterministic",
    "generator",
    "check_nan"
  ],
  "logger": [],
  "_get_device_capability": [],
  "maybe_contiguous": [
    "x"
  ],
  "_validate_tensor": [
    "t",
    "name",
    "expected_shape",
    "expected_dtype",
    "expected_device"
  ],
  "to_cute_tensor": [
    "t",
    "assumed_align",
    "leading_dim",
    "fully_dynamic"
  ],
  "torch2cute_dtype_map": [],
  "num_splits_heuristic": [
    "total_mblocks",
    "num_SMs",
    "num_n_blocks",
    "max_splits"
  ],
  "_flash_attn_fwd": [
    "q",
    "k",
    "v",
    "cu_seqlens_q",
    "cu_seqlens_k",
    "seqused_q",
    "seqused_k",
    "max_seqlen_q",
    "max_seqlen_k",
    "page_table",
    "softmax_scale",
    "causal",
    "softcap",
    "window_size_left",
    "window_size_right",
    "learnable_sink",
    "m_block_size",
    "n_block_size",
    "num_threads",
    "num_splits",
    "pack_gqa",
    "_compute_capability",
    "score_mod",
    "mask_mod",
    "block_sparse_tensors",
    "return_lse",
    "out",
    "lse",
    "aux_tensors"
  ],
  "_flash_attn_fwd_combine": [
    "out_partial",
    "lse_partial",
    "out",
    "lse",
    "cu_seqlens",
    "seqused",
    "num_splits_dynamic_ptr",
    "semaphore_to_reset"
  ],
  "warmup_flash_attn": [
    "f"
  ],
  "flash_attn_varlen_func": [
    "q",
    "k",
    "v",
    "cu_seqlens_q",
    "cu_seqlens_k",
    "seqused_q",
    "seqused_k",
    "page_table",
    "softmax_scale",
    "causal",
    "window_size",
    "learnable_sink",
    "softcap",
    "num_splits",
    "pack_gqa",
    "return_softmax_lse",
    "score_mod",
    "aux_tensors"
  ],
  "convert_vertical_slash_indexes": [
    "q_seqlens",
    "kv_seqlens",
    "vertical_indexes",
    "slash_indexes",
    "context_size",
    "block_size_M",
    "block_size_N",
    "causal"
  ],
  "convert_vertical_slash_indexes_mergehead": [
    "q_seqlens",
    "kv_seqlens",
    "vertical_indexes",
    "slash_indexes",
    "vertical_indices_count",
    "slash_indices_count",
    "context_size",
    "block_size_M",
    "block_size_N",
    "causal"
  ],
  "sparse_attn_func": [
    "q",
    "k",
    "v",
    "block_count",
    "block_offset",
    "column_count",
    "column_index",
    "dropout_p",
    "softmax_scale",
    "causal",
    "softcap",
    "alibi_slopes",
    "deterministic",
    "return_attn_probs"
  ],
  "sparse_attn_varlen_func": [
    "q",
    "k",
    "v",
    "block_count",
    "block_offset",
    "column_count",
    "column_index",
    "cu_seqlens_q",
    "cu_seqlens_k",
    "max_seqlen_q",
    "max_seqlen_k",
    "dropout_p",
    "softmax_scale",
    "causal",
    "softcap",
    "alibi_slopes",
    "deterministic",
    "return_attn_probs"
  ],
  "is_fa3_supported": [
    "device"
  ],
  "flash_attn_with_kvcache": [
    "q",
    "k_cache",
    "v_cache",
    "k",
    "v",
    "qv",
    "rotary_cos",
    "rotary_sin",
    "cache_seqlens",
    "cache_batch_idx",
    "cache_leftpad",
    "page_table",
    "cu_seqlens_q",
    "cu_seqlens_k_new",
    "max_seqlen_q",
    "rotary_seqlens",
    "q_descale",
    "k_descale",
    "v_descale",
    "softmax_scale",
    "causal",
    "window_size",
    "attention_chunk",
    "softcap",
    "rotary_interleaved",
    "scheduler_metadata",
    "num_splits",
    "pack_gqa",
    "sm_margin",
    "return_softmax_lse",
    "sinks",
    "score_mod",
    "aux_tensors",
    "ver"
  ],
  "hadamard_transform": [
    "x",
    "scale"
  ],
  "hadamard_transform_12n": [
    "x",
    "scale"
  ],
  "hadamard_transform_20n": [
    "x",
    "scale"
  ],
  "hadamard_transform_28n": [
    "x",
    "scale"
  ],
  "hadamard_transform_40n": [
    "x",
    "scale"
  ],
  "create_per_token_group_quant_test_data": [
    "num_tokens",
    "hidden_dim",
    "num_ranks",
    "flags"
  ],
  "_compute_balanced_split": [
    "total",
    "arr_len"
  ],
  "_compute_imbalanced_split": [
    "total",
    "arr_len",
    "gen_cpu",
    "dtype"
  ],
  "assert_all_close_or_tiny_diff": [
    "a",
    "b"
  ],
  "_get_cache_buf": [
    "name",
    "bytes",
    "device"
  ],
  "_to_tensor_scalar_tuple": [
    "x"
  ],
  "is_arch_support_pdl": [],
  "_get_compute_capability": [],
  "_filter_compiled_extensions": [
    "file_list"
  ],
  "_load_architecture_specific_ops": [],
  "_find_cuda_home": [],
  "_preload_cuda_library": [],
  "causal_conv1d_fwd": [
    "x",
    "weight",
    "bias_",
    "conv_states",
    "query_start_loc",
    "cache_indices",
    "has_initial_state",
    "silu_activation",
    "pad_slot_id"
  ],
  "causal_conv1d_update": [
    "x",
    "conv_state",
    "weight",
    "bias_",
    "silu_activation",
    "cache_seqlens",
    "conv_state_indices",
    "pad_slot_id"
  ],
  "common_ops": [],
  "create_greenctx_stream_by_value": [],
  "get_sm_available": [],
  "merge_state": [
    "v_a",
    "s_a",
    "v_b",
    "s_b",
    "v_merged",
    "s_merged"
  ],
  "merge_state_v2": [
    "v_a",
    "s_a",
    "v_b",
    "s_b",
    "v_merged",
    "s_merged"
  ],
  "cutlass_mla_decode": [
    "q_nope",
    "q_pe",
    "kv_c_and_k_pe_cache",
    "seq_lens",
    "page_table",
    "workspace",
    "sm_scale",
    "num_kv_splits"
  ],
  "cutlass_mla_get_workspace_size": [
    "max_seq_len",
    "num_batches",
    "sm_count",
    "num_kv_splits"
  ],
  "fast_topk": [
    "values",
    "topk",
    "dim"
  ],
  "fast_topk_v2": [
    "score",
    "lengths",
    "topk",
    "row_starts"
  ],
  "fast_topk_transform_fused": [
    "score",
    "lengths",
    "page_table_size_1",
    "cu_seqlens_q",
    "topk",
    "row_starts"
  ],
  "fast_topk_transform_ragged_fused": [
    "score",
    "lengths",
    "topk_indices_offset",
    "topk",
    "row_starts"
  ],
  "is_hip": [],
  "_is_hip": [],
  "transfer_kv_per_layer": [
    "src_k",
    "dst_k",
    "src_v",
    "dst_v",
    "src_indices",
    "dst_indices",
    "item_size",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_per_layer_pf_lf": [
    "src_k",
    "dst_k",
    "src_v",
    "dst_v",
    "src_indices",
    "dst_indices",
    "layer_id",
    "item_size",
    "src_layout_dim",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_per_layer_ph_lf": [
    "src_k",
    "dst_k",
    "src_v",
    "dst_v",
    "src_indices",
    "dst_indices",
    "layer_id",
    "item_size",
    "src_layout_dim",
    "page_size",
    "head_num",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_all_layer": [
    "src_k_layers",
    "dst_k_layers",
    "src_v_layers",
    "dst_v_layers",
    "src_indices",
    "dst_indices",
    "item_size",
    "num_layers",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_all_layer_lf_pf": [
    "src_k_layers",
    "dst_k",
    "src_v_layers",
    "dst_v",
    "src_indices",
    "dst_indices",
    "item_size",
    "dst_layout_dim",
    "num_layers",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_all_layer_lf_ph": [
    "src_k_layers",
    "dst_k",
    "src_v_layers",
    "dst_v",
    "src_indices",
    "dst_indices",
    "item_size",
    "dst_layout_dim",
    "num_layers",
    "page_size",
    "head_num",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_direct": [
    "src_layers",
    "dst_layers",
    "src_indices",
    "dst_indices",
    "page_size"
  ],
  "transfer_kv_per_layer_direct_pf_lf": [
    "src_ptrs",
    "dst_ptrs",
    "src_indices",
    "dst_indices",
    "layer_id",
    "page_size"
  ],
  "transfer_kv_all_layer_direct_lf_pf": [
    "src_ptrs",
    "dst_ptrs",
    "src_indices",
    "dst_indices",
    "page_size"
  ],
  "transfer_kv_per_layer_mla": [
    "src",
    "dst",
    "src_indices",
    "dst_indices",
    "item_size",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_per_layer_mla_pf_lf": [
    "src",
    "dst",
    "src_indices",
    "dst_indices",
    "layer_id",
    "item_size",
    "src_layout_dim",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_all_layer_mla": [
    "src_layers",
    "dst_layers",
    "src_indices",
    "dst_indices",
    "item_size",
    "num_layers",
    "block_quota",
    "num_warps_per_block"
  ],
  "transfer_kv_all_layer_mla_lf_pf": [
    "src_layers",
    "dst",
    "src_indices",
    "dst_indices",
    "item_size",
    "dst_layout_dim",
    "num_layers",
    "block_quota",
    "num_warps_per_block"
  ],
  "gptq_marlin_repack": [
    "b_q_weight",
    "perm",
    "size_k",
    "size_n",
    "num_bits"
  ],
  "awq_marlin_repack": [
    "b_q_weight",
    "size_k",
    "size_n",
    "num_bits"
  ],
  "awq_marlin_moe_repack": [
    "b_q_weight",
    "perm",
    "size_k",
    "size_n",
    "num_bits"
  ],
  "set_kv_buffer_kernel": [
    "k_cache",
    "v_cache",
    "loc",
    "k",
    "v",
    "fallback"
  ],
  "weak_ref_tensor": [
    "tensor"
  ],
  "awq_dequantize": [
    "qweight",
    "scales",
    "qzeros"
  ],
  "int8_scaled_mm": [
    "mat_a",
    "mat_b",
    "scales_a",
    "scales_b",
    "out_dtype",
    "bias"
  ],
  "fp8_blockwise_scaled_mm": [
    "mat_a",
    "mat_b",
    "scales_a",
    "scales_b",
    "out_dtype"
  ],
  "fp8_scaled_mm": [
    "mat_a",
    "mat_b",
    "scales_a",
    "scales_b",
    "out_dtype",
    "bias"
  ],
  "_bmm_fp8_internal": [
    "workspace_buffer",
    "A",
    "B",
    "D",
    "A_scale",
    "B_scale"
  ],
  "bmm_fp8": [
    "A",
    "B",
    "A_scale",
    "B_scale",
    "dtype",
    "out"
  ],
  "dsv3_fused_a_gemm": [
    "mat_a",
    "mat_b",
    "output"
  ],
  "sgl_per_token_group_quant_8bit": [
    "input",
    "output_q",
    "output_s",
    "group_size",
    "eps",
    "fp8_min",
    "fp8_max",
    "scale_ue8m0",
    "fuse_silu_and_mul",
    "masked_m",
    "enable_v2"
  ],
  "sgl_per_token_group_quant_fp8": [],
  "sgl_per_token_group_quant_int8": [],
  "sgl_per_tensor_quant_fp8": [
    "input",
    "output_q",
    "output_s",
    "is_static"
  ],
  "sgl_per_token_quant_fp8": [
    "input",
    "output_q",
    "output_s"
  ],
  "cutlass_scaled_fp4_mm": [
    "a",
    "b",
    "block_scale_a",
    "block_scale_b",
    "alpha",
    "out_dtype"
  ],
  "scaled_fp4_quant": [
    "input",
    "input_global_scale"
  ],
  "qserve_w4a8_per_chn_gemm": [
    "in_feats",
    "kernel",
    "wscales",
    "ascales",
    "w_szs",
    "a_ssums",
    "out_feats"
  ],
  "qserve_w4a8_per_group_gemm": [
    "in_feats",
    "kernel",
    "zeros",
    "scales_i8",
    "wscales",
    "ascales",
    "out_feats"
  ],
  "dsv3_router_gemm": [
    "hidden_states",
    "router_weights",
    "out_dtype"
  ],
  "shuffle_rows": [
    "input_tensor",
    "dst2src_map",
    "output_tensor_shape"
  ],
  "scaled_fp4_grouped_quant": [
    "input_tensor",
    "input_global_scale",
    "mask"
  ],
  "silu_and_mul_scaled_fp4_grouped_quant": [
    "input_tensor",
    "input_global_scale",
    "mask"
  ],
  "scaled_fp4_experts_quant": [
    "input_tensor",
    "input_global_scale",
    "expert_offsets",
    "blockscale_offsets",
    "topk",
    "expert_map"
  ],
  "gptq_marlin_gemm": [
    "a",
    "c",
    "b_q_weight",
    "b_scales",
    "global_scale",
    "b_zeros",
    "g_idx",
    "perm",
    "workspace",
    "b_q_type",
    "size_m",
    "size_n",
    "size_k",
    "is_k_full",
    "use_atomic_add",
    "use_fp32_reduce",
    "is_zp_float"
  ],
  "gptq_gemm": [
    "a",
    "b_q_weight",
    "b_gptq_qzeros",
    "b_gptq_scales",
    "b_g_idx",
    "use_shuffle",
    "bit"
  ],
  "gptq_shuffle": [
    "q_weight",
    "q_perm",
    "bit"
  ],
  "es_fp8_blockwise_scaled_grouped_mm": [
    "output",
    "a",
    "b",
    "scales_a",
    "scales_b",
    "stride_a",
    "stride_b",
    "stride_d",
    "problem_sizes",
    "expert_offsets",
    "workspace"
  ],
  "es_sm100_mxfp8_blockscaled_grouped_mm": [
    "output",
    "a",
    "b",
    "sfa",
    "sfb",
    "problem_sizes",
    "expert_offsets",
    "blockscale_offsets"
  ],
  "es_sm100_mxfp8_blockscaled_grouped_quant": [
    "input",
    "problem_sizes",
    "expert_offsets",
    "blockscale_offsets",
    "quant_output",
    "scale_factor"
  ],
  "get_cutlass_w4a8_moe_mm_data": [
    "topk_ids",
    "expert_offsets",
    "problem_sizes1",
    "problem_sizes2",
    "input_permutation",
    "output_permutation",
    "num_experts",
    "n",
    "k"
  ],
  "cutlass_w4a8_moe_mm": [
    "d",
    "a",
    "b",
    "a_scales",
    "b_scales",
    "experts_offsets",
    "problem_sizes",
    "a_strides",
    "b_strides",
    "d_strides",
    "s_strides",
    "chunk_size",
    "topk"
  ],
  "_IMPORT_ERROR": [],
  "get_mla_metadata": [
    "cache_seqlens",
    "num_q_tokens_per_head_k",
    "num_heads_k",
    "num_heads_q",
    "is_fp8_kvcache",
    "topk"
  ],
  "flash_mla_with_kvcache": [
    "q",
    "k_cache",
    "block_table",
    "cache_seqlens",
    "head_dim_v",
    "tile_scheduler_metadata",
    "num_splits",
    "softmax_scale",
    "causal",
    "descale_q",
    "descale_k",
    "is_fp8_kvcache",
    "indices"
  ],
  "flash_mla_sparse_fwd": [
    "q",
    "kv",
    "indices",
    "sm_scale",
    "d_v"
  ],
  "tree_speculative_sampling_target_only": [
    "predicts",
    "accept_index",
    "accept_token_num",
    "candidates",
    "retrive_index",
    "retrive_next_token",
    "retrive_next_sibling",
    "uniform_samples",
    "uniform_samples_for_final_sampling",
    "target_probs",
    "draft_probs",
    "threshold_single",
    "threshold_acc",
    "deterministic"
  ],
  "verify_tree_greedy": [
    "predicts",
    "accept_index",
    "accept_token_num",
    "candidates",
    "retrive_index",
    "retrive_next_token",
    "retrive_next_sibling",
    "target_predict"
  ],
  "build_tree_kernel_efficient": [
    "parent_list",
    "selected_index",
    "verified_seq_len",
    "tree_mask",
    "positions",
    "retrive_index",
    "retrive_next_token",
    "retrive_next_sibling",
    "topk",
    "depth",
    "draft_token_num",
    "tree_mask_mode"
  ],
  "reconstruct_indices_from_tree_mask": [
    "tree_mask",
    "verified_seq_len",
    "positions",
    "retrive_index",
    "retrive_next_token",
    "retrive_next_sibling",
    "batch_size",
    "draft_token_num"
  ],
  "segment_packbits": [
    "x",
    "input_indptr",
    "output_indptr",
    "y",
    "batch_size"
  ],
  "moe_align_block_size": [
    "topk_ids",
    "num_experts",
    "block_size",
    "sorted_token_ids",
    "experts_ids",
    "num_tokens_post_pad",
    "cumsum_buffer",
    "pad_sorted_token_ids"
  ],
  "topk_softmax": [
    "topk_weights",
    "topk_ids",
    "gating_output",
    "renormalize",
    "moe_softcapping",
    "correction_bias"
  ],
  "topk_sigmoid": [
    "topk_weights",
    "topk_ids",
    "gating_output",
    "renormalize",
    "correction_bias"
  ],
  "moe_sum_reduce": [
    "input_tensor",
    "output_tensor",
    "routed_scaling_factor"
  ],
  "moe_sum": [
    "input_tensor",
    "output_tensor"
  ],
  "moe_fused_gate": [
    "input_tensor",
    "bias",
    "num_expert_group",
    "topk_group",
    "topk",
    "num_fused_shared_experts",
    "routed_scaling_factor",
    "apply_routed_scaling_factor_on_output"
  ],
  "kimi_k2_moe_fused_gate": [
    "input_tensor",
    "bias",
    "topk",
    "renormalize",
    "routed_scaling_factor",
    "apply_routed_scaling_factor_on_output"
  ],
  "fp8_blockwise_scaled_grouped_mm": [
    "output",
    "a_ptrs",
    "b_ptrs",
    "out_ptrs",
    "a_scales_ptrs",
    "b_scales_ptrs",
    "a",
    "b",
    "scales_a",
    "scales_b",
    "stride_a",
    "stride_b",
    "stride_c",
    "layout_sfa",
    "layout_sfb",
    "problem_sizes",
    "expert_offsets",
    "workspace"
  ],
  "prepare_moe_input": [
    "topk_ids",
    "expert_offsets",
    "problem_sizes1",
    "problem_sizes2",
    "input_permutation",
    "output_permutation",
    "num_experts",
    "n",
    "k",
    "blockscale_offsets"
  ],
  "apply_shuffle_mul_sum": [
    "input",
    "output",
    "permutation",
    "factors"
  ],
  "fused_qk_norm_rope": [
    "qkv",
    "num_heads_q",
    "num_heads_k",
    "num_heads_v",
    "head_dim",
    "eps",
    "q_weight",
    "k_weight",
    "base",
    "is_neox",
    "position_ids",
    "factor",
    "low",
    "high",
    "attention_factor",
    "rotary_dim"
  ],
  "cutlass_fp4_group_mm": [
    "a_fp4",
    "b_fp4",
    "a_blockscale",
    "b_blockscale",
    "alphas",
    "out_dtype",
    "device",
    "params"
  ],
  "_SCALAR_TYPES_ID_MAP": [],
  "NanRepr": {
    "NONE": [],
    "IEEE_754": [],
    "EXTD_RANGE_MAX_MIN": []
  },
  "ScalarType": {
    "_floating_point_max_int": [
      "self"
    ],
    "_floating_point_max": [
      "self"
    ],
    "_raw_max": [
      "self"
    ],
    "_raw_min": [
      "self"
    ],
    "id": [
      "self"
    ],
    "size_bits": [
      "self"
    ],
    "min": [
      "self"
    ],
    "max": [
      "self"
    ],
    "is_signed": [
      "self"
    ],
    "is_floating_point": [
      "self"
    ],
    "is_integer": [
      "self"
    ],
    "has_bias": [
      "self"
    ],
    "has_infs": [
      "self"
    ],
    "has_nans": [
      "self"
    ],
    "is_ieee_754": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "int_": [
      "cls",
      "size_bits",
      "bias"
    ],
    "uint": [
      "cls",
      "size_bits",
      "bias"
    ],
    "float_IEEE754": [
      "cls",
      "exponent",
      "mantissa"
    ],
    "float_": [
      "cls",
      "exponent",
      "mantissa",
      "finite_values_only",
      "nan_repr"
    ],
    "from_id": [
      "cls",
      "scalar_type_id"
    ]
  },
  "scalar_types": {
    "int4": [],
    "uint4": [],
    "int8": [],
    "uint8": [],
    "float8_e4m3fn": [],
    "float8_e5m2": [],
    "float16_e8m7": [],
    "float16_e5m10": [],
    "float6_e3m2f": [],
    "float4_e2m1f": [],
    "uint2b2": [],
    "uint3b4": [],
    "uint4b8": [],
    "uint8b128": [],
    "bfloat16": [],
    "float16": []
  },
  "apply_token_bitmask_inplace_cuda": [
    "logits",
    "bitmask",
    "indices"
  ],
  "rmsnorm": [
    "input",
    "weight",
    "eps",
    "out",
    "enable_pdl"
  ],
  "fused_add_rmsnorm": [
    "input",
    "residual",
    "weight",
    "eps",
    "enable_pdl"
  ],
  "gemma_rmsnorm": [
    "input",
    "weight",
    "eps",
    "out",
    "enable_pdl"
  ],
  "gemma_fused_add_rmsnorm": [
    "input",
    "residual",
    "weight",
    "eps",
    "enable_pdl"
  ],
  "_check_shape": [
    "input",
    "output"
  ],
  "silu_and_mul": [
    "input",
    "out"
  ],
  "gelu_tanh_and_mul": [
    "input",
    "out"
  ],
  "gelu_and_mul": [
    "input",
    "out"
  ],
  "FusedSetKVBufferArg": {},
  "_view_3d": [
    "x",
    "head_size"
  ],
  "apply_rope_with_cos_sin_cache_inplace": [
    "positions",
    "query",
    "key",
    "head_size",
    "cos_sin_cache",
    "is_neox",
    "fused_set_kv_buffer_arg",
    "enable_pdl"
  ],
  "rotary_embedding": [
    "positions",
    "query",
    "key",
    "head_size",
    "cos_sin_cache",
    "is_neox"
  ],
  "downcast_fp8": [
    "k",
    "v",
    "k_out",
    "v_out",
    "k_scale",
    "v_scale",
    "loc",
    "mult",
    "offset"
  ],
  "copy_to_gpu_no_ce": [
    "input",
    "output"
  ],
  "concat_mla_k": [
    "k",
    "k_nope",
    "k_rope"
  ],
  "concat_mla_absorb_q": [
    "a",
    "b"
  ],
  "timestep_embedding": [
    "t",
    "dim",
    "flip_sin_to_cos",
    "downscale_freq_shift",
    "scale",
    "max_period",
    "dtype"
  ],
  "moe_wna16_marlin_gemm": [
    "a",
    "c_or_none",
    "b_q_weight",
    "b_bias_or_none",
    "b_scales",
    "global_scale_or_none",
    "b_zeros_or_none",
    "g_idx_or_none",
    "perm_or_none",
    "workspace",
    "sorted_token_ids",
    "expert_ids",
    "num_tokens_post_padded",
    "topk_weights",
    "moe_block_size",
    "top_k",
    "mul_topk_weights",
    "is_ep",
    "b_q_type_id",
    "size_m",
    "size_n",
    "size_k",
    "is_k_full",
    "use_atomic_add",
    "use_fp32_reduce",
    "is_zp_float"
  ],
  "_apply_rotary_emb": [
    "x",
    "cos",
    "sin",
    "is_neox_style"
  ],
  "RotaryEmbedding": {
    "__init__": [
      "self",
      "head_size",
      "rotary_dim",
      "max_position_embeddings",
      "base",
      "is_neox_style",
      "dtype"
    ],
    "_compute_inv_freq": [
      "self",
      "base"
    ],
    "_compute_cos_sin_cache": [
      "self"
    ],
    "forward_native": [
      "self",
      "positions",
      "query",
      "key",
      "offsets",
      "fused_set_kv_buffer_arg"
    ]
  },
  "FlashInferRotaryEmbedding": {
    "forward_cuda": [
      "self",
      "positions",
      "query",
      "key",
      "offsets",
      "fused_set_kv_buffer_arg"
    ]
  },
  "SglKernelRotaryEmbedding": {
    "forward_cuda": [
      "self",
      "positions",
      "query",
      "key",
      "offsets",
      "fused_set_kv_buffer_arg"
    ]
  },
  "MHATokenToKVPool": {
    "KV_POOL_SIZE": [],
    "__init__": [
      "self",
      "head_num",
      "head_dim"
    ],
    "_create_buffers": [
      "self"
    ],
    "set_kv_buffer": [
      "self",
      "loc",
      "cache_k",
      "cache_v"
    ]
  },
  "create_inputs": [
    "head_size",
    "batch_size",
    "seq_len",
    "device",
    "dtype",
    "num_q_heads",
    "num_kv_heads"
  ],
  "ggml_dequantize": [
    "weight",
    "quant_type",
    "M",
    "N",
    "dtype"
  ],
  "ggml_mul_mat_vec_a8": [
    "weight",
    "x",
    "quant_type",
    "row"
  ],
  "ggml_mul_mat_a8": [
    "weight",
    "x",
    "quant_type",
    "row"
  ],
  "ggml_moe_a8": [
    "input",
    "weight",
    "sorted_token_ids",
    "expert_ids",
    "num_token_post_padded",
    "type",
    "row",
    "topk",
    "tokens"
  ],
  "ggml_moe_a8_vec": [
    "input",
    "weight",
    "topk_ids",
    "top_k",
    "type",
    "row",
    "tokens"
  ],
  "ggml_moe_get_block_size": [
    "type"
  ],
  "FlashAttentionForwardCombine": {
    "__init__": [
      "self",
      "dtype",
      "dtype_partial",
      "head_dim",
      "m_block_size",
      "k_block_size",
      "log_max_splits",
      "num_threads",
      "stages"
    ],
    "can_implement": [
      "dtype",
      "dtype_partial",
      "head_dim",
      "m_block_size",
      "k_block_size",
      "log_max_splits",
      "num_threads"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "mO_partial",
      "mLSE_partial",
      "mO",
      "mLSE",
      "cu_seqlens",
      "seqused",
      "num_splits_dynamic_ptr",
      "semaphore_to_reset",
      "stream"
    ],
    "kernel": [
      "self",
      "mO_partial",
      "mLSE_partial",
      "mO",
      "mLSE",
      "cu_seqlens",
      "seqused",
      "num_splits_dynamic_ptr",
      "semaphore_to_reset",
      "SharedStorage",
      "smem_layout_lse",
      "smem_layout_o",
      "gmem_tiled_copy_O_partial",
      "gmem_tiled_copy_O",
      "gmem_tiled_copy_LSE",
      "s2r_tiled_copy_LSE",
      "seqlen_divmod",
      "head_divmod",
      "varlen"
    ],
    "load_O_partial": [
      "self",
      "gmem_tiled_copy_O_partial",
      "tOrOptr",
      "tOsO_partial",
      "tOhidx",
      "tOpO",
      "tOcO",
      "mO_cur_partial_layout",
      "split",
      "stage"
    ]
  },
  "FlashAttentionBackwardSm100": {
    "arch": [],
    "__init__": [
      "self",
      "head_dim",
      "head_dim_v",
      "is_causal",
      "is_local",
      "qhead_per_kvhead",
      "tile_m",
      "tile_n",
      "is_persistent",
      "deterministic",
      "cluster_size",
      "score_mod",
      "score_mod_bwd",
      "mask_mod",
      "has_aux_tensors",
      "subtile_factor"
    ],
    "_setup_attributes": [
      "self"
    ],
    "_get_tiled_mma": [
      "self"
    ],
    "_setup_smem_layout": [
      "self"
    ],
    "__call__": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mdO",
      "mLSE",
      "mdPsum",
      "mdQaccum",
      "mdK",
      "mdV",
      "softmax_scale",
      "stream",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "softcap",
      "window_size_left",
      "window_size_right",
      "mdQ_semaphore",
      "mdK_semaphore",
      "mdV_semaphore",
      "aux_tensors",
      "blocksparse_tensors"
    ],
    "kernel": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mLSE",
      "mdPsum",
      "mdO",
      "mdV",
      "mdK",
      "mdQaccum",
      "mdV_tma_tensor",
      "mdK_tma_tensor",
      "mdQ_semaphore",
      "mdK_semaphore",
      "mdV_semaphore",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "tma_atom_dO",
      "tma_atom_dV",
      "tma_atom_dK",
      "sQ_layout",
      "sQt_layout",
      "sK_layout",
      "sV_layout",
      "sLSE_layout",
      "sdPsum_layout",
      "sdO_layout",
      "sdOt_layout",
      "sdSt_layout",
      "sdS_layout",
      "sKt_layout",
      "sdQaccum_layout",
      "sdKV_layout",
      "tP_layout",
      "tdS_layout",
      "tiled_mma_S",
      "tiled_mma_dP",
      "tiled_mma_dV",
      "tiled_mma_dK",
      "tiled_mma_dQ",
      "tiled_copy_r2s_dKV",
      "softmax_scale",
      "softmax_scale_log2",
      "window_size_left",
      "window_size_right",
      "tile_sched_params",
      "aux_tensors",
      "fastdiv_mods",
      "blocksparse_tensors"
    ],
    "load": [
      "self",
      "thr_mma_S",
      "thr_mma_dP",
      "thr_mma_dV",
      "mQ",
      "mK",
      "mV",
      "mLSE",
      "mdPsum",
      "mdO",
      "sQ",
      "sK",
      "sV",
      "sLSE",
      "sdPsum",
      "sdO",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "tma_atom_dO",
      "pipeline_Q",
      "pipeline_dO",
      "pipeline_LSE",
      "pipeline_dPsum",
      "cluster_layout_vmnk",
      "block_info",
      "SeqlenInfoCls",
      "TileSchedulerCls",
      "blocksparse_tensors",
      "should_load_Q",
      "should_load_dO"
    ],
    "mma": [
      "self",
      "tiled_mma_S",
      "tiled_mma_dP",
      "tiled_mma_dV",
      "tiled_mma_dK",
      "tiled_mma_dQ",
      "sQ",
      "sQt",
      "sK",
      "sV",
      "sdO",
      "sdOt",
      "sdSt",
      "sdS",
      "sKt",
      "tP",
      "tdS",
      "tStS",
      "tdPtdP",
      "tdVtdV",
      "tdKtdK",
      "tdQtdQ",
      "pipeline_Q_consumer",
      "pipeline_dO",
      "pipeline_S_P",
      "pipeline_dS",
      "pipeline_dKV",
      "pipeline_dP",
      "pipeline_dQ",
      "block_info",
      "SeqlenInfoCls",
      "TileSchedulerCls",
      "blocksparse_tensors"
    ],
    "split_wg": [
      "self",
      "t",
      "wg_idx",
      "num_wg"
    ],
    "apply_score_mod": [
      "self",
      "tSrS_t2r",
      "thr_copy_t2r",
      "thr_mma_S",
      "batch_idx",
      "head_idx",
      "m_block",
      "n_block",
      "softmax_scale",
      "seqlen_info",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "apply_score_mod_bwd": [
      "self",
      "grad_tensor",
      "score_tensor",
      "index_tensor",
      "batch_idx",
      "head_idx",
      "softmax_scale",
      "seqlen_info",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "compute_loop": [
      "self",
      "thr_mma_S",
      "thr_mma_dP",
      "thr_mma_dV",
      "thr_mma_dK",
      "tStS",
      "sLSE",
      "sdPsum",
      "tdVtdV",
      "tdKtdK",
      "mdV",
      "mdK",
      "sdS",
      "tdPtdP",
      "pipeline_LSE",
      "pipeline_dPsum",
      "pipeline_S_P",
      "pipeline_dS",
      "pipeline_dKV",
      "pipeline_dP",
      "softmax_scale",
      "softmax_scale_log2",
      "block_info",
      "SeqlenInfoCls",
      "AttentionMaskCls",
      "TileSchedulerCls",
      "sdV",
      "sdK",
      "mdV_tma_tensor",
      "mdK_tma_tensor",
      "tma_atom_dV",
      "tma_atom_dK",
      "tiled_copy_r2s_dKV",
      "mdK_semaphore",
      "mdV_semaphore",
      "aux_tensors",
      "fastdiv_mods",
      "blocksparse_tensors"
    ],
    "dQacc_reduce": [
      "self",
      "mdQaccum",
      "sdQaccum",
      "thr_mma_dQ",
      "tdQtdQ",
      "pipeline_dQ",
      "block_info",
      "SeqlenInfoCls",
      "TileSchedulerCls",
      "mdQ_semaphore",
      "blocksparse_tensors"
    ],
    "epilogue_dKV": [
      "self",
      "tidx",
      "warp_idx",
      "batch_idx",
      "head_idx",
      "n_block",
      "seqlen",
      "thr_mma_dV",
      "thr_mma_dK",
      "tdVtdV",
      "tdKtdK",
      "mdV",
      "mdK",
      "pipeline_dKV",
      "consumer_state_dKV",
      "softmax_scale"
    ],
    "epilogue_dK_or_dV_tma": [
      "self",
      "tidx",
      "batch_idx",
      "head_idx",
      "n_block",
      "seqlen",
      "thr_mma",
      "tdKVtdKV",
      "mdKV",
      "sdKV",
      "tma_atom_dKV",
      "thr_copy_r2s_dKV",
      "pipeline_dKV",
      "consumer_state_dKV",
      "scale",
      "barrier_id",
      "mdKV_semaphore"
    ]
  },
  "StaticTypes": [],
  "load_cubin_module_data_og": [],
  "cute_compile_og": [],
  "get_max_active_clusters": [
    "cluster_size"
  ],
  "get_device_capacity": [
    "device"
  ],
  "ParamsBase": {
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "ArgumentsBase": {
    "__c_pointers__": [
      "self"
    ],
    "__get_mlir_types__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "load_cubin_module_data_patched": [
    "cubin_data",
    "filepath"
  ],
  "cute_compile_patched": [],
  "PagedKVManager": {
    "create": [
      "mPageTable",
      "mK_paged",
      "mV_paged",
      "page_size_divmod",
      "bidb",
      "bidh",
      "thread_idx",
      "seqlen_k",
      "leftpad_k",
      "n_block_size",
      "head_dim_padded",
      "head_dim_v_padded",
      "num_threads",
      "dtype"
    ],
    "load_page_table": [
      "self",
      "n_block"
    ],
    "load_KV": [
      "self",
      "n_block",
      "sX",
      "K_or_V"
    ]
  },
  "fill_swizzled": [
    "tensor",
    "value"
  ],
  "get_smem_layout_atom": [
    "dtype",
    "k_dim"
  ],
  "gemm": [
    "tiled_mma",
    "acc",
    "tCrA",
    "tCrB",
    "tCsA",
    "tCsB",
    "smem_thr_copy_A",
    "smem_thr_copy_B",
    "hook_fn",
    "A_in_regs",
    "B_in_regs",
    "swap_AB"
  ],
  "gemm_rs": [
    "tiled_mma",
    "acc",
    "tCrA",
    "tCrB",
    "tCsB",
    "smem_thr_copy_B",
    "hook_fn"
  ],
  "clz": [
    "x"
  ],
  "_flash_attn_bwd": [
    "q",
    "k",
    "v",
    "out",
    "dout",
    "lse",
    "softmax_scale",
    "causal",
    "softcap",
    "window_size_left",
    "window_size_right",
    "m_block_size",
    "n_block_size",
    "num_threads",
    "pack_gqa",
    "num_stages_Q",
    "num_stages_dO",
    "SdP_swapAB",
    "dKV_swapAB",
    "dQ_swapAB",
    "AtomLayoutMSdP",
    "AtomLayoutNdKV",
    "AtomLayoutMdQ",
    "V_in_regs",
    "cu_seqlens_q",
    "cu_seqlens_k",
    "seqused_q",
    "seqused_k",
    "max_seqlen_q",
    "max_seqlen_k",
    "deterministic",
    "dq",
    "dk",
    "dv",
    "score_mod",
    "score_mod_bwd",
    "mask_mod",
    "aux_tensors",
    "block_sparse_tensors"
  ],
  "FlashAttnFunc": {
    "forward": [
      "ctx",
      "q",
      "k",
      "v",
      "softmax_scale",
      "causal",
      "window_size",
      "learnable_sink",
      "softcap",
      "num_splits",
      "pack_gqa",
      "deterministic",
      "mask_mod",
      "full_block_cnt",
      "full_block_idx",
      "mask_block_cnt",
      "mask_block_idx"
    ],
    "backward": [
      "ctx",
      "dout"
    ]
  },
  "FlashAttnVarlenFunc": {
    "forward": [
      "ctx",
      "q",
      "k",
      "v",
      "cu_seqlens_q",
      "cu_seqlens_k",
      "seqused_q",
      "seqused_k",
      "max_seqlen_q",
      "max_seqlen_k",
      "page_table",
      "softmax_scale",
      "causal",
      "window_size",
      "learnable_sink",
      "softcap",
      "num_splits",
      "pack_gqa",
      "deterministic",
      "score_mod",
      "aux_tensors"
    ],
    "backward": [
      "ctx",
      "dout"
    ]
  },
  "flash_attn_func": [
    "q",
    "k",
    "v",
    "softmax_scale",
    "causal",
    "window_size",
    "learnable_sink",
    "softcap",
    "num_splits",
    "pack_gqa",
    "deterministic",
    "mask_mod",
    "full_block_cnt",
    "full_block_idx",
    "mask_block_cnt",
    "mask_block_idx"
  ],
  "flash_attn_combine": [
    "out_partial",
    "lse_partial",
    "out",
    "out_dtype",
    "cu_seqlens",
    "seqused",
    "return_lse"
  ],
  "mask_r2p": [
    "X",
    "col_limit",
    "arch",
    "rank1"
  ],
  "mask_r2p_transposed": [
    "X",
    "row_limit_top",
    "num_rep"
  ],
  "AttentionMask": {
    "seqlen_q": [
      "self"
    ],
    "seqlen_k": [
      "self"
    ],
    "apply_mask": [
      "self",
      "acc_S",
      "batch_idx",
      "head_idx",
      "m_block",
      "n_block",
      "thr_mma",
      "mask_seqlen",
      "mask_causal",
      "mask_local",
      "mask_mod",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "apply_mask_sm100": [
      "self",
      "acc_S",
      "m_block",
      "n_block",
      "thr_mma",
      "thr_tmem_load",
      "mask_seqlen",
      "mask_causal",
      "mask_local",
      "mask_mod",
      "batch_idx",
      "head_idx",
      "aux_tensors",
      "fastdiv_mods",
      "check_q_boundary"
    ],
    "apply_mask_sm100_transposed": [
      "self",
      "acc_S",
      "tScS_t2r",
      "t0ScS_t2r",
      "m_block",
      "n_block",
      "mask_seqlen",
      "mask_causal",
      "mask_local",
      "mask_mod",
      "batch_idx",
      "head_idx",
      "aux_tensors",
      "fastdiv_mods",
      "is_full_block",
      "check_m_boundary"
    ]
  },
  "BlockInfo": {
    "get_n_block_min_max": [
      "self",
      "seqlen_info",
      "m_block",
      "split_idx",
      "num_splits"
    ],
    "get_m_block_min_max": [
      "self",
      "seqlen_info",
      "n_block"
    ],
    "get_n_block_min_causal_local_mask": [
      "self",
      "seqlen_info",
      "m_block",
      "n_block_min"
    ],
    "get_n_block_min_before_local_mask": [
      "self",
      "seqlen_info",
      "m_block",
      "n_block_min"
    ]
  },
  "mma_partition_fragment_AB": [
    "thr_mma",
    "sA",
    "sB",
    "swap_AB"
  ],
  "FlashAttentionBackwardSm90": {
    "arch": [],
    "__init__": [
      "self",
      "dtype",
      "head_dim",
      "head_dim_v",
      "qhead_per_kvhead",
      "is_causal",
      "tile_m",
      "tile_n",
      "Q_stage",
      "dO_stage",
      "PdS_stage",
      "SdP_swapAB",
      "dKV_swapAB",
      "dQ_swapAB",
      "AtomLayoutMSdP",
      "AtomLayoutNdKV",
      "AtomLayoutMdQ",
      "num_threads",
      "V_in_regs",
      "score_mod",
      "score_mod_bwd",
      "mask_mod",
      "has_aux_tensors",
      "subtile_factor"
    ],
    "can_implement": [
      "dtype",
      "head_dim",
      "head_dim_v",
      "tile_m",
      "tile_n",
      "Q_stage",
      "num_threads",
      "V_in_regs"
    ],
    "_check_type": [
      "self",
      "mQ_type",
      "mK_type",
      "mV_type",
      "mdO_type",
      "mLSE_type",
      "mdPsum_type",
      "mdQaccum_type",
      "mdK_type",
      "mdV_type"
    ],
    "_setup_attributes": [
      "self"
    ],
    "_get_tiled_mma": [
      "self"
    ],
    "_get_shared_storage_cls": [
      "self"
    ],
    "__call__": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mdO",
      "mLSE",
      "mdPsum",
      "mdQaccum",
      "mdK",
      "mdV",
      "softmax_scale",
      "stream",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "softcap",
      "window_size_left",
      "window_size_right",
      "mdQ_semaphore",
      "mdK_semaphore",
      "mdV_semaphore",
      "aux_tensors",
      "blocksparse_tensors"
    ],
    "kernel": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mdO",
      "mdK",
      "mdV",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "tma_atom_dO",
      "tma_atom_dK",
      "tma_atom_dV",
      "mLSE",
      "mdPsum",
      "mdQaccum",
      "sQ_layout",
      "sK_layout",
      "sV_layout",
      "sPdS_layout",
      "sdO_layout",
      "sdQaccum_layout",
      "sdKVaccum_layout",
      "r2s_tiled_copy_dQaccum",
      "r2s_tiled_copy_dKVaccum",
      "tiled_mma_SdP",
      "tiled_mma_dK",
      "tiled_mma_dV",
      "tiled_mma_dQ",
      "softmax_scale_log2",
      "softmax_scale",
      "tile_sched_params",
      "TileScheduler",
      "SharedStorage",
      "aux_tensors",
      "fastdiv_mods",
      "blocksparse_tensors",
      "qhead_per_kvhead_divmod"
    ],
    "load": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mdO",
      "mLSE",
      "mdPsum",
      "sQ",
      "sK",
      "sV",
      "sdO",
      "sLSE",
      "sdPsum",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "tma_atom_dO",
      "pipeline_Q",
      "pipeline_dO",
      "block_info",
      "SeqlenInfoCls",
      "TileSchedulerCls",
      "blocksparse_tensors",
      "qhead_per_kvhead_divmod"
    ],
    "apply_score_mod": [
      "self",
      "acc_S",
      "thr_mma_SdP",
      "batch_idx",
      "head_idx",
      "m_block",
      "n_block",
      "softmax_scale",
      "seqlen_info",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "apply_score_mod_bwd": [
      "self",
      "grad_tensor",
      "score_tensor",
      "thr_mma_SdP",
      "batch_idx",
      "head_idx",
      "m_block",
      "n_block",
      "softmax_scale",
      "seqlen_info",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "mma": [
      "self",
      "tiled_mma_SdP",
      "tiled_mma_dK",
      "tiled_mma_dV",
      "tiled_mma_dQ",
      "mdK",
      "mdV",
      "mdQaccum",
      "sQ",
      "sK",
      "sV",
      "sdO",
      "sP",
      "sdS",
      "sLSE",
      "sdPsum",
      "sdQaccum",
      "pipeline_Q",
      "pipeline_dO",
      "tidx",
      "tma_atom_dK",
      "tma_atom_dV",
      "r2s_tiled_copy_dQaccum",
      "r2s_tiled_copy_dKVaccum",
      "sdKVaccum_layout",
      "softmax_scale_log2",
      "softmax_scale",
      "block_info",
      "SeqlenInfoCls",
      "AttentionMaskCls",
      "TileSchedulerCls",
      "aux_tensors",
      "fastdiv_mods",
      "blocksparse_tensors",
      "qhead_per_kvhead_divmod"
    ],
    "mma_one_m_block": [
      "self",
      "m_block",
      "consumer_state_Q",
      "consumer_state_dO",
      "warp_group_idx",
      "mma_qk_fn",
      "mma_dov_fn",
      "mma_pdo_fn",
      "mma_dsq_fn",
      "mma_dsk_fn",
      "pipeline_Q",
      "pipeline_dO",
      "tLSEsLSE",
      "tLSEsdPsum",
      "tPsP",
      "tdSsdS",
      "tdQsdQaccum",
      "smem_thr_copy_PdS",
      "smem_thr_copy_dQaccum",
      "softmax_scale_log2",
      "mask_fn",
      "dKV_accumulate",
      "thr_mma_SdP",
      "batch_idx",
      "head_idx",
      "n_block",
      "softmax_scale",
      "seqlen",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "epilogue_dKV": [
      "self",
      "acc_dV",
      "mdV",
      "sV",
      "acc_dK",
      "mdK",
      "sK",
      "seqlen",
      "tma_atom_dK",
      "tma_atom_dV",
      "tiled_mma_dK",
      "tiled_mma_dV",
      "r2s_tiled_copy_dKVaccum",
      "sdKVaccum_layout",
      "tidx",
      "n_block",
      "head_idx",
      "batch_idx",
      "qhead_per_kvhead_divmod"
    ],
    "dQaccum_store": [
      "self",
      "mdQaccum",
      "sdQaccum",
      "block_info",
      "TileSchedulerCls",
      "SeqlenInfoCls",
      "blocksparse_tensors"
    ]
  },
  "gemm_w_idx": [
    "tiled_mma",
    "acc",
    "tCrA",
    "tCrB",
    "A_idx",
    "B_idx",
    "zero_init",
    "swap_AB"
  ],
  "gemm_ptx_w_idx": [
    "tiled_mma",
    "acc",
    "tCrA",
    "tCrB",
    "sA",
    "sB",
    "A_idx",
    "B_idx",
    "zero_init"
  ],
  "i64_to_i32x2": [
    "i"
  ],
  "gemm_ptx": [
    "op",
    "acc",
    "tCrA",
    "tCrB",
    "sA",
    "sB",
    "zero_init"
  ],
  "gemm_ptx_loop": [
    "op",
    "acc",
    "tCrA",
    "tCrB",
    "sA",
    "sB",
    "zero_init"
  ],
  "gemm_ptx_partial": [
    "op",
    "acc_tmem_addr",
    "tCrA",
    "tCrB",
    "sA",
    "sB",
    "mbar_ptr",
    "mbar_phase",
    "zero_init",
    "tA_addr"
  ],
  "gemm_ptx_partial1": [
    "op",
    "acc_tmem_addr",
    "tCrA",
    "tCrB",
    "sA_base_addr_for_desc",
    "sA_addr_offset_for_desc",
    "sA_stage",
    "sB_base_addr_for_desc",
    "sB_addr_offset_for_desc",
    "sB_stage",
    "sA_layout",
    "sB_layout",
    "sA_swizzle",
    "sB_swizzle",
    "zero_init"
  ],
  "ceildiv": [
    "a",
    "b"
  ],
  "BlockSparseTensors": {
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "BlockSparseTensorsTorch": {},
  "_expand_sparsity_tensor": [
    "tensor",
    "expected_shape",
    "tensor_name",
    "context",
    "hint"
  ],
  "_check_and_expand_block": [
    "name",
    "cnt",
    "idx",
    "expected_count_shape",
    "expected_index_shape",
    "context",
    "hint"
  ],
  "get_block_sparse_expected_shapes": [
    "batch_size",
    "num_head",
    "seqlen_q",
    "seqlen_k",
    "m_block_size",
    "n_block_size",
    "q_stage"
  ],
  "get_block_sparse_expected_shapes_bwd": [
    "batch_size",
    "num_head",
    "seqlen_q",
    "seqlen_k",
    "m_block_size",
    "n_block_size",
    "subtile_factor"
  ],
  "normalize_block_sparse_tensors": [
    "tensors"
  ],
  "is_block_sparsity_enabled": [
    "tensors"
  ],
  "to_cute_block_sparse_tensors": [
    "tensors",
    "enable_tvm_ffi"
  ],
  "fast_sampling": [
    "mask_mod"
  ],
  "PackGQA": {
    "__init__": [
      "self",
      "m_block_size",
      "head_dim_padded",
      "check_hdim_oob",
      "qhead_per_kvhead"
    ],
    "compute_ptr": [
      "self",
      "tensor",
      "cRows",
      "tidx",
      "block",
      "threads_per_row",
      "num_threads"
    ],
    "load_Q": [
      "self",
      "mQ",
      "sQ",
      "gmem_tiled_copy",
      "tidx",
      "block",
      "seqlen"
    ],
    "store_LSE": [
      "self",
      "mLSE",
      "tLSErLSE",
      "tiled_mma",
      "tidx",
      "block",
      "seqlen"
    ],
    "store_O": [
      "self",
      "mO",
      "tOrO",
      "gmem_tiled_copy",
      "tidx",
      "block",
      "seqlen"
    ]
  },
  "fma_packed_f32x2": [],
  "mul_packed_f32x2": [],
  "add_packed_f32x2": [],
  "sub_packed_f32x2": [],
  "hash_callable": [
    "func"
  ],
  "create_softcap_scoremod": [
    "softcap_val"
  ],
  "convert_from_dlpack": [
    "x",
    "leading_dim",
    "alignment",
    "divisibility"
  ],
  "convert_from_dlpack_leading_static": [
    "x",
    "leading_dim",
    "alignment",
    "static_modes",
    "stride_order"
  ],
  "make_tiled_copy_A": [
    "copy_atom",
    "tiled_mma",
    "swapAB"
  ],
  "make_tiled_copy_B": [
    "copy_atom",
    "tiled_mma",
    "swapAB"
  ],
  "mma_make_fragment_A": [
    "smem",
    "thr_mma",
    "swapAB"
  ],
  "mma_make_fragment_B": [
    "smem",
    "thr_mma",
    "swapAB"
  ],
  "get_smem_store_atom": [
    "arch",
    "element_type",
    "transpose"
  ],
  "warp_reduce": [
    "val",
    "op",
    "width"
  ],
  "convert_layout_acc_mn": [
    "acc_layout",
    "transpose"
  ],
  "make_acc_tensor_mn_view": [
    "acc",
    "transpose"
  ],
  "convert_layout_acc_frgA": [
    "acc_layout"
  ],
  "make_acc_tensor_frgA_view": [
    "acc"
  ],
  "select": [
    "a",
    "mode"
  ],
  "transpose_view": [
    "a"
  ],
  "parse_swizzle_from_pointer": [
    "ptr"
  ],
  "exp2f": [
    "x"
  ],
  "log2f": [
    "a"
  ],
  "logf": [
    "a"
  ],
  "fmax": [
    "a",
    "b",
    "c"
  ],
  "fmax_reduce": [
    "x",
    "init_val",
    "arch"
  ],
  "fadd_reduce": [
    "x",
    "init_val",
    "arch"
  ],
  "atomic_add_fp32": [
    "a",
    "gmem_ptr"
  ],
  "elem_pointer": [
    "x",
    "coord"
  ],
  "elem_pointer_i64": [
    "x",
    "coord"
  ],
  "predicate_k": [
    "tAcA",
    "limit"
  ],
  "canonical_warp_group_idx": [
    "sync"
  ],
  "shuffle_sync": [
    "value",
    "offset",
    "width"
  ],
  "shr_u32": [
    "val",
    "shift"
  ],
  "warp_prefix_sum": [
    "val",
    "lane"
  ],
  "cvt_f16x2_f32": [
    "a",
    "b",
    "to_dtype"
  ],
  "cvt_f16": [
    "src",
    "dst_or_dtype"
  ],
  "evaluate_polynomial": [
    "x",
    "poly"
  ],
  "evaluate_polynomial_2": [
    "x",
    "y",
    "poly"
  ],
  "add_round_down": [
    "x",
    "y"
  ],
  "combine_int_frac_ex2": [
    "x_rounded",
    "frac_ex2"
  ],
  "ex2_emulation": [
    "x"
  ],
  "ex2_emulation_2": [
    "x",
    "y"
  ],
  "e2e_asm2": [
    "x",
    "y"
  ],
  "domain_offset_aligned": [
    "coord",
    "tensor"
  ],
  "domain_offset_i64": [
    "coord",
    "tensor"
  ],
  "coord_offset_i64": [
    "tensor",
    "idx",
    "dim"
  ],
  "scalar_to_ssa": [
    "a",
    "dtype"
  ],
  "ssa_to_scalar": [
    "val"
  ],
  "SeqlenInfo": {
    "create": [
      "batch_idx",
      "seqlen_static",
      "cu_seqlens",
      "seqused"
    ]
  },
  "SeqlenInfoQK": {
    "create": [
      "batch_idx",
      "seqlen_q_static",
      "seqlen_k_static",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "tile_m",
      "tile_n"
    ],
    "offset_batch_Q": [
      "self",
      "mQ",
      "batch_idx",
      "dim",
      "padded"
    ],
    "offset_batch_K": [
      "self",
      "mK",
      "batch_idx",
      "dim",
      "padded"
    ]
  },
  "load_block_list": [
    "block_indices",
    "block_count",
    "load_q_with_first",
    "first_block_preloaded",
    "kv_producer_state",
    "load_Q",
    "load_K",
    "load_V",
    "pipeline_k",
    "pipeline_v",
    "use_tma_q",
    "tma_q_bytes",
    "intra_wg_overlap"
  ],
  "finish_overlap_v_load": [
    "block_indices",
    "block_count",
    "load_V",
    "pipeline_v",
    "kv_producer_state"
  ],
  "sparse_tensor_m_block": [
    "m_block",
    "qhead_per_kvhead"
  ],
  "produce_block_sparse_loads": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "m_block",
    "kv_producer_state",
    "load_Q",
    "load_K",
    "load_V",
    "pipeline_k",
    "pipeline_v",
    "use_tma_q",
    "tma_q_bytes",
    "intra_wg_overlap",
    "qhead_per_kvhead"
  ],
  "consume_block_sparse_loads": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "m_block",
    "seqlen",
    "kv_consumer_state",
    "mma_pv_fn",
    "mma_one_n_block",
    "process_first_half_block",
    "process_last_half_block",
    "mask_fn",
    "score_mod_fn",
    "O_should_accumulate",
    "mask_mod",
    "fastdiv_mods",
    "intra_wg_overlap",
    "warp_scheduler_barrier_sync",
    "warp_scheduler_barrier_arrive",
    "qhead_per_kvhead"
  ],
  "load_block_list_sm100": [
    "block_indices",
    "block_count",
    "load_q_with_first",
    "m_block",
    "q_stage",
    "kv_producer_state",
    "load_Q",
    "load_K",
    "load_V",
    "pipeline_kv"
  ],
  "produce_block_sparse_loads_sm100": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "m_block",
    "kv_producer_state",
    "load_Q",
    "load_K",
    "load_V",
    "pipeline_kv",
    "q_stage",
    "q_producer_phase",
    "qhead_per_kvhead"
  ],
  "get_total_block_count": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "m_block",
    "qhead_per_kvhead"
  ],
  "handle_block_sparse_empty_tile_correction_sm100": [
    "tidx",
    "q_stage",
    "m_block_size",
    "qhead_per_kvhead",
    "pack_gqa",
    "is_split_kv",
    "learnable_sink",
    "mLSE",
    "seqlen",
    "m_block",
    "head_idx",
    "batch_idx",
    "split_idx",
    "sScale",
    "stats",
    "correction_epilogue",
    "thr_mma_pv",
    "tOtOs",
    "sO",
    "mbar_ptr",
    "mbar_softmax_corr_full_offset",
    "mbar_softmax_corr_empty_offset",
    "mbar_P_full_O_rescaled_offset",
    "mbar_P_full_2_offset",
    "mbar_corr_epi_full_offset",
    "mbar_corr_epi_empty_offset",
    "softmax_corr_consumer_phase",
    "o_corr_consumer_phase",
    "corr_epi_producer_phase",
    "softmax_scale_log2",
    "mO_cur",
    "gO",
    "gmem_tiled_copy_O"
  ],
  "softmax_block_sparse_sm100": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "m_block",
    "softmax_step",
    "mask_fn",
    "mask_fn_none",
    "mma_si_consumer_phase",
    "si_corr_producer_phase",
    "s0_s1_sequence_phase",
    "mbar_ptr",
    "mbar_softmax_corr_full_offset",
    "mbar_softmax_corr_empty_offset",
    "mbar_P_full_O_rescaled_offset",
    "mbar_P_full_2_offset",
    "q_stage",
    "stage_idx",
    "check_m_boundary",
    "qhead_per_kvhead"
  ],
  "get_total_q_block_count_bwd": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "n_block",
    "subtile_factor",
    "m_block_max"
  ],
  "produce_block_sparse_q_loads_bwd_sm100": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "n_block",
    "producer_state_Q_LSE",
    "producer_state_dO_dPsum",
    "pipeline_Q",
    "pipeline_LSE",
    "pipeline_dO",
    "pipeline_dPsum",
    "load_K",
    "load_V",
    "load_Q",
    "load_dO",
    "copy_stats",
    "gLSE",
    "sLSE",
    "gdPsum",
    "sdPsum",
    "tma_copy_bytes_K",
    "tma_copy_bytes_V",
    "should_load_Q",
    "should_load_dO",
    "subtile_factor",
    "m_block_max"
  ],
  "get_block_sparse_iteration_info_bwd": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "n_block",
    "subtile_factor",
    "m_block_max"
  ],
  "get_m_block_from_iter_bwd": [
    "iter_idx",
    "curr_q_cnt",
    "curr_q_idx",
    "curr_full_cnt",
    "curr_full_idx",
    "subtile_factor",
    "m_block_max"
  ],
  "_load_q_do_block_sm90": [
    "m_block",
    "producer_state_Q",
    "producer_state_dO",
    "pipeline_Q",
    "pipeline_dO",
    "load_K",
    "load_V",
    "load_Q",
    "load_dO",
    "load_LSE",
    "load_dPsum",
    "tma_copy_bytes_K",
    "tma_copy_bytes_V",
    "Q_stage_eq_dO_stage",
    "load_kv"
  ],
  "produce_block_sparse_q_loads_bwd_sm90": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "n_block",
    "producer_state_Q",
    "producer_state_dO",
    "pipeline_Q",
    "pipeline_dO",
    "load_K",
    "load_V",
    "load_Q",
    "load_dO",
    "load_LSE",
    "load_dPsum",
    "tma_copy_bytes_K",
    "tma_copy_bytes_V",
    "Q_stage_eq_dO_stage",
    "subtile_factor",
    "m_block_max"
  ],
  "consume_block_sparse_mma_bwd_sm90": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "n_block",
    "consumer_state_Q",
    "consumer_state_dO",
    "mma_one_m_block_fn",
    "mask",
    "mask_mod",
    "is_causal",
    "is_local",
    "thr_mma_SdP",
    "softmax_scale",
    "seqlen",
    "subtile_factor",
    "m_block_max",
    "aux_tensors",
    "fastdiv_mods"
  ],
  "_store_one_dQaccum_sm90": [
    "m_block",
    "sdQaccum",
    "gdQaccum",
    "num_mma_warp_groups",
    "num_threads_per_warp_group",
    "tma_copy_bytes_dQ"
  ],
  "dQaccum_store_block_sparse_bwd_sm90": [
    "blocksparse_tensors",
    "batch_idx",
    "head_idx",
    "n_block",
    "sdQaccum",
    "gdQaccum",
    "subtile_factor",
    "m_block_max",
    "num_mma_warp_groups",
    "num_threads_per_warp_group",
    "tma_copy_bytes_dQ"
  ],
  "Major": {
    "K": [],
    "MN": []
  },
  "ScaleIn": {
    "One": [],
    "Neg": []
  },
  "Saturate": {
    "False_": [],
    "True_": []
  },
  "CFormat": {
    "F16": [],
    "F32": [],
    "S32": []
  },
  "F16F32Format": {
    "F16": [],
    "BF16": [],
    "TF32": []
  },
  "S8Format": {
    "UINT8": [],
    "INT8": []
  },
  "MXF8F6F4Format": {
    "E4M3": [],
    "E5M2": [],
    "E2M3": [],
    "E3M2": [],
    "E2M1": []
  },
  "MaxShift": {
    "NoShift": [],
    "MaxShift8": [],
    "MaxShift16": [],
    "MaxShift32": []
  },
  "to_UMMA_format": [
    "cutlass_type"
  ],
  "to_C_format": [
    "cutlass_type"
  ],
  "make_instr_desc": [
    "a_type",
    "b_type",
    "c_type",
    "M",
    "N",
    "a_major",
    "b_major",
    "a_neg",
    "b_neg",
    "c_sat",
    "is_sparse",
    "max_shift"
  ],
  "mma_op_to_idesc": [
    "op"
  ],
  "LayoutType": {
    "SWIZZLE_NONE": [],
    "SWIZZLE_128B_BASE32B": [],
    "SWIZZLE_128B": [],
    "SWIZZLE_64B": [],
    "SWIZZLE_32B": []
  },
  "_layout_type": [
    "swizzle"
  ],
  "make_smem_desc_base": [
    "layout",
    "swizzle",
    "major"
  ],
  "make_smem_desc_start_addr": [
    "start_addr"
  ],
  "FlashAttentionBackwardPostprocess": {
    "__init__": [
      "self",
      "dtype",
      "head_dim",
      "arch",
      "tile_m",
      "num_threads",
      "AtomLayoutMdQ",
      "dQ_swapAB"
    ],
    "can_implement": [
      "dtype",
      "head_dim",
      "tile_m",
      "num_threads"
    ],
    "_get_tiled_mma": [
      "self"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "mdQaccum",
      "mdQ",
      "scale",
      "mCuSeqlensQ",
      "mSeqUsedQ",
      "stream"
    ],
    "kernel": [
      "self",
      "mdQaccum",
      "mdQ",
      "mCuSeqlensQ",
      "mSeqUsedQ",
      "scale",
      "tiled_mma",
      "dQ_swapAB",
      "sdQaccum_layout",
      "sdQ_layout",
      "g2s_tiled_copy_dQaccum",
      "s2r_tiled_copy_dQaccum",
      "gmem_tiled_copy_dQ",
      "tile_sched_params",
      "TileScheduler"
    ]
  },
  "cvt_copy": [
    "atom",
    "src",
    "dst"
  ],
  "load_s2r": [
    "src"
  ],
  "get_copy_atom": [
    "dtype",
    "num_copy_elems",
    "is_async"
  ],
  "make_tmem_copy": [
    "tmem_copy_atom",
    "num_wg"
  ],
  "copy": [
    "src",
    "dst"
  ],
  "tiled_copy_1d": [
    "dtype",
    "num_threads",
    "num_copy_elems",
    "is_async"
  ],
  "tiled_copy_2d": [
    "dtype",
    "major_mode_size",
    "num_threads",
    "is_async"
  ],
  "atomic_add_fp32x4": [
    "a",
    "b",
    "c",
    "d",
    "gmem_ptr"
  ],
  "set_block_rank": [
    "smem_ptr",
    "peer_cta_rank_in_cluster"
  ],
  "store_shared_remote_fp32x4": [
    "a",
    "b",
    "c",
    "d",
    "smem_ptr",
    "mbar_ptr",
    "peer_cta_rank_in_cluster"
  ],
  "cpasync_bulk_g2s": [
    "gmem_ptr",
    "smem_ptr",
    "tma_bar_ptr",
    "size"
  ],
  "cpasync_reduce_bulk_add_f32": [
    "smem_ptr",
    "gmem_ptr",
    "store_bytes"
  ],
  "cpasync_bulk_get_copy_fn": [
    "src_tensor",
    "dst_tensor",
    "single_stage"
  ],
  "tma_get_copy_fn": [
    "atom",
    "cta_coord",
    "cta_layout",
    "src_tensor",
    "dst_tensor",
    "filter_zeros",
    "single_stage"
  ],
  "tma_producer_copy_fn": [
    "copy",
    "pipeline"
  ],
  "__all__": [],
  "IndexFirstAxis": {
    "forward": [
      "ctx",
      "input",
      "indices"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "index_first_axis": [],
  "IndexPutFirstAxis": {
    "forward": [
      "ctx",
      "values",
      "indices",
      "first_axis_dim"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "index_put_first_axis": [],
  "unpad_input": [
    "hidden_states",
    "attention_mask",
    "unused_mask"
  ],
  "pad_input": [
    "hidden_states",
    "indices",
    "batch",
    "seqlen"
  ],
  "generate_random_padding_mask": [
    "max_seqlen",
    "batch_size",
    "device",
    "mode",
    "zero_lengths"
  ],
  "generate_qkv": [
    "q",
    "k",
    "v",
    "query_padding_mask",
    "key_padding_mask",
    "qv",
    "kvpacked",
    "qkvpacked",
    "query_unused_mask",
    "key_unused_mask"
  ],
  "construct_local_mask": [
    "seqlen_q",
    "seqlen_k",
    "window_size",
    "sink_token_length",
    "query_padding_mask",
    "key_padding_mask",
    "key_leftpad",
    "device"
  ],
  "construct_chunk_mask": [
    "seqlen_q",
    "seqlen_k",
    "attention_chunk",
    "query_padding_mask",
    "key_padding_mask",
    "key_leftpad",
    "device"
  ],
  "attention_ref": [
    "q",
    "k",
    "v",
    "query_padding_mask",
    "key_padding_mask",
    "key_leftpad",
    "attn_bias",
    "dropout_p",
    "dropout_mask",
    "causal",
    "qv",
    "q_descale",
    "k_descale",
    "v_descale",
    "window_size",
    "attention_chunk",
    "sink_token_length",
    "learnable_sink",
    "softcap",
    "upcast",
    "reorder_ops",
    "intermediate_dtype"
  ],
  "pipeline_init_wait": [
    "cta_layout_vmnk"
  ],
  "_sync": [
    "group"
  ],
  "PipelineStateSimple": {
    "__init__": [
      "self",
      "stages",
      "phase_index"
    ],
    "clone": [
      "self"
    ],
    "stages": [
      "self"
    ],
    "index": [
      "self"
    ],
    "phase": [
      "self"
    ],
    "advance": [
      "self"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "make_pipeline_state": [
    "type",
    "stages"
  ],
  "PipelineTmaAsync": {
    "create": [],
    "producer_acquire": [
      "self",
      "state",
      "try_acquire_token",
      "extra_tx_count"
    ]
  },
  "PipelineTmaUmma": {
    "create": [],
    "producer_acquire": [
      "self",
      "state",
      "try_acquire_token",
      "extra_tx_count"
    ]
  },
  "Softmax": {
    "create": [
      "scale_log2",
      "num_rows",
      "arch",
      "softmax_scale"
    ],
    "reset": [
      "self"
    ],
    "_compute_row_max": [
      "self",
      "acc_S_row",
      "init_val"
    ],
    "_compute_row_sum": [
      "self",
      "acc_S_row_exp",
      "init_val"
    ],
    "online_softmax": [
      "self",
      "acc_S",
      "is_first",
      "check_inf"
    ],
    "finalize": [
      "self",
      "final_scale",
      "sink_val"
    ],
    "rescale_O": [
      "self",
      "acc_O",
      "row_scale"
    ]
  },
  "SoftmaxSm100": {
    "create": [
      "scale_log2",
      "rescale_threshold",
      "softmax_scale"
    ],
    "update_row_max": [
      "self",
      "acc_S_row",
      "is_first"
    ],
    "update_row_sum": [
      "self",
      "acc_S_row_exp",
      "row_scale",
      "is_first"
    ],
    "scale_subtract_rowmax": [
      "self",
      "acc_S_row",
      "row_max"
    ],
    "apply_exp2_convert": [
      "self",
      "acc_S_row",
      "acc_S_row_converted",
      "e2e",
      "e2e_freq",
      "e2e_res",
      "e2e_frg_limit"
    ],
    "scale_apply_exp2_convert": [
      "self",
      "acc_S_row",
      "row_max",
      "acc_S_row_converted"
    ]
  },
  "floor_if_packed": [
    "q_idx",
    "qhead_per_kvhead"
  ],
  "apply_score_mod_inner": [
    "score_tensor",
    "index_tensor",
    "score_mod",
    "batch_idx",
    "head_idx",
    "softmax_scale",
    "vec_size",
    "qk_acc_dtype",
    "aux_tensors",
    "fastdiv_mods",
    "seqlen_info",
    "constant_q_idx",
    "qhead_per_kvhead",
    "transpose_indices"
  ],
  "apply_score_mod_bwd_inner": [
    "grad_tensor",
    "score_tensor",
    "index_tensor",
    "score_mod_bwd",
    "batch_idx",
    "head_idx",
    "softmax_scale",
    "vec_size",
    "qk_acc_dtype",
    "aux_tensors",
    "fastdiv_mods",
    "seqlen_info",
    "constant_q_idx",
    "qhead_per_kvhead",
    "transpose_indices"
  ],
  "FlashAttentionBackwardSm80": {
    "__init__": [
      "self",
      "dtype",
      "head_dim",
      "head_dim_v",
      "qhead_per_kvhead",
      "m_block_size",
      "n_block_size",
      "num_stages_Q",
      "num_stages_dO",
      "num_threads",
      "pack_gqa",
      "is_causal",
      "SdP_swapAB",
      "dKV_swapAB",
      "dQ_swapAB",
      "AtomLayoutMSdP",
      "AtomLayoutNdKV",
      "AtomLayoutMdQ",
      "V_in_regs"
    ],
    "can_implement": [
      "dtype",
      "head_dim",
      "head_dim_v",
      "m_block_size",
      "n_block_size",
      "num_stages_Q",
      "num_stages_dO",
      "num_threads",
      "is_causal",
      "V_in_regs"
    ],
    "_check_type": [
      "self",
      "mQ_type",
      "mK_type",
      "mV_type",
      "mdO_type",
      "mLSE_type",
      "mdPsum_type",
      "mdQaccum_type",
      "mdK_type",
      "mdV_type",
      "mCuSeqlensQ_type",
      "mCuSeqlensK_type",
      "mSeqUsedQ_type",
      "mSeqUsedK_type"
    ],
    "_setup_attributes": [
      "self"
    ],
    "_get_tiled_mma": [
      "self"
    ],
    "_get_shared_storage_cls": [
      "self"
    ],
    "__call__": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mdO",
      "mLSE",
      "mdPsum",
      "mdQaccum",
      "mdK",
      "mdV",
      "softmax_scale",
      "stream",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "softcap",
      "window_size_left",
      "window_size_right",
      "mdQ_semaphore"
    ],
    "kernel": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mdO",
      "mLSE",
      "mdPsum",
      "mdQaccum",
      "mdK",
      "mdV",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "softmax_scale",
      "softmax_scale_log2",
      "sQ_layout",
      "sK_layout",
      "sV_layout",
      "sdO_layout",
      "sPdS_layout",
      "sLSE_layout",
      "sLSEMma_layout",
      "gmem_tiled_copy_QK",
      "gmem_tiled_copy_VdO",
      "gmem_tiled_copy_dK",
      "gmem_tiled_copy_dV",
      "gmem_tiled_copy_LSE",
      "gmem_tiled_copy_dQaccum",
      "tiled_mma_sdp",
      "tiled_mma_dkv",
      "tiled_mma_dq",
      "SharedStorage",
      "tile_sched_params",
      "TileScheduler"
    ],
    "compute_one_m_block": [
      "self",
      "m_block",
      "smem_pipe_read_q",
      "smem_pipe_read_do",
      "smem_pipe_write_q",
      "smem_pipe_write_do",
      "mma_params",
      "smem_copy_params",
      "gmem_copy_params",
      "load_Q_LSE",
      "load_dO_dPsum",
      "m_block_max",
      "softmax_scale_log2",
      "mask_fn"
    ],
    "epilogue": [
      "self",
      "acc_dK",
      "acc_dV",
      "mdK",
      "mdV",
      "sdK",
      "sdV",
      "gmem_tiled_copy_dK",
      "gmem_tiled_copy_dV",
      "tiled_mma",
      "tidx",
      "n_block",
      "num_head",
      "batch_size",
      "seqlen",
      "d_head",
      "d_head_v"
    ],
    "advance_pipeline": [
      "self",
      "pipeline_index",
      "num_stages"
    ],
    "load_K": [
      "self",
      "gmem_thr_copy",
      "tKgK",
      "tKsK",
      "block",
      "seqlen",
      "headdim"
    ],
    "load_V": [
      "self",
      "gmem_thr_copy",
      "tVgV",
      "tVsV",
      "block",
      "seqlen",
      "headdim"
    ],
    "load_Q_LSE": [
      "self",
      "gmem_tiled_copy_Q",
      "gmem_tiled_copy_LSE",
      "tQgQ",
      "tQsQ",
      "tQcQ",
      "t0QcQ",
      "tQpQ",
      "tLSEgLSE",
      "tLSEsLSE",
      "tLSEcLSE",
      "block",
      "smem_pipe_write_q",
      "seqlen"
    ],
    "load_dO_dPsum": [
      "self",
      "gmem_tiled_copy_dO",
      "gmem_tiled_copy_dPsum",
      "tdOgdO",
      "tdOsdO",
      "tdOcdO",
      "t0dOcdO",
      "tdOpdO",
      "tdPsumgdPsum",
      "tdPsumsdPsum",
      "tdPsumcdPsum",
      "block",
      "smem_pipe_write_q",
      "seqlen"
    ]
  },
  "gemm_zero_init": [
    "tiled_mma",
    "shape",
    "tCrA",
    "tCrB",
    "A_idx",
    "B_idx",
    "wg_wait",
    "swap_AB"
  ],
  "make_smem_layout": [
    "dtype",
    "layout",
    "shape",
    "stage"
  ],
  "ld_acquire": [
    "lock_ptr"
  ],
  "red_relaxed": [
    "lock_ptr",
    "val"
  ],
  "red_release": [
    "lock_ptr",
    "val"
  ],
  "wait_eq": [
    "lock_ptr",
    "thread_idx",
    "flag_offset",
    "val"
  ],
  "arrive_inc": [
    "lock_ptr",
    "thread_idx",
    "flag_offset",
    "val"
  ],
  "BlockSparsityKernel": {
    "__init__": [
      "self",
      "mask_mod",
      "tile_mn",
      "compute_full_blocks",
      "use_aux_tensors",
      "use_fast_sampling"
    ],
    "__call__": [
      "self",
      "blocksparse_tensors",
      "seqlen_q",
      "seqlen_k",
      "aux_tensors"
    ],
    "kernel": [
      "self",
      "mask_cnt",
      "mask_idx",
      "full_cnt",
      "full_idx",
      "num_n_blocks",
      "seqlen_q",
      "seqlen_k",
      "aux_tensors"
    ]
  },
  "compute_block_sparsity": [
    "tile_m",
    "tile_n",
    "batch_size",
    "num_heads",
    "seqlen_q",
    "seqlen_k",
    "mask_mod",
    "aux_tensors",
    "device",
    "compute_full_blocks",
    "use_fast_sampling"
  ],
  "FlashAttentionBackwardPreprocess": {
    "__init__": [
      "self",
      "dtype",
      "head_dim",
      "arch",
      "m_block_size",
      "num_threads"
    ],
    "can_implement": [
      "dtype",
      "head_dim",
      "m_block_size",
      "num_threads"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "mO",
      "mdO",
      "mdPsum",
      "mLSE",
      "mLSElog2",
      "mdQaccum",
      "mCuSeqlensQ",
      "mSeqUsedQ",
      "stream"
    ],
    "kernel": [
      "self",
      "mO",
      "mdO",
      "mdPsum",
      "mLSE",
      "mLSElog2",
      "mdQaccum",
      "mCuSeqlensQ",
      "mSeqUsedQ",
      "gmem_tiled_copy_O",
      "gmem_tiled_copy_dQaccum",
      "tile_sched_params",
      "TileScheduler"
    ]
  },
  "WorkTileInfo": {
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "TileSchedulerArguments": {},
  "SingleTileScheduler": {
    "__init__": [
      "self",
      "params",
      "blk_coord"
    ],
    "to_underlying_arguments": [
      "args"
    ],
    "create": [
      "params"
    ],
    "get_grid_shape": [
      "params"
    ],
    "get_current_work": [
      "self"
    ],
    "initial_work_tile_info": [
      "self"
    ],
    "prefetch_next_work": [
      "self"
    ],
    "advance_to_next_work": [
      "self"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "StaticPersistentTileScheduler": {
    "__init__": [
      "self",
      "params",
      "tile_idx"
    ],
    "to_underlying_arguments": [
      "args"
    ],
    "create": [
      "params"
    ],
    "get_grid_shape": [
      "params"
    ],
    "get_current_work": [
      "self"
    ],
    "initial_work_tile_info": [
      "self"
    ],
    "prefetch_next_work": [
      "self"
    ],
    "advance_to_next_work": [
      "self"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "SingleTileLPTScheduler": {
    "__init__": [
      "self",
      "params",
      "tile_idx",
      "split_idx"
    ],
    "to_underlying_arguments": [
      "args"
    ],
    "create": [
      "params"
    ],
    "get_grid_shape": [
      "params"
    ],
    "get_current_work": [
      "self"
    ],
    "initial_work_tile_info": [
      "self"
    ],
    "prefetch_next_work": [
      "self"
    ],
    "advance_to_next_work": [
      "self"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "SingleTileLPTBwdScheduler": {
    "__init__": [
      "self",
      "params",
      "tile_idx"
    ],
    "to_underlying_arguments": [
      "args"
    ],
    "create": [
      "params"
    ],
    "get_grid_shape": [
      "params"
    ],
    "get_current_work": [
      "self"
    ],
    "initial_work_tile_info": [
      "self"
    ],
    "prefetch_next_work": [
      "self"
    ],
    "advance_to_next_work": [
      "self"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "SingleTileVarlenScheduler": {
    "__init__": [
      "self",
      "params",
      "tile_idx",
      "split_idx"
    ],
    "to_underlying_arguments": [
      "args"
    ],
    "create": [
      "params"
    ],
    "get_grid_shape": [
      "params"
    ],
    "_get_num_m_blocks": [
      "self",
      "lane",
      "bidb_start"
    ],
    "get_current_work": [
      "self"
    ],
    "initial_work_tile_info": [
      "self"
    ],
    "prefetch_next_work": [
      "self"
    ],
    "advance_to_next_work": [
      "self"
    ],
    "__extract_mlir_values__": [
      "self"
    ],
    "__new_from_mlir_values__": [
      "self",
      "values"
    ]
  },
  "FlashAttentionForwardBase": {
    "__init__": [
      "self",
      "dtype",
      "head_dim",
      "head_dim_v",
      "qhead_per_kvhead",
      "is_causal",
      "is_local",
      "pack_gqa",
      "tile_m",
      "tile_n",
      "num_stages",
      "num_threads",
      "Q_in_regs",
      "score_mod",
      "mask_mod",
      "has_aux_tensors"
    ],
    "can_implement": [
      "dtype",
      "head_dim",
      "head_dim_v",
      "tile_m",
      "tile_n",
      "num_stages",
      "num_threads",
      "is_causal",
      "Q_in_regs"
    ],
    "_check_type": [
      "self",
      "mQ_type",
      "mK_type",
      "mV_type",
      "mO_type",
      "mLSE_type",
      "mCuSeqlensQ_type",
      "mCuSeqlensK_type",
      "mSeqUsedQ_type",
      "mSeqUsedK_type"
    ],
    "_setup_attributes": [
      "self"
    ],
    "_get_smem_layout_atom": [
      "self"
    ],
    "_get_tiled_mma": [
      "self"
    ],
    "_get_shared_storage_cls": [
      "self"
    ],
    "__call__": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mO",
      "mLSE",
      "softmax_scale",
      "stream"
    ],
    "epilogue": [
      "self",
      "acc_O",
      "lse",
      "mO",
      "mLSE",
      "sO",
      "seqlen",
      "gmem_tiled_copy_O",
      "tma_atom_O",
      "tiled_mma",
      "tidx",
      "m_block",
      "head_idx",
      "batch_idx"
    ],
    "advance_pipeline": [
      "self",
      "pipeline_index"
    ],
    "load_Q": [
      "self",
      "gmem_thr_copy",
      "gQ",
      "sQ",
      "block",
      "seqlen",
      "headdim"
    ],
    "load_K": [
      "self",
      "gmem_tiled_copy",
      "tKgK",
      "tKsK",
      "tKcK",
      "t0KcK",
      "tKpK",
      "block",
      "smem_pipe_write",
      "seqlen",
      "need_predicates"
    ],
    "load_V": [
      "self",
      "gmem_tiled_copy",
      "tVgV",
      "tVsV",
      "tVcV",
      "t0VcV",
      "tVpV",
      "block",
      "smem_pipe_write",
      "seqlen",
      "need_predicates"
    ]
  },
  "FlashAttentionForwardSm80": {
    "_get_smem_layout_atom": [
      "self"
    ],
    "_get_tiled_mma": [
      "self"
    ],
    "_get_shared_storage_cls": [
      "self"
    ],
    "__call__": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mO",
      "mLSE",
      "stream",
      "softmax_scale",
      "window_size_left",
      "window_size_right",
      "learnable_sink",
      "aux_tensors"
    ],
    "kernel": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mO",
      "mLSE",
      "softmax_scale_log2",
      "softmax_scale",
      "window_size_left",
      "window_size_right",
      "sQ_layout",
      "sK_layout",
      "sV_layout",
      "sO_layout",
      "sP_layout",
      "gmem_tiled_copy_Q",
      "gmem_tiled_copy_K",
      "gmem_tiled_copy_V",
      "gmem_tiled_copy_O",
      "tiled_mma_qk",
      "tiled_mma_pv",
      "SharedStorage",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "compute_one_n_block": [
      "self",
      "n_block",
      "smem_pipe_read",
      "smem_pipe_write",
      "mma_params",
      "smem_copy_params",
      "softmax",
      "load_K",
      "load_V",
      "score_mod",
      "batch_idx",
      "head_idx",
      "m_block",
      "seqlen",
      "aux_tensors",
      "fastdiv_mods",
      "mask_fn",
      "is_first_n_block",
      "check_inf"
    ]
  },
  "FlashAttentionForwardSm90": {
    "arch": [],
    "__init__": [
      "self"
    ],
    "_get_smem_layout_atom": [
      "self"
    ],
    "_get_tiled_mma": [
      "self"
    ],
    "_get_shared_storage_cls": [
      "self"
    ],
    "__call__": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mO",
      "mLSE",
      "softmax_scale",
      "stream",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "mPageTable",
      "window_size_left",
      "window_size_right",
      "learnable_sink",
      "blocksparse_tensors",
      "aux_tensors"
    ],
    "kernel": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mO",
      "mLSE",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "tma_atom_O",
      "softmax_scale_log2",
      "softmax_scale",
      "window_size_left",
      "window_size_right",
      "learnable_sink",
      "blocksparse_tensors",
      "sQ_layout",
      "sK_layout",
      "sV_layout",
      "sO_layout",
      "sP_layout",
      "gmem_tiled_copy_Q",
      "gmem_tiled_copy_K",
      "gmem_tiled_copy_V",
      "gmem_tiled_copy_O",
      "tiled_mma_qk",
      "tiled_mma_pv",
      "tiled_mma_pv_rs",
      "tile_sched_params",
      "TileScheduler",
      "SharedStorage",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "load": [
      "self",
      "mQ",
      "mK",
      "mV",
      "sQ",
      "sK",
      "sV",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "pipeline_k",
      "pipeline_v",
      "mbar_ptr_Q",
      "blocksparse_tensors",
      "block_info",
      "SeqlenInfoCls",
      "TileSchedulerCls"
    ],
    "mma": [
      "self",
      "tiled_mma_qk",
      "tiled_mma_pv",
      "tiled_mma_pv_rs",
      "mQ",
      "mO",
      "mLSE",
      "sQ",
      "sK",
      "sVt",
      "sP",
      "sO",
      "learnable_sink",
      "pipeline_k",
      "pipeline_v",
      "mbar_ptr_Q",
      "gmem_tiled_copy_Q",
      "gmem_tiled_copy_O",
      "tma_atom_O",
      "tidx",
      "softmax_scale_log2",
      "softmax_scale",
      "block_info",
      "SeqlenInfoCls",
      "AttentionMaskCls",
      "TileSchedulerCls",
      "blocksparse_tensors",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "first_half_block_overlap": [
      "self",
      "n_block",
      "mma_qk_fn",
      "kv_consumer_state",
      "pipeline_k",
      "tOrP",
      "smem_copy_params",
      "softmax",
      "seqlen",
      "mask_fn",
      "score_mod_fn",
      "is_first_block"
    ],
    "last_half_block_overlap": [
      "self",
      "kv_consumer_state",
      "pipeline_v",
      "mma_pv_fn",
      "zero_init"
    ],
    "mma_one_n_block": [
      "self",
      "smem_pipe_read",
      "n_block",
      "mma_qk_fn",
      "mma_pv_fn",
      "tiled_mma_pv_rs",
      "pipeline_k",
      "pipeline_v",
      "acc_O",
      "tOrP",
      "smem_copy_params",
      "softmax",
      "seqlen",
      "score_mod_fn",
      "mask_fn",
      "is_first_n_block",
      "check_inf"
    ],
    "mma_one_n_block_intrawg_overlap": [
      "self",
      "smem_pipe_read",
      "n_block",
      "mma_qk_fn",
      "mma_pv_fn",
      "tiled_mma_pv_rs",
      "pipeline_k",
      "pipeline_v",
      "acc_O",
      "tOrP",
      "smem_copy_params",
      "softmax",
      "seqlen",
      "score_mod_fn",
      "mask_fn",
      "check_inf"
    ],
    "mma_init": [
      "self"
    ],
    "apply_score_mod": [
      "self",
      "thr_mma_qk",
      "batch_idx",
      "head_idx",
      "m_block",
      "acc_S",
      "n_block",
      "softmax_scale",
      "seqlen",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "warp_scheduler_barrier_sync": [
      "self"
    ],
    "warp_scheduler_barrier_arrive": [
      "self"
    ]
  },
  "benchmark_forward": [
    "fn"
  ],
  "benchmark_backward": [
    "fn"
  ],
  "benchmark_combined": [
    "fn"
  ],
  "benchmark_fwd_bwd": [
    "fn"
  ],
  "benchmark_all": [
    "fn"
  ],
  "pytorch_profiler": [
    "fn"
  ],
  "benchmark_memory": [
    "fn"
  ],
  "NamedBarrierFwd": {
    "Epilogue": []
  },
  "FlashAttentionForwardSm100": {
    "arch": [],
    "__init__": [
      "self",
      "head_dim",
      "head_dim_v",
      "qhead_per_kvhead",
      "is_causal",
      "is_local",
      "is_split_kv",
      "pack_gqa",
      "m_block_size",
      "n_block_size",
      "q_stage",
      "is_persistent",
      "score_mod",
      "mask_mod",
      "has_aux_tensors",
      "paged_kv_non_tma",
      "is_varlen_q"
    ],
    "_setup_attributes": [
      "self"
    ],
    "__call__": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mO",
      "mLSE",
      "softmax_scale",
      "stream",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "mPageTable",
      "window_size_left",
      "window_size_right",
      "learnable_sink",
      "blocksparse_tensors",
      "aux_tensors"
    ],
    "kernel": [
      "self",
      "mQ",
      "mK",
      "mV",
      "mO",
      "mLSE",
      "mCuSeqlensQ",
      "mCuSeqlensK",
      "mSeqUsedQ",
      "mSeqUsedK",
      "mPageTable",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "tma_atom_O",
      "softmax_scale_log2",
      "softmax_scale",
      "window_size_left",
      "window_size_right",
      "learnable_sink",
      "blocksparse_tensors",
      "sQ_layout",
      "sK_layout",
      "tP_layout",
      "sV_layout",
      "sO_layout",
      "gmem_tiled_copy_O",
      "tiled_mma_qk",
      "tiled_mma_pv",
      "tile_sched_params",
      "num_splits",
      "aux_tensors",
      "fastdiv_mods"
    ],
    "load": [
      "self",
      "thr_mma_qk",
      "thr_mma_pv",
      "mQ",
      "mK",
      "mV",
      "sQ",
      "sK",
      "sV",
      "mPageTable",
      "tma_atom_Q",
      "tma_atom_K",
      "tma_atom_V",
      "pipeline_kv",
      "mbar_ptr",
      "block_info",
      "num_splits",
      "SeqlenInfoCls",
      "TileSchedulerCls",
      "blocksparse_tensors"
    ],
    "mma": [
      "self",
      "tiled_mma_qk",
      "tiled_mma_pv",
      "sQ",
      "sK",
      "sV",
      "tStSs",
      "tOtOs",
      "tOrPs",
      "pipeline_kv",
      "mbar_ptr",
      "block_info",
      "num_splits",
      "SeqlenInfoCls",
      "TileSchedulerCls",
      "blocksparse_tensors"
    ],
    "softmax_loop": [
      "self",
      "stage",
      "softmax_scale_log2",
      "softmax_scale",
      "thr_mma_qk",
      "tStSi",
      "sScale",
      "mLSE",
      "learnable_sink",
      "mbar_ptr",
      "block_info",
      "num_splits",
      "SeqlenInfoCls",
      "AttentionMaskCls",
      "TileSchedulerCls",
      "aux_tensors",
      "fastdiv_mods",
      "blocksparse_tensors"
    ],
    "softmax_step": [
      "self",
      "mma_si_consumer_phase",
      "si_corr_producer_phase",
      "s0_s1_sequence_phase",
      "n_block",
      "softmax",
      "mbar_ptr",
      "mbar_s0_s1_sequence_offset",
      "thr_mma_qk",
      "thr_tmem_load",
      "thr_tmem_store",
      "thr_tmem_store_scale",
      "tStS_t2r",
      "tStScale_r2t",
      "tStP_r2t",
      "sScale",
      "stage",
      "batch_idx",
      "head_idx",
      "m_block",
      "seqlen",
      "aux_tensors",
      "fastdiv_mods",
      "mask_fn",
      "is_first"
    ],
    "correction_loop": [
      "self",
      "thr_mma_qk",
      "thr_mma_pv",
      "tStS",
      "tOtOs",
      "sScale",
      "mO",
      "mLSE",
      "sO",
      "learnable_sink",
      "gmem_tiled_copy_O",
      "tma_atom_O",
      "mbar_ptr",
      "softmax_scale_log2",
      "block_info",
      "num_splits",
      "SeqlenInfoCls",
      "TileSchedulerCls",
      "blocksparse_tensors"
    ],
    "correction_rescale": [
      "self",
      "thr_mma",
      "tOtO",
      "tidx",
      "scale"
    ],
    "correction_epilogue": [
      "self",
      "thr_mma",
      "tOtO",
      "tidx",
      "stage",
      "m_block",
      "seqlen_q",
      "scale",
      "sO",
      "mO_cur",
      "gO",
      "gmem_tiled_copy_O"
    ],
    "epilogue_s2g": [
      "self",
      "mO",
      "sO",
      "gmem_tiled_copy_O",
      "tma_atom_O",
      "mbar_ptr",
      "block_info",
      "num_splits",
      "SeqlenInfoCls",
      "TileSchedulerCls"
    ],
    "load_Q": [
      "self",
      "load_Q_fn",
      "mbar_full_ptr",
      "mbar_empty_ptr",
      "block",
      "stage",
      "phase"
    ],
    "load_KV": [
      "self",
      "tma_atom",
      "tXgX",
      "tXsX",
      "paged_kv_manager",
      "sX",
      "mbar_full_ptr",
      "mbar_empty_ptr",
      "block",
      "producer_state",
      "K_or_V",
      "page_idx"
    ],
    "offset_kv_smem": [
      "self",
      "sX",
      "stage",
      "phase"
    ],
    "make_and_init_load_kv_pipeline": [
      "self",
      "load_kv_mbar_ptr"
    ],
    "apply_score_mod": [
      "self",
      "tSrS_t2r",
      "thr_tmem_load",
      "thr_mma_qk",
      "batch_idx",
      "head_idx",
      "m_block",
      "n_block",
      "softmax",
      "seqlen",
      "aux_tensors",
      "fastdiv_mods"
    ]
  },
  "NamedBarrierBwd": {
    "Epilogue": [],
    "WarpSchedulerWG1": [],
    "WarpSchedulerWG2": [],
    "WarpSchedulerWG3": [],
    "PdS": [],
    "dQFullWG0": [],
    "dQFullWG1": [],
    "dQEmptyWG0": [],
    "dQEmptyWG1": []
  },
  "NamedBarrierBwdSm100": {
    "EpilogueWG1": [],
    "EpilogueWG2": [],
    "Compute": [],
    "dQaccReduce": []
  },
  "_dg_initialized": [],
  "_ensure_initialized": [],
  "_wrap_op": [
    "name"
  ],
  "set_num_sms": [],
  "get_num_sms": [],
  "set_compile_mode": [],
  "get_compile_mode": [],
  "set_tc_util": [],
  "get_tc_util": [],
  "fp8_gemm_nt": [],
  "fp8_gemm_nn": [],
  "fp8_gemm_tn": [],
  "fp8_gemm_tt": [],
  "m_grouped_fp8_gemm_nt_contiguous": [],
  "m_grouped_fp8_gemm_nn_contiguous": [],
  "m_grouped_fp8_gemm_nt_masked": [],
  "fp8_m_grouped_gemm_nt_masked": [],
  "k_grouped_fp8_gemm_nt_contiguous": [],
  "k_grouped_fp8_gemm_tn_contiguous": [],
  "bf16_gemm_nt": [],
  "bf16_gemm_nn": [],
  "bf16_gemm_tn": [],
  "bf16_gemm_tt": [],
  "m_grouped_bf16_gemm_nt_contiguous": [],
  "m_grouped_bf16_gemm_nn_contiguous": [],
  "m_grouped_bf16_gemm_nt_masked": [],
  "k_grouped_bf16_gemm_tn_contiguous": [],
  "cublaslt_gemm_nt": [],
  "cublaslt_gemm_nn": [],
  "cublaslt_gemm_tn": [],
  "cublaslt_gemm_tt": [],
  "fp8_gemm_nt_skip_head_mid": [],
  "fp8_mqa_logits": [],
  "get_paged_mqa_logits_metadata": [],
  "fp8_paged_mqa_logits": [],
  "einsum": [],
  "transform_sf_into_required_layout": [],
  "get_tma_aligned_size": [],
  "get_mk_alignment_for_contiguous_layout": [],
  "get_mn_major_tma_aligned_tensor": [],
  "get_mn_major_tma_aligned_packed_ue8m0_tensor": [],
  "get_k_grouped_mn_major_tma_aligned_packed_ue8m0_tensor": [],
  "_verify_ops_loaded": [],
  "bench": [
    "fn",
    "num_warmups",
    "num_tests",
    "high_precision"
  ],
  "empty_suppress": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "suppress_stdout_stderr": {
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "bench_kineto": [
    "fn",
    "kernel_names",
    "num_tests",
    "suppress_kineto_output",
    "trace_path",
    "flush_l2",
    "with_multiple_kernels"
  ],
  "get_arch_major": [],
  "test_filter": [
    "condition"
  ],
  "ignore_env": [
    "name",
    "condition"
  ],
  "calc_diff": [
    "x",
    "y"
  ],
  "count_bytes": [],
  "check_signal": [
    "num_local_expert",
    "max_m",
    "block_m",
    "threshold",
    "signal",
    "masked_m"
  ],
  "get_config_smem_size": [
    "config",
    "elem_bytes"
  ],
  "_gemm_configs": [],
  "get_m_grouped_gemm_configs": [],
  "get_k_grouped_gemm_configs": [],
  "a_fused_k_grouped_bf16_gemm_contiguous_tl_impl": [
    "a_ptr",
    "b_ptr",
    "d_ptr",
    "k_indices_ptr",
    "k_start_ptr",
    "k_end_ptr",
    "M",
    "N",
    "K",
    "ACC",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M"
  ],
  "a_fused_k_grouped_bf16_gemm_tn_contiguous_tl": [
    "a",
    "b",
    "d",
    "handle",
    "acc"
  ],
  "a_fused_m_grouped_bf16_gemm_contiguous_tl_impl": [
    "a_ptr",
    "b_ptr",
    "d_ptr",
    "m_indices_ptr",
    "m_row_indices_ptr",
    "M",
    "N",
    "K",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M",
    "IS_B_K_MAJOR"
  ],
  "a_fused_m_grouped_bf16_gemm_nt_contiguous_tl": [
    "a",
    "b",
    "d",
    "mappings"
  ],
  "a_fused_m_grouped_bf16_gemm_nn_contiguous_tl": [
    "a",
    "b",
    "d",
    "mappings"
  ],
  "b_fused_k_grouped_bf16_gemm_contiguous_tl_impl": [
    "a_ptr",
    "b_ptr",
    "d_ptr",
    "k_indices_ptr",
    "k_start_ptr",
    "k_end_ptr",
    "M",
    "N",
    "K",
    "ACC",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M"
  ],
  "b_fused_k_grouped_bf16_gemm_tn_contiguous_tl": [
    "a",
    "b",
    "d",
    "handle",
    "acc"
  ],
  "m_grouped_bf16_gemm_contiguous_tl_impl": [
    "a_ptr",
    "b_ptr",
    "d_ptr",
    "m_indices_ptr",
    "M",
    "N",
    "K",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K",
    "GROUP_SIZE_M",
    "IS_B_K_MAJOR"
  ],
  "m_grouped_bf16_gemm_nt_contiguous_tl": [
    "a",
    "b",
    "d",
    "m_indices"
  ],
  "m_grouped_bf16_gemm_nn_contiguous_tl": [
    "a",
    "b",
    "d",
    "m_indices"
  ],
  "ceil_div": [
    "x",
    "y"
  ],
  "align": [
    "x",
    "y"
  ],
  "ceil_to_ue8m0": [
    "x"
  ],
  "per_token_cast_to_fp8": [
    "x",
    "use_ue8m0"
  ],
  "per_channel_cast_to_fp8": [
    "x",
    "use_ue8m0"
  ],
  "per_block_cast_to_fp8": [
    "x",
    "use_ue8m0"
  ],
  "per_custom_dims_cast_to_fp8": [
    "x",
    "dims",
    "use_ue8m0"
  ],
  "get_m_alignment_for_contiguous_layout": [],
  "get_k_alignment_for_contiguous_layout": [],
  "_launch_metadata_allow_sync": [],
  "launch_metadata_allow_sync": [],
  "set_launch_metadata_allow_sync": [
    "allow_sync"
  ],
  "topk_forward": [
    "x",
    "k",
    "apply_softmax",
    "dim",
    "return_bitmatrix",
    "y_indx",
    "n_rows"
  ],
  "topk_backward": [
    "x",
    "y_indx",
    "dy_vals",
    "k",
    "n_rows",
    "apply_softmax"
  ],
  "TopK": {
    "forward": [
      "ctx",
      "x",
      "k",
      "apply_softmax",
      "dim",
      "return_bitmatrix",
      "y_indx",
      "n_rows"
    ],
    "backward": [
      "ctx",
      "dy_vals",
      "_0",
      "_1"
    ]
  },
  "topk": [
    "x",
    "k",
    "apply_softmax",
    "dim",
    "return_bitmatrix",
    "y_indx",
    "n_rows"
  ],
  "compaction": [
    "yv",
    "yi",
    "bitmask",
    "sentinel"
  ],
  "compaction_torch": [
    "yv",
    "yi",
    "bitmask",
    "sentinel"
  ],
  "MAX_FINITE_FLOAT8E5": [],
  "MAX_FINITE_FLOAT8E4NV": [],
  "MAX_FINITE_FLOAT8E4B8": [],
  "BaseFlexData": {
    "view": [
      "self",
      "x"
    ],
    "reinterpret": [
      "self",
      "x"
    ]
  },
  "InFlexData": {
    "is_per_batch": [
      "self"
    ]
  },
  "OutFlexData": {
    "__iter__": [
      "self"
    ]
  },
  "GatherIndx": {},
  "ScatterIndx": {},
  "ExptData": {
    "__post_init__": [
      "self"
    ]
  },
  "RoutingData": {
    "n_blocks": [
      "self",
      "n_rows",
      "block_m"
    ]
  },
  "SortTokens": {
    "forward": [
      "ctx",
      "expt_scal",
      "expt_indx",
      "n_expts_tot",
      "bitmatrix"
    ],
    "backward": [
      "ctx",
      "_0",
      "_1",
      "_2",
      "dgate_scal",
      "_3",
      "_4",
      "_5"
    ]
  },
  "sort_tokens": [
    "expt_scal",
    "expt_indx",
    "n_expts_tot",
    "bitmatrix"
  ],
  "PruneRouting": {
    "forward": [
      "ctx",
      "expt_scal",
      "expt_indx",
      "bitmatrix",
      "n_expts_tot",
      "simulated_ep"
    ]
  },
  "prune_routing": [
    "expt_scal",
    "expt_indx",
    "bitmatrix",
    "n_expts_tot",
    "simulated_ep"
  ],
  "log2_power_of_two": [
    "x"
  ],
  "block_m_log2_start": [],
  "_compute_expt_data_internal": [
    "expt_hist",
    "n_expts_tot",
    "n_gates"
  ],
  "_unpack_into_dict": [
    "x"
  ],
  "compute_expt_data": [
    "expt_hist",
    "n_expts_tot",
    "n_gates"
  ],
  "routing_from_bitmatrix": [
    "bitmatrix",
    "expt_scal",
    "expt_indx",
    "n_expts_tot",
    "n_expts_act"
  ],
  "routing": [
    "logits",
    "n_expts_act",
    "sm_first",
    "expt_indx",
    "simulated_ep",
    "n_rows"
  ],
  "compute_expt_data_torch": [
    "hist",
    "n_expts_tot",
    "n_gates"
  ],
  "topk_torch": [
    "vals",
    "k",
    "expt_indx",
    "has_user_provided_indx"
  ],
  "routing_torch": [
    "logits",
    "n_expts_act",
    "sm_first",
    "expt_indx",
    "n_rows"
  ],
  "assert_equal": [
    "ref",
    "tri"
  ],
  "assert_close": [
    "ref",
    "tri",
    "maxtol",
    "rmstol",
    "description",
    "verbose"
  ],
  "ComputeSanitizerTool": {
    "MEMCHECK": [],
    "RACECHECK": [],
    "SYNCCHECK": [],
    "INITCHECK": []
  },
  "compute_sanitizer": [],
  "compute_actual_scale": [
    "x",
    "dtype"
  ],
  "FnSpecs": {
    "default": []
  },
  "FusedActivation": {},
  "Epilogue": {},
  "FnName": {
    "QUANTIZE_MXFP8": []
  },
  "EpilogueSpecs": [],
  "_kernels": [],
  "get_kernels": [
    "epilogue",
    "fused_activation"
  ],
  "can_overflow_int32": [
    "tensor"
  ],
  "should_upcast_indices": [],
  "FlexCtx": {},
  "PrecisionConfig": {},
  "get_swap_xw": [
    "precision_config",
    "opt_flags"
  ],
  "MatmulAllocation": {},
  "init_allocation": [
    "x",
    "w",
    "precision_config",
    "fused_activation",
    "routing_data",
    "gather_indx",
    "scatter_indx",
    "opt_flags"
  ],
  "apply_allocation": [
    "allocation",
    "output"
  ],
  "_canonicalize_storage": [
    "storage",
    "out_ndim",
    "flex_data"
  ],
  "reduce_grouped": [
    "x",
    "indx",
    "out",
    "out_mx_scale",
    "fused_activation",
    "epilogue",
    "x_flex",
    "out_flex",
    "x_mx_scale",
    "out_dtype",
    "flexpoint_saturate_inf"
  ],
  "matmul_ogs_set_idle_sms": [
    "num_idle_sms"
  ],
  "matmul_ogs": [
    "x",
    "w",
    "bias",
    "routing_data",
    "gather_indx",
    "scatter_indx",
    "precision_config",
    "betas",
    "gammas",
    "out_alpha",
    "y",
    "fused_activation",
    "epilogue"
  ],
  "matmul_ogs_torch": [
    "x",
    "w",
    "bias",
    "routing_data",
    "gather_indx",
    "scatter_indx",
    "precision_config",
    "betas",
    "gammas",
    "round_x",
    "round_y"
  ],
  "swiglu_fn": [],
  "SwiGLU": {
    "forward": [
      "ctx",
      "a",
      "alpha",
      "precision_config",
      "routing_data"
    ]
  },
  "swiglu": [
    "a",
    "alpha",
    "precision_config",
    "routing_data"
  ],
  "swiglu_torch": [
    "a",
    "alpha",
    "precision_config"
  ],
  "Storage": {
    "__post_init__": [
      "self"
    ],
    "device": [
      "self"
    ],
    "is_tma_compliant": [
      "self"
    ],
    "make_dense_tma": [
      "self",
      "block_shape",
      "transpose"
    ],
    "make_tma": [
      "self",
      "block_shape",
      "mode",
      "transpose"
    ]
  },
  "IntegerType": {},
  "FloatType": {
    "__post_init__": [
      "self"
    ]
  },
  "BIT": [],
  "FP4": [],
  "bitwidth": [
    "type"
  ],
  "Tensor": {
    "__post_init__": [
      "self"
    ],
    "ndim": [
      "self"
    ],
    "device": [
      "self"
    ],
    "stride": [
      "self",
      "i"
    ],
    "data_ptr": [
      "self"
    ],
    "numel": [
      "self"
    ],
    "element_size": [
      "self"
    ],
    "data": [
      "self"
    ],
    "dim": [
      "self"
    ],
    "size": [
      "self",
      "i"
    ]
  },
  "Bitmatrix": {
    "__init__": [
      "self",
      "storage",
      "shape",
      "shape_max",
      "scratchpad"
    ],
    "sum": [
      "self",
      "partials_block_size"
    ]
  },
  "get_layout": [
    "tensor"
  ],
  "wrap_torch_tensor": [
    "torch_tensor",
    "dtype"
  ],
  "convert_layout": [
    "tensor",
    "layout_cls"
  ],
  "get_cdna_version": [],
  "has_tma_gather": [],
  "has_native_mxfp": [],
  "num_sms": [],
  "cacheable": [
    "f"
  ],
  "define_kernel": [
    "src",
    "module",
    "attrs"
  ],
  "specialize": [
    "fn",
    "module",
    "constants",
    "tuples",
    "name",
    "do_not_specialize"
  ],
  "make_default_matmul_mxfp4_w_layout": [
    "mx_axis"
  ],
  "make_default_matmul_mxfp4_w_scale_layout": [
    "mx_axis",
    "num_warps"
  ],
  "right_shift_unsigned": [
    "x",
    "shift"
  ],
  "_compress_fp4": [
    "x"
  ],
  "_compress_fourth": [
    "x"
  ],
  "_pack_bits": [
    "x",
    "mx_axis"
  ],
  "_bf16_to_fp4e2m1": [
    "x"
  ],
  "_bf16x2_to_fp4e2m1x2": [
    "x"
  ],
  "_unpack_bits": [
    "x",
    "mx_axis"
  ],
  "HopperMXValueLayout": {
    "__init__": [
      "self",
      "shape",
      "mx_axis",
      "mma_version"
    ],
    "_maybe_mT": [
      "self",
      "data"
    ],
    "swizzle_data": [
      "self",
      "data"
    ],
    "unswizzle_data": [
      "self",
      "data"
    ],
    "swizzle_block_shape": [
      "self",
      "block_shape"
    ]
  },
  "_unshuffle_triton": [
    "x",
    "mma_version"
  ],
  "_unpack_fp4_to_bf16_triton": [
    "x"
  ],
  "mxfp4_to_bf16_triton": [
    "x",
    "scale",
    "mx_axis"
  ],
  "Layout": {
    "__init__": [
      "self",
      "shape"
    ],
    "swizzle_data": [
      "self",
      "data"
    ],
    "unswizzle_data": [
      "self",
      "data"
    ],
    "swizzle_block_shape": [
      "self",
      "block_shape"
    ]
  },
  "NON_K_PRESHUFFLE_BLOCK_SIZE": [],
  "CDNA4MXScaleLayout": {
    "__init__": [
      "self",
      "shape"
    ],
    "swizzle_data": [
      "self",
      "data"
    ],
    "unswizzle_data": [
      "self",
      "data"
    ],
    "swizzle_block_shape": [
      "self",
      "block_shape"
    ]
  },
  "unswizzle_mx_scale_cdna4": [
    "x",
    "BLOCK_N",
    "MX_SCALE_BLOCK_K",
    "N_PRESHUFFLE_FACTOR"
  ],
  "BlackwellMXValueLayout": {
    "__init__": [
      "self",
      "shape"
    ],
    "swizzle_data": [
      "self",
      "data"
    ],
    "unswizzle_data": [
      "self",
      "data"
    ],
    "swizzle_block_shape": [
      "self",
      "block_shape"
    ]
  },
  "SWIZZLE_ALIGN_INNER": [],
  "SWIZZLE_SIZE_INNER": [],
  "SWIZZLE_SIZE_OUTER": [],
  "BlackwellMXScaleLayout": {
    "__init__": [
      "self",
      "shape"
    ],
    "swizzle_data": [
      "self",
      "data"
    ],
    "unswizzle_data": [
      "self",
      "data"
    ],
    "swizzle_block_shape": [
      "self",
      "block_shape"
    ]
  },
  "unswizzle_mx_scale_bw": [
    "x",
    "SIZE_OUTER",
    "SIZE_INNER",
    "ALIGN_INNER"
  ],
  "HopperMXScaleLayout": {
    "__init__": [
      "self",
      "shape",
      "mx_axis",
      "num_warps"
    ],
    "_maybe_mT": [
      "self",
      "data"
    ],
    "swizzle_data": [
      "self",
      "data"
    ],
    "unswizzle_data": [
      "self",
      "data"
    ],
    "swizzle_block_shape": [
      "self",
      "block_shape"
    ]
  },
  "unswizzle_mxfp4_scale_hopper": [
    "x",
    "mx_axis",
    "num_warps"
  ],
  "StridedLayout": {
    "__init__": [
      "self",
      "shape"
    ],
    "swizzle_data": [
      "self",
      "data"
    ],
    "unswizzle_data": [
      "self",
      "data"
    ],
    "swizzle_block_shape": [
      "self",
      "block_shape"
    ]
  },
  "_topk_backward": [
    "Yi",
    "stride_ym",
    "DY",
    "stride_dym",
    "X",
    "stride_xm",
    "DX",
    "stride_dxm",
    "n_rows",
    "NRows",
    "n_expts_tot",
    "APPLY_SOFTMAX",
    "N_EXPTS_ACT",
    "N_EXPTS_PAD"
  ],
  "get_topmask_and_fullmask": [
    "x"
  ],
  "fpval_to_key": [
    "x"
  ],
  "key_to_fpval": [
    "x"
  ],
  "indx_to_key": [
    "indx",
    "N_EXPTS_PAD"
  ],
  "key_to_indx": [
    "indx",
    "N_EXPTS_PAD"
  ],
  "streaming_topk": [
    "X",
    "stride_xm",
    "n_expts_tot",
    "offs_m",
    "mask_m",
    "N_EXPTS_PAD",
    "N_EXPTS_ACT",
    "BLOCK_N"
  ],
  "_topk_forward": [
    "X",
    "stride_xm",
    "Yv",
    "Yi",
    "stride_ym",
    "USE_PROVIDED_INDX",
    "Bits",
    "stride_rm",
    "stride_rn",
    "n_rows",
    "n_expts_tot",
    "S",
    "BLOCK_S",
    "s_blocks",
    "APPLY_SOFTMAX",
    "BLOCK_M",
    "N_EXPTS_PAD",
    "N_EXPTS_ACT",
    "BLOCK_N"
  ],
  "clip": [
    "x",
    "limit",
    "clip_lower"
  ],
  "thread_local_absmax": [
    "x",
    "BLOCK_SIZE",
    "NUM_THREADS"
  ],
  "swiglu_repr": [
    "specialization"
  ],
  "swiglu_launch_metadata": [
    "grid",
    "kernel",
    "args"
  ],
  "compute_swiglu": [
    "gelu",
    "linear",
    "scale",
    "alpha",
    "limit"
  ],
  "_swiglu_fn": [
    "input",
    "alpha",
    "limit"
  ],
  "_swiglu": [
    "Out",
    "OutExpectedScale",
    "OutActualScale",
    "OutChecksumScale",
    "A",
    "AScale",
    "alpha",
    "M",
    "N",
    "stride_am",
    "stride_an",
    "stride_outm",
    "stride_outn",
    "limit",
    "NTokens",
    "BLOCK_M",
    "BLOCK_N",
    "EVEN_N",
    "M_BLOCKS",
    "N_BLOCKS",
    "flexpoint_saturate_inf"
  ],
  "_routing_compute_expt_offs": [
    "ExpertHist",
    "FinalExpertOffs",
    "hist_size",
    "BLOCK_N"
  ],
  "_routing_compute_indx_offs": [
    "PartialHist",
    "shape_pm",
    "stride_pm",
    "stride_pn",
    "BLOCK_M",
    "expt_id"
  ],
  "_keyed_add": [
    "x",
    "y"
  ],
  "_routing_compute_indx": [
    "pid_m",
    "GatherIndx",
    "ScatterIndx",
    "GateScal",
    "ExptScal",
    "ExptIndx",
    "PartialOffs",
    "stride_pm",
    "stride_pn",
    "TokensStart",
    "n_tokens",
    "BLOCK_M",
    "N_EXPTS_ACT"
  ],
  "_combined_routing_compute": [
    "GatherIndx",
    "ScatterIndx",
    "GateScal",
    "ExptScal",
    "ExptIndx",
    "PartialOffs",
    "stride_pm",
    "stride_pn",
    "TokensStart",
    "n_tokens",
    "BLOCK_M",
    "N_EXPTS_ACT",
    "Hist",
    "MDTileStarts",
    "tile_starts_stridem",
    "MDTileInfo",
    "tile_info_stridem",
    "first_tile_dim_log2",
    "SIZES",
    "BLOCK",
    "blocks2a"
  ],
  "_routing_clear_bitmatrix": [
    "Bitmatrix",
    "stride_bm",
    "stride_bn",
    "shape_bn",
    "cutoff",
    "BLOCK_N"
  ],
  "_combined_routing_memset": [
    "Indx",
    "size",
    "sentinel",
    "BLOCK",
    "ExpertHist",
    "FinalExpertOffs",
    "hist_size",
    "n_expts_tot",
    "PartialHist",
    "shape_pm",
    "stride_pm",
    "stride_pn",
    "MDStarts",
    "tile_starts_stridem",
    "blocks1a",
    "MDTileInfo",
    "first_tile_dim_log2",
    "SIZES",
    "BLOCK_A",
    "BLOCK_N",
    "BLOCK_M"
  ],
  "_cdiv_pow2": [
    "n",
    "log2_k"
  ],
  "_expt_data_memset": [
    "Hist",
    "n_expts_tot",
    "MDStarts",
    "tile_starts_stridem",
    "MDTileInfo",
    "first_tile_dim_log2",
    "SIZES",
    "BLOCK"
  ],
  "_expt_data_compute": [
    "Hist",
    "MDTileStarts",
    "tile_starts_stridem",
    "MDTileInfo",
    "tile_info_stridem",
    "first_tile_dim_log2",
    "SIZES",
    "BLOCK"
  ],
  "TL_MAX_FINITE_FLOAT8E5": [],
  "TL_MAX_FINITE_FLOAT8E4NV": [],
  "TL_MAX_FINITE_FLOAT8E4B8": [],
  "TL_MAX_FINITE_FLOAT8E4B15": [],
  "TL_MAX_FINITE_FLOAT16": [],
  "TL_RCP_MAX_FINITE_FLOAT8E5": [],
  "TL_RCP_MAX_FINITE_FLOAT8E4NV": [],
  "TL_RCP_MAX_FINITE_FLOAT8E4B8": [],
  "TL_RCP_MAX_FINITE_FLOAT8E4B15": [],
  "TL_RCP_MAX_FINITE_FLOAT16": [],
  "max_finite": [
    "dtype"
  ],
  "rcp_max_finite": [
    "dtype"
  ],
  "sm86_min_nan_xorsign_abs_f32": [
    "a",
    "b"
  ],
  "sm86_max_nan_xorsign_abs_f32": [
    "a",
    "b"
  ],
  "load_scale": [
    "scale_ptr"
  ],
  "flex_to_float": [
    "x",
    "scale_ptr"
  ],
  "nan_propagating_absmax_reduce": [
    "x",
    "axis"
  ],
  "compute_scale": [
    "x",
    "Out"
  ],
  "update_scale": [
    "x",
    "scale_ptr",
    "Out"
  ],
  "float_to_flex": [
    "x",
    "expected_scale_ptr_or_val",
    "actual_scale_ptr",
    "checksum_scale_ptr",
    "mask",
    "Out",
    "saturate_infs"
  ],
  "DequantScaleRoundingMode": {
    "ROUND_UP": [],
    "ROUND_DOWN": []
  },
  "downcast_to_mxfp": [
    "src_tensor",
    "out_quant_type",
    "axis",
    "DEQUANT_SCALE_ROUNDING_MODE"
  ],
  "upcast_from_mxfp": [
    "tensor",
    "scale",
    "target_dtype",
    "axis"
  ],
  "get_max_quant_val": [
    "dtype"
  ],
  "downcast_to_mxfp_torch": [
    "src_tensor",
    "out_quant_type",
    "axis",
    "DEQUANT_SCALE_ROUNDING_MODE"
  ],
  "cvt_e2m1_to_fp32": [
    "input_tensor"
  ],
  "upcast_from_mxfp_torch": [
    "tensor",
    "scale",
    "target_dtype",
    "axis"
  ],
  "quantize_mxfp8_fn": [],
  "_upcast_from_mxfp": [
    "out_ptr",
    "stride_o_outer",
    "stride_o_quant",
    "mx_scale_ptr",
    "stride_scale_outer",
    "stride_scale_quant",
    "mx_tensor_ptr",
    "stride_tensor_outer",
    "stride_tensor_quant",
    "outer_dim",
    "quant_dim",
    "BLOCK_SIZE_OUT_DIM",
    "BLOCK_SIZE_QUANT_DIM"
  ],
  "MXFP_BLOCK_SIZE": [],
  "_get_max_quant_val": [
    "dtype"
  ],
  "_compute_quant_and_scale": [
    "src_tensor",
    "valid_src_mask",
    "mx_tensor_dtype",
    "DEQUANT_SCALE_ROUNDING_MODE"
  ],
  "_downcast_to_mxfp": [
    "mx_tensor_ptr",
    "stride_mxt_outer",
    "stride_mxt_quant",
    "mx_scale_ptr",
    "stride_mx_scale_outer",
    "stride_mx_scale_quant",
    "src_ptr",
    "stride_src_outer",
    "stride_src_quant",
    "outer_dim",
    "quant_dim",
    "BLOCK_SIZE_OUT_DIM",
    "BLOCK_SIZE_QUANT_DIM",
    "DEQUANT_SCALE_ROUNDING_MODE"
  ],
  "_quantize_mxfp8_fn": [
    "input",
    "mask",
    "pid"
  ],
  "_masked_compaction": [
    "Yv",
    "Yi",
    "BitMask",
    "stride_bm",
    "stride_bn",
    "RetYv",
    "RetYi",
    "sentinel",
    "K"
  ],
  "cuda_capability_geq": [
    "major",
    "minor"
  ],
  "get_dtype": [
    "tensor_or_desc"
  ],
  "_load_tile_attrs": [
    "tile_id",
    "num_tiles",
    "grid_m",
    "grid_n",
    "padding_m",
    "M",
    "ExptData",
    "ExptHist",
    "ExptOffs",
    "BLOCK_M",
    "BLOCK_N",
    "SPLIT_K",
    "GROUP_M",
    "XCD_SWIZZLE"
  ],
  "_load_writeback_idx_and_mask": [
    "WriteBackIndx",
    "writeback_size",
    "offs",
    "mask"
  ],
  "_matmul_ogs_repr": [],
  "_p_matmul_ogs": [
    "Y",
    "YPtr",
    "stride_y_k",
    "stride_y_z",
    "stride_y_m",
    "stride_y_n",
    "YExpectedScale",
    "YActualScale",
    "YChecksumScale",
    "stride_y_mx_z",
    "stride_y_mx_m",
    "stride_y_mx_n",
    "X",
    "XPtr",
    "stride_x_z",
    "stride_x_m",
    "stride_x_k",
    "XScale",
    "XMxScale",
    "stride_x_mx_z",
    "stride_x_mx_m",
    "stride_x_mx_k",
    "W",
    "WPtr",
    "stride_w_e",
    "stride_w_k",
    "stride_w_n",
    "W_TRANSPOSE",
    "WScale",
    "MxScale",
    "stride_mx_e",
    "stride_mx_k",
    "stride_mx_n",
    "B",
    "stride_b_e",
    "NRows",
    "M",
    "N",
    "K",
    "Betas",
    "Gammas",
    "GatherIndx",
    "ScatterSrcIndx",
    "num_idxs",
    "WriteBackIndx",
    "writeback_size",
    "ExptHist",
    "ExptOffs",
    "ExptOffsSum",
    "ExptData",
    "batch_size",
    "grid_m",
    "grid_n",
    "out_alpha",
    "ACTIVATION_FN",
    "activation_fn_args",
    "ACTIVATION_REDUCTION_N",
    "EPILOGUE_FN",
    "epilogue_fn_args",
    "N_EXPTS_TOT",
    "N_EXPTS_ACT",
    "MAX_NUM_IMPRECISE_ACC",
    "ALLOW_TF32",
    "FLEXPOINT_SATURATE_INF",
    "PER_BATCH_SCALE",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "GROUP_M",
    "XCD_SWIZZLE",
    "SWIZZLE_MX_VALUE",
    "SWIZZLE_MX_SCALE",
    "EPILOGUE_SUBTILE",
    "EVEN_K",
    "SPLIT_K",
    "W_CACHE_MODIFIER",
    "NUM_SMS",
    "X_TMA_MODE",
    "Y_TMA_MODE",
    "TOKENS_PER_EXPT_FOR_ANNOTATION",
    "UPCAST_INDICES",
    "SWAP_XW",
    "IS_EPILOGUE_QUANT_MXFP8"
  ],
  "_per_device_alloc_fns": [],
  "get_per_device_per_stream_alloc_fn": [
    "device"
  ],
  "get_scaled_dot_format_string": [
    "dtype"
  ],
  "xcd_swizzle": [
    "pid",
    "domain_size",
    "XCD_SWIZZLE"
  ],
  "swizzle2d": [
    "pid",
    "grid_m",
    "grid_n",
    "GROUP_M"
  ],
  "make_matmul_repr": [
    "base_name",
    "order"
  ],
  "matmul_launch_metadata": [
    "grid",
    "kernel",
    "args"
  ],
  "_reduce_grouped": [
    "X",
    "stride_xb",
    "stride_xm",
    "stride_xn",
    "XScale",
    "Out",
    "stride_om",
    "stride_on",
    "OutExpectedScale",
    "OutActualScale",
    "OutChecksumScale",
    "InIndx",
    "B",
    "N",
    "XMxScale",
    "stride_mxb",
    "stride_mxs",
    "OutMxScale",
    "stride_omxs",
    "ACTIVATION_FN",
    "activation_fn_args",
    "ACTIVATION_REDUCTION_N",
    "EPILOGUE_FN",
    "epilogue_fn_args",
    "HAS_IN_MX_SCALE",
    "HAS_OUT_MX_SCALE",
    "FLEXPOINT_SATURATE_INF",
    "K",
    "BLOCK_N"
  ],
  "OptFlags": {
    "__post_init__": [
      "self"
    ]
  },
  "make_default_opt_flags_amd": [
    "out_dtype",
    "lhs_dtype",
    "rhs_dtype",
    "precision_config",
    "m",
    "n",
    "k",
    "routing_data",
    "can_use_persistent_tma",
    "can_use_fused_scatter",
    "enforce_bitwise_invariance",
    "epilogue_effective_itemsize",
    "constraints"
  ],
  "make_default_opt_flags_nvidia": [
    "out_dtype",
    "lhs_dtype",
    "rhs_dtype",
    "precision_config",
    "m",
    "n",
    "k",
    "routing_data",
    "can_use_persistent_tma",
    "can_use_fused_scatter",
    "enforce_bitwise_invariance",
    "epilogue_effective_itemsize",
    "constraints"
  ],
  "update_opt_flags_constraints": [
    "constraints"
  ],
  "reset_opt_flags_constraints": [],
  "set_opt_flags": [
    "opt_flags"
  ],
  "InapplicableConstraint": {},
  "make_opt_flags": [
    "out_dtype",
    "lhs_dtype",
    "rhs_dtype",
    "precision_config",
    "m",
    "n",
    "k",
    "routing_data",
    "can_use_persistent_tma",
    "can_use_fused_scatter",
    "epilogue_effective_itemsize"
  ],
  "_zero_masked_rows": [
    "pid_m",
    "pid_n",
    "Y",
    "stride_y_m",
    "stride_y_n",
    "N",
    "ScatterSrcIndx",
    "num_idxs",
    "BLOCK_M",
    "BLOCK_N"
  ],
  "_matmul_ogs": [
    "Y",
    "YPtr",
    "stride_y_k",
    "stride_y_z",
    "stride_y_m",
    "stride_y_n",
    "YExpectedScale",
    "YActualScale",
    "YChecksumScale",
    "stride_y_mx_z",
    "stride_y_mx_m",
    "stride_y_mx_n",
    "X",
    "XPtr",
    "stride_x_z",
    "stride_x_m",
    "stride_x_k",
    "XScale",
    "XMxScale",
    "stride_x_mx_z",
    "stride_x_mx_m",
    "stride_x_mx_k",
    "W",
    "WPtr",
    "stride_w_e",
    "stride_w_k",
    "stride_w_n",
    "W_TRANSPOSE",
    "WScale",
    "WMxScale",
    "stride_w_mx_e",
    "stride_w_mx_k",
    "stride_w_mx_n",
    "B",
    "stride_b_e",
    "NRows",
    "M",
    "N",
    "K",
    "Betas",
    "Gammas",
    "GatherIndx",
    "ScatterSrcIndx",
    "num_idxs",
    "WriteBackIndx",
    "writeback_size",
    "ExptHist",
    "ExptOffs",
    "ExptOffsSum",
    "ExptData",
    "batch_size",
    "grid_m",
    "grid_n",
    "out_alpha",
    "ACTIVATION_FN",
    "activation_fn_args",
    "ACTIVATION_REDUCTION_N",
    "EPILOGUE_FN",
    "epilogue_fn_args",
    "N_EXPTS_TOT",
    "N_EXPTS_ACT",
    "MAX_NUM_IMPRECISE_ACC",
    "ALLOW_TF32",
    "FLEXPOINT_SATURATE_INF",
    "PER_BATCH_SCALE",
    "BLOCK_M",
    "BLOCK_N",
    "BLOCK_K",
    "GROUP_M",
    "XCD_SWIZZLE",
    "SWIZZLE_MX_VALUE",
    "SWIZZLE_MX_SCALE",
    "EPILOGUE_SUBTILE",
    "EVEN_K",
    "SPLIT_K",
    "W_CACHE_MODIFIER",
    "NUM_SMS",
    "X_TMA_MODE",
    "Y_TMA_MODE",
    "TOKENS_PER_EXPT_FOR_ANNOTATION",
    "UPCAST_INDICES",
    "SWAP_XW",
    "IS_EPILOGUE_QUANT_MXFP8"
  ],
  "compute_grid_size": [
    "routing_data",
    "m",
    "n",
    "block_m",
    "block_n"
  ],
  "compute_block_n": [
    "n",
    "arch",
    "precision_config"
  ],
  "compute_block_k": [
    "m",
    "k",
    "is_persistent",
    "lhs_dtype",
    "rhs_dtype",
    "precision_config"
  ],
  "compute_split_k": [
    "block_k",
    "k",
    "grid_size"
  ],
  "compute_num_warps": [
    "block_m",
    "block_n",
    "precision_config"
  ],
  "compute_num_stages": [
    "precision_config",
    "is_persistent",
    "block_m",
    "block_n",
    "block_k",
    "out_dtype",
    "lhs_dtype",
    "rhs_dtype",
    "epilogue_subtile",
    "epilogue_effective_itemsize"
  ],
  "compute_block_nk": [
    "n",
    "block_m",
    "grid_m",
    "num_xcds",
    "lhs_dtype",
    "rhs_dtype",
    "precision_config"
  ],
  "vpopc": [
    "x"
  ],
  "_sum_bitmatrix_memset": [
    "Ret",
    "BLOCK"
  ],
  "_sum_bitmatrix_rows": [
    "B",
    "shape_bm",
    "stride_bm",
    "stride_bn",
    "Ret",
    "Partials",
    "stride_pm",
    "stride_pn",
    "shape_pn",
    "BLOCK_MM",
    "BLOCK_M"
  ],
  "clear_sums": [
    "n_cols",
    "device",
    "MEMSET_BLOCK"
  ],
  "sum_bitmatrix_rows": [
    "x",
    "out_ret",
    "partials_block_size"
  ]
}