{
  "tabulate": [
    "rows",
    "headers"
  ],
  "ask_for_confirmation": [
    "message"
  ],
  "main": [],
  "StreamingDetokenizer": {
    "__slots__": [],
    "reset": [
      "self"
    ],
    "add_token": [
      "self",
      "token"
    ],
    "finalize": [
      "self"
    ],
    "last_segment": [
      "self"
    ]
  },
  "NaiveStreamingDetokenizer": {
    "__init__": [
      "self",
      "tokenizer"
    ],
    "reset": [
      "self"
    ],
    "add_token": [
      "self",
      "token"
    ],
    "finalize": [
      "self"
    ],
    "text": [
      "self"
    ]
  },
  "SPMStreamingDetokenizer": {
    "__init__": [
      "self",
      "tokenizer",
      "trim_space"
    ],
    "reset": [
      "self"
    ],
    "_try_flush": [
      "self",
      "force"
    ],
    "add_token": [
      "self",
      "token"
    ],
    "finalize": [
      "self"
    ]
  },
  "BPEStreamingDetokenizer": {
    "_byte_decoder": [],
    "_space_matches": [],
    "__init__": [
      "self",
      "tokenizer"
    ],
    "reset": [
      "self"
    ],
    "_decode_bytes": [
      "self",
      "seq"
    ],
    "_maybe_trim_space": [
      "self",
      "current_text"
    ],
    "add_token": [
      "self",
      "token"
    ],
    "finalize": [
      "self"
    ],
    "make_byte_decoder": [
      "cls"
    ]
  },
  "TokenizerWrapper": {
    "__init__": [
      "self",
      "tokenizer",
      "detokenizer_class",
      "eos_token_ids",
      "chat_template",
      "tool_call_start",
      "tool_call_end",
      "tool_parser"
    ],
    "apply_chat_template": [
      "self"
    ],
    "add_eos_token": [
      "self",
      "token"
    ],
    "has_thinking": [
      "self"
    ],
    "think_start": [
      "self"
    ],
    "think_start_id": [
      "self"
    ],
    "think_end": [
      "self"
    ],
    "think_end_id": [
      "self"
    ],
    "has_tool_calling": [
      "self"
    ],
    "tool_call_start": [
      "self"
    ],
    "tool_call_end": [
      "self"
    ],
    "tool_parser": [
      "self"
    ],
    "detokenizer": [
      "self"
    ],
    "__getattr__": [
      "self",
      "attr"
    ],
    "__setattr__": [
      "self",
      "attr",
      "value"
    ]
  },
  "NewlineTokenizer": {
    "__init__": [
      "self"
    ],
    "_preprocess_text": [
      "self",
      "text"
    ],
    "_postprocess_text": [
      "self",
      "text"
    ],
    "encode": [
      "self",
      "text"
    ],
    "encode_batch": [
      "self",
      "texts"
    ],
    "decode": [
      "self"
    ],
    "batch_decode": [
      "self"
    ]
  },
  "_match": [
    "a",
    "b"
  ],
  "_is_spm_decoder": [
    "decoder"
  ],
  "_is_spm_decoder_no_space": [
    "decoder"
  ],
  "_is_bpe_decoder": [
    "decoder"
  ],
  "_infer_tool_parser": [
    "chat_template"
  ],
  "load": [
    "model_path",
    "tokenizer_config_extra",
    "eos_token_ids"
  ],
  "no_bos_or_eos": [
    "sequence",
    "bos",
    "eos"
  ],
  "DEFAULT_PROMPT": [],
  "DEFAULT_MAX_TOKENS": [],
  "DEFAULT_TEMP": [],
  "DEFAULT_TOP_P": [],
  "DEFAULT_MIN_P": [],
  "DEFAULT_TOP_K": [],
  "DEFAULT_XTC_PROBABILITY": [],
  "DEFAULT_XTC_THRESHOLD": [],
  "DEFAULT_MIN_TOKENS_TO_KEEP": [],
  "DEFAULT_SEED": [],
  "DEFAULT_MODEL": [],
  "DEFAULT_QUANTIZED_KV_START": [],
  "str2bool": [
    "string"
  ],
  "setup_arg_parser": [],
  "generation_stream": [],
  "wired_limit": [
    "model",
    "streams"
  ],
  "GenerationResponse": {},
  "maybe_quantize_kv_cache": [
    "prompt_cache",
    "quantized_kv_start",
    "kv_group_size",
    "kv_bits"
  ],
  "generate_step": [
    "prompt",
    "model"
  ],
  "speculative_generate_step": [
    "prompt",
    "model",
    "draft_model"
  ],
  "stream_generate": [
    "model",
    "tokenizer",
    "prompt",
    "max_tokens",
    "draft_model"
  ],
  "generate": [
    "model",
    "tokenizer",
    "prompt",
    "verbose"
  ],
  "_left_pad_prompts": [
    "prompts",
    "max_length"
  ],
  "_right_pad_prompts": [
    "prompts",
    "max_length"
  ],
  "BatchStats": {},
  "BatchResponse": {},
  "Batch": {
    "__len__": [
      "self"
    ],
    "filter": [
      "self",
      "keep_idx"
    ],
    "extend": [
      "self",
      "other"
    ],
    "extract_cache": [
      "self",
      "idx"
    ]
  },
  "_make_cache": [
    "model",
    "left_padding",
    "max_kv_size"
  ],
  "_merge_caches": [
    "caches"
  ],
  "BatchGenerator": {
    "__init__": [
      "self",
      "model",
      "max_tokens",
      "stop_tokens",
      "sampler",
      "logits_processors",
      "completion_batch_size",
      "prefill_batch_size",
      "prefill_step_size",
      "prompt_progress_callback",
      "max_kv_size"
    ],
    "close": [
      "self"
    ],
    "__del__": [
      "self"
    ],
    "insert": [
      "self",
      "prompts",
      "max_tokens",
      "caches",
      "samplers",
      "logits_processors"
    ],
    "remove": [
      "self",
      "uids",
      "return_prompt_caches"
    ],
    "_process_prompts": [
      "self",
      "prompts"
    ],
    "_step": [
      "self",
      "input_tokens",
      "prompt_cache",
      "samplers",
      "logits_processors",
      "tokens"
    ],
    "stats": [
      "self"
    ],
    "_next": [
      "self"
    ],
    "next": [
      "self"
    ]
  },
  "batch_generate": [
    "model",
    "tokenizer",
    "prompts",
    "prompt_caches",
    "max_tokens",
    "verbose",
    "return_prompt_caches",
    "logits_processors"
  ],
  "yaml_loader": [],
  "CONFIG_DEFAULTS": [],
  "build_parser": [],
  "train_model": [
    "args",
    "model",
    "train_set",
    "valid_set",
    "training_callback"
  ],
  "evaluate_model": [
    "args",
    "model",
    "test_set"
  ],
  "run": [
    "args",
    "training_callback"
  ],
  "parse_arguments": [],
  "make_sampler": [
    "temp",
    "top_p",
    "min_p",
    "min_tokens_to_keep",
    "top_k",
    "xtc_probability",
    "xtc_threshold",
    "xtc_special_tokens"
  ],
  "make_logits_processors": [
    "logit_bias",
    "repetition_penalty",
    "repetition_context_size"
  ],
  "apply_top_k": [
    "logprobs",
    "top_k"
  ],
  "apply_min_p": [
    "logprobs",
    "min_p",
    "min_tokens_to_keep"
  ],
  "apply_top_p": [
    "logprobs",
    "top_p"
  ],
  "apply_xtc": [
    "logits",
    "xtc_probability",
    "xtc_threshold",
    "xtc_special_tokens"
  ],
  "categorical_sampling": [
    "logits",
    "temp"
  ],
  "make_repetition_penalty": [
    "penalty",
    "context_size"
  ],
  "TokenType": {
    "NORMAL": [],
    "UNKNOWN": [],
    "CONTROL": [],
    "USER_DEFINED": [],
    "UNUSED": [],
    "BYTE": []
  },
  "GGMLFileType": {
    "GGML_TYPE_F16": []
  },
  "HfVocab": {
    "__init__": [
      "self",
      "fname_tokenizer",
      "fname_added_tokens"
    ],
    "hf_tokens": [
      "self"
    ],
    "get_token_type": [
      "self",
      "token_id",
      "token_text",
      "special_ids"
    ],
    "get_token_score": [
      "self",
      "token_id"
    ],
    "added_tokens": [
      "self"
    ],
    "has_newline_token": [
      "self"
    ],
    "all_tokens": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "load": [
      "path"
    ]
  },
  "translate_weight_names": [
    "name"
  ],
  "permute_weights": [
    "weights",
    "n_head",
    "n_head_kv"
  ],
  "prepare_metadata": [
    "config",
    "vocab"
  ],
  "convert_to_gguf": [
    "model_path",
    "weights",
    "config",
    "output_file_path"
  ],
  "MODEL_REMAPPING": [],
  "MAX_FILE_SIZE_GB": [],
  "_unpack_awq_weights": [
    "qweight"
  ],
  "_transform_awq_weights": [
    "weights",
    "quantization_config"
  ],
  "_get_classes": [
    "config"
  ],
  "get_total_parameters": [
    "model"
  ],
  "compute_bits_per_weight": [
    "model"
  ],
  "_download": [
    "path_or_hf_repo",
    "revision",
    "allow_patterns"
  ],
  "hf_repo_to_path": [
    "hf_repo"
  ],
  "load_config": [
    "model_path"
  ],
  "load_model": [
    "model_path",
    "lazy",
    "strict",
    "model_config",
    "get_model_classes"
  ],
  "load_adapters": [
    "model",
    "adapter_path"
  ],
  "load_tokenizer": [
    "model_path",
    "tokenizer_config_extra",
    "eos_token_ids"
  ],
  "sharded_load": [
    "repo",
    "pipeline_group",
    "tensor_group",
    "return_config"
  ],
  "pipeline_load": [
    "repo",
    "return_config"
  ],
  "make_shards": [
    "weights",
    "max_file_size_gb"
  ],
  "create_model_card": [
    "path",
    "hf_path"
  ],
  "upload_to_hub": [
    "path",
    "upload_repo"
  ],
  "save_model": [
    "save_path",
    "model"
  ],
  "quantize_model": [
    "model",
    "config",
    "group_size",
    "bits",
    "mode",
    "quant_predicate"
  ],
  "dequantize_model": [
    "model"
  ],
  "save_config": [
    "config",
    "config_path"
  ],
  "save": [
    "dst_path",
    "src_path_or_repo",
    "model",
    "tokenizer",
    "config",
    "donate_model"
  ],
  "common_prefix_len": [
    "list1",
    "list2"
  ],
  "does_model_support_input_embeddings": [
    "model"
  ],
  "__all__": [],
  "mixed_quant_predicate_builder": [
    "recipe",
    "model",
    "group_size"
  ],
  "QUANT_RECIPES": [],
  "MODEL_CONVERSION_DTYPES": [],
  "convert": [
    "hf_path",
    "mlx_path",
    "quantize",
    "q_group_size",
    "q_bits",
    "q_mode",
    "dtype",
    "upload_repo",
    "revision",
    "dequantize",
    "quant_predicate",
    "trust_remote_code"
  ],
  "configure_parser": [],
  "get_system_fingerprint": [],
  "StopCondition": {},
  "stopping_criteria": [
    "tokens",
    "eos_token_ids",
    "stop_id_sequences",
    "stop_words"
  ],
  "sequence_overlap": [
    "s1",
    "s2"
  ],
  "convert_chat": [
    "messages",
    "role_mapping"
  ],
  "process_message_content": [
    "messages"
  ],
  "LRUPromptCache": {
    "__init__": [
      "self",
      "max_size"
    ],
    "_search": [
      "self",
      "model",
      "tokens"
    ],
    "_get": [
      "self",
      "model",
      "tokens"
    ],
    "_delete": [
      "self",
      "model",
      "tokens"
    ],
    "_extract": [
      "self",
      "model",
      "tokens"
    ],
    "fetch_nearest_cache": [
      "self",
      "model",
      "tokens"
    ],
    "insert_cache": [
      "self",
      "model",
      "tokens",
      "prompt_cache"
    ]
  },
  "ModelDescription": {},
  "SamplingArguments": {},
  "LogitsProcessorArguments": {},
  "GenerationArguments": {},
  "CompletionRequest": {},
  "GenerationContext": {
    "stop": [
      "self"
    ]
  },
  "Response": {},
  "TimeBudget": {
    "__init__": [
      "self",
      "budget",
      "iterations",
      "sync_frequency"
    ],
    "__iter__": [
      "self"
    ],
    "__next__": [
      "self"
    ]
  },
  "ModelProvider": {
    "__init__": [
      "self",
      "cli_args"
    ],
    "load": [
      "self",
      "model_path",
      "adapter_path",
      "draft_model_path"
    ]
  },
  "_make_sampler": [
    "args",
    "tokenizer"
  ],
  "_make_logits_processors": [
    "args"
  ],
  "_format_top_logprobs": [
    "logprobs",
    "top_logprobs",
    "tokenizer"
  ],
  "ResponseGenerator": {
    "__init__": [
      "self",
      "model_provider",
      "prompt_cache"
    ],
    "stop_and_join": [
      "self"
    ],
    "join": [
      "self"
    ],
    "_next_request": [
      "self",
      "timeout"
    ],
    "_share_object": [
      "self",
      "obj"
    ],
    "_share_request": [
      "self",
      "request"
    ],
    "_tokenize": [
      "self",
      "tokenizer",
      "request",
      "args"
    ],
    "_is_batchable": [
      "self",
      "args"
    ],
    "_generate": [
      "self"
    ],
    "_serve_single": [
      "self",
      "request"
    ],
    "generate": [
      "self",
      "request",
      "generation_args",
      "progress_callback"
    ],
    "cli_args": [
      "self"
    ]
  },
  "APIHandler": {
    "__init__": [
      "self",
      "response_generator"
    ],
    "_set_cors_headers": [
      "self"
    ],
    "_set_completion_headers": [
      "self",
      "status_code"
    ],
    "_set_stream_headers": [
      "self",
      "status_code"
    ],
    "do_OPTIONS": [
      "self"
    ],
    "do_POST": [
      "self"
    ],
    "validate_model_parameters": [
      "self"
    ],
    "generate_response": [
      "self",
      "text",
      "finish_reason",
      "prompt_token_count",
      "completion_token_count",
      "token_logprobs",
      "top_tokens",
      "tokens",
      "tool_calls",
      "reasoning_text"
    ],
    "handle_completion": [
      "self",
      "request",
      "stop_words"
    ],
    "completion_usage_response": [
      "self",
      "prompt_token_count",
      "completion_token_count"
    ],
    "handle_chat_completions": [
      "self"
    ],
    "handle_text_completions": [
      "self"
    ],
    "do_GET": [
      "self"
    ],
    "handle_health_check": [
      "self"
    ],
    "handle_models_request": [
      "self"
    ]
  },
  "_run_http_server": [
    "host",
    "port",
    "response_generator",
    "server_class",
    "handler_class"
  ],
  "load_data": [
    "tokenizer",
    "data_path",
    "num_samples",
    "sequence_length"
  ],
  "eval_ppl": [
    "model",
    "data",
    "batch_size"
  ],
  "_rstrip_until": [
    "s",
    "untils"
  ],
  "_lstrip": [
    "s",
    "pattern"
  ],
  "_pad_inputs": [
    "inputs"
  ],
  "chat_template_fn": [],
  "MLXLM": {
    "apply_chat_template": [],
    "__init__": [
      "self",
      "path_or_hf_repo",
      "max_tokens",
      "batch_size",
      "use_chat_template",
      "trust_remote_code",
      "sampler"
    ],
    "_process_prompt": [
      "self",
      "prompt",
      "step_size"
    ],
    "_score_fn": [
      "self",
      "inputs",
      "cache",
      "step_size"
    ],
    "_tokenize": [
      "self",
      "texts"
    ],
    "tokenizer_name": [
      "self"
    ],
    "loglikelihood": [
      "self",
      "requests"
    ],
    "loglikelihood_rolling": [
      "self",
      "requests"
    ],
    "generate_until": [
      "self",
      "requests"
    ]
  },
  "__version__": [],
  "_tool_call_regex": [],
  "tool_call_start": [],
  "tool_call_end": [],
  "parse_tool_call": [
    "text",
    "tools"
  ],
  "_func_name_regex": [],
  "_func_arg_regex": [],
  "_is_string_type": [
    "tool_name",
    "arg_name",
    "tools"
  ],
  "_deserialize": [
    "value"
  ],
  "_tool_call_split_regex": [],
  "_parse_single_tool": [
    "text"
  ],
  "_get_string_arg_names": [
    "tool_name",
    "tools"
  ],
  "_normalize_arguments": [
    "func_name",
    "arguments",
    "tools",
    "string_args"
  ],
  "_parse_json_tool_call": [
    "text",
    "tools"
  ],
  "_parse_key_value_pairs": [
    "text",
    "func_name",
    "tools",
    "string_args"
  ],
  "_parse_plain_text_tool_call": [
    "text",
    "tools"
  ],
  "_function_regex": [],
  "_parameter_regex": [],
  "_string_types": [],
  "_bool_types": [],
  "_obj_types": [],
  "_get_arguments_config": [
    "func_name",
    "tools"
  ],
  "_convert_param_value": [
    "param_value",
    "param_name",
    "param_config"
  ],
  "_parse_xml_function_call": [
    "function_call_str",
    "tools"
  ],
  "_tool_args_regex": [],
  "_invoke_complete_regex": [],
  "_parameter_complete_regex": [],
  "_extract_name": [
    "name_str"
  ],
  "_extract_types_from_schema": [
    "schema"
  ],
  "_convert_param_value_with_types": [
    "value",
    "param_types"
  ],
  "_get_param_types_from_config": [
    "param_name",
    "param_config"
  ],
  "can_run_metal": [],
  "_make_kl_forward_kernel": [],
  "_make_kl_backward_kernel": [],
  "_kl_forward_kernel": [],
  "_kl_backward_kernel": [],
  "_kl_div_loss": [
    "primals",
    "cotangent",
    "output"
  ],
  "kl_div_loss": [
    "logits_q",
    "logits_p"
  ],
  "_make_js_forward_kernel": [],
  "_make_js_backward_kernel": [],
  "_js_forward_kernel": [],
  "_js_backward_kernel": [],
  "_js_div_loss": [
    "primals",
    "cotangents",
    "outputs"
  ],
  "js_div_loss": [
    "logits_q",
    "logits_p"
  ],
  "TrainingCallback": {
    "on_train_loss_report": [
      "self",
      "train_info"
    ],
    "on_val_loss_report": [
      "self",
      "val_info"
    ]
  },
  "WandBCallback": {
    "__init__": [
      "self",
      "project_name",
      "log_dir",
      "config",
      "wrapped_callback"
    ],
    "_convert_to_serializable": [
      "self",
      "data"
    ],
    "on_train_loss_report": [
      "self",
      "train_info"
    ],
    "on_val_loss_report": [
      "self",
      "val_info"
    ]
  },
  "SwanLabCallback": {
    "__init__": [
      "self",
      "project_name",
      "log_dir",
      "config",
      "wrapped_callback"
    ],
    "_convert_to_serializable": [
      "self",
      "data"
    ],
    "on_train_loss_report": [
      "self",
      "train_info"
    ],
    "on_val_loss_report": [
      "self",
      "val_info"
    ]
  },
  "SUPPORT_CALLBACK": [],
  "get_reporting_callbacks": [
    "report_to",
    "project_name",
    "log_dir",
    "config"
  ],
  "LoRALinear": {
    "from_base": [
      "linear",
      "r",
      "dropout",
      "scale"
    ],
    "fuse": [
      "self",
      "dequantize"
    ],
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "r",
      "dropout",
      "scale",
      "bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LoRASwitchLinear": {
    "from_base": [
      "linear",
      "r",
      "dropout",
      "scale"
    ],
    "fuse": [
      "self",
      "dequantize"
    ],
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "num_experts",
      "r",
      "dropout",
      "scale",
      "bias"
    ],
    "__call__": [
      "self",
      "x",
      "indices",
      "sorted_indices"
    ]
  },
  "LoRAEmbedding": {
    "from_base": [
      "embedding",
      "r",
      "dropout",
      "scale"
    ],
    "fuse": [
      "self",
      "dequantize"
    ],
    "__init__": [
      "self",
      "num_embeddings",
      "dims",
      "r",
      "dropout",
      "scale"
    ],
    "__call__": [
      "self",
      "x"
    ],
    "as_linear": [
      "self",
      "x"
    ]
  },
  "grad_checkpoint": [
    "layer"
  ],
  "TrainingArgs": {},
  "default_loss": [
    "model",
    "batch",
    "lengths"
  ],
  "iterate_batches": [
    "dataset",
    "batch_size",
    "max_seq_length",
    "loop",
    "seed",
    "comm_group"
  ],
  "evaluate": [
    "model",
    "dataset",
    "batch_size",
    "num_batches",
    "max_seq_length",
    "loss",
    "iterate_batches"
  ],
  "train": [
    "model",
    "optimizer",
    "train_dataset",
    "val_dataset",
    "args",
    "loss",
    "iterate_batches",
    "training_callback"
  ],
  "DoRALinear": {
    "from_base": [
      "linear",
      "r",
      "dropout",
      "scale"
    ],
    "fuse": [
      "self",
      "dequantize"
    ],
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "r",
      "dropout",
      "scale",
      "bias"
    ],
    "set_linear": [
      "self",
      "linear"
    ],
    "_dequantized_weight": [
      "self"
    ],
    "_is_quantized": [
      "self"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DoRAEmbedding": {
    "from_base": [
      "embedding",
      "r",
      "dropout",
      "scale"
    ],
    "fuse": [
      "self",
      "dequantize"
    ],
    "__init__": [
      "self",
      "num_embeddings",
      "dims",
      "r",
      "dropout",
      "scale"
    ],
    "set_embedding": [
      "self",
      "embedding"
    ],
    "__call__": [
      "self",
      "x"
    ],
    "as_linear": [
      "self",
      "x"
    ]
  },
  "build_schedule": [
    "schedule_config"
  ],
  "linear_to_lora_layers": [
    "model",
    "num_layers",
    "config",
    "use_dora"
  ],
  "remove_lora_layers": [
    "model"
  ],
  "print_trainable_parameters": [
    "model"
  ],
  "TextDataset": {
    "__init__": [
      "self",
      "data",
      "tokenizer",
      "text_key"
    ],
    "process": [
      "self",
      "d"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__len__": [
      "self"
    ]
  },
  "ChatDataset": {
    "__init__": [
      "self",
      "data",
      "tokenizer",
      "chat_key",
      "mask_prompt"
    ],
    "process": [
      "self",
      "d"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__len__": [
      "self"
    ]
  },
  "CompletionsDataset": {
    "__init__": [
      "self",
      "data",
      "tokenizer",
      "prompt_key",
      "completion_key",
      "mask_prompt"
    ],
    "process": [
      "self",
      "d"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__len__": [
      "self"
    ]
  },
  "ConcatenatedDataset": {
    "__init__": [
      "self",
      "data"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "process": [
      "self",
      "d"
    ],
    "__len__": [
      "self"
    ]
  },
  "CacheDataset": {
    "__init__": [
      "self",
      "data"
    ],
    "itemlen": [
      "self",
      "idx"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "__len__": [
      "self"
    ]
  },
  "create_dataset": [
    "data",
    "tokenizer",
    "config"
  ],
  "load_local_dataset": [
    "data_path",
    "tokenizer",
    "config"
  ],
  "load_hf_dataset": [
    "data_id",
    "tokenizer",
    "config"
  ],
  "load_custom_hf_dataset": [
    "args",
    "tokenizer"
  ],
  "load_dataset": [
    "args",
    "tokenizer"
  ],
  "TOOLS_SYSTEM_TEMPLATE": [],
  "thinking_template": [],
  "tool_calls_template": [],
  "to_json": [
    "value"
  ],
  "tools_from_openai_format": [
    "tools"
  ],
  "tool_calls_from_openai_format": [
    "tool_calls"
  ],
  "encode_arguments_to_dsml": [
    "tool_call"
  ],
  "decode_dsml_to_arguments": [
    "tool_name",
    "tool_args"
  ],
  "render_tools": [
    "tools"
  ],
  "find_last_user_index": [
    "messages"
  ],
  "render_message": [
    "index",
    "messages",
    "thinking_mode",
    "tools"
  ],
  "drop_thinking_messages": [
    "messages",
    "last_user_idx"
  ],
  "encode_messages": [
    "messages",
    "thinking_mode",
    "context",
    "drop_thinking",
    "add_default_bos_token",
    "tools"
  ],
  "apply_chat_template": [
    "messages",
    "continue_final_message",
    "add_generation_prompt"
  ],
  "ScaleConfig": {},
  "AWQConfig": {},
  "update": [
    "cfg"
  ],
  "llama_awq": [],
  "gemma3_text_awq": [],
  "gemma3_awq": [],
  "deepseek_v2_awq": [],
  "AWQ_MODEL_CONFIGS": [],
  "mse": [
    "x",
    "y"
  ],
  "submodule_from_key": [
    "module",
    "key"
  ],
  "run_layer": [
    "layer",
    "x",
    "indices",
    "batch_size"
  ],
  "dist_split": [
    "x",
    "group"
  ],
  "search_best_scale": [
    "layers",
    "quantize_func",
    "block",
    "layer_kwargs",
    "n_grid"
  ],
  "apply_scale": [
    "prev_op",
    "layers",
    "scales"
  ],
  "scale_block": [
    "block",
    "configs",
    "quantize_func",
    "layer_kwargs",
    "n_grid"
  ],
  "search_best_clip": [
    "module",
    "quantize_func",
    "group_size",
    "n_grid",
    "max_shrink",
    "batch_size",
    "n_frames"
  ],
  "clip_block": [
    "block",
    "no_clip_keys",
    "quantize_func",
    "group_size",
    "n_grid"
  ],
  "awq_quantize": [
    "model",
    "inputs",
    "awq_config",
    "group_size",
    "bits",
    "embed_group_size",
    "embed_bits",
    "n_grid"
  ],
  "update_config": [
    "model",
    "config"
  ],
  "estimate_sensitivities": [
    "model",
    "data",
    "low_bits",
    "low_group_size",
    "high_bits",
    "high_group_size",
    "batch_size",
    "gradient_accum_dtype",
    "gradient_checkpoint"
  ],
  "estimate_threshold": [
    "model",
    "sensitivities",
    "target_bpw",
    "low_bits",
    "low_group_size",
    "high_bits",
    "high_group_size"
  ],
  "compute_dwq_targets": [
    "model",
    "save_dir",
    "train_data",
    "valid_data",
    "batch_size",
    "max_seq_length",
    "seed"
  ],
  "dwq_quantize": [
    "model",
    "target_fn",
    "opt",
    "train_data",
    "valid_data",
    "batch_size",
    "max_seq_length",
    "seed",
    "dtype",
    "gradient_checkpoint",
    "temperature"
  ],
  "quantize": [
    "w",
    "bits",
    "scales",
    "biases"
  ],
  "Catcher": {
    "__init__": [
      "self",
      "module"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "gptq_quantize": [
    "model",
    "data",
    "bits",
    "group_size",
    "fallback_bits",
    "fallback_group_size",
    "batch_size"
  ],
  "ModelArgs": {},
  "YoutuLLMAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "YoutuLLMMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "YoutuLLMDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "YoutuLLMModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Model": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ],
    "sanitize": [
      "self",
      "weights"
    ],
    "layers": [
      "self"
    ]
  },
  "Attention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "TransformerBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Qwen2Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "Qwen3NextRMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "Qwen3NextAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Qwen3NextMLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3NextGatedDeltaNet": {
    "__init__": [
      "self",
      "config"
    ],
    "fix_query_key_value_ordering": [
      "self",
      "mixed_qkvz",
      "mixed_ba"
    ],
    "__call__": [
      "self",
      "inputs",
      "mask",
      "cache"
    ]
  },
  "Qwen3NextSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3NextDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Qwen3NextModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "recurrent_gla": [
    "q",
    "k",
    "v",
    "g",
    "scale",
    "h"
  ],
  "GroupRMSNorm": {
    "__init__": [
      "self",
      "dims",
      "eps",
      "groups"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LinearAttention": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "_get_slopes": [
      "self"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "offset"
    ]
  },
  "group_expert_select": [
    "gates",
    "e_score_correction_bias",
    "top_k",
    "n_group",
    "topk_group",
    "routed_scaling_factor",
    "norm_topk_prob",
    "score_function"
  ],
  "Gate": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "SparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "offset"
    ]
  },
  "LanguageModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "LayerNorm2D": {
    "__init__": [
      "self",
      "d1",
      "d2",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "CohereModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "GPTBigCodeModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "GraniteMoeAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "GraniteMoeTopKGating": {
    "__init__": [
      "self",
      "input_size",
      "num_experts",
      "top_k"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteMoeMoE": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GraniteMoeDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "GraniteMoEModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "relu_squared": [
    "x"
  ],
  "NemotronLayerNorm1P": {
    "__call__": [
      "self",
      "x"
    ]
  },
  "NemotronModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "RMSNorm": {
    "__init__": [
      "self",
      "dims",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GemmaModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "GPTNeoXModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "rms_norm": [
    "x"
  ],
  "apply_rotary_emb": [
    "x",
    "offset",
    "base",
    "freqs"
  ],
  "NanoChatModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "softcap": [
    "logits",
    "cap"
  ],
  "KimiMLP": {
    "__init__": [
      "self",
      "args",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "_group_expert_select": [
    "gates",
    "bias",
    "top_k",
    "n_group",
    "topk_group",
    "routed_scaling_factor",
    "renormalize",
    "score_function"
  ],
  "KimiSparseMoE": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "KimiMLAAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "ShortConv1d": {
    "__init__": [
      "self",
      "channels",
      "kernel_size"
    ],
    "__call__": [
      "self",
      "x",
      "state",
      "mask",
      "lengths"
    ]
  },
  "KimiDeltaAttention": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "KimiDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "KimiLinearModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "MoEGate": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "bitnet_quantize": [
    "model",
    "quantization_config"
  ],
  "make_bitlinear_kernel": [],
  "_bitlinear_kernel": [],
  "BitLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "invert_weight_scales"
    ],
    "execute_matmul_kernel": [
      "self",
      "x",
      "packed_weights"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GraniteModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "ApertusMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ApertusAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "ApertusDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "ApertusModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "DynamicNTKScalingRoPE": {
    "__init__": [
      "self",
      "dims",
      "max_position_embeddings",
      "traditional",
      "base",
      "scale"
    ],
    "extra_repr": [
      "self"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "InternLM2Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Glm4MoeLiteAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Glm4MoeLiteMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4MoeLiteMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4MoeLiteDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Glm4MoeLiteModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "MiMoModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Transformer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "OlmoModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "OlmoeSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "OlmoeModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Qwen2MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen2MoeDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Qwen2MoeModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "BaseModelArgs": {
    "from_dict": [
      "cls",
      "params"
    ]
  },
  "create_causal_mask": [
    "N",
    "offset",
    "window_size",
    "right_padding",
    "left_padding"
  ],
  "create_attention_mask": [
    "h",
    "cache",
    "window_size",
    "return_array"
  ],
  "create_ssm_mask": [
    "h",
    "cache"
  ],
  "quantized_scaled_dot_product_attention": [
    "queries",
    "q_keys",
    "q_values",
    "scale",
    "mask",
    "group_size",
    "bits"
  ],
  "scaled_dot_product_attention": [
    "queries",
    "keys",
    "values",
    "cache",
    "scale",
    "mask",
    "sinks"
  ],
  "DeepseekAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "Olmo3Attention": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Olmo3MLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Olmo3DecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Olmo3Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "compute_dt": [
    "dt",
    "dt_bias",
    "time_step_limit"
  ],
  "make_ssm_kernel": [],
  "_ssm_kernel": [],
  "ssm_update_kernel": [
    "hidden_states",
    "A_log",
    "B",
    "C",
    "D",
    "dt",
    "dt_bias",
    "state",
    "time_step_limit"
  ],
  "segsum": [
    "x",
    "mask"
  ],
  "ssm_attn": [
    "x",
    "A_log",
    "B",
    "C",
    "D",
    "dt",
    "dt_bias",
    "state",
    "time_step_limit",
    "mask",
    "lengths",
    "step"
  ],
  "ssm_update": [
    "hidden_states",
    "A_log",
    "B",
    "C",
    "D",
    "dt",
    "dt_bias",
    "state",
    "time_step_limit",
    "mask",
    "lengths"
  ],
  "MambaRMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps",
      "group_size"
    ],
    "__call__": [
      "self",
      "x",
      "gate"
    ]
  },
  "NemotronHMamba2Mixer": {
    "__init__": [
      "self",
      "args"
    ],
    "_conv": [
      "self",
      "conv_input",
      "cache",
      "mask"
    ],
    "_ssm": [
      "self",
      "hidden_states",
      "B",
      "C",
      "dt",
      "cache",
      "mask"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "mask",
      "cache"
    ]
  },
  "NemotronHAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "NemotronHMLP": {
    "__init__": [
      "self",
      "args",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "NemotronHMoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "NemotronHBlock": {
    "__init__": [
      "self",
      "args",
      "block_type"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "NemotronHModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "HeliumAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "HeliumMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "HeliumDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "HeliumModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "_int_or_list": [
    "arg",
    "idx"
  ],
  "DynamicNTKAlphaRoPE": {
    "__init__": [
      "self",
      "dims",
      "base",
      "scaling_alpha"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "MoeBlock": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "HunYuanModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "swiglu": [
    "gate",
    "x"
  ],
  "xielu": [
    "x",
    "alpha_p",
    "alpha_n",
    "beta",
    "eps"
  ],
  "XieLU": {
    "__init__": [
      "self",
      "alpha_p_init",
      "alpha_n_init",
      "beta",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "make_prompt_cache": [
    "model",
    "max_kv_size"
  ],
  "save_prompt_cache": [
    "file_name",
    "cache",
    "metadata"
  ],
  "load_prompt_cache": [
    "file_name",
    "return_metadata"
  ],
  "can_trim_prompt_cache": [
    "cache"
  ],
  "trim_prompt_cache": [
    "cache",
    "num_tokens"
  ],
  "_BaseCache": {
    "state": [
      "self",
      "v"
    ],
    "meta_state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "size": [
      "self"
    ],
    "empty": [
      "self"
    ],
    "from_state": [
      "cls",
      "state",
      "meta_state"
    ]
  },
  "ConcatenateKVCache": {
    "__init__": [
      "self"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "make_mask": [
      "self"
    ],
    "empty": [
      "self"
    ]
  },
  "QuantizedKVCache": {
    "step": [],
    "__init__": [
      "self",
      "group_size",
      "bits"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "state": [
      "self",
      "v"
    ],
    "meta_state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "make_mask": [
      "self"
    ],
    "empty": [
      "self"
    ]
  },
  "KVCache": {
    "step": [],
    "__init__": [
      "self"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "size": [
      "self"
    ],
    "state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "to_quantized": [
      "self",
      "group_size",
      "bits"
    ],
    "make_mask": [
      "self"
    ],
    "merge": [
      "_",
      "caches"
    ],
    "empty": [
      "self"
    ]
  },
  "RotatingKVCache": {
    "step": [],
    "__init__": [
      "self",
      "max_size",
      "keep"
    ],
    "_trim": [
      "self",
      "trim_size",
      "v",
      "append"
    ],
    "_temporal_order": [
      "self",
      "v"
    ],
    "_update_concat": [
      "self",
      "keys",
      "values"
    ],
    "_update_in_place": [
      "self",
      "keys",
      "values"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "size": [
      "self"
    ],
    "state": [
      "self",
      "v"
    ],
    "meta_state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "to_quantized": [
      "self",
      "group_size",
      "bits"
    ],
    "make_mask": [
      "self",
      "N",
      "window_size",
      "return_array"
    ],
    "merge": [
      "_",
      "caches"
    ],
    "empty": [
      "self"
    ]
  },
  "ArraysCache": {
    "__new__": [
      "cls"
    ],
    "__init__": [
      "self",
      "size",
      "left_padding"
    ],
    "__setitem__": [
      "self",
      "idx",
      "value"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "state": [
      "self",
      "v"
    ],
    "filter": [
      "self",
      "batch_indices"
    ],
    "extend": [
      "self",
      "other"
    ],
    "extract": [
      "self",
      "idx"
    ],
    "prepare": [
      "self",
      "lengths"
    ],
    "finalize": [
      "self"
    ],
    "advance": [
      "self",
      "N"
    ],
    "make_mask": [
      "self",
      "N"
    ],
    "merge": [
      "cls",
      "caches"
    ],
    "empty": [
      "self"
    ]
  },
  "ChunkedKVCache": {
    "step": [],
    "__init__": [
      "self",
      "chunk_size"
    ],
    "maybe_trim_front": [
      "self"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "meta_state": [
      "self",
      "v"
    ],
    "empty": [
      "self"
    ]
  },
  "CacheList": {
    "__init__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "state": [
      "self",
      "v"
    ],
    "filter": [
      "self",
      "batch_indices"
    ],
    "extend": [
      "self",
      "other"
    ],
    "merge": [
      "cls",
      "caches"
    ],
    "extract": [
      "self",
      "idx"
    ],
    "prepare": [
      "self"
    ],
    "finalize": [
      "self"
    ],
    "size": [
      "self"
    ],
    "empty": [
      "self"
    ]
  },
  "dynamic_roll": [
    "x",
    "shifts",
    "axis"
  ],
  "BatchKVCache": {
    "step": [],
    "__init__": [
      "self",
      "left_padding"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "prepare": [
      "self"
    ],
    "finalize": [
      "self"
    ],
    "state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "make_mask": [
      "self",
      "N",
      "return_array"
    ],
    "filter": [
      "self",
      "batch_indices"
    ],
    "extend": [
      "self",
      "other"
    ],
    "extract": [
      "self",
      "idx"
    ],
    "merge": [
      "cls",
      "caches"
    ],
    "empty": [
      "self"
    ]
  },
  "BatchRotatingKVCache": {
    "step": [],
    "__init__": [
      "self",
      "max_size",
      "left_padding"
    ],
    "_trim": [
      "self",
      "trim_size",
      "v",
      "append"
    ],
    "_temporal_order": [
      "self"
    ],
    "_update_concat": [
      "self",
      "keys",
      "values"
    ],
    "_update_in_place": [
      "self",
      "keys",
      "values"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "prepare": [
      "self"
    ],
    "finalize": [
      "self"
    ],
    "state": [
      "self",
      "v"
    ],
    "meta_state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ],
    "to_quantized": [
      "self",
      "group_size",
      "bits"
    ],
    "make_mask": [
      "self",
      "N",
      "window_size",
      "return_array"
    ],
    "filter": [
      "self",
      "batch_indices"
    ],
    "extend": [
      "self",
      "other"
    ],
    "extract": [
      "self",
      "idx"
    ],
    "merge": [
      "cls",
      "caches"
    ],
    "empty": [
      "self"
    ]
  },
  "JambaMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "JambaAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "fma": [
    "a",
    "b",
    "c"
  ],
  "JambaMambaMixer": {
    "__init__": [
      "self",
      "args"
    ],
    "ssm_step": [
      "self",
      "x",
      "A",
      "state"
    ],
    "_process_sequence": [
      "self",
      "x",
      "conv_state",
      "ssm_state"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "JambaSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "JambaDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_type",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "JambaModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "ExaoneModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "PhiAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "PhiMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "PhiDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "PhiModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "NoPE": {
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "make_divisible": [
    "v",
    "divisor",
    "min_value"
  ],
  "OpenELMModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "gegelu_impl": [
    "a_gelu",
    "a_linear",
    "limit"
  ],
  "gegelu": [
    "x",
    "limit"
  ],
  "Phi3Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "MixtralAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MixtralSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MixtralDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MixtralModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Lille130mAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Lille130mMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Lille130Block": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Lille130": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "TextArgs": {},
  "LlamaModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "_get_llama_4_attn_scale": [
    "size",
    "offset",
    "beta",
    "max_position_embeddings"
  ],
  "Qwen3MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3MoeDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Qwen3MoeModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "ExaoneMoEModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Mamba2Block": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "_conv": [
      "self",
      "conv_input",
      "cache",
      "mask"
    ],
    "_ssm": [
      "self",
      "hidden_states",
      "B",
      "C",
      "dt",
      "cache",
      "mask"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "mask",
      "cache"
    ]
  },
  "ResidualBlock": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Mamba2": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "MambaBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "ssm_step": [
      "self",
      "x",
      "A",
      "state"
    ],
    "_process_sequence": [
      "self",
      "x",
      "conv_cache",
      "state_cache"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "Mamba": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "TextConfig": {},
  "RMSNoScale": {
    "__init__": [
      "self",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Gemma3nLaurelBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Gemma3nAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_kv_shared_layer"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "gelu_topk": [
    "inputs",
    "std_multiplier"
  ],
  "Gemma3nAltUp": {
    "__init__": [
      "self",
      "config"
    ],
    "compute_router_modalities": [
      "self",
      "x"
    ],
    "predict": [
      "self",
      "x"
    ],
    "correct": [
      "self",
      "predictions",
      "activated"
    ]
  },
  "Gemma3nDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_kv_shared_layer"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "per_layer_input"
    ]
  },
  "logit_softcap": [
    "softcap",
    "x"
  ],
  "Gemma3n": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ],
    "make_cache": [
      "self"
    ]
  },
  "Telechat3Attention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Telechat3MLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Telechat3DecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Telechat3Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "addcmul": [
    "x",
    "y",
    "z"
  ],
  "l2_norm": [
    "x"
  ],
  "_wkv7_step_ops": [
    "r",
    "w",
    "k",
    "v",
    "a",
    "b",
    "state"
  ],
  "_make_wkv7_kernel": [],
  "_wkv7_kernel": [],
  "wkv7_kernel": [
    "r",
    "w",
    "k",
    "v",
    "a",
    "b",
    "state"
  ],
  "LayerNormPerHead": {
    "__init__": [
      "self",
      "head_dim",
      "num_heads",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LoRA": {
    "__init__": [
      "self",
      "input_dim",
      "output_dim",
      "low_rank_dim",
      "bias",
      "activation"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "TokenShift": {
    "__call__": [
      "self",
      "x",
      "state"
    ]
  },
  "Rwkv7ChannelMixing": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "Rwkv7TimeMixing": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "_wkv7": [
      "self",
      "r",
      "w",
      "k",
      "v",
      "a",
      "b",
      "state"
    ],
    "__call__": [
      "self",
      "x",
      "v_first",
      "cache"
    ]
  },
  "Rwkv7Layer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "v_first",
      "cache"
    ]
  },
  "Rwkv7Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "GPT2Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "TextModelArgs": {
    "__post_init__": [
      "self"
    ]
  },
  "GatedDeltaNet": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "mask",
      "cache"
    ]
  },
  "Qwen3_5TextModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "TextModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ],
    "layers": [
      "self"
    ],
    "make_cache": [
      "self"
    ],
    "sanitize": [
      "self",
      "weights"
    ],
    "quant_predicate": [
      "self"
    ]
  },
  "FalconH1RMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps",
      "n_groups",
      "norm_before_gate"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "compute_mup_vector": [
    "args"
  ],
  "FalconH1Attention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "FalconH1Mixer": {
    "__init__": [
      "self",
      "args"
    ],
    "_conv": [
      "self",
      "conv_input",
      "cache",
      "mask"
    ],
    "_ssm": [
      "self",
      "hidden_states",
      "B",
      "C",
      "dt",
      "cache",
      "mask"
    ],
    "__call__": [
      "self",
      "input_states",
      "cache",
      "mask"
    ]
  },
  "FalconH1MLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "FalconH1DecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "h",
      "cache",
      "attn_mask",
      "mamba_mask"
    ]
  },
  "FalconH1Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "ShortConv": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Lfm2MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Lfm2DecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Lfm2Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "compute_g": [
    "A_log",
    "a",
    "dt_bias"
  ],
  "_make_gated_delta_kernel": [
    "has_mask",
    "vectorized"
  ],
  "_gated_delta_kernel": [],
  "_gated_delta_kernel_masked": [],
  "_gated_delta_kernel_vec": [],
  "_gated_delta_kernel_vec_masked": [],
  "_gated_delta_step_ops": [
    "q",
    "k",
    "v",
    "g",
    "beta",
    "state",
    "mask"
  ],
  "gated_delta_kernel": [
    "q",
    "k",
    "v",
    "g",
    "beta",
    "state",
    "mask"
  ],
  "gated_delta_ops": [
    "q",
    "k",
    "v",
    "g",
    "beta",
    "state",
    "mask"
  ],
  "gated_delta_update": [
    "q",
    "k",
    "v",
    "a",
    "b",
    "A_log",
    "dt_bias",
    "state",
    "mask",
    "use_kernel"
  ],
  "QwenModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "RoPEAttention": {
    "__init__": [
      "self",
      "dims",
      "num_heads",
      "rotary_dim"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MOE": {
    "__init__": [
      "self",
      "args",
      "dim",
      "hidden_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ParallelBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "TransformerDecoder": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Embd": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "OutputHead": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "GraniteMoeHybridRMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "GraniteMoeHybridMamba2Mixer": {
    "__init__": [
      "self",
      "args"
    ],
    "_conv": [
      "self",
      "conv_input",
      "cache",
      "mask"
    ],
    "_ssm": [
      "self",
      "hidden_states",
      "B",
      "C",
      "dt",
      "cache",
      "mask"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "mask",
      "cache"
    ]
  },
  "GraniteMoeHybridAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "GraniteMoeHybridTopKGating": {
    "__init__": [
      "self",
      "input_size",
      "num_experts",
      "top_k"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "GraniteMoeHybridMoE": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GraniteMoeHybridSharedMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GraniteMoeHybridMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GraniteMoeHybridLayer": {
    "__init__": [
      "self",
      "args",
      "layer_type"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "GraniteMoeHybridModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "PipelineMixin": {
    "__init__": [
      "self"
    ],
    "pipeline_layers": [
      "self"
    ],
    "pipeline": [
      "self",
      "group"
    ]
  },
  "_compute_gate": [
    "query",
    "weight",
    "bias"
  ],
  "_silu_mul": [
    "gate",
    "up"
  ],
  "_mix_attention": [
    "gate",
    "attn_global",
    "attn_local"
  ],
  "LoopGateProjection": {
    "__init__": [
      "self",
      "num_heads",
      "head_dim"
    ],
    "__call__": [
      "self",
      "query"
    ]
  },
  "IQuestLoopCoderModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "rnn_scan": [
    "x",
    "a",
    "h0"
  ],
  "Conv1d": {
    "__init__": [
      "self",
      "channels",
      "kernel_size"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "RGLRU": {
    "__init__": [
      "self",
      "width",
      "num_heads"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "RecurrentBlock": {
    "__init__": [
      "self",
      "width",
      "num_heads",
      "lru_width",
      "conv1d_temporal_width"
    ],
    "__call__": [
      "self",
      "x",
      "cache",
      "mask"
    ]
  },
  "LocalAttentionBlock": {
    "__init__": [
      "self",
      "width",
      "num_heads",
      "window_size"
    ],
    "__call__": [
      "self",
      "x",
      "cache",
      "mask"
    ]
  },
  "MLPBlock": {
    "__init__": [
      "self",
      "width",
      "expanded_width"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Griffin": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "tokens",
      "cache"
    ]
  },
  "AttentionModule": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MiniCPMModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "LongcatFlashMLA": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "LongcatFlashMLP": {
    "__init__": [
      "self",
      "args",
      "is_expert"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LongcatFlashTopkRouter": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "LongcatFlashMoE": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "LongcatFlashDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "LongcatFlashModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "MiniCPM3Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "mask",
      "cache"
    ]
  },
  "_gather_sort": [
    "x",
    "indices"
  ],
  "_scatter_unsort": [
    "x",
    "inv_order",
    "shape"
  ],
  "QuantizedSwitchLinear": {
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "num_experts",
      "bias",
      "group_size",
      "bits",
      "mode"
    ],
    "input_dims": [
      "self"
    ],
    "output_dims": [
      "self"
    ],
    "num_experts": [
      "self"
    ],
    "__call__": [
      "self",
      "x",
      "indices",
      "sorted_indices"
    ]
  },
  "SwitchLinear": {
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "num_experts",
      "bias"
    ],
    "input_dims": [
      "self"
    ],
    "output_dims": [
      "self"
    ],
    "num_experts": [
      "self"
    ],
    "__call__": [
      "self",
      "x",
      "indices",
      "sorted_indices"
    ],
    "to_quantized": [
      "self",
      "group_size",
      "bits",
      "mode"
    ]
  },
  "SwiGLU": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "x",
      "gate"
    ]
  },
  "SwitchGLU": {
    "__init__": [
      "self",
      "input_dims",
      "hidden_dims",
      "num_experts",
      "activation",
      "bias"
    ],
    "__call__": [
      "self",
      "x",
      "indices"
    ]
  },
  "SwitchMLP": {
    "__init__": [
      "self",
      "input_dims",
      "hidden_dims",
      "num_experts",
      "activation",
      "bias"
    ],
    "__call__": [
      "self",
      "x",
      "indices"
    ]
  },
  "Ernie4_5_MLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim",
      "use_bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Ernie4_5_MoeMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Ernie4_5_DecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Ernie45Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Indexer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "qr",
      "mask",
      "cache"
    ]
  },
  "DeepseekV32Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV32MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV32MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV32DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV32Model": {
    "__init__": [
      "self",
      "config"
    ],
    "pipeline": [
      "self",
      "group"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "Dots1Attention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Dots1TopkRouter": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Dots1MLP": {
    "__init__": [
      "self",
      "args",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Dots1MoE": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Dots1DecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Dots1Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "NormAttnNorm": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Router": {
    "__init__": [
      "self",
      "d_model",
      "num_experts"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DBRX": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "PhiMoESparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "PhiMoEDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "PhiMoEModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "PlamoDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache"
    ]
  },
  "PlamoDecoder": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "PlamoModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "MultiLinear": {
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "num_heads"
    ],
    "__call__": [
      "self",
      "x",
      "transpose"
    ],
    "to_quantized": [
      "self",
      "group_size",
      "bits",
      "mode"
    ]
  },
  "QuantizedMultiLinear": {
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "num_heads",
      "group_size",
      "bits",
      "mode"
    ],
    "__call__": [
      "self",
      "x",
      "transpose"
    ]
  },
  "yarn_find_correction_dim": [
    "num_rotations",
    "dim",
    "base",
    "max_position_embeddings"
  ],
  "yarn_find_correction_range": [
    "low_rot",
    "high_rot",
    "dim",
    "base",
    "max_position_embeddings"
  ],
  "yarn_get_mscale": [
    "scale",
    "mscale"
  ],
  "yarn_linear_ramp_mask": [
    "min_val",
    "max_val",
    "dim"
  ],
  "DeepseekV2YarnRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "scaling_factor",
      "original_max_position_embeddings",
      "beta_fast",
      "beta_slow",
      "mscale",
      "mscale_all_dim"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "DeepseekV2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV2MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV2MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "AttentionConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "FFNConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "BlockConfig": {
    "from_dict": [
      "cls",
      "data"
    ]
  },
  "_find_multiple": [
    "n",
    "k"
  ],
  "_ffn_mult_to_intermediate_size": [
    "ffn_mult",
    "n_embd"
  ],
  "_ACT2FN": [],
  "LinearSubblockReplacement": {
    "__init__": [
      "self",
      "hidden_size",
      "bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "NemotronNASModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "is_mamba": [
    "config",
    "i"
  ],
  "HunyuanV1DenseModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "NgramEmbedding": {
    "__init__": [
      "self",
      "args"
    ],
    "_compute_vocab_mods": [
      "self"
    ],
    "_shift_right": [
      "self",
      "x",
      "n"
    ],
    "_get_ngram_ids": [
      "self",
      "input_ids",
      "shifted_ids",
      "vocab_mods",
      "ngram"
    ],
    "__call__": [
      "self",
      "input_ids",
      "cache"
    ]
  },
  "LongcatFlashNgramModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "input_ids",
      "cache"
    ]
  },
  "Qwen3Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "aggregate_expert_outputs": [
    "expert_outputs",
    "scores"
  ],
  "BailingMoeMLP": {
    "__init__": [
      "self",
      "args",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "BailingMoeAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "BailingMoeGate": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "BailingMoeSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "BailingMoeDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "BailingMoeModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "SuScaledRoPE": {
    "__init__": [
      "self",
      "dims",
      "base",
      "max_position_embeddings",
      "original_max_position_embeddings",
      "short_factor",
      "long_factor",
      "short_mscale",
      "long_mscale"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "Llama3RoPE": {
    "__init__": [
      "self",
      "dims",
      "max_position_embeddings",
      "traditional",
      "base",
      "scaling_config"
    ],
    "extra_repr": [
      "self"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "YarnRoPE": {
    "__init__": [
      "self",
      "dims",
      "traditional",
      "max_position_embeddings",
      "base",
      "scaling_factor",
      "original_max_position_embeddings",
      "beta_fast",
      "beta_slow",
      "mscale",
      "mscale_all_dim"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "initialize_rope": [
    "dims",
    "base",
    "traditional",
    "scaling_config",
    "max_position_embeddings"
  ],
  "MiniMaxAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MiniMaxSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MiniMaxDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MiniMaxModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "mask",
      "cache"
    ]
  },
  "GLMAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "GLMMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GLMBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "GLMModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "KlearAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "KlearMLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "KlearSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "KlearDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "KlearModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "FusedLoRALinear": {
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "r",
      "dropout",
      "scale"
    ],
    "fuse": [
      "self",
      "dequantize"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "FusedQuantizedLinear": {
    "__init__": [
      "self",
      "input_dims",
      "output_dims",
      "group_size",
      "bits"
    ],
    "input_dims": [
      "self"
    ],
    "output_dims": [
      "self"
    ],
    "__call__": [
      "self",
      "x"
    ],
    "to_lora": [
      "self",
      "r",
      "dropout",
      "scale"
    ]
  },
  "FusedLinear": {
    "__init__": [
      "self",
      "input_dims",
      "output_dims"
    ],
    "input_dims": [
      "self"
    ],
    "output_dims": [
      "self"
    ],
    "__call__": [
      "self",
      "x"
    ],
    "to_quantized": [
      "self",
      "group_size",
      "bits"
    ],
    "to_lora": [
      "self",
      "r",
      "dropout",
      "scale"
    ]
  },
  "fake_8bit_quant": [
    "x",
    "scale"
  ],
  "KVReuseAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "keys",
      "values",
      "mask"
    ]
  },
  "KVReuseTransformerBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "keys",
      "values",
      "mask"
    ]
  },
  "AFMModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "clamped_swiglu": [
    "x",
    "gate",
    "limit"
  ],
  "ClampedSwiGLU": {
    "__init__": [
      "self",
      "limit"
    ],
    "__call__": [
      "self",
      "x",
      "gate"
    ]
  },
  "ZeroCenteredRMSNorm": {
    "__init__": [
      "self",
      "dims",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Step3p5MLP": {
    "__init__": [
      "self",
      "args",
      "intermediate_size",
      "swiglu_limit"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "moe_gate_select": [
    "gates",
    "router_bias",
    "top_k",
    "routed_scaling_factor",
    "norm_topk_prob"
  ],
  "Step3p5MoEGate": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Step3p5MoE": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Step3p5Attention": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Step3p5DecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Step3p5Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "MoERouter": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "AfmoeMoE": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "AfmoeModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "clip_residual": [
    "x",
    "y"
  ],
  "Gemma3Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "BaichuanModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "mlx_topk": [
    "a",
    "k",
    "axis"
  ],
  "AttentionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "GptOssMoeModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "input_embeddings"
    ]
  },
  "SeedModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Starcoder2Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "Glm4MLP": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4Attention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Glm4DecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Glm4Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache"
    ]
  },
  "StableLM": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  },
  "DeepseekV3Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV3MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV3MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "cache"
    ]
  }
}