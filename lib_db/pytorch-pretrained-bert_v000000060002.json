{
  "CONFIG_NAME": [],
  "WEIGHTS_NAME": [],
  "logger": [],
  "url_to_filename": [
    "url",
    "etag"
  ],
  "filename_to_url": [
    "filename",
    "cache_dir"
  ],
  "cached_path": [
    "url_or_filename",
    "cache_dir"
  ],
  "split_s3_path": [
    "url"
  ],
  "s3_request": [
    "func"
  ],
  "s3_etag": [
    "url"
  ],
  "s3_get": [
    "url",
    "temp_file"
  ],
  "http_get": [
    "url",
    "temp_file"
  ],
  "get_from_cache": [
    "url",
    "cache_dir"
  ],
  "read_set_from_file": [
    "filename"
  ],
  "get_file_extension": [
    "path",
    "dot",
    "lower"
  ],
  "PRETRAINED_MODEL_ARCHIVE_MAP": [],
  "PRETRAINED_CONFIG_ARCHIVE_MAP": [],
  "TF_WEIGHTS_NAME": [],
  "build_tf_to_pytorch_map": [
    "model",
    "config"
  ],
  "load_tf_weights_in_transfo_xl": [
    "model",
    "config",
    "tf_path"
  ],
  "TransfoXLConfig": {
    "__init__": [
      "self",
      "vocab_size_or_config_json_file",
      "cutoffs",
      "d_model",
      "d_embed",
      "n_head",
      "d_head",
      "d_inner",
      "div_val",
      "pre_lnorm",
      "n_layer",
      "tgt_len",
      "ext_len",
      "mem_len",
      "clamp_len",
      "same_length",
      "proj_share_all_but_first",
      "attn_type",
      "sample_softmax",
      "adaptive",
      "tie_weight",
      "dropout",
      "dropatt",
      "untie_r",
      "init",
      "init_range",
      "proj_init_std",
      "init_std"
    ],
    "from_dict": [
      "cls",
      "json_object"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "__repr__": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ]
  },
  "PositionalEmbedding": {
    "__init__": [
      "self",
      "demb"
    ],
    "forward": [
      "self",
      "pos_seq",
      "bsz"
    ]
  },
  "PositionwiseFF": {
    "__init__": [
      "self",
      "d_model",
      "d_inner",
      "dropout",
      "pre_lnorm"
    ],
    "forward": [
      "self",
      "inp"
    ]
  },
  "MultiHeadAttn": {
    "__init__": [
      "self",
      "n_head",
      "d_model",
      "d_head",
      "dropout",
      "dropatt",
      "pre_lnorm",
      "r_r_bias",
      "r_w_bias"
    ],
    "forward": [
      "self",
      "h",
      "attn_mask",
      "mems"
    ]
  },
  "RelMultiHeadAttn": {
    "__init__": [
      "self",
      "n_head",
      "d_model",
      "d_head",
      "dropout",
      "dropatt",
      "tgt_len",
      "ext_len",
      "mem_len",
      "pre_lnorm",
      "r_r_bias",
      "r_w_bias"
    ],
    "_parallelogram_mask": [
      "self",
      "h",
      "w",
      "left"
    ],
    "_shift": [
      "self",
      "x",
      "qlen",
      "klen",
      "mask",
      "left"
    ],
    "_rel_shift": [
      "self",
      "x",
      "zero_triu"
    ],
    "forward": [
      "self",
      "w",
      "r",
      "attn_mask",
      "mems"
    ]
  },
  "RelPartialLearnableMultiHeadAttn": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "w",
      "r",
      "attn_mask",
      "mems"
    ]
  },
  "RelLearnableMultiHeadAttn": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "w",
      "r_emb",
      "r_w_bias",
      "r_bias",
      "attn_mask",
      "mems"
    ]
  },
  "DecoderLayer": {
    "__init__": [
      "self",
      "n_head",
      "d_model",
      "d_head",
      "d_inner",
      "dropout"
    ],
    "forward": [
      "self",
      "dec_inp",
      "dec_attn_mask",
      "mems"
    ]
  },
  "RelLearnableDecoderLayer": {
    "__init__": [
      "self",
      "n_head",
      "d_model",
      "d_head",
      "d_inner",
      "dropout"
    ],
    "forward": [
      "self",
      "dec_inp",
      "r_emb",
      "r_w_bias",
      "r_bias",
      "dec_attn_mask",
      "mems"
    ]
  },
  "RelPartialLearnableDecoderLayer": {
    "__init__": [
      "self",
      "n_head",
      "d_model",
      "d_head",
      "d_inner",
      "dropout"
    ],
    "forward": [
      "self",
      "dec_inp",
      "r",
      "dec_attn_mask",
      "mems"
    ]
  },
  "AdaptiveEmbedding": {
    "__init__": [
      "self",
      "n_token",
      "d_embed",
      "d_proj",
      "cutoffs",
      "div_val",
      "sample_softmax"
    ],
    "forward": [
      "self",
      "inp"
    ]
  },
  "TransfoXLPreTrainedModel": {
    "__init__": [
      "self",
      "config"
    ],
    "init_weight": [
      "self",
      "weight"
    ],
    "init_bias": [
      "self",
      "bias"
    ],
    "init_weights": [
      "self",
      "m"
    ],
    "set_num_special_tokens": [
      "self",
      "num_special_tokens"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "state_dict",
      "cache_dir",
      "from_tf"
    ]
  },
  "TransfoXLModel": {
    "__init__": [
      "self",
      "config"
    ],
    "backward_compatible": [
      "self"
    ],
    "reset_length": [
      "self",
      "tgt_len",
      "ext_len",
      "mem_len"
    ],
    "init_mems": [
      "self",
      "data"
    ],
    "_update_mems": [
      "self",
      "hids",
      "mems",
      "qlen",
      "mlen"
    ],
    "_forward": [
      "self",
      "dec_inp",
      "mems"
    ],
    "forward": [
      "self",
      "input_ids",
      "mems"
    ]
  },
  "TransfoXLLMHeadModel": {
    "__init__": [
      "self",
      "config"
    ],
    "tie_weights": [
      "self"
    ],
    "reset_length": [
      "self",
      "tgt_len",
      "ext_len",
      "mem_len"
    ],
    "init_mems": [
      "self",
      "data"
    ],
    "forward": [
      "self",
      "input_ids",
      "target",
      "mems"
    ]
  },
  "load_tf_weights_in_gpt2": [
    "model",
    "gpt2_checkpoint_path"
  ],
  "gelu": [
    "x"
  ],
  "GPT2Config": {
    "__init__": [
      "self",
      "vocab_size_or_config_json_file",
      "n_positions",
      "n_ctx",
      "n_embd",
      "n_layer",
      "n_head",
      "layer_norm_epsilon",
      "initializer_range"
    ],
    "from_dict": [
      "cls",
      "json_object"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "__repr__": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ]
  },
  "Conv1D": {
    "__init__": [
      "self",
      "nf",
      "nx"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Attention": {
    "__init__": [
      "self",
      "nx",
      "n_ctx",
      "config",
      "scale"
    ],
    "_attn": [
      "self",
      "q",
      "k",
      "v"
    ],
    "merge_heads": [
      "self",
      "x"
    ],
    "split_heads": [
      "self",
      "x",
      "k"
    ],
    "forward": [
      "self",
      "x",
      "layer_past"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "n_state",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Block": {
    "__init__": [
      "self",
      "n_ctx",
      "config",
      "scale"
    ],
    "forward": [
      "self",
      "x",
      "layer_past"
    ]
  },
  "GPT2LMHead": {
    "__init__": [
      "self",
      "model_embeddings_weights",
      "config"
    ],
    "set_embeddings_weights": [
      "self",
      "model_embeddings_weights"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "GPT2MultipleChoiceHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mc_token_ids"
    ]
  },
  "GPT2PreTrainedModel": {
    "__init__": [
      "self",
      "config"
    ],
    "set_tied": [
      "self"
    ],
    "init_weights": [
      "self",
      "module"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "state_dict",
      "cache_dir",
      "from_tf"
    ]
  },
  "GPT2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "token_type_ids",
      "past"
    ]
  },
  "GPT2LMHeadModel": {
    "__init__": [
      "self",
      "config"
    ],
    "set_tied": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "token_type_ids",
      "lm_labels",
      "past"
    ]
  },
  "GPT2DoubleHeadsModel": {
    "__init__": [
      "self",
      "config"
    ],
    "set_tied": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "mc_token_ids",
      "lm_labels",
      "mc_labels",
      "token_type_ids",
      "position_ids",
      "past"
    ]
  },
  "convert_openai_checkpoint_to_pytorch": [
    "openai_checkpoint_folder_path",
    "openai_config_file",
    "pytorch_dump_folder_path"
  ],
  "load_tf_weights_in_openai_gpt": [
    "model",
    "openai_checkpoint_folder_path"
  ],
  "swish": [
    "x"
  ],
  "ACT_FNS": [],
  "OpenAIGPTConfig": {
    "__init__": [
      "self",
      "vocab_size_or_config_json_file",
      "n_special",
      "n_positions",
      "n_ctx",
      "n_embd",
      "n_layer",
      "n_head",
      "afn",
      "resid_pdrop",
      "embd_pdrop",
      "attn_pdrop",
      "layer_norm_epsilon",
      "initializer_range"
    ],
    "total_tokens_embeddings": [
      "self"
    ],
    "from_dict": [
      "cls",
      "json_object"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "__repr__": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ]
  },
  "OpenAIGPTLMHead": {
    "__init__": [
      "self",
      "model_embeddings_weights",
      "config"
    ],
    "set_embeddings_weights": [
      "self",
      "model_embeddings_weights"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "OpenAIGPTMultipleChoiceHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "mc_token_ids"
    ]
  },
  "OpenAIGPTPreTrainedModel": {
    "__init__": [
      "self",
      "config"
    ],
    "init_weights": [
      "self",
      "module"
    ],
    "set_num_special_tokens": [
      "self",
      "num_special_tokens"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "num_special_tokens",
      "state_dict",
      "cache_dir",
      "from_tf"
    ]
  },
  "OpenAIGPTModel": {
    "__init__": [
      "self",
      "config"
    ],
    "set_num_special_tokens": [
      "self",
      "num_special_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "token_type_ids"
    ]
  },
  "OpenAIGPTLMHeadModel": {
    "__init__": [
      "self",
      "config"
    ],
    "set_num_special_tokens": [
      "self",
      "num_special_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "token_type_ids",
      "lm_labels"
    ]
  },
  "OpenAIGPTDoubleHeadsModel": {
    "__init__": [
      "self",
      "config"
    ],
    "set_num_special_tokens": [
      "self",
      "num_special_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "mc_token_ids",
      "lm_labels",
      "mc_labels",
      "token_type_ids",
      "position_ids"
    ]
  },
  "convert_transfo_xl_checkpoint_to_pytorch": [
    "tf_checkpoint_path",
    "transfo_xl_config_file",
    "pytorch_dump_folder_path",
    "transfo_xl_dataset_file"
  ],
  "_LRSchedule": {
    "warn_t_total": [],
    "__init__": [
      "self",
      "warmup",
      "t_total"
    ],
    "get_lr": [
      "self",
      "step",
      "nowarn"
    ],
    "get_lr_": [
      "self",
      "progress"
    ]
  },
  "ConstantLR": {
    "get_lr_": [
      "self",
      "progress"
    ]
  },
  "WarmupCosineSchedule": {
    "warn_t_total": [],
    "__init__": [
      "self",
      "warmup",
      "t_total",
      "cycles"
    ],
    "get_lr_": [
      "self",
      "progress"
    ]
  },
  "WarmupCosineWithHardRestartsSchedule": {
    "__init__": [
      "self",
      "warmup",
      "t_total",
      "cycles"
    ],
    "get_lr_": [
      "self",
      "progress"
    ]
  },
  "WarmupCosineWithWarmupRestartsSchedule": {
    "__init__": [
      "self",
      "warmup",
      "t_total",
      "cycles"
    ],
    "get_lr_": [
      "self",
      "progress"
    ]
  },
  "WarmupConstantSchedule": {
    "get_lr_": [
      "self",
      "progress"
    ]
  },
  "WarmupLinearSchedule": {
    "warn_t_total": [],
    "get_lr_": [
      "self",
      "progress"
    ]
  },
  "SCHEDULES": [],
  "BertAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "warmup",
      "t_total",
      "schedule",
      "b1",
      "b2",
      "e",
      "weight_decay",
      "max_grad_norm"
    ],
    "get_lr": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "__version__": [],
  "BERT_CONFIG_NAME": [],
  "load_tf_weights_in_bert": [
    "model",
    "tf_checkpoint_path"
  ],
  "ACT2FN": [],
  "BertConfig": {
    "__init__": [
      "self",
      "vocab_size_or_config_json_file",
      "hidden_size",
      "num_hidden_layers",
      "num_attention_heads",
      "intermediate_size",
      "hidden_act",
      "hidden_dropout_prob",
      "attention_probs_dropout_prob",
      "max_position_embeddings",
      "type_vocab_size",
      "initializer_range"
    ],
    "from_dict": [
      "cls",
      "json_object"
    ],
    "from_json_file": [
      "cls",
      "json_file"
    ],
    "__repr__": [
      "self"
    ],
    "to_dict": [
      "self"
    ],
    "to_json_string": [
      "self"
    ],
    "to_json_file": [
      "self",
      "json_file_path"
    ]
  },
  "BertEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids"
    ]
  },
  "BertSelfAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "transpose_for_scores": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "BertSelfOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BertAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "attention_mask"
    ]
  },
  "BertIntermediate": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertOutput": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "input_tensor"
    ]
  },
  "BertLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "BertEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "output_all_encoded_layers"
    ]
  },
  "BertPooler": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertPredictionHeadTransform": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertLMPredictionHead": {
    "__init__": [
      "self",
      "config",
      "bert_model_embedding_weights"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BertOnlyMLMHead": {
    "__init__": [
      "self",
      "config",
      "bert_model_embedding_weights"
    ],
    "forward": [
      "self",
      "sequence_output"
    ]
  },
  "BertOnlyNSPHead": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pooled_output"
    ]
  },
  "BertPreTrainingHeads": {
    "__init__": [
      "self",
      "config",
      "bert_model_embedding_weights"
    ],
    "forward": [
      "self",
      "sequence_output",
      "pooled_output"
    ]
  },
  "BertPreTrainedModel": {
    "__init__": [
      "self",
      "config"
    ],
    "init_bert_weights": [
      "self",
      "module"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "BertModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "output_all_encoded_layers"
    ]
  },
  "BertForPreTraining": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "masked_lm_labels",
      "next_sentence_label"
    ]
  },
  "BertForMaskedLM": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "masked_lm_labels"
    ]
  },
  "BertForNextSentencePrediction": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "next_sentence_label"
    ]
  },
  "BertForSequenceClassification": {
    "__init__": [
      "self",
      "config",
      "num_labels"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels"
    ]
  },
  "BertForMultipleChoice": {
    "__init__": [
      "self",
      "config",
      "num_choices"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels"
    ]
  },
  "BertForTokenClassification": {
    "__init__": [
      "self",
      "config",
      "num_labels"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "labels"
    ]
  },
  "BertForQuestionAnswering": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids",
      "attention_mask",
      "start_positions",
      "end_positions"
    ]
  },
  "ProjectedAdaptiveLogSoftmax": {
    "__init__": [
      "self",
      "n_token",
      "d_embed",
      "d_proj",
      "cutoffs",
      "div_val",
      "keep_order"
    ],
    "_compute_logit": [
      "self",
      "hidden",
      "weight",
      "bias",
      "proj"
    ],
    "forward": [
      "self",
      "hidden",
      "target",
      "keep_order"
    ],
    "log_prob": [
      "self",
      "hidden"
    ]
  },
  "LogUniformSampler": {
    "__init__": [
      "self",
      "range_max",
      "n_sample"
    ],
    "sample": [
      "self",
      "labels"
    ]
  },
  "sample_logits": [
    "embedding",
    "bias",
    "labels",
    "inputs",
    "sampler"
  ],
  "OpenAIAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "schedule",
      "warmup",
      "t_total",
      "b1",
      "b2",
      "e",
      "weight_decay",
      "vector_l2",
      "max_grad_norm"
    ],
    "get_lr": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "PRETRAINED_VOCAB_ARCHIVE_MAP": [],
  "PRETRAINED_MERGES_ARCHIVE_MAP": [],
  "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP": [],
  "VOCAB_NAME": [],
  "MERGES_NAME": [],
  "SPECIAL_TOKENS_NAME": [],
  "get_pairs": [
    "word"
  ],
  "text_standardize": [
    "text"
  ],
  "OpenAIGPTTokenizer": {
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir"
    ],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "special_tokens",
      "max_len"
    ],
    "__len__": [
      "self"
    ],
    "set_special_tokens": [
      "self",
      "special_tokens"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "tokenize": [
      "self",
      "text"
    ],
    "convert_tokens_to_ids": [
      "self",
      "tokens"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "encode": [
      "self",
      "text"
    ],
    "decode": [
      "self",
      "ids",
      "skip_special_tokens",
      "clean_up_tokenization_spaces"
    ],
    "save_vocabulary": [
      "self",
      "vocab_path"
    ]
  },
  "bytes_to_unicode": [],
  "GPT2Tokenizer": {
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir"
    ],
    "__init__": [
      "self",
      "vocab_file",
      "merges_file",
      "errors",
      "special_tokens",
      "max_len"
    ],
    "__len__": [
      "self"
    ],
    "set_special_tokens": [
      "self",
      "special_tokens"
    ],
    "bpe": [
      "self",
      "token"
    ],
    "tokenize": [
      "self",
      "text"
    ],
    "convert_tokens_to_ids": [
      "self",
      "tokens"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "encode": [
      "self",
      "text"
    ],
    "decode": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "vocab_path"
    ]
  },
  "convert_tf_checkpoint_to_pytorch": [
    "tf_checkpoint_path",
    "bert_config_file",
    "pytorch_dump_path"
  ],
  "main": [],
  "PRETRAINED_CORPUS_ARCHIVE_MAP": [],
  "CORPUS_NAME": [],
  "TransfoXLTokenizer": {
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir"
    ],
    "__init__": [
      "self",
      "special",
      "min_freq",
      "max_size",
      "lower_case",
      "delimiter",
      "vocab_file",
      "never_split"
    ],
    "count_file": [
      "self",
      "path",
      "verbose",
      "add_eos"
    ],
    "count_sents": [
      "self",
      "sents",
      "verbose"
    ],
    "_build_from_file": [
      "self",
      "vocab_file"
    ],
    "save_vocabulary": [
      "self",
      "vocab_path"
    ],
    "build_vocab": [
      "self"
    ],
    "encode_file": [
      "self",
      "path",
      "ordered",
      "verbose",
      "add_eos",
      "add_double_eos"
    ],
    "encode_sents": [
      "self",
      "sents",
      "ordered",
      "verbose"
    ],
    "add_special": [
      "self",
      "sym"
    ],
    "add_symbol": [
      "self",
      "sym"
    ],
    "get_sym": [
      "self",
      "idx"
    ],
    "get_idx": [
      "self",
      "sym"
    ],
    "convert_ids_to_tokens": [
      "self",
      "indices"
    ],
    "convert_tokens_to_ids": [
      "self",
      "symbols"
    ],
    "convert_to_tensor": [
      "self",
      "symbols"
    ],
    "decode": [
      "self",
      "indices",
      "exclude"
    ],
    "__len__": [
      "self"
    ],
    "tokenize": [
      "self",
      "line",
      "add_eos",
      "add_double_eos"
    ]
  },
  "LMOrderedIterator": {
    "__init__": [
      "self",
      "data",
      "bsz",
      "bptt",
      "device",
      "ext_len"
    ],
    "get_batch": [
      "self",
      "i",
      "bptt"
    ],
    "get_fixlen_iter": [
      "self",
      "start"
    ],
    "get_varlen_iter": [
      "self",
      "start",
      "std",
      "min_len",
      "max_deviation"
    ],
    "__iter__": [
      "self"
    ]
  },
  "LMShuffledIterator": {
    "__init__": [
      "self",
      "data",
      "bsz",
      "bptt",
      "device",
      "ext_len",
      "shuffle"
    ],
    "get_sent_stream": [
      "self"
    ],
    "stream_iterator": [
      "self",
      "sent_stream"
    ],
    "__iter__": [
      "self"
    ]
  },
  "LMMultiFileIterator": {
    "__init__": [
      "self",
      "paths",
      "vocab",
      "bsz",
      "bptt",
      "device",
      "ext_len",
      "shuffle"
    ],
    "get_sent_stream": [
      "self",
      "path"
    ],
    "__iter__": [
      "self"
    ]
  },
  "TransfoXLCorpus": {
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir"
    ],
    "__init__": [
      "self"
    ],
    "build_corpus": [
      "self",
      "path",
      "dataset"
    ],
    "get_iterator": [
      "self",
      "split"
    ]
  },
  "get_lm_corpus": [
    "datadir",
    "dataset"
  ],
  "convert_gpt2_checkpoint_to_pytorch": [
    "gpt2_checkpoint_path",
    "gpt2_config_file",
    "pytorch_dump_folder_path"
  ],
  "load_vocab": [
    "vocab_file"
  ],
  "whitespace_tokenize": [
    "text"
  ],
  "BertTokenizer": {
    "__init__": [
      "self",
      "vocab_file",
      "do_lower_case",
      "max_len",
      "do_basic_tokenize",
      "never_split"
    ],
    "tokenize": [
      "self",
      "text"
    ],
    "convert_tokens_to_ids": [
      "self",
      "tokens"
    ],
    "convert_ids_to_tokens": [
      "self",
      "ids"
    ],
    "save_vocabulary": [
      "self",
      "vocab_path"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "cache_dir"
    ]
  },
  "BasicTokenizer": {
    "__init__": [
      "self",
      "do_lower_case",
      "never_split"
    ],
    "tokenize": [
      "self",
      "text"
    ],
    "_run_strip_accents": [
      "self",
      "text"
    ],
    "_run_split_on_punc": [
      "self",
      "text"
    ],
    "_tokenize_chinese_chars": [
      "self",
      "text"
    ],
    "_is_chinese_char": [
      "self",
      "cp"
    ],
    "_clean_text": [
      "self",
      "text"
    ]
  },
  "WordpieceTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "unk_token",
      "max_input_chars_per_word"
    ],
    "tokenize": [
      "self",
      "text"
    ]
  },
  "_is_whitespace": [
    "char"
  ],
  "_is_control": [
    "char"
  ],
  "_is_punctuation": [
    "char"
  ]
}