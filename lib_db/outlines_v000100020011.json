{
  "Image": {
    "__post_init__": [
      "self"
    ]
  },
  "Video": {},
  "Audio": {},
  "Chat": {
    "__post_init__": [
      "self"
    ],
    "append": [
      "self",
      "message"
    ],
    "extend": [
      "self",
      "messages"
    ],
    "pop": [
      "self"
    ],
    "add_system_message": [
      "self",
      "content"
    ],
    "add_user_message": [
      "self",
      "content"
    ],
    "add_assistant_message": [
      "self",
      "content"
    ],
    "__str__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "GRAMMAR_PATH": [],
  "read_grammar": [
    "grammar_file_name",
    "base_grammar_path"
  ],
  "arithmetic": [],
  "json": [],
  "_caching_enabled": [],
  "CloudpickleDisk": {
    "__init__": [
      "self",
      "directory",
      "compress_level"
    ],
    "put": [
      "self",
      "key"
    ],
    "get": [
      "self",
      "key",
      "raw"
    ],
    "store": [
      "self",
      "value",
      "read",
      "key"
    ],
    "fetch": [
      "self",
      "mode",
      "filename",
      "value",
      "read"
    ]
  },
  "get_cache": [],
  "cache": [
    "expire",
    "typed",
    "ignore"
  ],
  "disable_cache": [],
  "clear_cache": [],
  "cache_disabled": [],
  "BlackBoxGenerator": {
    "__init__": [
      "self",
      "model",
      "output_type"
    ],
    "__call__": [
      "self",
      "prompt"
    ],
    "batch": [
      "self",
      "prompts"
    ],
    "stream": [
      "self",
      "prompt"
    ]
  },
  "AsyncBlackBoxGenerator": {
    "__init__": [
      "self",
      "model",
      "output_type"
    ],
    "__call__": [
      "self",
      "prompt"
    ],
    "batch": [
      "self",
      "prompts"
    ],
    "stream": [
      "self",
      "prompt"
    ]
  },
  "SteerableGenerator": {
    "__init__": [
      "self",
      "model",
      "output_type",
      "backend_name"
    ],
    "from_processor": [
      "cls",
      "model",
      "processor"
    ],
    "__call__": [
      "self",
      "prompt"
    ],
    "batch": [
      "self",
      "prompts"
    ],
    "stream": [
      "self",
      "prompt"
    ]
  },
  "Generator": [
    "model",
    "output_type",
    "backend"
  ],
  "Application": {
    "__init__": [
      "self",
      "template",
      "output_type"
    ],
    "__call__": [
      "self",
      "model",
      "template_vars"
    ]
  },
  "Vision": [
    "prompt",
    "image"
  ],
  "Template": {
    "__call__": [
      "self"
    ],
    "from_string": [
      "cls",
      "content",
      "filters"
    ],
    "from_file": [
      "cls",
      "path",
      "filters"
    ]
  },
  "build_template_from_string": [
    "content",
    "filters"
  ],
  "build_template_from_file": [
    "path",
    "filters"
  ],
  "create_jinja_env": [
    "loader",
    "filters"
  ],
  "get_fn_name": [
    "fn"
  ],
  "get_fn_args": [
    "fn"
  ],
  "get_fn_description": [
    "fn"
  ],
  "get_fn_source": [
    "fn"
  ],
  "get_fn_signature": [
    "fn"
  ],
  "get_schema": [
    "model"
  ],
  "get_schema_dict": [
    "model"
  ],
  "get_schema_pydantic": [
    "model"
  ],
  "parse_pydantic_schema": [
    "raw_schema",
    "definitions"
  ],
  "__all__": [],
  "TYPE_CHECKING": [],
  "__version__": [],
  "version": [],
  "__version_tuple__": [],
  "version_tuple": [],
  "__commit_id__": [],
  "commit_id": [],
  "XGrammarLogitsProcessor": {
    "__init__": [
      "self",
      "compiled_grammar",
      "tensor_library_name"
    ],
    "reset": [
      "self"
    ],
    "_setup": [
      "self",
      "batch_size",
      "vocab_size"
    ],
    "_bias_logits_torch": [
      "self",
      "input_ids",
      "logits"
    ],
    "_bias_logits_mlx": [
      "self",
      "input_ids",
      "logits"
    ],
    "process_logits": [
      "self",
      "input_ids",
      "logits"
    ]
  },
  "XGrammarBackend": {
    "__init__": [
      "self",
      "model"
    ],
    "get_json_schema_logits_processor": [
      "self",
      "json_schema"
    ],
    "get_regex_logits_processor": [
      "self",
      "regex"
    ],
    "get_cfg_logits_processor": [
      "self",
      "grammar"
    ]
  },
  "LogitsProcessorType": [],
  "BaseBackend": {
    "get_json_schema_logits_processor": [
      "self",
      "json_schema"
    ],
    "get_regex_logits_processor": [
      "self",
      "regex"
    ],
    "get_cfg_logits_processor": [
      "self",
      "grammar"
    ]
  },
  "CFG_DEFAULT_BACKEND": [],
  "JSON_SCHEMA_DEFAULT_BACKEND": [],
  "REGEX_DEFAULT_BACKEND": [],
  "_get_backend": [
    "backend_name",
    "model"
  ],
  "get_json_schema_logits_processor": [
    "backend_name",
    "model",
    "json_schema"
  ],
  "get_regex_logits_processor": [
    "backend_name",
    "model",
    "regex"
  ],
  "get_cfg_logits_processor": [
    "backend_name",
    "model",
    "grammar"
  ],
  "SUPPORTED_TENSOR_LIBRARIES": [],
  "LLGuidanceLogitsProcessor": {
    "__init__": [
      "self",
      "grammar",
      "llg_tokenizer",
      "tensor_library_name"
    ],
    "reset": [
      "self"
    ],
    "_setup": [
      "self",
      "batch_size"
    ],
    "_bias_logits_mlx": [
      "self",
      "input_ids",
      "logits"
    ],
    "_bias_logits_torch": [
      "self",
      "input_ids",
      "logits"
    ],
    "_bias_logits_numpy": [
      "self",
      "input_ids",
      "logits"
    ],
    "process_logits": [
      "self",
      "input_ids",
      "logits"
    ]
  },
  "LLGuidanceBackend": {
    "__init__": [
      "self",
      "model"
    ],
    "_create_llg_tokenizer": [
      "self",
      "model"
    ],
    "get_json_schema_logits_processor": [
      "self",
      "json_schema"
    ],
    "get_regex_logits_processor": [
      "self",
      "regex"
    ],
    "get_cfg_logits_processor": [
      "self",
      "grammar"
    ]
  },
  "OutlinesCoreLogitsProcessor": {
    "__init__": [
      "self",
      "index",
      "tensor_library_name"
    ],
    "reset": [
      "self"
    ],
    "_setup": [
      "self",
      "batch_size",
      "vocab_size"
    ],
    "_bias_logits_mlx": [
      "self",
      "batch_size",
      "logits"
    ],
    "_bias_logits_torch": [
      "self",
      "batch_size",
      "logits"
    ],
    "_bias_logits_numpy": [
      "self",
      "batch_size",
      "logits"
    ],
    "process_logits": [
      "self",
      "input_ids",
      "logits"
    ]
  },
  "OutlinesCoreBackend": {
    "__init__": [
      "self",
      "model"
    ],
    "get_json_schema_logits_processor": [
      "self",
      "json_schema"
    ],
    "get_regex_logits_processor": [
      "self",
      "regex"
    ],
    "get_cfg_logits_processor": [
      "self",
      "grammar"
    ],
    "create_outlines_core_vocabulary": [
      "vocab",
      "eos_token_id",
      "eos_token",
      "token_to_str"
    ]
  },
  "get_country_flags": [],
  "ALPHA_2_CODE": [],
  "Alpha2": [],
  "ALPHA_3_CODE": [],
  "Alpha3": [],
  "NUMERIC_CODE": [],
  "Numeric": [],
  "NAME": [],
  "Name": [],
  "flag_mapping": [],
  "FLAG": [],
  "Flag": [],
  "is_int": [
    "value"
  ],
  "is_int_instance": [
    "value"
  ],
  "is_float": [
    "value"
  ],
  "is_float_instance": [
    "value"
  ],
  "is_str": [
    "value"
  ],
  "is_str_instance": [
    "value"
  ],
  "is_bool": [
    "value"
  ],
  "is_dict_instance": [
    "value"
  ],
  "is_datetime": [
    "value"
  ],
  "is_date": [
    "value"
  ],
  "is_time": [
    "value"
  ],
  "is_native_dict": [
    "value"
  ],
  "is_typing_dict": [
    "value"
  ],
  "is_typing_list": [
    "value"
  ],
  "is_typing_tuple": [
    "value"
  ],
  "is_union": [
    "value"
  ],
  "is_literal": [
    "value"
  ],
  "is_dataclass": [
    "value"
  ],
  "is_typed_dict": [
    "value"
  ],
  "is_pydantic_model": [
    "value"
  ],
  "is_genson_schema_builder": [
    "value"
  ],
  "is_enum": [
    "value"
  ],
  "is_callable": [
    "value"
  ],
  "get_enum_from_literal": [
    "value"
  ],
  "get_enum_from_choice": [
    "value"
  ],
  "get_schema_from_signature": [
    "fn"
  ],
  "get_schema_from_enum": [
    "myenum"
  ],
  "string": [],
  "integer": [],
  "boolean": [],
  "number": [],
  "date": [],
  "time": [],
  "datetime": [],
  "digit": [],
  "char": [],
  "newline": [],
  "whitespace": [],
  "hex_str": [],
  "uuid4": [],
  "ipv4": [],
  "sentence": [],
  "paragraph": [],
  "email": [],
  "isbn": [],
  "AIRPORT_IATA_LIST": [],
  "IATA": [],
  "Term": {
    "__add__": [
      "self",
      "other"
    ],
    "__radd__": [
      "self",
      "other"
    ],
    "__or__": [
      "self",
      "other"
    ],
    "__ror__": [
      "self",
      "other"
    ],
    "__get_validator__": [
      "self",
      "_core_schema"
    ],
    "__get_pydantic_core_schema__": [
      "self",
      "source_type",
      "handler"
    ],
    "__get_pydantic_json_schema__": [
      "self",
      "core_schema",
      "handler"
    ],
    "validate": [
      "self",
      "value"
    ],
    "matches": [
      "self",
      "value"
    ],
    "display_ascii_tree": [
      "self",
      "indent",
      "is_last"
    ],
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__str__": [
      "self"
    ],
    "optional": [
      "self"
    ],
    "exactly": [
      "self",
      "count"
    ],
    "at_least": [
      "self",
      "count"
    ],
    "at_most": [
      "self",
      "count"
    ],
    "between": [
      "self",
      "min_count",
      "max_count"
    ],
    "one_or_more": [
      "self"
    ],
    "zero_or_more": [
      "self"
    ]
  },
  "String": {
    "_display_node": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Regex": {
    "_display_node": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "CFG": {
    "_display_node": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "from_file": [
      "cls",
      "path"
    ]
  },
  "JsonSchema": {
    "__init__": [
      "self",
      "schema",
      "whitespace_pattern",
      "ensure_ascii"
    ],
    "is_json_schema": [
      "cls",
      "obj"
    ],
    "convert_to": [
      "cls",
      "schema",
      "target_types"
    ],
    "_display_node": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "from_file": [
      "cls",
      "path"
    ]
  },
  "Choice": {
    "_display_node": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "KleeneStar": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "KleenePlus": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Optional": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Alternatives": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Sequence": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "QuantifyExact": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "QuantifyMinimum": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "QuantifyMaximum": {
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "QuantifyBetween": {
    "__post_init__": [
      "self"
    ],
    "_display_node": [
      "self"
    ],
    "_display_children": [
      "self",
      "indent"
    ],
    "__repr__": [
      "self"
    ]
  },
  "regex": [
    "pattern"
  ],
  "cfg": [
    "definition"
  ],
  "json_schema": [
    "schema"
  ],
  "either": [],
  "optional": [
    "term"
  ],
  "exactly": [
    "count",
    "term"
  ],
  "at_least": [
    "count",
    "term"
  ],
  "at_most": [
    "count",
    "term"
  ],
  "between": [
    "min_count",
    "max_count",
    "term"
  ],
  "zero_or_more": [
    "term"
  ],
  "one_or_more": [
    "term"
  ],
  "python_types_to_terms": [
    "ptype",
    "recursion_depth"
  ],
  "_get_enum_members": [
    "ptype"
  ],
  "_handle_literal": [
    "args"
  ],
  "_handle_union": [
    "args",
    "recursion_depth"
  ],
  "_handle_list": [
    "args",
    "recursion_depth"
  ],
  "_handle_tuple": [
    "args",
    "recursion_depth"
  ],
  "_handle_dict": [
    "args",
    "recursion_depth"
  ],
  "to_regex": [
    "term"
  ],
  "schema_type_to_python": [
    "schema",
    "caller_target_type"
  ],
  "json_schema_dict_to_typeddict": [
    "schema",
    "name"
  ],
  "json_schema_dict_to_pydantic": [
    "schema",
    "name"
  ],
  "json_schema_dict_to_dataclass": [
    "schema",
    "name"
  ],
  "zip_code": [],
  "phone_number": [],
  "TensorType": [],
  "OutlinesLogitsProcessor": {
    "__init__": [
      "self",
      "tensor_library_name"
    ],
    "reset": [
      "self"
    ],
    "process_logits": [
      "self",
      "input_ids",
      "logits"
    ],
    "__call__": [
      "self",
      "input_ids",
      "logits"
    ]
  },
  "TorchTensorAdapter": {
    "library_name": [],
    "__init__": [
      "self"
    ],
    "shape": [
      "self",
      "tensor"
    ],
    "unsqueeze": [
      "self",
      "tensor"
    ],
    "squeeze": [
      "self",
      "tensor"
    ],
    "to_list": [
      "self",
      "tensor"
    ],
    "to_scalar": [
      "self",
      "tensor"
    ],
    "full_like": [
      "self",
      "tensor",
      "fill_value"
    ],
    "concatenate": [
      "self",
      "tensors"
    ],
    "get_device": [
      "self",
      "tensor"
    ],
    "to_device": [
      "self",
      "tensor",
      "device"
    ],
    "boolean_ones_like": [
      "self",
      "tensor"
    ],
    "apply_mask": [
      "self",
      "tensor",
      "mask",
      "value"
    ],
    "argsort_descending": [
      "self",
      "tensor"
    ]
  },
  "TensorAdapter": {
    "shape": [
      "self",
      "tensor"
    ],
    "unsqueeze": [
      "self",
      "tensor"
    ],
    "squeeze": [
      "self",
      "tensor"
    ],
    "to_list": [
      "self",
      "tensor"
    ],
    "to_scalar": [
      "self",
      "tensor"
    ],
    "full_like": [
      "self",
      "tensor",
      "fill_value"
    ],
    "concatenate": [
      "self",
      "tensors"
    ],
    "get_device": [
      "self",
      "tensor"
    ],
    "to_device": [
      "self",
      "tensor",
      "device"
    ],
    "boolean_ones_like": [
      "self",
      "tensor"
    ],
    "apply_mask": [
      "self",
      "tensor",
      "mask",
      "value"
    ],
    "argsort_descending": [
      "self",
      "tensor"
    ]
  },
  "tensor_adapters": [],
  "TensorAdapterImplementation": [],
  "MLXTensorAdapter": {
    "library_name": [],
    "__init__": [
      "self"
    ],
    "shape": [
      "self",
      "tensor"
    ],
    "unsqueeze": [
      "self",
      "tensor"
    ],
    "squeeze": [
      "self",
      "tensor"
    ],
    "to_list": [
      "self",
      "tensor"
    ],
    "to_scalar": [
      "self",
      "tensor"
    ],
    "full_like": [
      "self",
      "tensor",
      "fill_value"
    ],
    "concatenate": [
      "self",
      "tensors"
    ],
    "get_device": [
      "self",
      "tensor"
    ],
    "to_device": [
      "self",
      "tensor",
      "device"
    ],
    "boolean_ones_like": [
      "self",
      "tensor"
    ],
    "apply_mask": [
      "self",
      "tensor",
      "mask",
      "value"
    ],
    "argsort_descending": [
      "self",
      "tensor"
    ]
  },
  "NumpyTensorAdapter": {
    "library_name": [],
    "__init__": [
      "self"
    ],
    "shape": [
      "self",
      "tensor"
    ],
    "unsqueeze": [
      "self",
      "tensor"
    ],
    "squeeze": [
      "self",
      "tensor"
    ],
    "to_list": [
      "self",
      "tensor"
    ],
    "to_scalar": [
      "self",
      "tensor"
    ],
    "full_like": [
      "self",
      "tensor",
      "fill_value"
    ],
    "concatenate": [
      "self",
      "tensors"
    ],
    "get_device": [
      "self",
      "tensor"
    ],
    "to_device": [
      "self",
      "tensor",
      "device"
    ],
    "boolean_ones_like": [
      "self",
      "tensor"
    ],
    "apply_mask": [
      "self",
      "tensor",
      "mask",
      "value"
    ],
    "argsort_descending": [
      "self",
      "tensor"
    ]
  },
  "GeminiTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_model_input": [
      "self",
      "model_input"
    ],
    "format_list_model_input": [
      "self",
      "model_input"
    ],
    "format_chat_model_input": [
      "self",
      "model_input"
    ],
    "_create_message": [
      "self",
      "role",
      "content"
    ],
    "_create_text_part": [
      "self",
      "text"
    ],
    "_create_img_part": [
      "self",
      "image"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ],
    "format_enum_output_type": [
      "self",
      "output_type"
    ],
    "format_json_output_type": [
      "self",
      "output_type"
    ],
    "format_list_output_type": [
      "self",
      "output_type"
    ]
  },
  "Gemini": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_gemini": [
    "client",
    "model_name"
  ],
  "SGLangTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "SGLang": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_build_client_args": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncSGLang": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_build_client_args": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_sglang": [
    "client",
    "model_name"
  ],
  "MistralTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_model_input": [
      "self",
      "model_input"
    ],
    "format_list_model_input": [
      "self",
      "model_input"
    ],
    "format_chat_model_input": [
      "self",
      "model_input"
    ],
    "_create_message_content": [
      "self",
      "content"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ],
    "format_json_schema_type": [
      "self",
      "schema",
      "schema_name"
    ]
  },
  "Mistral": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncMistral": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_mistral": [
    "client",
    "model_name",
    "async_client"
  ],
  "LlamaCppTokenizer": {
    "__init__": [
      "self",
      "model"
    ],
    "decode": [
      "self",
      "token_ids"
    ],
    "encode": [
      "self",
      "prompt",
      "add_bos",
      "special"
    ],
    "convert_token_to_string": [
      "self",
      "token"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ]
  },
  "LlamaCppTypeAdapter": {
    "__init__": [
      "self",
      "has_chat_template"
    ],
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_input": [
      "self",
      "model_input"
    ],
    "format_chat_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "LlamaCpp": {
    "tensor_library_name": [],
    "__init__": [
      "self",
      "model",
      "chat_mode"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_llamacpp": [
    "model",
    "chat_mode"
  ],
  "ModelTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "Model": {
    "__call__": [
      "self",
      "model_input",
      "output_type",
      "backend"
    ],
    "batch": [
      "self",
      "model_input",
      "output_type",
      "backend"
    ],
    "stream": [
      "self",
      "model_input",
      "output_type",
      "backend"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncModel": {
    "__call__": [
      "self",
      "model_input",
      "output_type",
      "backend"
    ],
    "batch": [
      "self",
      "model_input",
      "output_type",
      "backend"
    ],
    "stream": [
      "self",
      "model_input",
      "output_type",
      "backend"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "OllamaTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_model_input": [
      "self",
      "model_input"
    ],
    "format_list_model_input": [
      "self",
      "model_input"
    ],
    "format_chat_model_input": [
      "self",
      "model_input"
    ],
    "_create_message": [
      "self",
      "role",
      "content"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "Ollama": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncOllama": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_ollama": [
    "client",
    "model_name"
  ],
  "set_additional_properties_false_json_schema": [
    "schema"
  ],
  "VLLMOfflineTypeAdapter": {
    "__init__": [
      "self",
      "has_chat_template"
    ],
    "format_input": [
      "self",
      "model_input"
    ],
    "format_input_str": [
      "self",
      "model_input"
    ],
    "format_input_chat": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "VLLMOffline": {
    "__init__": [
      "self",
      "model"
    ],
    "_build_generation_args": [
      "self",
      "inference_kwargs",
      "output_type"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_check_chat_template": [
      "self"
    ]
  },
  "from_vllm_offline": [
    "model"
  ],
  "Tokenizer": {
    "encode": [
      "self",
      "prompt"
    ],
    "decode": [
      "self",
      "token_ids"
    ],
    "convert_token_to_string": [
      "self",
      "token"
    ]
  },
  "_check_hf_chat_template": [
    "tokenizer"
  ],
  "get_llama_tokenizer_types": [],
  "TransformerTokenizer": {
    "__init__": [
      "self",
      "tokenizer"
    ],
    "encode": [
      "self",
      "prompt"
    ],
    "decode": [
      "self",
      "token_ids"
    ],
    "convert_token_to_string": [
      "self",
      "token"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ]
  },
  "TransformersTypeAdapter": {
    "__init__": [
      "self",
      "tokenizer",
      "has_chat_template"
    ],
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_input": [
      "self",
      "model_input"
    ],
    "format_chat_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "Transformers": {
    "__init__": [
      "self",
      "model",
      "tokenizer"
    ],
    "_prepare_model_inputs": [
      "self",
      "model_input",
      "is_batch"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_generate_output_seq": [
      "self",
      "prompts",
      "inputs"
    ],
    "_decode_generation": [
      "self",
      "generated_ids"
    ]
  },
  "TransformersMultiModalTypeAdapter": {
    "__init__": [
      "self"
    ],
    "format_input": [
      "self",
      "model_input"
    ],
    "format_chat_input": [
      "self",
      "model_input"
    ],
    "_prepare_message": [
      "self",
      "role",
      "content"
    ],
    "_extract_assets_from_content": [
      "self",
      "content"
    ],
    "_format_asset_for_template": [
      "self",
      "asset"
    ],
    "format_list_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "TransformersMultiModal": {
    "__init__": [
      "self",
      "model",
      "processor"
    ],
    "_prepare_model_inputs": [
      "self",
      "model_input",
      "is_batch"
    ]
  },
  "from_transformers": [
    "model",
    "tokenizer_or_processor"
  ],
  "SteerableModel": [],
  "BlackBoxModel": [],
  "AsyncBlackBoxModel": [],
  "TGITypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "TGI": {
    "__init__": [
      "self",
      "client"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_build_client_args": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncTGI": {
    "__init__": [
      "self",
      "client"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_build_client_args": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_tgi": [
    "client"
  ],
  "MLXLMTypeAdapter": {
    "__init__": [
      "self",
      "tokenizer",
      "has_chat_template"
    ],
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_input": [
      "self",
      "model_input"
    ],
    "format_chat_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "MLXLM": {
    "tensor_library_name": [],
    "__init__": [
      "self",
      "model",
      "tokenizer"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_mlxlm": [
    "model",
    "tokenizer"
  ],
  "DottxtTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "Dottxt": {
    "__init__": [
      "self",
      "client",
      "model_name",
      "model_revision"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_dottxt": [
    "client",
    "model_name",
    "model_revision"
  ],
  "OpenAITypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_model_input": [
      "self",
      "model_input"
    ],
    "format_list_model_input": [
      "self",
      "model_input"
    ],
    "format_chat_model_input": [
      "self",
      "model_input"
    ],
    "_create_message": [
      "self",
      "role",
      "content"
    ],
    "_create_img_content": [
      "self",
      "image"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ],
    "format_json_output_type": [
      "self",
      "schema"
    ],
    "format_json_mode_type": [
      "self"
    ]
  },
  "OpenAI": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncOpenAI": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_openai": [
    "client",
    "model_name"
  ],
  "VLLMTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "VLLM": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_build_client_args": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncVLLM": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ],
    "_build_client_args": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_vllm": [
    "client",
    "model_name"
  ],
  "AnthropicTypeAdapter": {
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_model_input": [
      "self",
      "model_input"
    ],
    "format_list_model_input": [
      "self",
      "model_input"
    ],
    "format_chat_model_input": [
      "self",
      "model_input"
    ],
    "_create_message": [
      "self",
      "role",
      "content"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "Anthropic": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_anthropic": [
    "client",
    "model_name"
  ],
  "LMStudioTypeAdapter": {
    "_prepare_lmstudio_image": [
      "self",
      "image"
    ],
    "format_input": [
      "self",
      "model_input"
    ],
    "format_str_model_input": [
      "self",
      "model_input"
    ],
    "format_list_model_input": [
      "self",
      "model_input"
    ],
    "format_chat_model_input": [
      "self",
      "model_input"
    ],
    "format_output_type": [
      "self",
      "output_type"
    ]
  },
  "LMStudio": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "AsyncLMStudio": {
    "__init__": [
      "self",
      "client",
      "model_name"
    ],
    "close": [
      "self"
    ],
    "generate": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_batch": [
      "self",
      "model_input",
      "output_type"
    ],
    "generate_stream": [
      "self",
      "model_input",
      "output_type"
    ]
  },
  "from_lmstudio": [
    "client",
    "model_name"
  ]
}