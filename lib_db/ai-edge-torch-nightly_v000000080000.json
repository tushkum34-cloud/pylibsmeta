{
  "__version__": [],
  "__all__": [],
  "_get_bool_env_var": [
    "name",
    "default"
  ],
  "_Config": {
    "use_torch_xla": [
      "self"
    ],
    "in_oss": [
      "self"
    ],
    "enable_group_norm_composite": [
      "self",
      "value"
    ],
    "layout_optimize_partitioner": [
      "self",
      "value"
    ]
  },
  "config": [],
  "pytest_configure": [
    "config"
  ],
  "load": [
    "path"
  ],
  "DEFAULT_SIGNATURE_NAME": [],
  "Model": {
    "__call__": [
      "self"
    ],
    "export": [
      "self",
      "path"
    ],
    "load": [
      "path"
    ]
  },
  "TfLiteModel": {
    "__init__": [
      "self",
      "tflite_model"
    ],
    "tflite_model": [
      "self"
    ],
    "set_interpreter_builder": [
      "self",
      "builder"
    ],
    "__call__": [
      "self"
    ],
    "export": [
      "self",
      "path"
    ],
    "load": [
      "path"
    ]
  },
  "export_with_tensor_inputs_only": [
    "model",
    "args",
    "kwargs",
    "dynamic_shapes"
  ],
  "_flatten": [
    "data"
  ],
  "_torch_tensors_to_np": [],
  "compare_tflite_torch": [
    "edge_model",
    "torch_eval_func",
    "args",
    "kwargs"
  ],
  "_conv1d_bn_example_inputs": [],
  "_conv2d_bn_example_inputs": [],
  "QuantizationConfig": {},
  "OperatorPatternType": [],
  "AnnotatorType": [],
  "register_annotator": [
    "op"
  ],
  "OperatorConfig": {},
  "_is_annotated": [
    "nodes"
  ],
  "_mark_nodes_as_annotated": [
    "nodes"
  ],
  "get_input_act_qspec": [
    "quantization_config"
  ],
  "get_output_act_qspec": [
    "quantization_config"
  ],
  "get_weight_qspec": [
    "quantization_config"
  ],
  "get_bias_qspec": [
    "quantization_config"
  ],
  "get_fixed_qparams_qspec": [
    "quantization_config"
  ],
  "_annotate_linear": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_addmm": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_bn": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_conv_bn_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_do_annotate_conv_bn": [
    "gm",
    "quantization_config",
    "filter_fn",
    "has_relu"
  ],
  "_annotate_gru_io_only": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_max_pool2d": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_adaptive_avg_pool2d": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_fixed_qparams": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_add_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_add": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_mul_relu": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_mul": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_annotate_cat": [
    "gm",
    "quantization_config",
    "filter_fn"
  ],
  "_is_share_obs_or_fq_op": [
    "op"
  ],
  "propagate_annotation": [
    "model"
  ],
  "_convert_scalars_to_attrs": [
    "model"
  ],
  "QuantConfig": {
    "__init__": [
      "self",
      "pt2e_quantizer",
      "generative_recipe"
    ]
  },
  "_supported_symmetric_quantized_operators": [],
  "_get_supported_symmetric_config_and_operators": [],
  "get_symmetric_quantization_config": [
    "is_per_channel",
    "is_qat",
    "is_dynamic"
  ],
  "_get_supported_config_and_operators": [],
  "_get_module_name_filter": [
    "module_name"
  ],
  "_get_module_type_filter": [
    "tp"
  ],
  "_get_not_module_type_or_name_filter": [
    "tp_list",
    "module_name_list"
  ],
  "PT2EQuantizer": {
    "supported_config_and_operators": [],
    "STATIC_QAT_ONLY_OPS": [],
    "STATIC_OPS": [],
    "DYNAMIC_OPS": [],
    "__init__": [
      "self"
    ],
    "get_supported_quantization_configs": [
      "cls"
    ],
    "get_supported_operator_for_quantization_config": [
      "cls",
      "quantization_config"
    ],
    "set_global": [
      "self",
      "quantization_config"
    ],
    "set_operator_type": [
      "self",
      "operator_type",
      "quantization_config"
    ],
    "set_module_type": [
      "self",
      "module_type",
      "quantization_config"
    ],
    "set_module_name": [
      "self",
      "module_name",
      "quantization_config"
    ],
    "transform_for_annotation": [
      "self",
      "model"
    ],
    "annotate": [
      "self",
      "model"
    ],
    "_annotate_all_static_patterns": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_all_dynamic_patterns": [
      "self",
      "model",
      "quantization_config",
      "filter_fn"
    ],
    "_annotate_for_static_quantization_config": [
      "self",
      "model"
    ],
    "_annotate_for_dynamic_quantization_config": [
      "self",
      "model"
    ],
    "validate": [
      "self",
      "model"
    ],
    "get_supported_operators": [
      "cls"
    ]
  },
  "_ComputePrecision": [],
  "_QuantGranularity": [],
  "_OpName": [],
  "_TensorQuantConfig": [],
  "_OpQuantConfig": [],
  "_DEFAULT_REGEX_STR": [],
  "_SINGULAR_TRANSFORMER_BLOCK_REGEX_STR": [],
  "_IDX_TRANSFORMER_BLOCKS_REGEX_STR": [],
  "_ATTENTION_REGEX_STR": [],
  "_FEEDFORWARD_REGEX_STR": [],
  "_EMBEDDING_REGEX_STR": [],
  "_DECODE_LOGITS_REGEX_STR": [],
  "_ANY_TWO_DIGITS_REGEX_STR": [],
  "_get_nbits_from_dtype": [
    "dtype"
  ],
  "_get_dtype_from_dtype": [
    "dtype"
  ],
  "_get_compute_precision_from_mode": [
    "mode"
  ],
  "_get_explicit_dequant_from_mode": [
    "mode"
  ],
  "_get_granularity": [
    "granularity"
  ],
  "_get_algorithm_key_from_algorithm": [
    "algo"
  ],
  "_set_quant_config": [
    "rm",
    "layer_recipe",
    "regex",
    "operation_name"
  ],
  "translate_to_ai_edge_recipe": [
    "recipe"
  ],
  "quantize_model": [
    "model",
    "recipe"
  ],
  "_flatten_list": [
    "l"
  ],
  "flat_dict_names": [
    "tree_spec",
    "context"
  ],
  "_torch_to_tf_variable": [
    "torch_tensor"
  ],
  "_get_states": [
    "exported_programs",
    "signatures"
  ],
  "_tensor_unique_id": [
    "tensor"
  ],
  "gather_state_dict": [
    "exported_programs",
    "signatures"
  ],
  "exported_programs_to_tflite": [
    "exported_programs",
    "signatures"
  ],
  "_extract_backend_configs": [
    "mlir"
  ],
  "assert_string_count": [
    "test_case",
    "mlir",
    "torch_xla_pattern_counter",
    "odml_torch_pattern_counter",
    "odml_torch_attr_counter"
  ],
  "MlirBundle": [],
  "MergedBundle": {},
  "exported_program_to_mlir": [
    "exported_program",
    "sample_args"
  ],
  "merge_mlir_bundles": [
    "bundles",
    "signatures",
    "exported_programs"
  ],
  "_get_shape_with_dynamic": [
    "signature"
  ],
  "_wrap_as_tf_func": [
    "func",
    "bundle",
    "exported_program"
  ],
  "_make_tf_signature": [
    "meta",
    "signature"
  ],
  "exported_program_to_mlir_text": [
    "exported_program"
  ],
  "merged_bundle_to_tfl_model": [
    "merged_bundle",
    "signatures"
  ],
  "torch_dtype_to_tf": [
    "dtype"
  ],
  "_extract_call_args": [
    "bundle",
    "args",
    "tf_state_dict"
  ],
  "_torch_float_dtypes": [],
  "_torch_int_dtypes": [],
  "_fx_op_runner": [],
  "_CULPRIT_GRAPH_MODULE_NAME": [],
  "_get_shape_str": [
    "t"
  ],
  "_tensor_to_random_tensor_call": [
    "t"
  ],
  "_tensor_to_buffer": [
    "t"
  ],
  "SearchResult": {
    "graph": [
      "self",
      "fx_g"
    ]
  },
  "Culprit": {
    "stack_traces": [
      "self"
    ],
    "print_readable": [
      "self",
      "print_output"
    ],
    "print_code": [
      "self",
      "print_output"
    ],
    "code": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "_normalize_getitem_nodes": [
    "fx_gm"
  ],
  "_erase_unused_inputs": [
    "fx_gm",
    "inputs"
  ],
  "_lift_dead_ops_to_outputs": [
    "fx_gm"
  ],
  "_erase_trivial_outputs": [
    "fx_gm"
  ],
  "_erase_sub_gm_from_gm": [
    "fx_gm",
    "fx_inputs",
    "sub_gm",
    "sub_inputs"
  ],
  "_normalize_minified_fx_gm": [
    "fx_gm",
    "inputs"
  ],
  "_fx_minifier_checker": [
    "fx_gm",
    "inputs",
    "runtime_errors"
  ],
  "_search_model": [
    "predicate_f",
    "model",
    "export_args"
  ],
  "find_culprits": [
    "torch_model",
    "args",
    "max_granularity",
    "runtime_errors"
  ],
  "exported_program_to_fx_graph_module_and_inputs": [
    "ep"
  ],
  "redirect_stdio": [
    "stdout",
    "stderr"
  ],
  "_test_culprit_lib": [],
  "non_lowerable_op": [
    "x"
  ],
  "non_lowerable_op_meta": [
    "x"
  ],
  "BadModel": {
    "forward": [
      "self",
      "x"
    ]
  },
  "TestCulprit": {
    "setUp": [
      "self"
    ],
    "test_find_culprits": [
      "self"
    ],
    "test_valid_culprit_readable": [
      "self"
    ],
    "test_valid_culprit_code": [
      "self"
    ],
    "test_find_multiple_culprits": [
      "self"
    ],
    "test_find_culprits_with_trivial_inputs_outputs": [
      "self"
    ]
  },
  "TestSearchModel": {
    "test_search_model_with_ops": [
      "self"
    ]
  },
  "_export_and_decomp": [
    "mod",
    "args"
  ],
  "_to_mlir": [
    "ep"
  ],
  "TestMarkPattern": {
    "test_mark_pattern": [
      "self"
    ],
    "test_mark_pattern_with_clone_inputs": [
      "self"
    ],
    "test_mark_pattern_with_attr_builder": [
      "self"
    ],
    "test_mark_pattern_with_scalar_attr_tracker": [
      "self"
    ],
    "test_mark_tangent_model_and_pattern_input": [
      "self"
    ]
  },
  "_get_uuid": [],
  "_prepose_placeholder_nodes": [
    "graph"
  ],
  "_insert_marker": [
    "graph_module",
    "node",
    "name",
    "pos",
    "id",
    "is_input",
    "attr"
  ],
  "mark_pattern": [
    "graph_module",
    "pattern"
  ],
  "remove_dangling_args": [],
  "remove_assert_tensor_metadata_nodes": [],
  "is_clone_op": [
    "node"
  ],
  "remove_clone_ops": [
    "gm"
  ],
  "Graph": [],
  "GraphModule": [],
  "TensorArgument": [],
  "InternalMatch": [],
  "SubgraphMatcher": [],
  "_are_equal": [
    "x",
    "y"
  ],
  "ScalarAttrTracker": {
    "track": [
      "self"
    ]
  },
  "ScalarAttrLocation": {
    "index": [
      "self"
    ],
    "key": [
      "self"
    ]
  },
  "_find_scalar_attr": [
    "pattern_module",
    "export_args",
    "tracker",
    "decomp_table"
  ],
  "Pattern": {
    "__init__": [
      "self",
      "name",
      "module",
      "export_args"
    ],
    "register_attr_builder": [
      "self",
      "attr_builder"
    ],
    "match": [
      "self",
      "graph_module"
    ],
    "_get_attr_value_from_pattern_match": [
      "self",
      "match",
      "loc"
    ]
  },
  "apply_tfl_converter_flags": [
    "converter",
    "tfl_converter_flags"
  ],
  "set_tfl_converter_quant_flags": [
    "converter",
    "quant_config"
  ],
  "_run_convert_passes": [
    "exported_program"
  ],
  "_warn_training_modules": [
    "signatures"
  ],
  "convert_signatures": [
    "signatures"
  ],
  "aot_compile": [
    "compilation_configs",
    "cpu_model"
  ],
  "Converter": {
    "__init__": [
      "self"
    ],
    "signature": [
      "self",
      "name",
      "module",
      "sample_args",
      "sample_kwargs"
    ],
    "add_signature": [
      "self",
      "name",
      "module",
      "sample_args",
      "sample_kwargs"
    ],
    "experimental_add_compilation_backend": [
      "self",
      "target"
    ],
    "convert": [
      "self",
      "module",
      "sample_args",
      "sample_kwargs"
    ]
  },
  "signature": [
    "name",
    "module",
    "sample_args",
    "sample_kwargs",
    "dynamic_shapes"
  ],
  "experimental_add_compilation_backend": [
    "target"
  ],
  "convert": [
    "module",
    "sample_args",
    "sample_kwargs"
  ],
  "Signature": {
    "_normalized_sample_args_kwargs": [
      "self"
    ],
    "flat_arg_names": [
      "self"
    ],
    "flat_args": [
      "self"
    ],
    "args": [
      "self"
    ],
    "kwargs": [
      "self"
    ]
  },
  "ChannelLastIOWrapper": {
    "__init__": [
      "self",
      "wrapped"
    ],
    "_to_channel_last": [
      "self",
      "x"
    ],
    "_to_channel_first": [
      "self",
      "x"
    ],
    "forward": [
      "self"
    ]
  },
  "to_channel_last_io": [
    "module",
    "args",
    "outputs"
  ],
  "FullyConnectedModel": {
    "__init__": [
      "self",
      "input_size",
      "hidden_size",
      "output_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FullyConvModel": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "TestConvertMultiSignature": {
    "setUp": [
      "self"
    ],
    "test_convert_with_default": [
      "self"
    ],
    "test_convert_no_default": [
      "self"
    ],
    "test_convert_signature_helper": [
      "self"
    ],
    "test_convert_separate_modules": [
      "self"
    ]
  },
  "TestContainer1": {},
  "TestConvert": {
    "setUp": [
      "self"
    ],
    "test_convert_add": [
      "self"
    ],
    "test_convert_dot_add": [
      "self"
    ],
    "test_convert_resnet18": [
      "self"
    ],
    "test_signature_args_ordering": [
      "self"
    ],
    "test_multi_output_model": [
      "self"
    ],
    "test_12_outputs_model": [
      "self"
    ],
    "test_apply_tfl_converter_flags": [
      "self"
    ],
    "test_convert_add_converter_flags": [
      "self"
    ],
    "test_convert_conv_transpose_batch_norm": [
      "self"
    ],
    "test_convert_model_with_dynamic_batch": [
      "self"
    ],
    "test_convert_model_with_kwargs": [
      "self"
    ],
    "test_convert_model_with_args_kwargs": [
      "self"
    ],
    "test_convert_model_with_args_nested_kwargs_1": [
      "self"
    ],
    "test_convert_model_with_args_nested_kwargs_2": [
      "self"
    ],
    "test_convert_model_with_args_nested_kwargs_3": [
      "self"
    ],
    "test_convert_model_non_flat_output_dict": [
      "self"
    ],
    "_compare_tflite_torch_args_kwargs": [
      "self",
      "model",
      "args",
      "kwargs",
      "flat_inputs"
    ],
    "test_convert_model_with_input_mutation": [
      "self"
    ],
    "test_convert_resnet18_pt2e_per_layer": [
      "self"
    ],
    "test_convert_resnet18_pt2e_per_channel": [
      "self"
    ],
    "test_convert_model_with_bfloat16_inputs": [
      "self"
    ],
    "test_convert_model_with_torch_linspace_operation": [
      "self"
    ],
    "test_convert_model_with_torch_div_operation": [
      "self"
    ],
    "test_convert_model_with_torch_div_operation_6d_inputs": [
      "self"
    ],
    "test_convert_model_with_slice_6d_inputs": [
      "self"
    ],
    "test_convert_model_with_strided_slice_6d_inputs": [
      "self"
    ],
    "test_compile_model": [
      "self"
    ]
  },
  "_func_to_torch_module": [
    "func"
  ],
  "TestConvertComposites": {
    "test_convert_hardswish": [
      "self"
    ],
    "test_convert_avg_pool2d": [
      "self",
      "input_size"
    ],
    "test_convert_upsample_bilinear_functional": [
      "self",
      "input_size",
      "kwargs"
    ],
    "test_convert_upsample_bilinear": [
      "self",
      "input_size",
      "kwargs"
    ],
    "test_convert_interpolate_bilinear_functional": [
      "self",
      "input_size",
      "kwargs"
    ],
    "test_convert_gelu": [
      "self"
    ],
    "test_convert_gelu_approximate": [
      "self"
    ],
    "test_convert_embedding_lookup": [
      "self"
    ]
  },
  "Identity": {
    "forward": [
      "self",
      "x"
    ]
  },
  "TestToChannelLastIO": {
    "test_no_transformations": [
      "self"
    ],
    "test_args": [
      "self"
    ],
    "test_outputs": [
      "self"
    ],
    "test_args_outputs": [
      "self"
    ],
    "test_args_5d": [
      "self"
    ],
    "test_outputs_5d": [
      "self"
    ],
    "test_chained_wrappers": [
      "self"
    ],
    "test_list_args": [
      "self"
    ],
    "test_list_outputs": [
      "self"
    ]
  },
  "RemoveNonUserOutputsPass": {
    "call": [
      "self",
      "exported_program"
    ]
  },
  "cast_f32": [
    "x"
  ],
  "CastInputsBf16ToF32Pass": {
    "call": [
      "self",
      "exported_program"
    ]
  },
  "EliminateDeadCodePass": {
    "call": [
      "self",
      "graph_module"
    ]
  },
  "_register_composite_builder": [
    "op"
  ],
  "_tree_map_to_composite_attr_values": [
    "values"
  ],
  "TorchOpArgumentsMapper": {
    "__init__": [
      "self",
      "op"
    ],
    "get_full_kwargs": [
      "self",
      "args",
      "kwargs"
    ]
  },
  "_aten_hardswish": [
    "_",
    "node"
  ],
  "_aten_gelu": [
    "_",
    "node"
  ],
  "_aten_avg_pool2d": [
    "_",
    "node"
  ],
  "_aten_embedding": [
    "gm",
    "node"
  ],
  "_aten_upsample_bilinear2d_vec": [
    "_",
    "node"
  ],
  "_aten_upsample_nearest2d_vec": [
    "_",
    "node"
  ],
  "BuildAtenCompositePass": {
    "call": [
      "self",
      "graph_module"
    ]
  },
  "_get_mlir_debuginfo": [
    "node"
  ],
  "_wrap_call_function_node_with_debuginfo_writer": [
    "node"
  ],
  "InjectMlirDebuginfoPass": {
    "call": [
      "self",
      "graph_module"
    ]
  },
  "TransposeFunc": [],
  "OptimizeLayoutTransposesPass": {
    "get_source_meta": [
      "self",
      "node"
    ],
    "insert_t_q_dq": [
      "self",
      "graph",
      "input_dq",
      "target",
      "transpose_func",
      "transpose_node_meta"
    ],
    "insert_dq_t_q": [
      "self",
      "graph",
      "input_q",
      "target",
      "transpose_func",
      "transpose_node_meta"
    ],
    "insert_layout_transpose": [
      "self",
      "graph",
      "input_node",
      "target_node",
      "transpose_func",
      "transpose_node_meta"
    ],
    "input_to_nhwc": [
      "self",
      "graph",
      "input_node",
      "target_node"
    ],
    "input_to_nchw": [
      "self",
      "graph",
      "input_node",
      "target_node"
    ],
    "mark_const_nodes": [
      "self",
      "exported_program"
    ],
    "call": [
      "self",
      "exported_program"
    ]
  },
  "IS_NHWC_NODE": [],
  "IS_CONST_NODE": [],
  "mark_as_nhwc_node": [
    "node"
  ],
  "mark_as_nchw_node": [
    "node"
  ],
  "is_nhwc_node": [
    "node"
  ],
  "is_nchw_node": [
    "node"
  ],
  "mark_as_const_node": [
    "node"
  ],
  "is_const_node": [
    "node"
  ],
  "OpFuncRegistry": {
    "register": [
      "self",
      "op"
    ]
  },
  "aten": [],
  "LayoutSensitiveInputsGettersRegistry": {
    "__missing__": [
      "self",
      "op"
    ]
  },
  "NHWCable": {
    "__bool__": [
      "self"
    ]
  },
  "NHWCableNodeCheckersRegistry": {
    "__init__": [
      "self"
    ],
    "__missing__": [
      "self",
      "op"
    ]
  },
  "nhwcable_node_checkers": [],
  "layout_sensitive_inputs_getters": [],
  "can_be_nhwc": [
    "node"
  ],
  "must_be_nhwc": [
    "node"
  ],
  "get_layout_sensitive_inputs": [
    "node"
  ],
  "get_no_rewriter_nhwc_ops": [],
  "is_4d": [
    "node"
  ],
  "all_layout_sensitive_inputs_are_4d": [
    "node"
  ],
  "_qdq_layout_sensitive_inputs_getter": [
    "node"
  ],
  "_first_arg_getter": [
    "node"
  ],
  "_all_layout_sensitive_inputs_are_4d_checker": [
    "node"
  ],
  "_aten_norm_checker": [
    "node"
  ],
  "_aten_group_norm_checker": [
    "node"
  ],
  "_aten_native_group_norm_checker": [
    "node"
  ],
  "_not_nhwc": [
    "node"
  ],
  "_aten_index_layout_sensitive_inputs_getter": [
    "node"
  ],
  "_aten_index_checker": [
    "node"
  ],
  "_getitem_checker": [
    "node"
  ],
  "tensor_to_nhwc": [
    "t"
  ],
  "tensor_to_nchw": [
    "t"
  ],
  "flatten_torch_op_overloads": [
    "op"
  ],
  "_TORCH_Q_OPS": [],
  "_TORCH_DQ_OPS": [],
  "is_q_node": [
    "node"
  ],
  "is_dq_node": [
    "node"
  ],
  "get_paired_q_dq_ops": [
    "op"
  ],
  "_OPS_TO_KEEP": [],
  "StableHLOCompositeBuilder": [],
  "NHWCNodeRewritersRegistry": {
    "__missing__": [
      "self",
      "op"
    ]
  },
  "rewriters": [],
  "rewrite_nhwc_node": [
    "node"
  ],
  "has_nhwc_rewriter": [
    "node"
  ],
  "noop": [
    "node"
  ],
  "_qdq_per_channel_rewriter": [
    "node"
  ],
  "transpose_first_arg_rewriter": [
    "node"
  ],
  "_aten_convolution_rewriter": [
    "node"
  ],
  "dim_attr_rewriter": [
    "node"
  ],
  "_aten__native_batch_norm_legit_no_training": [
    "node"
  ],
  "_aten_group_norm": [
    "node"
  ],
  "_aten_native_group_norm": [
    "node"
  ],
  "_aten_index": [
    "node"
  ],
  "_aten_reflection_pad2d": [
    "node"
  ],
  "can_partition": [
    "graph_module"
  ],
  "MinCutSolver": {
    "INF_COST": [],
    "__init__": [
      "self"
    ],
    "_next_nid": [
      "self"
    ],
    "nodes": [
      "self"
    ],
    "edges_map": [
      "self"
    ],
    "edges": [
      "self"
    ],
    "graph": [
      "self"
    ],
    "get_nid": [
      "self",
      "obj"
    ],
    "get_obj": [
      "self",
      "nid"
    ],
    "add_edge": [
      "self",
      "a_id",
      "b_id",
      "cost"
    ],
    "solve": [
      "self"
    ]
  },
  "MultiUsersDummyNode": {},
  "partition": [
    "graph_module"
  ],
  "CanonicalizePass": {
    "call": [
      "self",
      "exported_program"
    ]
  },
  "_DUMMY_DECOMP_TABLE": [],
  "_BUILTIN_OPERATORS": [],
  "_require_decomp": [
    "exported_program",
    "decomp_table"
  ],
  "_FORCE_DECOMP_ATTR": [],
  "annotate_force_decomp": [
    "decomp"
  ],
  "safe_run_decompositions": [
    "exported_program",
    "decomp_table",
    "can_skip"
  ],
  "_pre_convert_decomp": [],
  "_pre_lower_decomp": [],
  "_get_ops": [
    "op"
  ],
  "pre_convert_decomp": [],
  "pre_lower_decomp": [],
  "remove_pre_lower_decomp": [
    "op"
  ],
  "remove_pre_convert_decomp": [
    "op"
  ],
  "add_pre_convert_decomp": [
    "op",
    "decomp"
  ],
  "add_pre_lower_decomp": [
    "op",
    "decomp"
  ],
  "update_pre_convert_decomp": [
    "decomps"
  ],
  "update_pre_lower_decomp": [
    "decomps"
  ],
  "PassBase": [],
  "PassResult": [],
  "FxPassBase": [],
  "FxPassResult": [],
  "ExportedProgramPassBase": [],
  "ExportedProgramPassResult": [],
  "run_passes": [],
  "is_torch_version_under": [
    "torch_version"
  ],
  "reset_from_node_meta": [
    "ep"
  ],
  "LoweringContext": [],
  "_build_flat_inputs": [
    "exported_program"
  ],
  "_get_output_metas": [
    "exported_program"
  ],
  "LoweringInterpreter": {
    "__init__": [
      "self",
      "module",
      "lctx"
    ],
    "_build_loc": [
      "self",
      "node"
    ],
    "run_node": [
      "self",
      "node"
    ],
    "call_function": [
      "self",
      "target",
      "args",
      "kwargs"
    ],
    "output": [
      "self",
      "target",
      "args",
      "kwargs"
    ]
  },
  "InputSpec": {
    "parameter": [
      "cls",
      "name"
    ],
    "user_input": [
      "cls",
      "i"
    ],
    "is_parameter": [
      "self"
    ],
    "is_user_input": [
      "self"
    ]
  },
  "VariableSignature": {},
  "MlirLowered": {
    "__str__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "get_text": [
      "self",
      "enable_debug_info"
    ],
    "module_bytecode": [
      "self"
    ],
    "module_bytecode_vhlo": [
      "self"
    ],
    "tf_function": [
      "self"
    ],
    "__call__": [
      "self"
    ]
  },
  "_convert_i64_to_i32": [
    "exported_program"
  ],
  "_convert_q_dq_per_channel_args_to_list": [
    "exported_program"
  ],
  "IR_DYNAMIC": [],
  "is_ir_dynamic": [
    "v"
  ],
  "is_torch_dynamic": [
    "v"
  ],
  "is_iterable": [
    "v"
  ],
  "create_ir_context": [],
  "inline": [
    "symbol_table",
    "block"
  ],
  "clone_func_body_ops": [
    "func_op",
    "ir_inputs"
  ],
  "sanitize_aten_op_name": [
    "op",
    "chars"
  ],
  "build_ir_attr": [
    "val"
  ],
  "torch_dtype_to_ir_element_type": [],
  "ir_element_type_to_torch_dtype": [
    "ty"
  ],
  "optimization_barrier_op": [],
  "optimization_barrier": [],
  "_optimization_barrier_impl": [
    "inputs"
  ],
  "_optimization_barrier_fake": [
    "inputs"
  ],
  "_optimization_barrier_lowering": [
    "lctx",
    "inputs"
  ],
  "graph_module_flat_inputs": [
    "ep",
    "args",
    "kwargs"
  ],
  "_mangle_tf_root_scope_name": [
    "name"
  ],
  "_build_tf_state_dict": [
    "lowered"
  ],
  "_make_input_signatures": [
    "lowered"
  ],
  "mlir_to_tf_function": [
    "lowered"
  ],
  "ODML_TORCH_LIB": [],
  "custom_op_with_fake": [
    "name"
  ],
  "tfl_batch_matmul": [
    "x",
    "y",
    "adj_x",
    "adj_y"
  ],
  "tfl_add": [
    "x",
    "y"
  ],
  "tfl_sub": [
    "x",
    "y"
  ],
  "tfl_mul": [
    "x",
    "y"
  ],
  "tfl_div": [
    "x",
    "y"
  ],
  "tfl_pow": [
    "x",
    "y"
  ],
  "tfl_logical_and": [
    "x",
    "y"
  ],
  "tfl_mean": [
    "x",
    "dims",
    "keepdim"
  ],
  "tfl_greater": [
    "x",
    "y"
  ],
  "tfl_less": [
    "x",
    "y"
  ],
  "tfl_maximum": [
    "x",
    "y"
  ],
  "tfl_minimum": [
    "x",
    "y"
  ],
  "tfl_sin": [
    "x"
  ],
  "tfl_cos": [
    "x"
  ],
  "tfl_rsqrt": [
    "x"
  ],
  "tfl_neg": [
    "x"
  ],
  "tfl_gelu": [
    "x",
    "approximate"
  ],
  "tfl_transpose": [
    "input",
    "perm"
  ],
  "tfl_concatenation": [
    "tensors",
    "dim"
  ],
  "tfl_fill": [
    "dims",
    "fill_value"
  ],
  "_normalize_shape": [
    "tensor_input",
    "shape"
  ],
  "tfl_reshape": [
    "input",
    "shape"
  ],
  "tfl_reshape_fake": [
    "input",
    "shape"
  ],
  "tfl_range": [
    "start",
    "limit",
    "delta"
  ],
  "tfl_split_v": [
    "input",
    "size_splits",
    "split_dim"
  ],
  "tfl_expand_dims": [
    "x",
    "dim"
  ],
  "tfl_broadcast_to": [
    "x",
    "shape"
  ],
  "tfl_squeeze": [
    "x",
    "squeeze_dims"
  ],
  "tfl_strided_slice": [
    "input",
    "begin",
    "end",
    "strides"
  ],
  "tfl_select_v2": [
    "condition",
    "x",
    "y"
  ],
  "tfl_embedding_lookup": [
    "indices",
    "weight"
  ],
  "tfl_gather": [
    "input",
    "indices",
    "axis"
  ],
  "tfl_softmax": [
    "x"
  ],
  "tfl_topk_v2": [
    "x",
    "k"
  ],
  "tfl_multinomial": [
    "logits",
    "num_samples",
    "replacement"
  ],
  "tfl_slice": [
    "input",
    "begin",
    "size"
  ],
  "tfl_slice_tensor": [
    "input",
    "begin",
    "size"
  ],
  "tfl_slice_tensor_fake": [
    "input",
    "begin",
    "size"
  ],
  "decomps": [],
  "lower": [],
  "_ir_operation": [
    "name",
    "results",
    "operands",
    "attributes"
  ],
  "_tfl_batch_matmul_lowering": [
    "lctx",
    "x",
    "y",
    "adj_x",
    "adj_y"
  ],
  "_tfl_add_lowering": [
    "lctx",
    "lhs",
    "rhs",
    "fused_activation_function"
  ],
  "_tfl_sub_lowering": [
    "lctx",
    "lhs",
    "rhs",
    "fused_activation_function"
  ],
  "_tfl_mul_lowering": [
    "lctx",
    "lhs",
    "rhs",
    "fused_activation_function"
  ],
  "_tfl_div_lowering": [
    "lctx",
    "lhs",
    "rhs",
    "fused_activation_function"
  ],
  "_tfl_pow_lowering": [
    "lctx",
    "lhs",
    "rhs"
  ],
  "_tfl_logical_and_lowering": [
    "lctx",
    "lhs",
    "rhs"
  ],
  "_tfl_mean_lowering": [
    "lctx",
    "x",
    "dims",
    "keepdim"
  ],
  "_tfl_greater_lowering": [
    "lctx",
    "lhs",
    "rhs"
  ],
  "_tfl_less_lowering": [
    "lctx",
    "lhs",
    "rhs"
  ],
  "_tfl_maximum_lowering": [
    "lctx",
    "lhs",
    "rhs"
  ],
  "_tfl_minimum_lowering": [
    "lctx",
    "lhs",
    "rhs"
  ],
  "_tfl_sin_lowering": [
    "lctx",
    "x"
  ],
  "_tfl_cos_lowering": [
    "lctx",
    "x"
  ],
  "_tfl_rsqrt_lowering": [
    "lctx",
    "x"
  ],
  "_tfl_neg_lowering": [
    "lctx",
    "x"
  ],
  "_tfl_gelu_lowering": [
    "lctx",
    "x",
    "approximate"
  ],
  "_tfl_transpose_lowering": [
    "lctx",
    "x",
    "perm"
  ],
  "_tfl_concatenation_lowering": [
    "lctx",
    "tensors",
    "axis",
    "fused_activation_function"
  ],
  "_tfl_fill_lowering": [
    "lctx",
    "dims",
    "fill_value"
  ],
  "_tfl_reshape_lowering": [
    "lctx",
    "x",
    "shape"
  ],
  "_tfl_range_lowering": [
    "lctx",
    "start",
    "limit",
    "delta"
  ],
  "_tfl_split_v_lowering": [
    "lctx",
    "x",
    "size_splits",
    "dim"
  ],
  "_tfl_slice_lowering": [
    "lctx",
    "x",
    "begin",
    "size"
  ],
  "_tfl_expand_dims_lowering": [
    "lctx",
    "x",
    "dim"
  ],
  "_tfl_broadcast_to_lowering": [
    "lctx",
    "x",
    "shape"
  ],
  "_tfl_squeeze_lowering": [
    "lctx",
    "x",
    "squeeze_dims"
  ],
  "_tfl_strided_slice_lowering": [
    "lctx",
    "x",
    "begin",
    "end",
    "strides",
    "begin_mask",
    "end_mask",
    "ellipsis_mask",
    "new_axis_mask",
    "shrink_axis_mask",
    "offset"
  ],
  "_tfl_select_v2_lowering": [
    "lctx",
    "condition",
    "x",
    "y"
  ],
  "_tfl_embedding_lookup_lowering": [
    "lctx",
    "indices",
    "weight"
  ],
  "_tfl_gather_lowering": [
    "lctx",
    "x",
    "indices",
    "axis",
    "batch_dims"
  ],
  "_tfl_softmax_lowering": [
    "lctx",
    "x",
    "beta"
  ],
  "_tfl_topk_v2_lowering": [
    "lctx",
    "x",
    "k"
  ],
  "_tfl_multinomial_lowering": [
    "lctx",
    "logits",
    "num_samples",
    "replacement"
  ],
  "register_decomp": [
    "op"
  ],
  "_aten_mm_decomp": [
    "x",
    "y"
  ],
  "_aten_bmm_decomp": [
    "x",
    "y"
  ],
  "_promote_types_for_binary_op": [
    "x",
    "y"
  ],
  "_aten_add_tensor_decomp": [
    "x",
    "y",
    "alpha"
  ],
  "_aten_sub_tensor_decomp": [
    "x",
    "y",
    "alpha"
  ],
  "_aten_mul_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_mul_scalar_decomp": [
    "x",
    "y"
  ],
  "_aten_div_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_pow_scalar_decomp": [
    "x",
    "y"
  ],
  "_aten_pow_tensor_scalar_decomp": [
    "x",
    "y"
  ],
  "_aten_pow_tensor_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_bitwise_and_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_mean_dim_decomp": [
    "x",
    "dim",
    "keepdim"
  ],
  "_aten_gt_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_lt_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_maximum_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_minimum_tensor_decomp": [
    "x",
    "y"
  ],
  "_aten_sin_decomp": [
    "x"
  ],
  "_aten_cos_decomp": [
    "x"
  ],
  "_aten_rsqrt_decomp": [
    "x"
  ],
  "_aten_neg_decomp": [
    "x"
  ],
  "_aten_gelu_decomp": [
    "x",
    "approximate"
  ],
  "_aten_permute_decomp": [
    "x",
    "dims"
  ],
  "_prepare_tensors_for_concatenation": [
    "tensors",
    "axis"
  ],
  "_aten_cat_decomp": [
    "tensors",
    "dim"
  ],
  "_aten_full_decomp": [
    "size",
    "fill_value",
    "dtype",
    "layout",
    "device",
    "pin_memory"
  ],
  "_aten_full_like_decomp": [
    "x",
    "fill_value",
    "dtype",
    "layout",
    "device",
    "pin_memory",
    "memory_format"
  ],
  "_aten_view_decomp": [
    "x",
    "shape"
  ],
  "_aten_arange_start_step_decomp": [
    "start",
    "end",
    "step",
    "dtype",
    "layout",
    "device",
    "pin_memory"
  ],
  "_aten_split_with_sizes_decomp": [
    "x",
    "split_sizes",
    "dim"
  ],
  "_aten_unsqueeze_decomp": [
    "x",
    "dim"
  ],
  "_aten_expand_decomp": [
    "x",
    "shape"
  ],
  "_aten_squeeze_dims_decomp": [
    "x",
    "squeeze_dims"
  ],
  "_aten_select_int_decomp": [
    "x",
    "dim",
    "index"
  ],
  "_aten_slice_tensor_decomp": [
    "x",
    "dim",
    "start",
    "end",
    "step"
  ],
  "_aten_where_self_decomp": [
    "condition",
    "x",
    "y"
  ],
  "_aten_embedding_decomp": [
    "weight",
    "indices",
    "padding_idx"
  ],
  "_aten__softmax_decomp": [
    "x",
    "dim",
    "half_to_float"
  ],
  "_aten_multinomial_decomp": [
    "x",
    "num_samples",
    "replacement",
    "generator"
  ],
  "_aten_topk_decomp": [
    "self",
    "k",
    "dim",
    "largest",
    "sorted"
  ],
  "_native_batch_norm_legit_no_training": [
    "lctx",
    "input_tensor",
    "weight",
    "bias",
    "running_mean",
    "running_var",
    "momentum",
    "eps"
  ],
  "make_padding": [
    "padding"
  ],
  "create_conv_dimension_numbers": [
    "lhs",
    "transposed"
  ],
  "build_transpose_conv": [
    "lctx",
    "output_type",
    "lhs",
    "rhs",
    "stride",
    "padding",
    "dilation",
    "output_padding",
    "groups"
  ],
  "_aten_convolution": [
    "lctx",
    "lhs",
    "rhs",
    "bias",
    "stride",
    "padding",
    "dilation",
    "transposed",
    "output_padding",
    "groups"
  ],
  "node_meta_to_ir_types": [
    "node"
  ],
  "splat": [
    "val",
    "ty",
    "shape"
  ],
  "get_common_broadcast_shape": [
    "shape_1",
    "shape_2"
  ],
  "get_broadcast_dimensions": [
    "shape_from",
    "shape_to"
  ],
  "broadcast_args_if_needed": [
    "val_1",
    "val_2"
  ],
  "upcast_to_same_type": [],
  "minmax": [
    "ty"
  ],
  "convert_int_to_float": [
    "t"
  ],
  "IrValues": [],
  "numpy_dtype_to_ir_type": [
    "dtype"
  ],
  "numpy_array_constant": [
    "x"
  ],
  "convert_to_ir_value": [
    "value"
  ],
  "convert_shape_to_ir_value": [
    "shape"
  ],
  "_aten_add": [
    "lctx",
    "x",
    "y",
    "alpha"
  ],
  "_aten_mul_tensor": [
    "lctx",
    "self",
    "other"
  ],
  "_hann_window_impl": [
    "lctx",
    "size",
    "periodic",
    "dtype"
  ],
  "_aten_hann_window_default": [
    "lctx",
    "size"
  ],
  "_aten_hann_window_periodic": [
    "lctx",
    "size",
    "periodic"
  ],
  "_aten_cat": [
    "lctx",
    "tensors",
    "dim"
  ],
  "_aten_view": [
    "lctx",
    "self",
    "size"
  ],
  "_aten_hardtanh": [
    "lctx",
    "self",
    "min_val",
    "max_val"
  ],
  "_aten_mean_dim": [
    "lctx",
    "self",
    "dim",
    "keepdim"
  ],
  "_aten_clone": [
    "lctx",
    "x"
  ],
  "_aten_permute": [
    "lctx",
    "x",
    "dims"
  ],
  "_aten_mm": [
    "mod",
    "mat1",
    "mat2"
  ],
  "_aten_div": [
    "mod",
    "x",
    "y"
  ],
  "_aten_floor": [
    "lctx",
    "x"
  ],
  "_aten_abs": [
    "lctx",
    "input"
  ],
  "_aten_unfold": [
    "lctx",
    "x",
    "dim",
    "size",
    "step"
  ],
  "_aten_slice_scatter": [
    "lctx",
    "self",
    "src",
    "dim",
    "start",
    "end",
    "step"
  ],
  "_aten_to_copy": [
    "lctx",
    "x",
    "dtype"
  ],
  "_aten_sym_size_int": [
    "lctx",
    "x",
    "dim"
  ],
  "_operator_add": [
    "lctx",
    "self",
    "other"
  ],
  "_operator_sub": [
    "lctx",
    "self",
    "other"
  ],
  "_operator_mul": [
    "lctx",
    "self",
    "other"
  ],
  "upsample_compute_output_size": [
    "input_size",
    "output_size",
    "scale_factors"
  ],
  "_compute_upsample_nearest_indices": [
    "input",
    "output_size",
    "scales",
    "exact"
  ],
  "_upsample_nearest2d_common": [
    "input",
    "h_indices",
    "w_indices"
  ],
  "upsample_nearest2d": [
    "input",
    "output_size",
    "scales_h",
    "scales_w"
  ],
  "get_scale_value": [
    "scales",
    "idx"
  ],
  "_log_usage": [
    "op"
  ],
  "lower_by_jax": [
    "op",
    "ir_input_names"
  ],
  "_TORCH_XLA2_IMPLS": [],
  "lower_by_torch_xla2": [
    "op"
  ],
  "_ceil_mode_padding": [
    "padding",
    "input_shape",
    "kernel_size",
    "stride",
    "dilation",
    "ceil_mode"
  ],
  "max_pool": [
    "inputs",
    "kernel_size",
    "strides",
    "padding",
    "dilation",
    "ceil_mode",
    "with_index"
  ],
  "_aten_max_pool2d_with_indices": [
    "self",
    "kernel_size",
    "stride",
    "padding",
    "dilation",
    "ceil_mode"
  ],
  "_aten_max_pool3d_with_indices": [
    "self",
    "kernel_size",
    "stride",
    "padding",
    "dilation",
    "ceil_mode"
  ],
  "_aten_pixel_shuffle": [
    "x",
    "upscale_factor"
  ],
  "_aten_copy": [
    "self",
    "src"
  ],
  "_aten_elu": [
    "self",
    "alpha",
    "scale",
    "input_scale"
  ],
  "_aten_add_scalar": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_add_tensor": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_sub_scalar": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_sub_tensor": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_lt_scalar": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_lt_tensor": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_mul_scalar": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_div_scalar": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_div_scalar_mode": [
    "lctx",
    "self",
    "other",
    "rounding_mode"
  ],
  "_aten_div_tensor": [
    "lctx",
    "self",
    "other"
  ],
  "_aten_div_tensor_mode": [
    "lctx",
    "self",
    "other",
    "rounding_mode"
  ],
  "_aten_where_self": [
    "lctx",
    "condition",
    "self",
    "other"
  ],
  "_aten_einsum_default": [
    "lctx",
    "equation",
    "tensors",
    "path"
  ],
  "_aten_topk": [
    "lctx",
    "self",
    "k",
    "dim",
    "largest",
    "sorted"
  ],
  "_random_lowering": [
    "lctx",
    "size",
    "generator",
    "dtype",
    "rand_tensor",
    "composite_name"
  ],
  "_aten_rand": [
    "lctx",
    "size",
    "generator",
    "dtype",
    "layout",
    "device",
    "pin_memory"
  ],
  "_aten_randn": [
    "lctx",
    "size",
    "generator",
    "dtype",
    "layout",
    "device",
    "pin_memory"
  ],
  "_aten_native_layer_norm": [
    "lctx",
    "data",
    "normalized_shape",
    "weight",
    "bias",
    "eps"
  ],
  "LoweringRegistry": {
    "__init__": [
      "self"
    ],
    "lookup": [
      "self",
      "op_or_name"
    ],
    "_get_lowering": [
      "self",
      "op"
    ],
    "register": [
      "self",
      "op",
      "lowering"
    ]
  },
  "global_registry": [],
  "lookup": [
    "op"
  ],
  "_uniform_quantized_type": [
    "stored_type",
    "expressed_type"
  ],
  "_quantize_per_tensor": [
    "lctx",
    "input",
    "scale",
    "zero_point",
    "quant_min",
    "quant_max",
    "dtype"
  ],
  "_quantize_per_channel": [
    "lctx",
    "input",
    "scale",
    "zero_point",
    "axis",
    "quant_min",
    "quant_max",
    "dtype"
  ],
  "_dequantize": [
    "lctx",
    "input"
  ],
  "CompositeAttrType": [],
  "_assert_valid_composite_attr": [
    "attr"
  ],
  "serialize_composite_attr": [
    "attr"
  ],
  "deserialize_composite_attr": [
    "serialized_attr"
  ],
  "mark_tensor_op": [],
  "mark_tensor": [
    "x",
    "name",
    "pos",
    "id",
    "is_input",
    "attr"
  ],
  "mark_tensor_meta": [
    "x",
    "name",
    "pos",
    "id",
    "is_input",
    "attr"
  ],
  "mark_tensor_lowering": [
    "lctx",
    "x",
    "name",
    "pos",
    "id",
    "is_input",
    "attr"
  ],
  "run_pass": [
    "pipeline",
    "module"
  ],
  "canonicalize": [
    "module"
  ],
  "cse": [
    "module"
  ],
  "strip_debuginfo": [
    "module"
  ],
  "_lower_to_ir_text": [
    "jaxfn",
    "args",
    "kwargs",
    "ir_input_names"
  ],
  "wrap": [
    "jaxfn",
    "ir_input_names"
  ],
  "t2j_dtype": [
    "dtype"
  ],
  "is_ir_variable": [
    "value"
  ],
  "ir_variable_to_jax": [
    "value"
  ],
  "tree_map_list_to_tuple": [
    "value"
  ],
  "_class_fullname": [
    "cls"
  ],
  "_get_hierarchy": [
    "node"
  ],
  "_get_canonical_filename": [
    "filename"
  ],
  "_get_canoical_nodename": [
    "node"
  ],
  "build_mlir_file_debuginfo": [
    "node"
  ],
  "build_nodename_debuginfo": [
    "node"
  ],
  "build_mlir_debuginfo": [
    "node"
  ],
  "write_mlir_debuginfo_op": [],
  "write_mlir_debuginfo": [
    "x",
    "_"
  ],
  "write_mlir_debuginfo_meta": [
    "x",
    "_"
  ],
  "write_mlir_debuginfo_lowering": [
    "lctx",
    "x",
    "_"
  ],
  "ModelConfig": [],
  "LayerQuantRecipe": {
    "__str__": [
      "self"
    ],
    "__repr__": [],
    "verify": [
      "self"
    ]
  },
  "GenerativeQuantRecipe": {
    "__str__": [
      "self"
    ],
    "__repr__": [],
    "verify": [
      "self"
    ]
  },
  "Dtype": {
    "FP32": [],
    "FP16": [],
    "INT8": [],
    "INT4": []
  },
  "Algorithm": {
    "MIN_MAX": [],
    "FLOAT_CAST": []
  },
  "Mode": {
    "DYNAMIC_RANGE": [],
    "WEIGHT_ONLY": []
  },
  "Granularity": {
    "NONE": [],
    "CHANNELWISE": [],
    "BLOCKWISE_32": [],
    "BLOCKWISE_64": [],
    "BLOCKWISE_128": [],
    "BLOCKWISE_256": []
  },
  "create_layer_quant_dynamic": [
    "weight_dtype",
    "granularity"
  ],
  "create_layer_quant_weight_only": [
    "weight_dtype",
    "granularity"
  ],
  "create_layer_quant_fp16": [],
  "full_dynamic_recipe": [
    "mcfg",
    "weight_dtype",
    "granularity"
  ],
  "full_weight_only_recipe": [
    "mcfg",
    "weight_dtype",
    "granularity"
  ],
  "full_fp16_recipe": [
    "mcfg"
  ],
  "main": [],
  "get_supported_layer_schemes": [],
  "ResidualBlockTensorNames": {},
  "AttentionBlockTensorNames": {},
  "CrossAttentionBlockTensorNames": {},
  "TimeEmbeddingTensorNames": {},
  "FeedForwardBlockTensorNames": {},
  "TransformerBlockTensorNames": {},
  "MidBlockTensorNames": {},
  "DownEncoderBlockTensorNames": {},
  "UpDecoderBlockTensorNames": {},
  "SkipUpDecoderBlockTensorNames": {},
  "_map_to_converted_state": [
    "state",
    "state_param",
    "converted_state",
    "converted_state_param",
    "squeeze_dims"
  ],
  "BaseLoader": {
    "_map_residual_block": [
      "self",
      "state",
      "converted_state",
      "tensor_names",
      "converted_state_param_prefix",
      "config"
    ],
    "_map_attention_block": [
      "self",
      "state",
      "converted_state",
      "tensor_names",
      "converted_state_param_prefix",
      "config"
    ],
    "_map_cross_attention_block": [
      "self",
      "state",
      "converted_state",
      "tensor_names",
      "converted_state_param_prefix",
      "config"
    ],
    "_map_feedforward_block": [
      "self",
      "state",
      "converted_state",
      "tensor_names",
      "converted_state_param_prefix",
      "config"
    ],
    "_map_transformer_block": [
      "self",
      "state",
      "converted_state",
      "tensor_names",
      "converted_state_param_prefix",
      "config"
    ],
    "_map_mid_block": [
      "self",
      "state",
      "converted_state",
      "tensor_names",
      "converted_state_param_prefix",
      "config"
    ],
    "_map_down_encoder_block": [
      "self",
      "state",
      "converted_state",
      "converted_state_param_prefix",
      "config",
      "tensor_names"
    ],
    "_map_up_decoder_block": [
      "self",
      "state",
      "converted_state",
      "converted_state_param_prefix",
      "config",
      "tensor_names"
    ],
    "_map_skip_up_decoder_block": [
      "self",
      "state",
      "converted_state",
      "converted_state_param_prefix",
      "config",
      "tensor_names"
    ]
  },
  "ClipModelLoader": [],
  "AutoEncoderModelLoader": {
    "__init__": [
      "self",
      "file_name",
      "names"
    ],
    "load": [
      "self",
      "model",
      "strict"
    ]
  },
  "build_attention_config": [
    "num_heads",
    "dim",
    "num_query_groups",
    "rotary_percentage",
    "qkv_transpose_before_split",
    "qkv_use_bias",
    "output_proj_use_bias",
    "enable_kv_cache",
    "qkv_fused_interleaved"
  ],
  "DiffusionModelLoader": {
    "__init__": [
      "self",
      "file_name",
      "names"
    ],
    "load": [
      "self",
      "model",
      "strict"
    ],
    "_map_time_embedding": [
      "self",
      "state",
      "converted_state",
      "converted_state_param_prefix",
      "tensor_names"
    ]
  },
  "initialize_kv_cache_all_zeros": [
    "kv_shape",
    "layout"
  ],
  "get_custom_loader": [
    "checkpoint_path",
    "checkpoint_format"
  ],
  "maybe_get_custom_loader": [
    "checkpoint_path",
    "use_custom_loader"
  ],
  "load_safetensors": [
    "full_path"
  ],
  "load_pytorch_statedict": [
    "full_path"
  ],
  "ModelLoader": {
    "__init__": [
      "self",
      "file_name",
      "names",
      "custom_loader"
    ],
    "get_state": [
      "self"
    ],
    "load": [
      "self",
      "model",
      "strict"
    ],
    "_get_loader": [
      "self"
    ],
    "_map_feedforward": [
      "self",
      "idx",
      "config",
      "state",
      "converted_state"
    ],
    "_map_attention": [
      "self",
      "idx",
      "config",
      "state",
      "converted_state"
    ],
    "_map_norm": [
      "self",
      "idx",
      "config",
      "state",
      "converted_state"
    ],
    "_fuse_qkv": [
      "self",
      "attn_config",
      "q",
      "k",
      "v"
    ]
  },
  "ExportConfig": [],
  "ExportableModule": {
    "__init__": [
      "self",
      "module"
    ],
    "forward": [
      "self"
    ]
  },
  "QuantizationName": {
    "NONE": [],
    "DYNAMIC_INT8": [],
    "WEIGHT_ONLY_INT8": [],
    "FP16": [],
    "DYNAMIC_INT4_BLOCK32": [],
    "DYNAMIC_INT4_BLOCK128": []
  },
  "define_conversion_flags": [
    "model_name",
    "default_mask_as_input",
    "default_transpose_kv_cache"
  ],
  "_CONTEXT_LENGTH_TO_VERIFY_MAGIC_NUMBERS": [],
  "_LONG_PREFILL_LENGTH_TO_VERIFY_MAGIC_NUMBERS": [],
  "_SHORT_PREFILL_LENGTH_TO_VERIFY_MAGIC_NUMBERS": [],
  "is_magic_number_": [
    "num"
  ],
  "get_magic_number_for": [
    "org_number"
  ],
  "get_mask_cache_size_from_flags": [],
  "get_quant_recipe_from_flag": [
    "quantize",
    "model_config"
  ],
  "create_quantize_suffix": [
    "quantize"
  ],
  "_build_mask": [
    "mask_len",
    "kv_cache_max_len",
    "causal_mask_value"
  ],
  "convert_to_tflite": [
    "pytorch_model",
    "output_path",
    "output_name_prefix",
    "prefill_seq_len",
    "kv_cache_max_len",
    "pixel_values_size",
    "pixel_seq_len",
    "quantize",
    "config",
    "lora_ranks",
    "export_config",
    "extra_model",
    "extra_prefill_seq_lens",
    "extra_kv_cache_max_len",
    "extra_signature_prefix"
  ],
  "_add_signatures": [
    "converter",
    "pytorch_model",
    "prefill_seq_lens",
    "kv_cache_max_len",
    "pixel_values_size",
    "pixel_seq_len",
    "config",
    "loras",
    "export_config",
    "signature_prefix"
  ],
  "build_and_convert_to_tflite_from_flags": [
    "model_builder",
    "checkpoint_path",
    "output_name_prefix"
  ],
  "convert_to_litert": [
    "pytorch_model",
    "output_path",
    "output_name_prefix",
    "prefill_seq_len",
    "kv_cache_max_len",
    "pixel_values_size",
    "pixel_seq_len",
    "quantize",
    "config",
    "lora_ranks",
    "export_config",
    "output_format"
  ],
  "transpose_if_needed": [
    "t"
  ],
  "load_h5_statedict": [
    "full_path"
  ],
  "TensorDims": {
    "BATCH": [],
    "SEQUENCE": [],
    "NUM_HEADS": [],
    "HEAD_DIM": [],
    "MODEL_DIM": []
  },
  "DIM_TO_LETTER": [],
  "TensorDimensionMeta": {
    "__new__": [
      "cls",
      "name",
      "bases",
      "attrs",
      "dimensions"
    ],
    "__init__": [
      "cls",
      "name",
      "bases",
      "attrs",
      "dimensions"
    ],
    "__repr__": [
      "cls"
    ],
    "__iter__": [
      "cls"
    ]
  },
  "create_tensor_dimension_order_class": [
    "dims"
  ],
  "BTNH": [],
  "BNTH": [],
  "BNHT": [],
  "dataclass": [],
  "get_from_flags": [],
  "is_litertlm_builder_available": [],
  "build_litertlm": [
    "tflite_model_path",
    "workdir",
    "output_path",
    "context_length",
    "model_prompt_prefix",
    "model_prompt_suffix",
    "user_prompt_prefix",
    "user_prompt_suffix",
    "tokenizer_model_path",
    "hf_tokenizer_model_path",
    "start_token",
    "start_token_id",
    "stop_tokens",
    "stop_token_ids",
    "llm_model_type",
    "jinja_prompt_template",
    "base_llm_metadata_path"
  ],
  "DEFAULT_KV_CACHE_MAX_LEN": [],
  "ModelWrapper": {
    "__init__": [
      "self",
      "model"
    ],
    "forward": [
      "self",
      "tokens",
      "pixel_values"
    ],
    "generate": [
      "self",
      "prompts",
      "max_new_tokens",
      "pixel_values"
    ]
  },
  "ReauthoredModelWrapper": {
    "__init__": [
      "self",
      "model",
      "mask_as_input",
      "kv_layout",
      "kv_cache_max_len"
    ],
    "_init_kv_cache": [
      "self"
    ],
    "_get_extra_args_for_forward": [
      "self"
    ],
    "_build_mask": [
      "self",
      "input_pos"
    ],
    "_forward_with_kv_cache": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "pixel_values"
    ],
    "forward": [
      "self",
      "tokens",
      "pixel_values"
    ],
    "generate": [
      "self",
      "prompts",
      "max_new_tokens",
      "pixel_values",
      "eos_token_id"
    ]
  },
  "TokenizerWrapper": {
    "__init__": [
      "self",
      "tokenizer"
    ],
    "encode": [
      "self",
      "prompts"
    ],
    "decode": [
      "self",
      "token_ids"
    ]
  },
  "verify_with_input_ids": [
    "original_model",
    "reauthored_model",
    "input_ids",
    "total_seq_len",
    "rtol",
    "atol"
  ],
  "verify_model_with_prompts": [
    "original_model",
    "reauthored_model",
    "tokenizer",
    "prompts",
    "max_new_tokens"
  ],
  "verify_reauthored_model": [
    "original_model",
    "reauthored_model",
    "tokenizer",
    "generate_prompts",
    "max_new_tokens",
    "forward_input_ids",
    "rtol",
    "atol",
    "continue_on_failure",
    "verify_inputs",
    "verify_prompts"
  ],
  "TENSOR_NAMES": [],
  "TENSOR_NAMES_WITH_SEPARATE_LM_HEAD": [],
  "DecoderOnlyModel": {
    "__init__": [
      "self",
      "config",
      "mask_cache_size"
    ],
    "build_mask_cache": [
      "self",
      "mask_cache_size"
    ],
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "mask",
      "lora",
      "export_config"
    ],
    "_forward_with_embeds": [
      "self",
      "input_embeds",
      "rope",
      "mask",
      "input_pos",
      "kv_cache",
      "lora",
      "export_config"
    ]
  },
  "build_decoder_only_model": [
    "checkpoint_path",
    "config",
    "tensor_names",
    "model_class",
    "custom_loader",
    "mask_cache_size"
  ],
  "TransformersModelWrapper": {
    "forward": [
      "self",
      "tokens"
    ],
    "generate": [
      "self",
      "inputs",
      "max_new_tokens"
    ]
  },
  "TestModelConversion": {
    "setUp": [
      "self"
    ],
    "_get_params": [
      "self",
      "enable_hlfb",
      "kv_layout"
    ],
    "_test_model_with_kv_cache": [
      "self",
      "enable_hlfb",
      "kv_layout"
    ],
    "test_toy_model_with_kv_cache": [
      "self"
    ],
    "test_toy_model_with_kv_cache_with_hlfb": [
      "self"
    ],
    "test_toy_model_with_kv_cache_transposed": [
      "self"
    ],
    "test_toy_model_has_dus_op": [
      "self"
    ],
    "_test_multisig_model": [
      "self",
      "config",
      "pytorch_model",
      "atol",
      "rtol",
      "kv_layout"
    ],
    "test_tiny_llama_multisig": [
      "self"
    ],
    "test_tiny_llama_multisig_kv_layout_transposed": [
      "self"
    ]
  },
  "compare_logits": [
    "edge_logits",
    "torch_logits",
    "atol",
    "rtol"
  ],
  "TestKVLayers": {
    "_get_test_config": [
      "self",
      "num_layers",
      "head_dim",
      "num_query_groups"
    ],
    "_assert_kv_cache_entry_equal": [
      "self",
      "kv1",
      "kv2"
    ],
    "_assert_kv_cache_equal": [
      "self",
      "kv1",
      "kv2"
    ],
    "test_cache_udpate": [
      "self"
    ],
    "test_serialization": [
      "self"
    ],
    "test_pytree_roundtrip_kv_cache": [
      "self"
    ],
    "test_pytree_roundtrip_kv_cache_derived": [
      "self"
    ],
    "test_pytree_roundtrip_kv_entry": [
      "self"
    ],
    "test_pytree_roundtrip_kv_entry_derived": [
      "self"
    ]
  },
  "TestLoader": {
    "test_load_safetensors": [
      "self"
    ],
    "test_load_statedict": [
      "self"
    ],
    "test_model_loader": [
      "self"
    ]
  },
  "updated_slice_matches": [
    "buffer",
    "update",
    "index"
  ],
  "intT": [
    "x"
  ],
  "DUSMod": {
    "forward": [
      "self",
      "buffer",
      "update",
      "index"
    ]
  },
  "TestCustomDUS": {
    "test_opcheck_dynamic_update_slice": [
      "self",
      "buffer",
      "update",
      "indices"
    ],
    "test_exported_program": [
      "self"
    ]
  },
  "TestVerifyRecipes": {
    "test_verify_invalid_recipes": [
      "self",
      "activation",
      "weight"
    ],
    "test_verify_valid_recipes": [
      "self",
      "activation",
      "weight",
      "mode",
      "algo",
      "granularity"
    ]
  },
  "TestQuantizeConvert": {
    "setUp": [
      "self"
    ],
    "_attention_int8_dynamic_recipe": [],
    "_feedforward_int8_dynamic_recipe": [],
    "test_quantize_convert_toy_sizes": [
      "self",
      "quant_config"
    ],
    "test_quantize_convert_toy_weight_sharing": [
      "self"
    ],
    "test_quantize_convert_toy_blockwise": [
      "self"
    ],
    "test_quantize_convert_compare_toy": [
      "self"
    ]
  },
  "TestLora": {
    "test_safetensors_builder": [
      "self"
    ],
    "test_torch_export": [
      "self"
    ],
    "test_lora_tflite_serialization": [
      "self"
    ],
    "_get_test_config": [
      "self",
      "num_layers",
      "head_dim",
      "num_query_groups"
    ]
  },
  "apply_rope": [
    "x",
    "cos",
    "sin"
  ],
  "build_rope": [
    "input_pos",
    "n_elem",
    "base"
  ],
  "apply_rope_inline": [
    "q",
    "k",
    "cos",
    "sin"
  ],
  "AttentionUtilsTest": {
    "test_get_alibi_slopes": [
      "self"
    ],
    "test_build_alibi_bias": [
      "self"
    ],
    "test_build_causal_mask_cache": [
      "self"
    ],
    "test_build_sliding_window_mask_cache": [
      "self"
    ],
    "test_build_relative_position_buckets": [
      "self"
    ]
  },
  "_TFLITE_SCHEMA_VERSION": [],
  "_TFLITE_FILE_IDENTIFIER": [],
  "LoRAWeight": {
    "__eq__": [
      "self",
      "other",
      "rtol",
      "atol"
    ]
  },
  "AttentionLoRA": {
    "__eq__": [
      "self",
      "other",
      "rtol",
      "atol"
    ]
  },
  "LoRAEntry": {
    "__eq__": [
      "self",
      "other",
      "rtol",
      "atol"
    ]
  },
  "LoRATensorNames": {},
  "LoRA": {
    "__eq__": [
      "self",
      "other",
      "rtol",
      "atol"
    ],
    "get_rank": [
      "self"
    ],
    "from_safetensors": [
      "cls",
      "path",
      "scale",
      "config",
      "lora_tensor_names",
      "dtype"
    ],
    "from_flatbuffers": [
      "cls",
      "flatbuffer_model",
      "dtype"
    ],
    "zeros": [
      "cls",
      "rank",
      "config",
      "dtype"
    ],
    "random": [
      "cls",
      "rank",
      "config",
      "dtype"
    ],
    "_from_tensor_generator": [
      "cls",
      "tensor_generator",
      "rank",
      "config",
      "dtype"
    ],
    "to_tflite": [
      "self"
    ]
  },
  "apply_lora": [
    "x",
    "lora_weight",
    "shape"
  ],
  "_flatten_attention_lora": [
    "lora",
    "block_index"
  ],
  "_flatten_lora": [
    "lora"
  ],
  "_flatten_lora_with_keys": [
    "lora"
  ],
  "_unflatten_lora": [
    "values",
    "context"
  ],
  "_add_buffer": [
    "builder",
    "data"
  ],
  "_add_tensor": [
    "builder",
    "name",
    "shape",
    "buffer_idx"
  ],
  "_lora_to_flatbuffers": [
    "lora"
  ],
  "KVLayout": [],
  "KV_LAYOUT_DEFAULT": [],
  "KV_LAYOUT_TRANSPOSED": [],
  "KVCacheEntry": {
    "construct_kv_shape_from_layout": [
      "cls",
      "shape_spec",
      "kv_cache_max",
      "config",
      "batch_size"
    ],
    "from_model_config": [
      "cls",
      "kv_cache_max",
      "config",
      "dtype",
      "device",
      "batch_size",
      "kv_layout"
    ],
    "get_max_seq_len": [
      "self"
    ]
  },
  "KVCache": {
    "from_model_config": [
      "cls",
      "kv_cache_max",
      "config",
      "dtype",
      "device",
      "batch_size",
      "kv_layout"
    ],
    "flatten": [
      "self"
    ],
    "get_max_seq_len": [
      "self"
    ]
  },
  "_flatten_kvc": [
    "kvc"
  ],
  "_flatten_kvc_with_keys": [
    "kvc"
  ],
  "_unflatten_kvc": [
    "values",
    "context"
  ],
  "_flatten_kv_entry": [
    "kv_e"
  ],
  "_unflatten_kv_entry": [
    "values",
    "context"
  ],
  "update": [
    "cache",
    "input_pos",
    "k_slice",
    "v_slice",
    "use_dus"
  ],
  "_update_kv_base_impl": [
    "cache",
    "input_pos",
    "k_slice",
    "v_slice"
  ],
  "_get_slice_indices": [
    "positions"
  ],
  "_update_kv_impl": [
    "cache",
    "input_pos",
    "k_slice",
    "v_slice"
  ],
  "update_transposed": [
    "cache",
    "input_pos",
    "k_slice",
    "v_slice"
  ],
  "_get_slice_indices_transposed": [
    "positions",
    "cache_dim",
    "ts_idx"
  ],
  "_update_kv_impl_transposed": [
    "cache",
    "input_pos",
    "k_slice",
    "v_slice"
  ],
  "FeedForwardTest": {
    "test_einsum": [
      "self"
    ]
  },
  "Einsum": {
    "__init__": [
      "self",
      "shape",
      "einsum_str",
      "init_fn"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_get_alibi_slopes": [
    "n_heads"
  ],
  "build_alibi_bias": [
    "n_heads",
    "k_size",
    "dtype",
    "device"
  ],
  "build_rope_cache": [
    "size",
    "dim",
    "base",
    "condense_ratio",
    "dtype",
    "device"
  ],
  "build_causal_mask_cache": [
    "size",
    "dtype",
    "device",
    "mask_value"
  ],
  "build_sliding_window_mask_cache": [
    "size",
    "window_size",
    "dtype",
    "device",
    "mask_value"
  ],
  "relative_position_bucket": [
    "relative_position",
    "bidirectional",
    "num_buckets",
    "max_distance"
  ],
  "build_relative_position_buckets": [
    "query_length",
    "key_length",
    "bidirectional",
    "num_buckets",
    "max_distance"
  ],
  "sdpa_with_kv_update": [
    "query",
    "key",
    "value",
    "kv",
    "input_pos",
    "mask",
    "config",
    "enable_hlfb",
    "alibi_bias"
  ],
  "_sdpa_with_kv_update_transposed": [
    "query",
    "key",
    "value",
    "kv",
    "input_pos",
    "mask",
    "config",
    "alibi_bias"
  ],
  "_sdpa_with_kv_update_default": [
    "query",
    "key",
    "value",
    "kv",
    "input_pos",
    "mask",
    "config",
    "enable_hlfb",
    "alibi_bias"
  ],
  "RMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps",
      "zero_centered_gamma",
      "with_scale",
      "scale_shift",
      "enable_hlfb",
      "init_fn"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GroupNorm": {
    "__init__": [
      "self",
      "group_num",
      "dim",
      "eps",
      "enable_hlfb"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LayerNorm": {
    "__init__": [
      "self",
      "dim",
      "eps",
      "use_bias",
      "enable_hlfb"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "rms_norm_with_hlfb": [
    "x",
    "w",
    "eps",
    "final_scale"
  ],
  "layer_norm_with_hlfb": [
    "x",
    "normalized_shape",
    "w",
    "b",
    "eps"
  ],
  "TransformerBlock": {
    "__init__": [
      "self",
      "config",
      "model_config"
    ],
    "forward": [
      "self",
      "x",
      "rope",
      "mask",
      "input_pos",
      "kv_cache",
      "lora"
    ]
  },
  "CausalSelfAttentionBase": {
    "__init__": [
      "self",
      "dim",
      "config",
      "enable_hlfb"
    ],
    "forward": [
      "self",
      "x",
      "rope",
      "mask",
      "input_pos",
      "kv_cache",
      "lora"
    ]
  },
  "CausalSelfAttention": {
    "__init__": [
      "self",
      "dim",
      "config",
      "enable_hlfb"
    ],
    "forward": [
      "self",
      "x",
      "rope",
      "mask",
      "input_pos",
      "kv_cache",
      "lora"
    ]
  },
  "SelfAttention": {
    "forward": [
      "self",
      "x",
      "rope",
      "input_pos",
      "kv_cache",
      "lora"
    ]
  },
  "CrossAttention": {
    "__init__": [
      "self",
      "query_dim",
      "cross_dim",
      "hidden_dim",
      "output_dim",
      "config",
      "enable_hlfb"
    ],
    "forward": [
      "self",
      "x",
      "y",
      "rope",
      "mask",
      "input_pos",
      "kv_cache",
      "lora"
    ]
  },
  "GeGLU": {
    "__init__": [
      "self",
      "d_in",
      "d_out"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SwiGLU": {
    "forward": [
      "self",
      "x"
    ]
  },
  "build_norm": [
    "dim",
    "config",
    "init_fn"
  ],
  "build_ff": [
    "dim",
    "config"
  ],
  "get_activation": [
    "config"
  ],
  "ScaledDotProductAttentionTest": {
    "test_scaled_dot_product_attention": [
      "self"
    ],
    "test_scaled_dot_product_attention_transposed": [
      "self"
    ],
    "test_scaled_dot_product_attention_with_hlfb": [
      "self"
    ]
  },
  "AttentionTest": {
    "test_causal_self_attention": [
      "self",
      "attn_type",
      "use_alibi",
      "expected_shape"
    ],
    "test_cross_attention": [
      "self"
    ],
    "test_transformer_block": [
      "self"
    ]
  },
  "NormalizationTest": {
    "test_rms_norm": [
      "self",
      "model_dim",
      "with_scale",
      "scale_shift",
      "enable_hlfb",
      "expected_values"
    ]
  },
  "ActivationType": {
    "LINEAR": [],
    "SILU": [],
    "GELU": [],
    "GELU_TANH": [],
    "GELU_QUICK": [],
    "GE_GLU": [],
    "RELU": [],
    "SILU_GLU": []
  },
  "NormalizationType": {
    "NONE": [],
    "RMS_NORM": [],
    "LAYER_NORM": [],
    "GROUP_NORM": []
  },
  "FeedForwardType": {
    "SEQUENTIAL": [],
    "GATED": []
  },
  "AttentionType": {
    "GLOBAL": [],
    "LOCAL_SLIDING": []
  },
  "NormalizationConfig": {},
  "KVCacheUpdateStrategy": {
    "INPLACE": [],
    "PREPEND_LEFT": []
  },
  "AttentionConfig": {},
  "ActivationConfig": {},
  "FeedForwardConfig": {},
  "TransformerBlockConfig": {},
  "ImageEmbeddingConfig": {},
  "FeedForwardBase": {
    "__init__": [
      "self",
      "dim",
      "activation",
      "config",
      "pre_ff_norm",
      "post_ff_norm"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SequentialFeedForward": {
    "__init__": [
      "self",
      "dim",
      "activation",
      "config",
      "pre_ff_norm",
      "post_ff_norm"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GatedFeedForward": {
    "__init__": [
      "self",
      "dim",
      "activation",
      "config",
      "pre_ff_norm",
      "post_ff_norm"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "scaled_dot_product_attention": [
    "q",
    "k",
    "v",
    "head_size",
    "mask",
    "scale",
    "softcap",
    "alibi_bias"
  ],
  "scaled_dot_product_attention_with_hlfb": [
    "q",
    "k",
    "v",
    "head_size",
    "mask",
    "scale",
    "softcap",
    "alibi_bias"
  ],
  "scaled_dot_product_attention_transposed": [
    "query",
    "key",
    "value",
    "head_size",
    "mask",
    "scale",
    "softcap",
    "alibi_bias"
  ],
  "build_upsampling": [
    "config"
  ],
  "build_downsampling": [
    "config"
  ],
  "ResidualBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "time_emb"
    ]
  },
  "AttentionBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor"
    ]
  },
  "CrossAttentionBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "context_tensor"
    ]
  },
  "FeedForwardBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor"
    ]
  },
  "TransformerBlock2D": {
    "__init__": [
      "self",
      "config",
      "dim_override"
    ],
    "forward": [
      "self",
      "x",
      "context"
    ]
  },
  "DownEncoderBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "time_emb",
      "context_tensor",
      "output_hidden_states"
    ]
  },
  "UpDecoderBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "time_emb",
      "context_tensor"
    ]
  },
  "SkipUpDecoderBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "skip_connection_tensors",
      "time_emb",
      "context_tensor"
    ]
  },
  "MidBlock2D": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_tensor",
      "time_emb",
      "context_tensor"
    ]
  },
  "SamplingType": {
    "NEAREST": [],
    "BILINEAR": [],
    "AVERAGE": [],
    "CONVOLUTION": []
  },
  "UpSamplingConfig": {},
  "DownSamplingConfig": {},
  "ResidualBlock2DConfig": {},
  "AttentionBlock2DConfig": {},
  "CrossAttentionBlock2DConfig": {},
  "FeedForwardBlock2DConfig": {},
  "TransformerBlock2DConfig": {},
  "UpDecoderBlock2DConfig": {},
  "SkipUpDecoderBlock2DConfig": {},
  "DownEncoderBlock2DConfig": {},
  "MidBlock2DConfig": {},
  "AutoEncoderConfig": {},
  "DiffusionModelConfig": {},
  "dynamic_update_slice": [
    "in_tensor",
    "update",
    "start_indices"
  ],
  "_": [
    "in_tensor",
    "update",
    "start_indices"
  ],
  "_dynamic_update_slice_lower": [
    "lctx",
    "in_tensor",
    "update",
    "start_indices"
  ],
  "bmm_4d": [
    "lhs",
    "rhs"
  ],
  "_bmm_4d_lower": [
    "lctx",
    "lhs",
    "rhs"
  ],
  "export": [
    "model",
    "output_dir",
    "prefill_lengths",
    "cache_length",
    "quantization_recipe",
    "enable_dynamic_shape",
    "auto_model_override",
    "trust_remote_code",
    "use_jinja_template"
  ],
  "KeyCache": [],
  "KeySlice": [],
  "ValueCache": [],
  "ValueSlice": [],
  "LiteRTLMCacheLayer": {
    "is_compileable": [],
    "is_sliding": [],
    "__init__": [
      "self",
      "key_cache",
      "value_cache",
      "batch_size",
      "reverse_kv"
    ],
    "get_batch_size": [
      "self"
    ],
    "lazy_initialization": [
      "self",
      "key_states"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "get_seq_length": [
      "self"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "_infer_cache_shape_from_config": [
      "cls",
      "model_config",
      "layer_index",
      "cache_length",
      "batch_size",
      "reverse_kv"
    ],
    "create_from_config": [
      "cls",
      "model_config",
      "layer_index",
      "cache_length",
      "batch_size",
      "reverse_kv"
    ]
  },
  "LiteRTLMCache": {
    "create_from_config": [
      "cls",
      "model_config",
      "cache_length",
      "batch_size",
      "reverse_kv"
    ]
  },
  "_flatten_kvc_t": [
    "kvc"
  ],
  "_unflatten_kvc_t": [
    "values",
    "context"
  ],
  "_flatten_kvc_t_with_keys": [
    "kvc"
  ],
  "verify_model_compatibility": [
    "model",
    "model_config",
    "text_model_config"
  ],
  "load_model": [
    "model_path",
    "trust_remote_code",
    "auto_model_override"
  ],
  "get_prefill_decode_exportable_cls": [
    "export_config"
  ],
  "export_text_prefill_decode_model": [
    "model",
    "text_model_config",
    "export_config",
    "work_dir",
    "quantization_recipe"
  ],
  "maybe_quantize_model": [
    "model_path",
    "quantization_recipe"
  ],
  "export_embedder_model": [
    "model",
    "text_model_config",
    "export_config",
    "work_dir",
    "quantization_recipe"
  ],
  "export_tokenizer": [
    "tokenizer",
    "work_dir"
  ],
  "create_sliding_mask": [
    "segment_pos",
    "cache_len",
    "sliding_window_size",
    "mask_value"
  ],
  "WARNING_MESSAGE": [],
  "ERROR_MESSAGE": [],
  "has_local_rope": [
    "model"
  ],
  "has_sliding_attention": [
    "model"
  ],
  "transposed_attention": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "scaling",
    "softcap"
  ],
  "_use_kernel_forward_from_hub": [
    "layer_name"
  ],
  "original_use_kernel_forward_from_hub": [],
  "_adapt_inputs": [
    "attention_implementation",
    "key",
    "value"
  ],
  "DummyAttentionModule": {
    "__init__": [
      "self",
      "attention_implementation",
      "num_key_value_groups",
      "scaling",
      "softcap"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ]
  },
  "ExportableModuleConfig": {},
  "ExportableModuleBase": {
    "get_sample_inputs": [
      "self",
      "model_config",
      "export_config"
    ]
  },
  "LiteRTExportableModuleForDecoderOnlyLM": {
    "__init__": [
      "self",
      "model"
    ],
    "adapt_inputs": [
      "self",
      "tokens",
      "embeddings",
      "input_pos",
      "kv_cache",
      "mask"
    ],
    "get_sample_kv_cache": [
      "self",
      "model_config",
      "export_config"
    ]
  },
  "LiteRTExportableModuleForDecoderOnlyLMPrefill": {
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "mask"
    ],
    "_get_input": [
      "self",
      "batch_size",
      "prefill_length",
      "prefill_length_dim",
      "model_config"
    ],
    "get_sample_inputs": [
      "self",
      "model_config",
      "export_config"
    ]
  },
  "LiteRTExportableModuleForDecoderOnlyLMGenerate": {
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "mask"
    ],
    "_get_input": [
      "self",
      "batch_size",
      "decode_length",
      "decode_length_dim",
      "model_config"
    ],
    "get_sample_inputs": [
      "self",
      "model_config",
      "export_config"
    ]
  },
  "build_cache_data": [
    "batch_size",
    "num_layers",
    "context_len",
    "head_dim",
    "all_ones"
  ],
  "update_cache": [
    "slices",
    "kv_cache",
    "cache_kwargs"
  ],
  "CacheTest": {
    "assert_cache_equals": [
      "self",
      "kv_cache",
      "expected_kv_cache",
      "num_layers"
    ],
    "assert_cache_slice_equals": [
      "self",
      "slices",
      "kv_cache",
      "num_layers",
      "time_step",
      "input_seq"
    ],
    "test_accessors": [
      "self"
    ],
    "test_update": [
      "self"
    ],
    "test_flatten_round_trip": [
      "self"
    ],
    "test_flatten_with_keys": [
      "self"
    ]
  },
  "LiteRTLMCacheLayerMixin": {
    "get_batch_size": [
      "self"
    ],
    "create_from_config": [
      "cls",
      "model_config",
      "layer_index",
      "cache_length",
      "batch_size"
    ]
  },
  "LiteRTLMCacheMixin": {
    "create_from_config": [
      "cls",
      "model_config",
      "cache_length",
      "batch_size"
    ]
  },
  "register_cache_implementation": [
    "cls"
  ],
  "_PH": [],
  "parse_chat_template": [
    "tokenizer"
  ],
  "build_llm_metadata": [
    "model",
    "tokenizer",
    "chat_templates",
    "context_length"
  ],
  "pack_to_litert_lm": [
    "model",
    "tokenizer",
    "tflite_model_path",
    "tokenizer_model_path",
    "cache_length",
    "work_dir",
    "output_dir",
    "use_jinja_template"
  ],
  "package_model": [
    "model",
    "tokenizer",
    "tflite_model_path",
    "tokenizer_model_path",
    "cache_length",
    "work_dir",
    "output_dir",
    "use_jinja_template"
  ],
  "is_mu_available": [],
  "call_pass": [
    "input_model"
  ],
  "update_model": [
    "input_model_path",
    "output_model_path"
  ],
  "KeyCacheEntry": [],
  "ValueCacheEntry": [],
  "LiteRTLMSplitCacheLayer": {
    "is_compileable": [],
    "is_sliding": [],
    "__init__": [
      "self",
      "key_cache",
      "value_cache",
      "batch_size"
    ],
    "get_batch_size": [
      "self"
    ],
    "lazy_initialization": [
      "self",
      "key_states"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "cache_kwargs"
    ],
    "get_mask_sizes": [
      "self",
      "cache_position"
    ],
    "get_seq_length": [
      "self"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "_infer_cache_shape_from_config": [
      "cls",
      "model_config",
      "layer_index",
      "cache_length",
      "batch_size"
    ],
    "create_from_config": [
      "cls",
      "model_config",
      "layer_index",
      "cache_length",
      "batch_size"
    ]
  },
  "LiteRTLMSplitCache": {
    "create_from_config": [
      "cls",
      "model_config",
      "cache_length",
      "batch_size"
    ]
  },
  "_scaled_dot_product_attention": [
    "query",
    "key_cache",
    "value_cache",
    "head_size",
    "mask",
    "scale",
    "softcap"
  ],
  "split_cache_attention": [
    "module",
    "query",
    "key",
    "value",
    "attention_mask",
    "scaling",
    "softcap"
  ],
  "LiteRTSplitCacheExportableModuleForDecoderOnlyLM": {
    "adapt_inputs": [
      "self",
      "embeddings",
      "pos_emb",
      "mask",
      "kv_cache"
    ],
    "post_process_kv_cache": [
      "self",
      "output_cache"
    ],
    "_get_input": [
      "self",
      "batch_size",
      "input_length",
      "cache_length"
    ]
  },
  "LiteRTSplitCacheExportableModuleForDecoderOnlyLMPrefill": {
    "forward": [
      "self",
      "embeddings",
      "pos_emb",
      "mask",
      "kv_cache"
    ],
    "get_sample_inputs": [
      "self",
      "model_config",
      "export_config"
    ]
  },
  "LiteRTSplitCacheExportableModuleForDecoderOnlyLMGenerate": {
    "forward": [
      "self",
      "embeddings",
      "pos_emb",
      "mask",
      "kv_cache"
    ],
    "get_sample_inputs": [
      "self",
      "model_config",
      "export_config"
    ]
  },
  "SplitAttentionMaskBuilder": {
    "__init__": [
      "self",
      "context_size",
      "sliding_window_sizes",
      "pad_token"
    ],
    "forward": [
      "self",
      "input_tokens",
      "time_step"
    ],
    "get_sample_inputs": [
      "cls",
      "model_config",
      "export_config"
    ]
  },
  "CacheUpdate": {
    "forward": [
      "self",
      "kv_slice",
      "kv_cache",
      "input_pos"
    ],
    "_get_input": [
      "cls",
      "model_config",
      "input_length",
      "cache_length",
      "batch_size"
    ],
    "get_sample_inputs": [
      "cls",
      "model_config",
      "export_config"
    ]
  },
  "Mask": {
    "_create_mask": [
      "cls",
      "mask_len",
      "kv_cache_max_len"
    ]
  },
  "SplitMask": {
    "_create_mask": [
      "cls",
      "mask_len",
      "kv_cache_max_len"
    ]
  },
  "_MaskPyTreeContext": [],
  "_flatten_mask": [
    "mask"
  ],
  "_unflatten_mask": [
    "values",
    "context"
  ],
  "_flatten_mask_with_keys": [
    "mask"
  ],
  "generate_causal_right": [
    "input_tokens",
    "W",
    "pad_token"
  ],
  "generate_causal_left": [
    "input_tokens",
    "W",
    "S",
    "time_step"
  ],
  "build_full_mask": [
    "input_tokens",
    "W",
    "S",
    "time_step",
    "pad_token"
  ],
  "SplitAttentionMask": {
    "__init__": [
      "self",
      "context_size",
      "sliding_window_size",
      "pad_token"
    ],
    "forward": [
      "self",
      "input_tokens",
      "time_step"
    ]
  },
  "RotaryPositionEmbeddingInjector": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "inject_rotary_position_embedding": [
    "model"
  ],
  "RoPEEmbedder": {
    "__init__": [
      "self",
      "model"
    ],
    "forward": [
      "self",
      "input_pos"
    ],
    "get_sample_inputs": [
      "cls",
      "model_config",
      "export_config"
    ]
  },
  "LiteRTExportableModuleForDecoderOnlyLMPrefillExternalEmbedder": {
    "forward": [
      "self",
      "embeddings",
      "input_pos",
      "kv_cache",
      "mask"
    ],
    "_get_input": [
      "self",
      "batch_size",
      "prefill_length",
      "prefill_length_dim",
      "model_config"
    ]
  },
  "LiteRTExportableModuleForDecoderOnlyLMGenerateExternalEmbedder": {
    "forward": [
      "self",
      "embeddings",
      "input_pos",
      "kv_cache",
      "mask"
    ],
    "_get_input": [
      "self",
      "batch_size",
      "decode_length",
      "decode_length_dim",
      "model_config"
    ]
  },
  "LiteRTExportableModuleForEmbedder": {
    "__init__": [
      "self",
      "model"
    ],
    "forward": [
      "self",
      "token_ids"
    ]
  },
  "RemoveSDPACompositeZeroMaskPass": {
    "is_zero_tensor_node": [
      "self",
      "node"
    ],
    "call": [
      "self",
      "exported_program"
    ]
  },
  "run_generative_passes": [
    "exported_program"
  ],
  "OpenELM": {},
  "get_model_config": [],
  "get_fake_model_config": [],
  "build_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "_PROMPTS": [],
  "_MAX_NEW_TOKENS": [],
  "DEFAULT_PROMPTS": [],
  "verify_openelm": [
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "flags": [],
  "_CHECKPOINT": [],
  "_VARIANT": [],
  "_WEIGHT_FILENAME": [],
  "find_first_ckpt": [
    "folder"
  ],
  "PROJECTION_TENSOR_NAME": [],
  "Gemma3MMConfig": {},
  "Gemma3MM": {
    "__init__": [
      "self",
      "config",
      "mask_cache_size"
    ],
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "image_indices",
      "image_feat_indices",
      "pixel_values",
      "export_config"
    ]
  },
  "build_model_1b": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "build_model_270m": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "TENSOR_NAMES_SEP_QKV": [],
  "TENSOR_NAMES_FUSED_QKV": [],
  "TENSOR_NAMES_DICT": [],
  "DecoderBlock": {
    "forward": [
      "self",
      "x",
      "rope",
      "mask",
      "input_pos",
      "kv_cache"
    ]
  },
  "Decoder": {
    "__init__": [
      "self",
      "config",
      "mask_cache_size"
    ],
    "build_mask_cache": [
      "self",
      "mask_cache_size"
    ],
    "get_local_global_attention_mask": [
      "self",
      "attention_mask",
      "attn_type",
      "segment_pos",
      "sliding_window_size"
    ],
    "create_sliding_mask": [
      "self",
      "segment_pos",
      "cache_len",
      "sliding_window_size"
    ],
    "compose_mask": [
      "self",
      "mask",
      "pixel_mask",
      "attn_type"
    ],
    "build_pixel_mask": [
      "self",
      "image_indices",
      "max_seq_len"
    ],
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "input_embeds",
      "mask",
      "image_indices",
      "export_config"
    ],
    "_forward_with_embeds": [
      "self",
      "input_embeds",
      "rope",
      "mask",
      "input_pos",
      "kv_cache",
      "pixel_mask",
      "export_config"
    ]
  },
  "get_decoder_config_1b": [],
  "get_decoder_config_270m": [],
  "get_fake_decoder_config_1b": [],
  "_MODEL_SIZE": [],
  "_get_actual_input_len": [
    "tokens"
  ],
  "GemmaWrapper": {
    "_get_kv_caches": [
      "self",
      "max_seq_len"
    ],
    "forward": [
      "self",
      "tokens"
    ],
    "generate": [
      "self",
      "tokens",
      "max_new_tokens"
    ]
  },
  "UnifiedGemma3Wrapper": {
    "__init__": [
      "self",
      "model",
      "kv_cache_max_len"
    ],
    "_init_kv_cache": [
      "self"
    ],
    "forward": [
      "self",
      "tokens",
      "pixel_values"
    ],
    "generate": [
      "self",
      "prompts",
      "max_new_tokens",
      "pixel_values",
      "eos_token_id"
    ]
  },
  "GemmaTokenizerWrapper": {
    "encode": [
      "self",
      "text"
    ],
    "decode": [
      "self",
      "tokens"
    ]
  },
  "verify_reauthored_gemma_model": [
    "checkpoint",
    "variant",
    "reauthored_model",
    "generate_prompts",
    "forward_input_ids",
    "weight_filename",
    "custom_loader",
    "tokenizer_filename",
    "max_new_tokens",
    "rtol",
    "atol"
  ],
  "verify_gemma3": [
    "checkpoint",
    "prompts",
    "max_new_tokens",
    "variant",
    "weight_filename",
    "custom_loader"
  ],
  "verify_gemma3_with_custom_loader": [
    "checkpoint"
  ],
  "SiglipExit": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "SiglipVisionEncoderWithExit": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "get_image_encoder_config": [],
  "get_fake_image_encoder_config": [],
  "build_image_encoder": [
    "checkpoint_path"
  ],
  "QwenVLConfig": {},
  "QwenVL": {
    "__init__": [
      "self",
      "config",
      "mask_cache_size"
    ],
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "mask",
      "pixel_values",
      "export_config"
    ],
    "_build_rope": [
      "self",
      "rope_pos"
    ],
    "_build_text_rope": [
      "self",
      "input_pos"
    ],
    "_build_multimodal_rope": [
      "self",
      "input_pos",
      "grid_thw"
    ],
    "_build_image_text_rope": [
      "self",
      "image_pos",
      "text_pos"
    ],
    "_merge_ropes": [
      "self",
      "a",
      "b",
      "c"
    ]
  },
  "get_decoder_config": [],
  "get_fake_decoder_config": [],
  "build_decoder": [
    "checkpoint_path",
    "mask_cache_size"
  ],
  "_IMAGE_URL": [],
  "_PROMPTS_WITH_IMAGE": [],
  "_PROMPTS_TEXT_ONLY": [],
  "ReauthoredQwenVLWrapper": {
    "_init_kv_cache": [
      "self"
    ]
  },
  "DecoderWrapper": {
    "__init__": [
      "self",
      "model",
      "lm_head"
    ],
    "forward": [
      "self",
      "tokens"
    ]
  },
  "_IMAGE_HEIGHT": [],
  "_IMAGE_WIDTH": [],
  "MERGER_TENSOR_NAMES": [],
  "QwenVLMergerConfig": {},
  "QwenVLImageConfig": {},
  "QwenVLMerger": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "QwenVLImageEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ],
    "set_image_size": [
      "self",
      "image_size"
    ],
    "get_grid_thw": [
      "self",
      "num_images"
    ],
    "get_pixel_values_size": [
      "self",
      "grid_thw"
    ],
    "_get_rope": [
      "self",
      "grid_thw",
      "window_index"
    ],
    "_get_window_index": [
      "self",
      "grid_thw"
    ],
    "_rearrange": [
      "self",
      "x",
      "window_index"
    ],
    "_get_mask": [
      "self",
      "grid_thw",
      "cu_seqlens"
    ]
  },
  "load_image_encoder": [
    "checkpoint_path",
    "encoder",
    "custom_loader"
  ],
  "verify_amd_llama_135m": [
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "AmdLlama": {},
  "_MODEL_SAFETENSORS": [],
  "_DENSE1_DIR": [],
  "_DENSE2_DIR": [],
  "_HF_EMBED_TOKENS": [],
  "_HF_NORM": [],
  "_HF_LAYERS_PREFIX": [],
  "_HF_INPUT_LAYERNORM": [],
  "_HF_POST_ATTENTION_LAYERNORM": [],
  "_HF_PRE_FF_LAYERNORM": [],
  "_HF_POST_FF_LAYERNORM": [],
  "_HF_ATTN_Q_PROJ": [],
  "_HF_ATTN_K_PROJ": [],
  "_HF_ATTN_V_PROJ": [],
  "_HF_ATTN_O_PROJ": [],
  "_HF_ATTN_Q_NORM": [],
  "_HF_ATTN_K_NORM": [],
  "_HF_MLP_GATE_PROJ": [],
  "_HF_MLP_UP_PROJ": [],
  "_HF_MLP_DOWN_PROJ": [],
  "_ST_LINEAR_WEIGHT": [],
  "_AIET_EMBEDDER": [],
  "_AIET_FINAL_NORM": [],
  "_AIET_BLOCKS_PREFIX": [],
  "_AIET_PRE_ATTEN_NORM": [],
  "_AIET_POST_ATTEN_NORM": [],
  "_AIET_FF_PRE_FF_NORM": [],
  "_AIET_FF_POST_FF_NORM": [],
  "_AIET_ATTN_QKV_PROJ": [],
  "_AIET_ATTN_OUTPUT_PROJ": [],
  "_AIET_ATTN_Q_NORM": [],
  "_AIET_ATTN_K_NORM": [],
  "_AIET_MLP_W1": [],
  "_AIET_MLP_W2": [],
  "_AIET_MLP_W3": [],
  "_AIET_DENSE1": [],
  "_AIET_DENSE2": [],
  "EncoderBlock": {
    "forward": [
      "self",
      "x",
      "rope",
      "mask"
    ]
  },
  "EmbeddingGemma": {
    "__init__": [
      "self",
      "config"
    ],
    "create_sliding_mask": [
      "self",
      "segment_pos",
      "sequence_length",
      "sliding_window_size"
    ],
    "mean_pool": [
      "self",
      "hidden_states",
      "attention_mask"
    ],
    "forward": [
      "self",
      "tokens",
      "attention_mask"
    ]
  },
  "_LONG_INPUT_PROMPT_PATH": [],
  "_MODEL_PATH": [],
  "verify_embedding_gemma_300m": [
    "checkpoint_dir",
    "prompts",
    "long_input_prompt_path",
    "atol"
  ],
  "FLAGS": [],
  "BATCH_SIZE": [],
  "EncoderDecoderBlock": {
    "__init__": [
      "self",
      "config",
      "model_config",
      "has_relative_attention_bias"
    ],
    "forward": [
      "self",
      "x",
      "input_pos",
      "mask",
      "relative_position",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "encoder_decoder_position_bias"
    ]
  },
  "T5Attention": {
    "__init__": [
      "self",
      "batch",
      "dim",
      "config",
      "norm_config",
      "kv_cache_max",
      "enable_hlfb",
      "has_relative_attention_bias"
    ],
    "forward": [
      "self",
      "x",
      "input_pos",
      "key_value_states",
      "mask",
      "relative_position",
      "position_bias"
    ]
  },
  "convert_t5_to_tflite_singlesig": [
    "checkpoint_path"
  ],
  "convert_t5_to_tflite_multisig": [
    "checkpoint_path"
  ],
  "ENCDEC_TENSOR_NAMES": [],
  "T5Stack": {
    "__init__": [
      "self",
      "config",
      "embed_tokens"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_pos",
      "attention_mask",
      "relative_position",
      "encoder_hidden_states",
      "encoder_attention_mask"
    ]
  },
  "T5": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_pos",
      "decoder_input_ids",
      "decoder_input_pos",
      "pad_mask"
    ]
  },
  "T5Encoder": {
    "__init__": [
      "self",
      "config",
      "embedding_layer"
    ],
    "forward": [
      "self",
      "input_ids",
      "input_pos",
      "pad_mask"
    ]
  },
  "T5Decoder": {
    "__init__": [
      "self",
      "config",
      "embedding_layer"
    ],
    "forward": [
      "self",
      "encoder_hidden_states",
      "decoder_input_ids",
      "decoder_input_pos",
      "pad_mask"
    ]
  },
  "get_model_config_t5": [],
  "build_t5_model": [
    "checkpoint_path"
  ],
  "build_t5_encoder_model": [
    "config",
    "embedding_layer",
    "checkpoint_path"
  ],
  "build_t5_decoder_model": [
    "config",
    "embedding_layer",
    "checkpoint_path"
  ],
  "get_sample_encoder_input_ids": [],
  "define_and_run_t5": [
    "checkpoint_path"
  ],
  "define_and_run_t5_split": [
    "checkpoint_path"
  ],
  "verify_tiny_llama": [
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "TinyLlama": {},
  "_BUILDER": [],
  "Qwen": {},
  "get_3b_model_config": [],
  "get_1_5b_model_config": [],
  "get_0_5b_model_config": [],
  "_build_model": [
    "checkpoint_path",
    "config",
    "custom_loader",
    "mask_cache_size"
  ],
  "build_3b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "build_1_5b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "build_0_5b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "_BUILDER_V2": [],
  "_BUILDER_V3": [],
  "verify_qwen": [
    "model_size",
    "model_version",
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "Qwen3": {},
  "get_4b_model_config": [],
  "get_1_7b_model_config": [],
  "get_0_6b_model_config": [],
  "build_4b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "build_1_7b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "build_0_6b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "Gemma2Block": {
    "forward": [
      "self",
      "x",
      "rope",
      "mask",
      "input_pos",
      "kv_cache"
    ]
  },
  "Gemma2": {
    "__init__": [
      "self",
      "config",
      "mask_cache_size"
    ],
    "build_mask_cache": [
      "self",
      "mask_cache_size"
    ],
    "get_attention_mask": [
      "self",
      "attn_type",
      "input_pos"
    ],
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "mask",
      "export_config"
    ],
    "_forward_with_embeds": [
      "self",
      "input_embeds",
      "rope",
      "mask",
      "input_pos",
      "kv_cache",
      "export_config"
    ]
  },
  "get_model_config_2b": [],
  "build_2b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "verify_gemma2": [
    "checkpoint_dir",
    "weight_filename",
    "prompts",
    "max_new_tokens",
    "mask_as_input",
    "kv_layout",
    "custom_loader"
  ],
  "verify_gemma1_with_custom_loader": [
    "checkpoint_dir"
  ],
  "verify_gemma2_with_custom_loader": [
    "checkpoint_dir"
  ],
  "Gemma1": {},
  "_MASK_AS_INPUT": [],
  "_TRANSPOSE_KV_CACHE": [],
  "DeepSeekDistillQwen": {},
  "verify_deepseek_r1_distill_1_5b": [
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "_CHECKPOINT_PATH": [],
  "_TFLITE_PATH": [],
  "_MODEL_VERSION": [],
  "verify_smollm_135m": [
    "model_version",
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "SmolLM": {},
  "SmolLM2": {},
  "get_model_config_v2": [],
  "get_fake_model_config_v2": [],
  "build_model_v2": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "Phi2": {},
  "ROPE_SCALE_FACTOR": [],
  "ROPE_SHORT_FACTOR": [],
  "_build_phi4_rope": [
    "input_pos",
    "n_elem",
    "base",
    "condense_ratio",
    "dtype",
    "device",
    "theta_factors",
    "scale"
  ],
  "Phi4Mini": {},
  "_build_phi3_rope": [
    "input_pos",
    "n_elem",
    "base",
    "condense_ratio",
    "dtype",
    "device",
    "theta_factors",
    "scale"
  ],
  "Phi3_5Mini": {},
  "verify_phi": [
    "version",
    "checkpoint_dir",
    "max_new_tokens",
    "prompts",
    "atol",
    "initialize_from_local",
    "custom_loader"
  ],
  "verify_falcon_1b": [
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "Falcon": {},
  "PaliGemmaConfig": {},
  "PaliGemma": {
    "__init__": [
      "self",
      "config",
      "decoder_class",
      "mask_cache_size"
    ],
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "mask",
      "pixel_values",
      "export_config"
    ]
  },
  "_VERSION": [],
  "ReauthoredPaliGemmaWrapper": {
    "_init_kv_cache": [
      "self"
    ]
  },
  "Decoder2": {
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "input_embeds",
      "mask",
      "export_config"
    ]
  },
  "get_decoder2_config": [],
  "get_fake_decoder2_config": [],
  "build_decoder2": [
    "checkpoint_path",
    "mask_cache_size"
  ],
  "SiglipVisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "pixel_values"
    ]
  },
  "Hammer": {},
  "verify_hammer": [
    "model_size",
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "initialize_from_local",
    "prompts"
  ],
  "verify_llama_3_2": [
    "model_size",
    "checkpoint_dir",
    "weight_filename",
    "max_new_tokens",
    "prompts",
    "initialize_from_local",
    "custom_loader"
  ],
  "_build_llama3_rope_cache": [
    "input_pos",
    "n_elem",
    "base",
    "condense_ratio",
    "dtype",
    "device",
    "factor",
    "low_freq_factor",
    "high_freq_factor",
    "max_seq_len"
  ],
  "Llama": {},
  "get_1b_model_config": [],
  "build_1b_model": [
    "checkpoint_path",
    "custom_loader",
    "mask_cache_size"
  ],
  "create_bytes_table": [],
  "pairwise": [
    "seq"
  ],
  "Tokenizer": {
    "__init__": [
      "self",
      "vocab_dir"
    ],
    "encode": [
      "self",
      "text"
    ],
    "encode_batch": [
      "self",
      "texts"
    ],
    "bpe": [
      "self",
      "chunk"
    ]
  },
  "triu": [
    "a"
  ],
  "CLIP": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "tokens"
    ]
  },
  "arg_parser": [],
  "StableDiffusion": {
    "__init__": [
      "self"
    ]
  },
  "run_tflite_pipeline": [
    "model",
    "prompt",
    "output_path",
    "uncond_prompt",
    "cfg_scale",
    "height",
    "width",
    "sampler",
    "n_inference_steps",
    "seed",
    "strength",
    "input_image"
  ],
  "get_time_embedding": [
    "timestep"
  ],
  "get_alphas_cumprod": [
    "beta_start",
    "beta_end",
    "n_training_steps"
  ],
  "get_file_path": [
    "filename",
    "url"
  ],
  "move_channel": [
    "image",
    "to"
  ],
  "rescale": [
    "x",
    "old_range",
    "new_range",
    "clamp"
  ],
  "AttentionBlock": {
    "__init__": [
      "self",
      "channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ResidualBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Encoder": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x",
      "noise"
    ]
  },
  "_down_encoder_blocks_tensor_names": [],
  "_mid_block_tensor_names": [],
  "_up_decoder_blocks_tensor_names": [],
  "TimeEmbedding": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Diffusion": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "latents",
      "context",
      "time_emb"
    ]
  },
  "_CLIP_CKPT": [],
  "_DIFFUSION_CKPT": [],
  "_DECODER_CKPT": [],
  "_OUTPUT_DIR": [],
  "_QUANTIZE": [],
  "_DEVICE_TYPE": [],
  "convert_stable_diffusion_to_tflite": [
    "output_dir",
    "clip_ckpt_path",
    "diffusion_ckpt_path",
    "decoder_ckpt_path",
    "image_height",
    "image_width",
    "quantize"
  ],
  "KEulerAncestralSampler": {
    "__init__": [
      "self",
      "n_inference_steps",
      "n_training_steps"
    ],
    "get_input_scale": [
      "self",
      "step_count"
    ],
    "set_strength": [
      "self",
      "strength"
    ],
    "step": [
      "self",
      "latents",
      "output"
    ]
  },
  "KEulerSampler": {
    "__init__": [
      "self",
      "n_inference_steps",
      "n_training_steps"
    ],
    "get_input_scale": [
      "self",
      "step_count"
    ],
    "set_strength": [
      "self",
      "strength"
    ],
    "step": [
      "self",
      "latents",
      "output"
    ]
  },
  "KLMSSampler": {
    "__init__": [
      "self",
      "n_inference_steps",
      "n_training_steps",
      "lms_order"
    ],
    "get_input_scale": [
      "self",
      "step_count"
    ],
    "set_strength": [
      "self",
      "strength"
    ],
    "step": [
      "self",
      "latents",
      "output"
    ]
  },
  "SamplerInterface": {
    "get_input_scale": [
      "self",
      "step_count"
    ],
    "set_strength": [
      "self",
      "strength"
    ],
    "step": [
      "self",
      "latents",
      "output"
    ]
  },
  "RoPECache": [],
  "ToyModelWithKVCache": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "tokens",
      "input_pos",
      "kv_cache",
      "mask",
      "export_config"
    ]
  },
  "get_sample_prefill_inputs": [],
  "get_sample_decode_inputs": [],
  "KV_CACHE_MAX_LEN": [],
  "ToySingleLayerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "idx",
      "input_pos",
      "mask"
    ]
  },
  "ToySingleLayerModelWeightSharing": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "idx",
      "input_pos",
      "mask"
    ]
  },
  "convert_toy_model": [
    "_"
  ],
  "_export_stablehlo_mlir": [
    "model",
    "args"
  ],
  "convert_toy_model_with_kv_cache": [
    "_"
  ],
  "_bytes_to_unicode": [],
  "_BYTE_DECODE_MAP": [],
  "_normalize_gpt2": [
    "token"
  ],
  "_NORMALIZE_FUNCS": [],
  "_add_token": [
    "token",
    "id_",
    "tokenizer",
    "sp_model",
    "tokens_seen",
    "counts",
    "normalize_tokens"
  ],
  "_build_spm_model_from_tokenizer": [
    "tokenizer",
    "normalize_tokens"
  ],
  "_is_same_ids": [
    "ids_by_tokenizer",
    "ids"
  ],
  "_log_not_matched": [
    "num_not_matched_strict",
    "num_not_matched_loose",
    "total"
  ],
  "_encode_by_spm": [
    "spm_tokenizer",
    "string"
  ],
  "verify_spm_tokenizer": [
    "tokenizer",
    "spm_tokenizer",
    "strings_to_verify",
    "num_pairs_to_verify"
  ],
  "_OUTPUT_PATH": [],
  "_STRINGS_TO_VERIFY": [],
  "_NORMALIZE_TOKENS": [],
  "_NUM_PAIRS_TO_VERIFY": [],
  "_CHECKPOINT_ROOT_PATH": [],
  "_MODELS": [],
  "_PREFILL_SEQ_LENS": [],
  "_KV_CACHE_MAX_LEN": [],
  "_PRECISIONS": [],
  "ExportPrecision": {
    "INT8": [],
    "FP32": []
  },
  "ConversionConfig": {
    "print_config": [
      "self"
    ]
  },
  "get_conversion_config": [
    "model_name",
    "input_checkpoint_subdir",
    "tflite_output_subdir",
    "model_builder",
    "model_size"
  ],
  "prepare_conversion_configs": [],
  "get_output_filename": [
    "model_name",
    "model_size",
    "precision",
    "kv_cache_max_len"
  ],
  "convert_models": [
    "conversion_configs"
  ],
  "_DepthwiseConv2D": [],
  "SelfieSegmentation": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "image"
    ],
    "load_from_pth": [
      "self",
      "pth_path"
    ]
  }
}