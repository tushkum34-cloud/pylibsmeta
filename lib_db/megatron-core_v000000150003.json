{
  "MAJOR": [],
  "MINOR": [],
  "PATCH": [],
  "PRE_RELEASE": [],
  "VERSION": [],
  "__shortversion__": [],
  "__version__": [],
  "__package_name__": [],
  "__contact_names__": [],
  "__contact_emails__": [],
  "__homepage__": [],
  "__repository_url__": [],
  "__download_url__": [],
  "__description__": [],
  "__license__": [],
  "__keywords__": [],
  "HAVE_TE": [],
  "HAVE_TE_FP8_TENSOR_CLASS": [],
  "is_float8tensor": [
    "tensor"
  ],
  "is_mxfp8tensor": [
    "tensor"
  ],
  "dequantize_fp8_tensor": [
    "fp8_tensor"
  ],
  "get_fp8_align_size": [
    "fp8_recipe"
  ],
  "modify_underlying_storage": [
    "tensor",
    "new_raw_data"
  ],
  "quantize_param_shard": [
    "model_params",
    "main_params",
    "start_offsets",
    "data_parallel_group",
    "fsdp_shard_model_params"
  ],
  "correct_amax_history_if_needed": [
    "model"
  ],
  "is_first_last_bf16_layer": [
    "config",
    "layer_no"
  ],
  "logger": [],
  "copy_tensors_in_struct": [
    "src"
  ],
  "clone_tensors_in_struct": [
    "tgt",
    "src"
  ],
  "StaticBufferLoader": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs",
      "stage",
      "microbatch"
    ]
  },
  "FullCudaGraphWrapper": {
    "curr_iteration": [],
    "cuda_graph": [],
    "result": [],
    "__init__": [
      "self",
      "forward_backward_func",
      "cuda_graph_warmup_steps"
    ],
    "data_read": [
      "self",
      "data_iterator",
      "model",
      "training",
      "num_microbatches"
    ],
    "__call__": [
      "self"
    ],
    "curr_iter": [
      "self",
      "stage"
    ],
    "next_iter": [
      "self",
      "stage"
    ]
  },
  "_allocator": [],
  "_build_nccl_allocator": [],
  "get_func_args": [
    "func"
  ],
  "create_nccl_mem_pool": [
    "symmetric"
  ],
  "init": [],
  "nccl_mem": {
    "__init__": [
      "self",
      "pool",
      "enabled",
      "device",
      "group"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "MultiGroupMemPoolAllocator": {
    "__init__": [
      "self",
      "pool",
      "groups"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "OptimizerParamScheduler": {
    "__init__": [
      "self",
      "optimizer",
      "init_lr",
      "max_lr",
      "min_lr",
      "lr_warmup_steps",
      "lr_decay_steps",
      "lr_decay_style",
      "start_wd",
      "end_wd",
      "wd_incr_steps",
      "wd_incr_style",
      "use_checkpoint_opt_param_scheduler",
      "override_opt_param_scheduler",
      "wsd_decay_steps",
      "lr_wsd_decay_style"
    ],
    "get_wd": [
      "self"
    ],
    "get_lr": [
      "self",
      "param_group"
    ],
    "step": [
      "self",
      "increment"
    ],
    "state_dict": [
      "self"
    ],
    "_check_and_set": [
      "self",
      "cls_value",
      "sd_value",
      "name"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "PackedSeqParams": {},
  "squared_relu": [
    "x"
  ],
  "quick_gelu": [
    "x"
  ],
  "fast_gelu": [
    "x"
  ],
  "jit_fuser": [],
  "HAVE_TE_FP4_TENSOR_CLASS": [],
  "is_nvfp4tensor": [
    "tensor"
  ],
  "dequantize_fp4_tensor": [
    "fp4_tensor"
  ],
  "_te_version": [],
  "_fa_version": [],
  "null_decorator": [],
  "ExperimentalNotEnabledError": {},
  "experimental_fn": [
    "introduced_with_version"
  ],
  "experimental_cls": [
    "introduced_with_version"
  ],
  "get_torch_version": [],
  "get_te_version": [],
  "is_te_min_version": [
    "version",
    "check_equality"
  ],
  "is_torch_min_version": [
    "version",
    "check_equality"
  ],
  "get_fa_version": [],
  "is_fa_min_version": [
    "version",
    "check_equality"
  ],
  "ensure_divisibility": [
    "numerator",
    "denominator"
  ],
  "divide": [
    "numerator",
    "denominator"
  ],
  "deprecate_inference_params": [
    "inference_context",
    "inference_params"
  ],
  "get_tensor_model_parallel_group_if_none": [
    "tp_group",
    "is_expert",
    "check_initialized"
  ],
  "get_pg_size": [
    "group"
  ],
  "get_pg_rank": [
    "group"
  ],
  "get_attr_wrapped_model": [
    "model",
    "attr",
    "allow_none",
    "return_model_obj"
  ],
  "get_model_type": [
    "model"
  ],
  "get_model_xattn": [
    "model"
  ],
  "get_model_config": [
    "model"
  ],
  "GlobalMemoryBuffer": {
    "__init__": [
      "self"
    ],
    "get_tensor": [
      "self",
      "tensor_shape",
      "dtype",
      "name",
      "mem_alloc_context"
    ]
  },
  "_kernel_make_viewless_tensor": [
    "inp",
    "requires_grad"
  ],
  "WrappedTensor": {
    "__init__": [
      "self",
      "tensor"
    ],
    "unwrap": [
      "self"
    ]
  },
  "MakeViewlessTensor": {
    "forward": [
      "ctx",
      "inp",
      "requires_grad"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "make_viewless_tensor": [
    "inp",
    "requires_grad",
    "keep_graph"
  ],
  "assert_viewless_tensor": [
    "tensor",
    "extra_msg"
  ],
  "safely_set_viewless_tensor_data": [
    "tensor",
    "new_data_tensor"
  ],
  "init_method_normal": [
    "sigma"
  ],
  "scaled_init_method_normal": [
    "sigma",
    "num_layers",
    "multiplier"
  ],
  "log_single_rank": [
    "logger"
  ],
  "log_on_each_pipeline_stage": [
    "logger"
  ],
  "check_param_hashes_across_dp_replicas": [
    "model",
    "cross_check"
  ],
  "make_tp_sharded_tensor_for_checkpoint": [
    "tensor",
    "key",
    "tp_axis",
    "replica_id",
    "prepend_offsets"
  ],
  "make_sharded_tensor_for_checkpoint": [
    "tensor",
    "key",
    "prepend_offsets",
    "replica_id"
  ],
  "get_full_tensor_if_necessary": [
    "tensor"
  ],
  "to_local_if_dtensor": [
    "tensor"
  ],
  "get_data_parallel_group_if_dtensor": [
    "tensor",
    "data_parallel_group"
  ],
  "prepare_input_tensors_for_wgrad_compute": [
    "grad_output",
    "all_gathered_input"
  ],
  "drain_embedding_wgrad_compute": [
    "config",
    "embedding_activation_buffer",
    "grad_output_buffer",
    "weight",
    "tp_group"
  ],
  "local_multi_tensor_applier": [
    "op",
    "noop_flag_buffer",
    "tensor_lists"
  ],
  "local_multi_tensor_l2_norm": [
    "chunk_size",
    "noop_flag",
    "tensor_lists",
    "per_tensor"
  ],
  "local_multi_tensor_scale": [
    "chunk_size",
    "noop_flag",
    "tensor_lists",
    "scale"
  ],
  "_ValueWithRank": {
    "__init__": [
      "self",
      "value",
      "rank",
      "unit"
    ],
    "__lt__": [
      "self",
      "other"
    ],
    "__gt__": [
      "self",
      "other"
    ],
    "__call__": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "_StragglerData": {
    "min_elapsed": [],
    "max_elapsed": [],
    "min_btime": [],
    "max_btime": [],
    "min_temp": [],
    "max_temp": [],
    "min_power": [],
    "max_power": [],
    "min_util": [],
    "max_util": [],
    "min_clock": [],
    "max_clock": []
  },
  "StragglerDetector": {
    "_configured": [],
    "__new__": [
      "cls"
    ],
    "__init__": [
      "self"
    ],
    "configure": [
      "self",
      "world",
      "rank",
      "mmcnt",
      "amp",
      "port",
      "prefill",
      "enabled"
    ],
    "reset": [
      "self"
    ],
    "start_method": [
      "self"
    ],
    "stop_method": [
      "self"
    ],
    "elapsed": [
      "self"
    ],
    "report": [
      "self",
      "total_flops",
      "log_interval"
    ],
    "_check_toggle": [
      "self"
    ],
    "_handler": [
      "self"
    ],
    "_controller": [
      "self"
    ],
    "_min_max": [
      "self",
      "ptime",
      "btime",
      "temp",
      "power",
      "util",
      "clock",
      "flops"
    ],
    "enabled": [
      "self"
    ],
    "configured": [
      "self"
    ],
    "my_rank": [
      "self"
    ],
    "world_size": [
      "self"
    ],
    "null_method": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__call__": [
      "self",
      "bdata"
    ],
    "__exit__": [
      "self",
      "ex_type",
      "ex_val",
      "ex_tb"
    ]
  },
  "__straggler__": [],
  "is_submodule": [
    "module",
    "parent_module",
    "strict"
  ],
  "get_batch_on_this_cp_rank": [
    "batch"
  ],
  "configure_nvtx_profiling": [
    "enabled"
  ],
  "_nvtx_range_get_func_path": [],
  "nvtx_range_push": [
    "msg",
    "suffix"
  ],
  "nvtx_range_pop": [
    "msg",
    "suffix"
  ],
  "_nvtx_decorator_get_func_path": [
    "func"
  ],
  "nvtx_decorator": [
    "message",
    "color"
  ],
  "unwrap_model": [
    "model",
    "module_instances"
  ],
  "get_asyncio_loop": [],
  "mpu": [],
  "__all__": [],
  "_FeatureFlag": {
    "__init__": [
      "self",
      "default"
    ],
    "enable": [
      "self"
    ],
    "disable": [
      "self"
    ],
    "is_enabled": [
      "self"
    ],
    "import_package": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ]
  },
  "MultiStorageClientFeature": [],
  "open_file": [],
  "ENABLE_EXPERIMENTAL": [],
  "set_experimental_flag": [
    "flag"
  ],
  "is_experimental_enabled": [],
  "EnergyMonitor": {
    "__init__": [
      "self"
    ],
    "setup": [
      "self"
    ],
    "shutdown": [
      "self"
    ],
    "pause": [
      "self"
    ],
    "resume": [
      "self"
    ],
    "_get_energy": [
      "self"
    ],
    "lap": [
      "self"
    ],
    "get_total": [
      "self"
    ]
  },
  "_TENSOR_MODEL_PARALLEL_GROUP": [],
  "_PIPELINE_MODEL_PARALLEL_GROUP": [],
  "_MODEL_PARALLEL_GROUP": [],
  "_EMBEDDING_GROUP": [],
  "_POSITION_EMBEDDING_GROUP": [],
  "_DATA_PARALLEL_GROUP": [],
  "_DATA_PARALLEL_GROUP_GLOO": [],
  "_TENSOR_AND_DATA_PARALLEL_GROUP": [],
  "_EXPERT_MODEL_PARALLEL_GROUP": [],
  "_EXPERT_TENSOR_PARALLEL_GROUP": [],
  "_EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP": [],
  "_EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP": [],
  "_EXPERT_DATA_PARALLEL_GROUP": [],
  "_EXPERT_DATA_PARALLEL_GROUP_GLOO": [],
  "_INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP": [],
  "_INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO": [],
  "_INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP": [],
  "_MPU_EXPERT_MODEL_PARALLEL_WORLD_SIZE": [],
  "_MPU_EXPERT_MODEL_PARALLEL_RANK": [],
  "_MPU_EXPERT_TENSOR_PARALLEL_WORLD_SIZE": [],
  "_MPU_EXPERT_TENSOR_PARALLEL_RANK": [],
  "_VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK": [],
  "_VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE": [],
  "_MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE": [],
  "_MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE": [],
  "_MPU_DATA_PARALLEL_WORLD_SIZE": [],
  "_MPU_DATA_PARALLEL_RANK": [],
  "_MPU_TENSOR_MODEL_PARALLEL_RANK": [],
  "_MPU_PIPELINE_MODEL_PARALLEL_RANK": [],
  "_EMBEDDING_GLOBAL_RANKS": [],
  "_POSITION_EMBEDDING_GLOBAL_RANKS": [],
  "_PIPELINE_GLOBAL_RANKS": [],
  "_DATA_PARALLEL_GLOBAL_RANKS": [],
  "_TENSOR_MODEL_PARALLEL_GLOBAL_RANKS": [],
  "_MODEL_PARALLEL_GLOBAL_RANKS": [],
  "_CONTEXT_PARALLEL_GROUP": [],
  "_CONTEXT_PARALLEL_GLOBAL_RANKS": [],
  "_HIERARCHICAL_CONTEXT_PARALLEL_GROUPS": [],
  "_DATA_PARALLEL_GROUP_WITH_CP": [],
  "_DATA_PARALLEL_GROUP_WITH_CP_GLOO": [],
  "_DATA_PARALLEL_GLOBAL_RANKS_WITH_CP": [],
  "_INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP": [],
  "_INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO": [],
  "_TENSOR_AND_CONTEXT_PARALLEL_GROUP": [],
  "_TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP": [],
  "_INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP": [],
  "_GLOBAL_MEMORY_BUFFER": [],
  "_global_process_group_list": [],
  "get_nccl_options": [
    "pg_name",
    "nccl_comm_cfgs"
  ],
  "update_pg_timeout": [
    "timeout",
    "pg"
  ],
  "create_group": [
    "ranks",
    "timeout",
    "backend",
    "pg_options",
    "use_local_synchronization",
    "group_desc"
  ],
  "generate_masked_orthogonal_rank_groups": [
    "world_size",
    "parallel_size",
    "mask"
  ],
  "create_hierarchical_groups": [
    "rank",
    "ranks",
    "hierarchical_group_sizes",
    "create_gloo_process_groups",
    "pg_options",
    "timeout",
    "group_desc"
  ],
  "RankGenerator": {
    "__init__": [
      "self",
      "tp",
      "ep",
      "dp",
      "pp",
      "cp",
      "order",
      "rank_offset"
    ],
    "get_mask": [
      "self",
      "order",
      "token"
    ],
    "get_ranks": [
      "self",
      "token"
    ]
  },
  "default_embedding_ranks": [
    "pp_ranks"
  ],
  "default_position_embedding_ranks": [
    "pp_ranks"
  ],
  "overwrite_nccl_comm_cfgs": [
    "nccl_comm_cfgs",
    "pg_name",
    "key_value_pair"
  ],
  "initialize_model_parallel": [
    "tensor_model_parallel_size",
    "pipeline_model_parallel_size",
    "virtual_pipeline_model_parallel_size",
    "pipeline_model_parallel_comm_backend",
    "use_sharp",
    "context_parallel_size",
    "hierarchical_context_parallel_sizes",
    "expert_model_parallel_size",
    "num_distributed_optimizer_instances",
    "expert_tensor_parallel_size",
    "nccl_communicator_config_path",
    "distributed_timeout_minutes",
    "order",
    "get_embedding_ranks",
    "get_position_embedding_ranks",
    "create_gloo_process_groups",
    "high_priority_stream_groups",
    "sharp_enabled_group"
  ],
  "is_initialized": [],
  "is_unitialized": [],
  "model_parallel_is_initialized": [],
  "get_model_parallel_group": [
    "check_initialized"
  ],
  "get_tensor_model_parallel_group": [
    "check_initialized"
  ],
  "get_pipeline_model_parallel_group": [
    "check_initialized"
  ],
  "get_data_parallel_group": [
    "with_context_parallel",
    "partial_data_parallel"
  ],
  "get_data_parallel_group_gloo": [
    "with_context_parallel",
    "partial_data_parallel"
  ],
  "get_context_parallel_group": [
    "check_initialized"
  ],
  "get_context_parallel_global_ranks": [
    "check_initialized"
  ],
  "get_hierarchical_context_parallel_groups": [
    "check_initialized"
  ],
  "get_embedding_group": [
    "check_initialized"
  ],
  "get_position_embedding_group": [
    "check_initialized"
  ],
  "get_amax_reduction_group": [
    "with_context_parallel",
    "tp_only_amax_red"
  ],
  "get_tensor_and_data_parallel_group": [
    "check_initialized",
    "with_context_parallel"
  ],
  "get_tensor_and_context_parallel_group": [
    "check_initialized"
  ],
  "set_tensor_model_parallel_world_size": [
    "world_size"
  ],
  "set_pipeline_model_parallel_world_size": [
    "world_size"
  ],
  "set_virtual_pipeline_model_parallel_world_size": [
    "world_size"
  ],
  "get_tensor_model_parallel_world_size": [],
  "get_pipeline_model_parallel_world_size": [],
  "set_tensor_model_parallel_rank": [
    "rank"
  ],
  "set_pipeline_model_parallel_rank": [
    "rank"
  ],
  "get_tensor_model_parallel_rank": [],
  "get_pipeline_model_parallel_rank": [],
  "is_pipeline_first_stage": [
    "ignore_virtual",
    "vp_stage"
  ],
  "is_pipeline_last_stage": [
    "ignore_virtual",
    "vp_stage"
  ],
  "is_rank_in_embedding_group": [
    "ignore_virtual",
    "vp_stage"
  ],
  "is_rank_in_position_embedding_group": [],
  "get_virtual_pipeline_model_parallel_rank": [],
  "set_virtual_pipeline_model_parallel_rank": [
    "rank"
  ],
  "get_virtual_pipeline_model_parallel_world_size": [],
  "get_tensor_model_parallel_src_rank": [],
  "get_model_parallel_src_rank": [],
  "get_data_parallel_src_rank": [
    "with_context_parallel"
  ],
  "get_pipeline_model_parallel_first_rank": [],
  "get_pipeline_model_parallel_last_rank": [],
  "get_pipeline_model_parallel_next_rank": [],
  "get_pipeline_model_parallel_prev_rank": [],
  "get_data_parallel_world_size": [
    "with_context_parallel",
    "partial_data_parallel"
  ],
  "set_data_parallel_rank": [
    "rank"
  ],
  "get_data_parallel_rank": [
    "with_context_parallel",
    "partial_data_parallel"
  ],
  "get_context_parallel_world_size": [],
  "get_context_parallel_rank": [],
  "get_tensor_and_context_parallel_world_size": [],
  "get_tensor_and_context_parallel_rank": [],
  "get_expert_model_parallel_group": [
    "check_initialized"
  ],
  "get_expert_model_parallel_world_size": [],
  "set_expert_model_parallel_world_size": [
    "world_size"
  ],
  "get_expert_model_parallel_rank": [],
  "set_expert_model_parallel_rank": [
    "rank"
  ],
  "get_expert_tensor_parallel_group": [
    "check_initialized"
  ],
  "get_expert_tensor_parallel_world_size": [],
  "set_expert_tensor_parallel_world_size": [
    "world_size"
  ],
  "get_expert_tensor_parallel_rank": [],
  "set_expert_tensor_parallel_rank": [
    "rank"
  ],
  "get_expert_tensor_and_model_parallel_group": [
    "check_initialized"
  ],
  "get_expert_tensor_and_model_parallel_world_size": [],
  "get_expert_tensor_and_model_parallel_rank": [],
  "get_expert_tensor_model_pipeline_parallel_group": [
    "check_initialized"
  ],
  "get_expert_data_parallel_group": [
    "check_initialized",
    "partial_expert_data_parallel"
  ],
  "get_data_modulo_expert_parallel_group": [
    "partial_expert_data_parallel"
  ],
  "get_expert_data_parallel_group_gloo": [
    "partial_expert_data_parallel"
  ],
  "get_expert_data_parallel_rank": [
    "partial_expert_data_parallel"
  ],
  "get_expert_data_parallel_world_size": [
    "partial_expert_data_parallel"
  ],
  "get_intra_distributed_optimizer_instance_group": [
    "check_initialized"
  ],
  "get_inter_distributed_optimizer_instance_group": [
    "check_initialized"
  ],
  "_set_global_memory_buffer": [],
  "get_global_memory_buffer": [],
  "destroy_global_memory_buffer": [],
  "get_all_ranks": [],
  "destroy_model_parallel": [],
  "ModelType": {
    "encoder_or_decoder": [],
    "retro_encoder": [],
    "retro_decoder": [],
    "encoder_and_decoder": [
      "self"
    ]
  },
  "Fp8Recipe": {
    "delayed": [],
    "tensorwise": [],
    "mxfp8": [],
    "blockwise": []
  },
  "Fp4Recipe": {
    "nvfp4": []
  },
  "TimerBase": {
    "__init__": [
      "self",
      "name"
    ],
    "start": [
      "self",
      "barrier"
    ],
    "stop": [
      "self",
      "barrier"
    ],
    "reset": [
      "self"
    ],
    "elapsed": [
      "self",
      "reset",
      "barrier"
    ]
  },
  "DummyTimer": {
    "__init__": [
      "self"
    ],
    "start": [
      "self",
      "barrier"
    ],
    "stop": [
      "self",
      "barrier"
    ],
    "reset": [
      "self"
    ],
    "elapsed": [
      "self",
      "reset",
      "barrier"
    ],
    "active_time": [
      "self"
    ]
  },
  "Timer": {
    "__init__": [
      "self",
      "name"
    ],
    "set_barrier_group": [
      "self",
      "barrier_group"
    ],
    "start": [
      "self",
      "barrier"
    ],
    "stop": [
      "self",
      "barrier"
    ],
    "reset": [
      "self"
    ],
    "elapsed": [
      "self",
      "reset",
      "barrier"
    ],
    "active_time": [
      "self"
    ]
  },
  "Timers": {
    "__init__": [
      "self",
      "log_level",
      "log_option"
    ],
    "__call__": [
      "self",
      "name",
      "log_level"
    ],
    "_get_elapsed_time_all_ranks": [
      "self",
      "names",
      "reset",
      "barrier"
    ],
    "_get_global_min_max_time": [
      "self",
      "names",
      "reset",
      "barrier",
      "normalizer"
    ],
    "_get_global_min_max_time_string": [
      "self",
      "names",
      "reset",
      "barrier",
      "normalizer",
      "max_only"
    ],
    "_get_all_ranks_time_string": [
      "self",
      "names",
      "reset",
      "barrier",
      "normalizer"
    ],
    "get_all_timers_string": [
      "self",
      "names",
      "normalizer",
      "reset",
      "barrier"
    ],
    "log": [
      "self",
      "names",
      "rank",
      "normalizer",
      "reset",
      "barrier"
    ],
    "write": [
      "self",
      "names",
      "writer",
      "iteration",
      "normalizer",
      "reset",
      "barrier"
    ]
  },
  "HyperCommGrid": {
    "__init__": [
      "self",
      "shape",
      "dim_names",
      "rank_offset",
      "backend"
    ],
    "create_pg": [
      "self",
      "dims"
    ],
    "get_pg": [
      "self",
      "dims"
    ],
    "_gen_rank_enum": [
      "self",
      "dims"
    ],
    "_order_dims": [
      "self",
      "dims"
    ]
  },
  "get_num_microbatches": [],
  "get_current_global_batch_size": [],
  "get_micro_batch_size": [],
  "get_current_running_global_batch_size": [],
  "update_num_microbatches": [
    "consumed_samples",
    "consistency_check",
    "verbose"
  ],
  "unset_num_microbatches_calculator": [],
  "init_num_microbatches_calculator": [
    "rank",
    "rampup_batch_size",
    "global_batch_size",
    "micro_batch_size",
    "data_parallel_size",
    "decrease_batch_size_if_needed"
  ],
  "destroy_num_microbatches_calculator": [],
  "reconfigure_num_microbatches_calculator": [
    "rank",
    "rampup_batch_size",
    "global_batch_size",
    "micro_batch_size",
    "data_parallel_size",
    "decrease_batch_size_if_needed"
  ],
  "_configure_global_num_microbatches_calculator": [
    "rank",
    "rampup_batch_size",
    "global_batch_size",
    "micro_batch_size",
    "data_parallel_size",
    "decrease_batch_size_if_needed",
    "init"
  ],
  "_build_num_microbatches_calculator": [
    "rank",
    "rampup_batch_size",
    "global_batch_size",
    "micro_batch_size",
    "data_parallel_size",
    "decrease_batch_size_if_needed"
  ],
  "_round": [
    "batch_size",
    "divisor"
  ],
  "NumMicroBatchesCalculator": {
    "__init__": [
      "self"
    ],
    "get": [
      "self"
    ],
    "get_current_global_batch_size": [
      "self"
    ],
    "get_micro_batch_size": [
      "self"
    ],
    "get_current_running_global_batch_size": [
      "self"
    ],
    "update": [
      "self",
      "consumed_samples",
      "consistency_check",
      "verbose"
    ]
  },
  "ConstantNumMicroBatchesCalculator": {
    "__init__": [
      "self",
      "global_batch_size",
      "micro_batch_size",
      "data_parallel_size",
      "decrease_batch_size_if_needed",
      "rank"
    ],
    "update": [
      "self",
      "consumed_samples",
      "consistency_check",
      "verbose"
    ]
  },
  "RampupBatchsizeNumMicroBatchesCalculator": {
    "__init__": [
      "self",
      "global_batch_size",
      "micro_batch_size",
      "data_parallel_size",
      "decrease_batch_size_if_needed",
      "rank",
      "start_global_batch_size",
      "batch_size_increment",
      "ramup_samples"
    ],
    "update": [
      "self",
      "consumed_samples",
      "consistency_check",
      "verbose"
    ]
  },
  "ProcessGroupHelperMeta": {
    "__setattr__": [
      "cls",
      "name",
      "value"
    ]
  },
  "ProcessGroupCollection": {
    "__init__": [
      "self"
    ],
    "use_mpu_process_groups": [
      "cls",
      "required_pgs"
    ],
    "setup_process_groups_for_optimizer": [
      "pg_collection",
      "model_chunks",
      "use_gloo_process_groups"
    ],
    "setup_process_groups_for_ddp": [
      "pg_collection",
      "config",
      "ddp_config"
    ]
  },
  "SerializableStateType": [],
  "DataIteratorArgType": [],
  "Caller": {},
  "Call": {},
  "RerunDiagnostic": {
    "CORRECT_RESULT": [],
    "TRANSIENT_ERROR": [],
    "PERSISTENT_ERROR": []
  },
  "RerunMode": {
    "DISABLED": [],
    "VALIDATE_RESULTS": [],
    "REPORT_DETERMINISM_STATS": []
  },
  "RerunState": {
    "NOT_RUNNING_YET": [],
    "INITIAL_RUN": [],
    "RERUNNING_IN_PLACE": [],
    "WILL_RERUN_FROM_CHECKPOINT": [],
    "RERUNNING_FROM_CHECKPOINT": [],
    "RERUNNING_AGAIN_FROM_CHECKPOINT": []
  },
  "RerunValidationStatus": {
    "RERUN_DISABLED": [],
    "INITIAL_RUN": [],
    "FIRST_RERUN_NOT_REPRODUCIBLE": [],
    "FIRST_RERUN_REPRODUCIBLE": [],
    "SECOND_RERUN_NOT_REPRODUCIBLE": [],
    "SECOND_RERUN_REPRODUCIBLE": []
  },
  "RerunStateMachine": {
    "__init__": [
      "self",
      "state_save_func",
      "state_restore_func",
      "mode",
      "error_injector",
      "result_rejected_tracker_filename"
    ],
    "set_mode": [
      "self",
      "mode"
    ],
    "get_mode": [
      "self"
    ],
    "should_run_forward_backward": [
      "self",
      "data_iterator"
    ],
    "should_checkpoint_and_exit": [
      "self"
    ],
    "validate_result": [
      "self",
      "result",
      "rejection_func",
      "message",
      "comparison_func",
      "tolerance",
      "fatal"
    ],
    "is_unexpectedly_large": [
      "self",
      "result",
      "threshold",
      "context",
      "num_samples",
      "resample"
    ],
    "state_dict": [
      "self",
      "data_iterator",
      "ckpt_format",
      "force"
    ],
    "validate_state_dict": [
      "self",
      "state_dict"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "_sanitize_data_iterators": [
      "self",
      "data_iterator"
    ],
    "_get_validation_call_info": [
      "self",
      "message"
    ],
    "_save_state": [
      "self"
    ],
    "_restore_state": [
      "self"
    ],
    "_maybe_report_stats": [
      "self"
    ],
    "_log_validation_error_to_file": [
      "self",
      "status",
      "result",
      "message"
    ],
    "get_skipped_iterations_from_tracker_file": [
      "cls",
      "tracker_file_name"
    ]
  },
  "RerunDataIterator": {
    "__init__": [
      "self",
      "iterable"
    ],
    "__next__": [
      "self"
    ],
    "rewind": [
      "self"
    ],
    "advance": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "QuickStats": {
    "__init__": [
      "self",
      "max_size"
    ],
    "record": [
      "self",
      "data"
    ],
    "combine": [
      "self",
      "others"
    ],
    "reset": [
      "self"
    ],
    "print_stats": [
      "self"
    ],
    "__getstate_": [
      "self"
    ],
    "__setstate": [
      "self",
      "state"
    ]
  },
  "RerunErrorInjector": {
    "__init__": [
      "self",
      "error_injection_rate",
      "error_injection_type"
    ],
    "maybe_inject": [
      "self"
    ],
    "maybe_miscompare": [
      "self",
      "comparison_func",
      "initial_result",
      "result",
      "state"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "initialize_rerun_state_machine": [],
  "destroy_rerun_state_machine": [],
  "get_rerun_state_machine": [],
  "_set_rerun_state_machine": [
    "rerun_state_machine"
  ],
  "_safe_get_rank": [],
  "_compare_floats": [
    "a",
    "b"
  ],
  "ModelParallelConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "SAFE_GLOBALS": [],
  "register_safe_globals": [],
  "get_config_logger_path": [
    "config"
  ],
  "has_config_logger_enabled": [
    "config"
  ],
  "__config_logger_path_counts": [],
  "get_path_count": [
    "path"
  ],
  "get_path_with_count": [
    "path"
  ],
  "JSONEncoderWithMcoreTypes": {
    "default": [
      "self",
      "o"
    ]
  },
  "log_config_to_disk": [
    "config",
    "dict_data",
    "prefix",
    "rank_str"
  ],
  "MLPLayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "hidden_dropout",
      "pg_collection"
    ]
  },
  "MambaContextParallel": {
    "__init__": [
      "self",
      "cp_group",
      "d_inner_local_tp",
      "nheads_local_tp",
      "ngroups_local_tp",
      "d_state",
      "conv1d_cp1",
      "dt_bias_cp1",
      "A_log_cp1",
      "D_cp1",
      "D_has_hdim"
    ],
    "pre_conv_ssm": [
      "self",
      "input_"
    ],
    "post_conv_ssm": [
      "self",
      "input_"
    ],
    "conv1d": [
      "self",
      "input_"
    ],
    "conv1d_channels": [
      "self"
    ],
    "get_conv1d_weight": [
      "self"
    ],
    "get_conv1d_bias": [
      "self"
    ],
    "get_dt_bias": [
      "self"
    ],
    "get_A_log": [
      "self"
    ],
    "get_D": [
      "self"
    ],
    "_slice_conv_param": [
      "self",
      "param"
    ],
    "_slice_vector_param": [
      "self",
      "param",
      "has_hdim"
    ]
  },
  "_all_to_all_cp2hp": [
    "input_",
    "cp_group"
  ],
  "_all_to_all_hp2cp": [
    "input_",
    "cp_group"
  ],
  "_undo_attention_load_balancing": [
    "input_",
    "cp_size"
  ],
  "_redo_attention_load_balancing": [
    "input_",
    "cp_size"
  ],
  "_version_no_greater_than": [
    "version",
    "version_limit"
  ],
  "default_cache_dir": [],
  "ParallelFileCacheManager": {
    "put": [
      "self",
      "data",
      "filename",
      "binary"
    ]
  },
  "Symbols": {
    "MAMBA": [],
    "ATTENTION": [],
    "MLP": [],
    "VALID": []
  },
  "_allocate_auto": [
    "total_layers_count",
    "target_attention_ratio",
    "target_mlp_ratio"
  ],
  "_allocate_override": [
    "total_layers_count",
    "override_pattern"
  ],
  "_layer_counts_match": [
    "a",
    "b"
  ],
  "allocate_layers": [
    "total_layers_count",
    "target_attention_ratio",
    "target_mlp_ratio",
    "override_pattern"
  ],
  "ExtendedRMSNorm": {
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "MambaMixerSubmodules": {},
  "MambaMixer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "d_model",
      "d_conv",
      "conv_init",
      "expand",
      "A_init_range",
      "D_has_hdim",
      "rmsnorm",
      "norm_before_gate",
      "dt_min",
      "dt_max",
      "dt_init",
      "dt_scale",
      "dt_init_floor",
      "bias",
      "conv_bias",
      "chunk_size",
      "layer_number",
      "use_mem_eff_path",
      "d_state",
      "headdim",
      "ngroups",
      "pg_collection"
    ],
    "forward": [
      "self",
      "hidden_states",
      "inference_context"
    ],
    "dynamic_inference": [
      "self",
      "hidden_states",
      "inference_context"
    ],
    "decode": [
      "self",
      "hidden_states",
      "conv_state",
      "ssm_state"
    ],
    "ssm_training": [
      "self",
      "zxBCdt"
    ],
    "ssm_prefill": [
      "self",
      "zxBCdt",
      "conv_state",
      "ssm_state"
    ],
    "ssm_decode": [
      "self",
      "zxBCdt",
      "conv_state",
      "ssm_state"
    ],
    "allocate_inference_cache": [
      "self",
      "batch_size",
      "max_seqlen",
      "dtype"
    ],
    "_get_states_from_cache": [
      "self",
      "inference_context",
      "batch_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "_split_tensor_factory": [
    "orig_sh_ten",
    "split_sections",
    "split_names",
    "split_dim"
  ],
  "_init_weights": [
    "module",
    "n_layer",
    "initializer_range",
    "rescale_prenorm_residual",
    "n_residuals_per_layer"
  ],
  "MambaStackSubmodules": {},
  "MambaStack": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "residual_in_fp32",
      "pre_process",
      "hybrid_attention_ratio",
      "hybrid_mlp_ratio",
      "hybrid_override_pattern",
      "post_layer_norm",
      "post_process",
      "device",
      "dtype",
      "pg_collection"
    ],
    "_select_layers_for_pipeline_parallel": [
      "self",
      "layer_type_list"
    ],
    "allocate_inference_cache": [
      "self",
      "batch_size",
      "max_seqlen",
      "dtype"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "inference_context",
      "rotary_pos_emb"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "MambaLayerSubmodules": {},
  "MambaLayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "residual_in_fp32",
      "pg_collection"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "inference_context",
      "rotary_pos_emb"
    ],
    "allocate_inference_cache": [
      "self",
      "batch_size",
      "max_seqlen",
      "dtype"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "_te_cuda_graph_replay": [
      "self"
    ],
    "_should_call_local_cudagraph": [
      "self"
    ]
  },
  "_get_extra_te_kwargs": [
    "config"
  ],
  "condition_init_method": [
    "config",
    "init_method"
  ],
  "split_te_layernorm_column_parallel_linear": [
    "fused_layer",
    "config",
    "init_method",
    "tp_group"
  ],
  "TENorm": {
    "__new__": [
      "cls",
      "config",
      "hidden_size",
      "eps"
    ]
  },
  "TELinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "forward": [
      "self",
      "x"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "TELayerNormColumnParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "forward": [
      "self",
      "x"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "__repr__": [
      "self"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "TEColumnParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "__repr__": [
      "self"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "TERowParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "__repr__": [
      "self"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "TEDotProductAttention": {
    "__init__": [
      "self",
      "config",
      "layer_number",
      "attn_mask_type",
      "attention_type",
      "attention_dropout",
      "softmax_scale",
      "k_channels",
      "v_channels",
      "cp_comm_type",
      "pg_collection"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "attn_mask_type",
      "attention_bias",
      "packed_seq_params"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "TEDelayedScaling": {
    "__init__": [
      "self",
      "config",
      "fp8_format",
      "override_linear_precision"
    ]
  },
  "TECudaRNGStatesTracker": {
    "__init__": [
      "self",
      "is_inference_rng_tracker"
    ],
    "is_initialized": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "set_states": [
      "self",
      "states"
    ],
    "add": [
      "self",
      "name",
      "seed"
    ]
  },
  "te_checkpoint": [
    "forward_func",
    "distribute_saved_activations",
    "get_rng_state_tracker",
    "tp_group"
  ],
  "set_save_original_input": [
    "module"
  ],
  "TESpecProvider": {
    "linear": [
      "self"
    ],
    "column_parallel_linear": [
      "self"
    ],
    "row_parallel_linear": [
      "self"
    ],
    "fuse_layernorm_and_linear": [
      "self"
    ],
    "column_parallel_layer_norm_linear": [
      "self"
    ],
    "layer_norm": [
      "self",
      "rms_norm",
      "for_qk"
    ],
    "core_attention": [
      "self"
    ],
    "grouped_mlp_modules": [
      "self",
      "moe_use_grouped_gemm",
      "moe_use_legacy_grouped_gemm"
    ],
    "activation_func": [
      "self"
    ]
  },
  "_KITCHEN_CONFIG_TYPE_KEY": [],
  "KitchenConfigType": {
    "QLINEAR_PARAMS": []
  },
  "QLinearParamsConfigSchema": {
    "parse_config_dict": [
      "cls",
      "config_dict"
    ],
    "get_expected_keys": [
      "cls"
    ],
    "__post_init__": [
      "self"
    ],
    "to_kitchen_qlinear": [
      "self"
    ]
  },
  "KitchenQuantizationParams": {
    "parse_from_config": [
      "quant_config"
    ]
  },
  "_get_extra_kitchen_kwargs": [
    "config"
  ],
  "KitchenLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "finish_init": [
      "self",
      "quantization_config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "KitchenColumnParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "__repr__": [
      "self"
    ]
  },
  "KitchenRowParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "__repr__": [
      "self"
    ]
  },
  "KitchenGroupedLinear": {
    "__init__": [
      "self",
      "num_gemms",
      "input_size",
      "output_size"
    ],
    "finish_init": [
      "self",
      "quantization_config"
    ],
    "forward": [
      "self",
      "x",
      "m_splits"
    ],
    "_encode_extra_state": [
      "self",
      "state"
    ],
    "_decode_extra_state": [
      "self",
      "state"
    ],
    "_split_extra_state": [
      "self",
      "state"
    ],
    "_sharded_state_dict_grouped": [
      "self",
      "tp_axis_map",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "KitchenColumnParallelGroupedLinear": {
    "__init__": [
      "self",
      "num_gemms",
      "input_size",
      "output_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "KitchenRowParallelGroupedLinear": {
    "__init__": [
      "self",
      "num_gemms",
      "input_size",
      "output_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "KitchenLayerNormColumnParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "finish_init": [
      "self",
      "quantization_config"
    ],
    "forward": [
      "self",
      "x"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "__repr__": [
      "self"
    ]
  },
  "KitchenSpecProvider": {
    "__init__": [
      "self",
      "fallback"
    ],
    "column_parallel_linear": [
      "self"
    ],
    "row_parallel_linear": [
      "self"
    ],
    "fuse_layernorm_and_linear": [
      "self"
    ],
    "column_parallel_layer_norm_linear": [
      "self"
    ],
    "layer_norm": [
      "self",
      "rms_norm",
      "for_qk"
    ],
    "core_attention": [
      "self"
    ],
    "grouped_mlp_modules": [
      "self",
      "moe_use_grouped_gemm",
      "moe_use_legacy_grouped_gemm"
    ],
    "activation_func": [
      "self"
    ]
  },
  "MultimodalDatasetConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "MockMultimodalDataset": {
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "S3_PREFIX": [],
  "MSC_PREFIX": [],
  "ObjectStorageConfig": {},
  "S3Config": [],
  "S3Client": {
    "download_file": [
      "self",
      "Bucket",
      "Key",
      "Filename"
    ],
    "upload_file": [
      "self",
      "Filename",
      "Bucket",
      "Key"
    ],
    "head_object": [
      "self",
      "Bucket",
      "Key"
    ],
    "get_object": [
      "self",
      "Bucket",
      "Key",
      "Range"
    ],
    "close": [
      "self"
    ]
  },
  "_remove_s3_prefix": [
    "path"
  ],
  "_is_s3_path": [
    "path"
  ],
  "_remove_msc_prefix": [
    "path"
  ],
  "_is_msc_path": [
    "path"
  ],
  "_s3_download_file": [
    "client",
    "s3_path",
    "local_path"
  ],
  "_s3_object_exists": [
    "client",
    "path"
  ],
  "is_object_storage_path": [
    "path"
  ],
  "get_index_cache_path": [
    "idx_path",
    "object_storage_config"
  ],
  "parse_s3_path": [
    "path"
  ],
  "get_object_storage_access": [
    "path"
  ],
  "dataset_exists": [
    "path_prefix",
    "idx_path",
    "bin_path"
  ],
  "cache_index_file": [
    "remote_path",
    "local_path"
  ],
  "BERTMaskedWordPieceDatasetConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "BERTMaskedWordPieceDataset": {
    "__init__": [
      "self",
      "indexed_dataset",
      "dataset_path",
      "indexed_indices",
      "num_samples",
      "index_split",
      "config"
    ],
    "_key_config_attributes": [],
    "__getitem__": [
      "self",
      "idx"
    ],
    "_get_token_mask": [
      "self",
      "numpy_random_state"
    ]
  },
  "Split": {
    "train": [],
    "valid": [],
    "test": []
  },
  "compile_helpers": [],
  "normalize": [
    "weights"
  ],
  "get_blend_from_list": [
    "blend"
  ],
  "build_sample_idx": [
    "sizes",
    "document_indices",
    "sequence_length",
    "num_epochs",
    "tokens_per_epoch",
    "drop_last_partial_sequence",
    "add_extra_token_to_sequence"
  ],
  "MaskedWordPieceDatasetConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "MaskedWordPieceDataset": {
    "__init__": [
      "self",
      "indexed_dataset",
      "dataset_path",
      "indexed_indices",
      "num_samples",
      "index_split",
      "config"
    ],
    "numel_low_level_dataset": [
      "low_level_dataset"
    ],
    "build_low_level_dataset": [
      "dataset_path",
      "config"
    ],
    "_key_config_attributes": [],
    "__len__": [
      "self"
    ],
    "_build_sample_index": [
      "self",
      "sequence_length",
      "min_sentences_per_sample"
    ],
    "_create_masked_lm_predictions": [
      "self",
      "token_ids",
      "target_sequence_length",
      "numpy_random_state"
    ],
    "_get_token_mask": [
      "self",
      "numpy_random_state"
    ]
  },
  "LowLevelDataset": [],
  "MegatronDataset": {
    "__init__": [
      "self",
      "dataset",
      "dataset_path",
      "indices",
      "num_samples",
      "index_split",
      "config"
    ],
    "numel_low_level_dataset": [
      "low_level_dataset"
    ],
    "build_low_level_dataset": [
      "dataset_path",
      "config"
    ],
    "_key_config_attributes": [],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "MidLevelDataset": [],
  "TopLevelDataset": [],
  "DistributedDataset": [],
  "BlendedMegatronDatasetBuilder": {
    "__init__": [
      "self",
      "cls",
      "sizes",
      "is_built_on_rank",
      "config"
    ],
    "build": [
      "self"
    ],
    "_build_blended_dataset_splits": [
      "self"
    ],
    "_build_megatron_datasets_parallel": [
      "self",
      "prefixes",
      "split",
      "sizes_per_dataset"
    ],
    "_build_megatron_dataset_splits": [
      "self",
      "dataset_path",
      "split",
      "sizes",
      "synchronize_ranks"
    ],
    "build_generic_dataset": [
      "cls",
      "is_built_on_rank",
      "synchronize_ranks"
    ]
  },
  "_get_size_per_split_per_dataset": [
    "normalized_weights",
    "target_size_per_split",
    "surplus"
  ],
  "_PAD_TOKEN_ID": [],
  "GPTDatasetConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "GPTDataset": {
    "__init__": [
      "self",
      "indexed_dataset",
      "dataset_path",
      "indexed_indices",
      "num_samples",
      "index_split",
      "config"
    ],
    "numel_low_level_dataset": [
      "low_level_dataset"
    ],
    "build_low_level_dataset": [
      "dataset_path",
      "config"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "_query_document_sample_shuffle_indices": [
      "self",
      "idx"
    ],
    "_build_document_sample_shuffle_indices": [
      "self"
    ],
    "_get_num_tokens_per_epoch": [
      "self"
    ],
    "_get_num_epochs": [
      "self",
      "num_tokens_per_epoch"
    ]
  },
  "_build_document_index": [
    "documents",
    "num_epochs",
    "numpy_random_state",
    "separate_final_epoch"
  ],
  "_build_shuffle_index": [
    "num_samples",
    "total_size",
    "numpy_random_state"
  ],
  "_get_ltor_masks_and_position_ids": [
    "data",
    "eod_token",
    "reset_position_ids",
    "reset_attention_mask",
    "eod_mask_loss",
    "create_attention_mask"
  ],
  "MockGPTLowLevelDataset": {
    "__init__": [
      "self",
      "tokenizer"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "get": [
      "self",
      "idx",
      "offset",
      "length"
    ]
  },
  "MockGPTDataset": {
    "__init__": [
      "self",
      "dataset",
      "dataset_path",
      "indices",
      "num_samples",
      "index_split",
      "config"
    ],
    "numel_low_level_dataset": [
      "low_level_dataset"
    ],
    "build_low_level_dataset": [
      "dataset_path",
      "config"
    ]
  },
  "_VERBOSE": [],
  "BlendedDataset": {
    "__init__": [
      "self",
      "datasets",
      "weights",
      "size",
      "config"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "_build_indices": [
      "self"
    ]
  },
  "MegatronLegacyTokenizer": {
    "__init__": [
      "self"
    ],
    "tokenize": [
      "self",
      "text"
    ],
    "detokenize": [
      "self",
      "ids"
    ],
    "offsets": [
      "self",
      "ids",
      "text"
    ],
    "vocab": [
      "self"
    ],
    "inv_vocab": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "cls": [
      "self"
    ],
    "sep": [
      "self"
    ],
    "pad": [
      "self"
    ],
    "eod": [
      "self"
    ],
    "bos": [
      "self"
    ],
    "eos": [
      "self"
    ],
    "mask": [
      "self"
    ]
  },
  "BlendedMegatronDatasetConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "parse_and_normalize_split": [
    "split"
  ],
  "convert_split_vector_to_split_matrix": [
    "vector_a",
    "vector_b"
  ],
  "T5MaskedWordPieceDatasetConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "T5MaskedWordPieceDataset": {
    "__init__": [
      "self",
      "indexed_dataset",
      "dataset_path",
      "indexed_indices",
      "num_samples",
      "index_split",
      "config"
    ],
    "_key_config_attributes": [],
    "_build_b1ss_attention_mask": [
      "source_block",
      "target_block",
      "make_history_mask"
    ],
    "config_attention_mask": [
      "encoder_tokens",
      "decoder_tokens",
      "encoder_mask",
      "decoder_mask",
      "use_local",
      "test_te_version"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "_get_token_mask": [
      "self",
      "numpy_random_state"
    ]
  },
  "_INDEX_HEADER": [],
  "DType": {
    "uint8": [],
    "int8": [],
    "int16": [],
    "int32": [],
    "int64": [],
    "float64": [],
    "float32": [],
    "uint16": [],
    "code_from_dtype": [
      "cls",
      "value"
    ],
    "dtype_from_code": [
      "cls",
      "value"
    ],
    "size": [
      "key"
    ],
    "optimal_dtype": [
      "cardinality"
    ]
  },
  "_IndexWriter": {
    "__init__": [
      "self",
      "idx_path",
      "dtype"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_val",
      "exc_tb"
    ],
    "write": [
      "self",
      "sequence_lengths",
      "sequence_modes",
      "document_indices"
    ],
    "_sequence_pointers": [
      "self",
      "sequence_lengths"
    ]
  },
  "_IndexReader": {
    "__init__": [
      "self",
      "idx_path",
      "multimodal"
    ],
    "__del__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "_BinReader": {
    "read": [
      "self",
      "dtype",
      "count",
      "offset"
    ]
  },
  "_MMapBinReader": {
    "__init__": [
      "self",
      "bin_path"
    ],
    "read": [
      "self",
      "dtype",
      "count",
      "offset"
    ],
    "__del__": [
      "self"
    ]
  },
  "_FileBinReader": {
    "__init__": [
      "self",
      "bin_path"
    ],
    "read": [
      "self",
      "dtype",
      "count",
      "offset"
    ]
  },
  "_S3BinReader": {
    "__init__": [
      "self",
      "bin_path",
      "object_storage_config"
    ],
    "_extract_from_cache": [
      "self",
      "offset",
      "size"
    ],
    "read": [
      "self",
      "dtype",
      "count",
      "offset"
    ],
    "__del__": [
      "self"
    ]
  },
  "_MultiStorageClientBinReader": {
    "__init__": [
      "self",
      "bin_path",
      "object_storage_config"
    ],
    "read": [
      "self",
      "dtype",
      "count",
      "offset"
    ]
  },
  "OBJECT_STORAGE_BIN_READERS": [],
  "IndexedDataset": {
    "__init__": [
      "self",
      "path_prefix",
      "multimodal",
      "mmap",
      "object_storage_config",
      "s3_config"
    ],
    "initialize": [
      "self",
      "path_prefix",
      "multimodal",
      "mmap",
      "object_storage_config"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "__del__": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "get": [
      "self",
      "idx",
      "offset",
      "length"
    ],
    "sequence_lengths": [
      "self"
    ],
    "document_indices": [
      "self"
    ],
    "get_document_indices": [
      "self"
    ],
    "set_document_indices": [
      "self",
      "document_indices"
    ],
    "sequence_modes": [
      "self"
    ],
    "exists": [
      "path_prefix"
    ]
  },
  "IndexedDatasetBuilder": {
    "__init__": [
      "self",
      "bin_path",
      "dtype",
      "multimodal"
    ],
    "add_item": [
      "self",
      "tensor",
      "mode"
    ],
    "add_document": [
      "self",
      "tensor",
      "lengths",
      "modes"
    ],
    "end_document": [
      "self"
    ],
    "add_index": [
      "self",
      "path_prefix"
    ],
    "finalize": [
      "self",
      "idx_path"
    ]
  },
  "get_idx_path": [
    "path_prefix"
  ],
  "get_bin_path": [
    "path_prefix"
  ],
  "Block": {},
  "log_retro_rank_0": [
    "message"
  ],
  "retro_makedir": [
    "config",
    "path"
  ],
  "extract_data_config": [
    "config"
  ],
  "get_num_chunks_per_sample": [
    "sample_length",
    "chunk_length"
  ],
  "GPTToTextDataset": {
    "__init__": [
      "self",
      "gpt_dataset",
      "gpt_tokenizer"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "get_blocks": [
    "dirname",
    "n_samples",
    "block_size",
    "validate"
  ],
  "get_blocks_by_rank": [
    "dirname",
    "n_samples",
    "block_size",
    "validate",
    "sample",
    "process_group"
  ],
  "BlockPathMap": {
    "from_dir": [
      "cls",
      "dir",
      "block_size",
      "ext"
    ],
    "__init__": [
      "self",
      "block_paths",
      "block_size"
    ],
    "__str__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "required_libs": [],
  "get_index": [
    "config",
    "ondisk"
  ],
  "embed_block": [
    "config",
    "gpt_dataset",
    "block"
  ],
  "query_embeddings": [
    "config",
    "db_dataset",
    "index",
    "embeddings",
    "chunk_id_range",
    "sample_map",
    "n_chunks_per_sample",
    "verbose"
  ],
  "query_embedding_block": [
    "config",
    "db_dataset",
    "index",
    "embeddings",
    "chunk_id_range",
    "sample_map",
    "n_chunks_per_sample"
  ],
  "query_block_neighbors": [
    "config",
    "db_dataset",
    "query_dataset",
    "index",
    "block"
  ],
  "query_dataset_neighbors": [
    "config",
    "db_dataset",
    "query_dataset",
    "num_active_chunks",
    "prefix",
    "neighbor_dir",
    "index"
  ],
  "query_neighbors": [
    "config"
  ],
  "GPTChunkDataset": {
    "__init__": [
      "self",
      "sample_dataset",
      "sample_length",
      "chunk_length"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "build_gpt_chunk_datasets_from_gpt_datasets": [
    "project_dir",
    "gpt_datasets",
    "sample_length",
    "chunk_length"
  ],
  "get_query_dir": [
    "project_dir"
  ],
  "get_neighbor_dir": [
    "project_dir",
    "key",
    "dataset"
  ],
  "MultiSplitGPTDatasetConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "MultiSplitGPTDataset": {
    "__init__": [
      "self",
      "indexed_dataset",
      "dataset_path",
      "indexed_indices",
      "num_samples",
      "index_split",
      "config"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "_key_config_attributes": []
  },
  "RetroDataset": {
    "__init__": [
      "self",
      "num_queried_samples",
      "num_neighbors",
      "num_retrieved_chunks",
      "block_size",
      "db_dataset",
      "chunk_dataset",
      "neighbor_path_map"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "sample_idx"
    ]
  },
  "get_retro_datasets": [
    "config",
    "gpt_datasets",
    "sample_length",
    "eod_token_id"
  ],
  "Index": {
    "make_object_verbose": [
      "cls",
      "index",
      "verbose"
    ],
    "get_empty_index_path": [
      "self",
      "config"
    ],
    "get_empty_index": [
      "self",
      "config"
    ],
    "get_added_index_path": [
      "self",
      "config"
    ],
    "get_added_index": [
      "self",
      "config"
    ],
    "train": [
      "self",
      "config"
    ],
    "add": [
      "self",
      "config",
      "text_dataset"
    ],
    "embed_text_dataset_block": [
      "self",
      "embedder",
      "text_dataset",
      "_range"
    ]
  },
  "IndexFactory": {
    "get_index_class": [
      "cls",
      "index_type"
    ],
    "get_index": [
      "cls",
      "index_type"
    ]
  },
  "get_index_dir": [
    "config"
  ],
  "num_samples_to_block_ranges": [
    "config",
    "num_samples"
  ],
  "get_training_data_root_dir": [
    "config"
  ],
  "get_training_data_block_dir": [
    "config"
  ],
  "get_training_data_block_paths": [
    "config"
  ],
  "get_training_data_merged_path": [
    "config"
  ],
  "get_added_codes_dir": [
    "config"
  ],
  "get_added_code_paths": [
    "config"
  ],
  "validate_training_embeddings": [
    "config"
  ],
  "validate_added_encodings": [
    "config"
  ],
  "validate_index": [
    "config"
  ],
  "get_empty_index_path": [
    "config"
  ],
  "get_block_nload": [
    "block_path",
    "load_fraction"
  ],
  "merge_embedding_blocks": [
    "config"
  ],
  "get_text_dataset_for_training": [
    "config"
  ],
  "embed_training_chunks": [
    "config"
  ],
  "train_on_embeddings": [
    "config"
  ],
  "remove_embeddings": [
    "config"
  ],
  "_train_index": [
    "config"
  ],
  "train_index": [
    "config"
  ],
  "get_text_dataset_for_adding": [
    "config"
  ],
  "_add_to_index": [
    "config"
  ],
  "add_to_index": [
    "config"
  ],
  "build_index": [
    "config"
  ],
  "FaissBaseIndex": {
    "_train": [
      "self",
      "config"
    ],
    "train": [
      "self",
      "config"
    ],
    "_add": [
      "self",
      "config",
      "text_dataset"
    ],
    "add": [
      "self",
      "config",
      "text_dataset"
    ]
  },
  "FaissParallelAddIndex": {
    "encode_block": [
      "self",
      "index",
      "embedder",
      "text_dataset",
      "block"
    ],
    "save_block": [
      "self",
      "config",
      "block",
      "codes"
    ],
    "encode": [
      "self",
      "config",
      "text_dataset"
    ],
    "add_codes": [
      "self",
      "config"
    ],
    "remove_codes": [
      "self",
      "config"
    ],
    "add": [
      "self",
      "config",
      "text_dataset"
    ]
  },
  "RetroTokenizers": {},
  "Embedder": {
    "embed_text_dataset": [
      "self",
      "text_dataset"
    ],
    "embed_text": [
      "self",
      "text"
    ]
  },
  "RetroBertEmbedders": {},
  "RetroPreprocessingConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "RetroGPTChunkDatasets": {},
  "get_db_dir": [
    "project_dir"
  ],
  "init_indexed_dataset_infos": [
    "config"
  ],
  "get_indexed_dataset_infos_path": [
    "project_dir"
  ],
  "save_indexed_dataset_infos": [
    "project_dir",
    "indexed_dataset_infos"
  ],
  "load_indexed_datasets": [
    "project_dir",
    "indexed_dataset_infos"
  ],
  "get_indexed_dataset_infos": [
    "project_dir"
  ],
  "get_individual_db_dir": [
    "project_dir",
    "prefix"
  ],
  "get_individual_db_paths": [
    "project_dir",
    "prefix"
  ],
  "get_individual_chunk_db": [
    "project_dir",
    "ds_id",
    "ds_info"
  ],
  "get_individual_doc_offsets": [
    "project_dir",
    "ds_id",
    "ds_info"
  ],
  "get_merged_db_path_map": [
    "project_dir"
  ],
  "get_merged_dataset": [
    "project_dir",
    "chunk_length",
    "eod_token_id",
    "db_type",
    "indexed_dataset_infos"
  ],
  "get_merged_sampled_dataset": [
    "project_dir",
    "chunk_length",
    "eod_token_id",
    "indexed_dataset_infos"
  ],
  "get_merged_train_dataset": [
    "project_dir",
    "chunk_length",
    "eod_token_id",
    "indexed_dataset_infos"
  ],
  "get_merged_valid_dataset": [
    "project_dir",
    "chunk_length",
    "eod_token_id",
    "indexed_dataset_infos"
  ],
  "get_merged_datasets": [
    "project_dir",
    "chunk_length",
    "eod_token_id"
  ],
  "DBDataset": {
    "__init__": [
      "self",
      "db_path",
      "indexed_datasets",
      "chunks",
      "chunk_length",
      "eod_token_id"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "chunk_id"
    ],
    "load_doc_tuples": [
      "self"
    ]
  },
  "build_partial_db": [
    "config",
    "dataset_idx",
    "n_datasets",
    "indexed_dataset",
    "block_id",
    "n_blocks",
    "block",
    "proc_id",
    "n_procs"
  ],
  "build_block_db": [
    "config",
    "dataset_idx",
    "n_datasets",
    "indexed_dataset",
    "n_procs",
    "executor",
    "n_missing_blocks",
    "block_idx",
    "block"
  ],
  "save_block_db": [
    "block",
    "chunk_db_valid",
    "chunk_db_invalid",
    "doc_offsets"
  ],
  "build_individual_db": [
    "config",
    "dataset_idx",
    "n_datasets",
    "dataset_info"
  ],
  "build_individual_dbs": [
    "config",
    "indexed_dataset_infos"
  ],
  "update_chunk_counts": [
    "config",
    "indexed_dataset_infos"
  ],
  "merge_dbs": [
    "project_dir",
    "indexed_dataset_infos",
    "db_type"
  ],
  "build_merged_dbs": [
    "project_dir",
    "indexed_dataset_infos"
  ],
  "build_db": [
    "config"
  ],
  "FusedLayerNorm": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "eps",
      "persist_layer_norm",
      "zero_centered_gamma",
      "normalization"
    ],
    "reset_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "_bias_dropout_add_func": [
    "x_with_bias",
    "residual",
    "prob",
    "training"
  ],
  "bias_dropout_add_unfused": [
    "training"
  ],
  "bias_dropout_add_fused_train": [
    "x_with_bias",
    "residual",
    "prob"
  ],
  "bias_dropout_add_fused_inference": [
    "x_with_bias",
    "residual",
    "prob"
  ],
  "get_bias_dropout_add": [
    "training",
    "fused"
  ],
  "_indices_to_multihot_kernel": [
    "indices_ptr",
    "probs_in_indices_ptr",
    "multihot_indices_ptr",
    "probs_in_multihot_ptr",
    "position_map_ptr",
    "num_of_local_experts",
    "num_of_local_experts_next_power_of_2",
    "topk",
    "topk_next_power_of_2",
    "BLOCK_SIZE"
  ],
  "_multihot_to_indices_kernel": [
    "probs_in_multihot_ptr",
    "position_map_ptr",
    "probs_indices_ptr",
    "num_of_local_experts",
    "num_of_local_experts_next_power_of_2",
    "topk",
    "topk_next_power_of_2",
    "BLOCK_SIZE"
  ],
  "IndicesToMultihot": {
    "forward": [
      "ctx",
      "indices",
      "probs_indices",
      "num_of_local_experts"
    ],
    "backward": [
      "ctx",
      "grad_multihot_indices",
      "grad_probs_in_multihot"
    ]
  },
  "fused_indices_to_multihot": [
    "indices",
    "probs_indices",
    "num_of_local_experts"
  ],
  "bias_gelu": [
    "bias",
    "y"
  ],
  "bias_gelu_back": [
    "g",
    "bias",
    "y"
  ],
  "GeLUFunction": {
    "forward": [
      "ctx",
      "input",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ],
    "apply": [
      "cls"
    ]
  },
  "bias_gelu_impl": [],
  "_get_thd_token_idx": [
    "cu_seqlens",
    "pid_m",
    "seq_num",
    "cp_rank",
    "cp_size"
  ],
  "rotary_fwd_q_kernel": [
    "Q",
    "COS",
    "SIN",
    "qk_head_dim",
    "emb_dim",
    "head_num",
    "batch_size",
    "seq_num",
    "cu_seqlens_q",
    "stride_x_seq",
    "stride_x_nheads",
    "cp_rank",
    "cp_size",
    "BLOCK_H"
  ],
  "rotary_bwd_q_kernel": [
    "DO",
    "COS",
    "SIN",
    "qk_head_dim",
    "emb_dim",
    "head_num",
    "batch_size",
    "seq_num",
    "cu_seqlens_q",
    "stride_x_seq",
    "stride_x_nheads",
    "cp_rank",
    "cp_size",
    "BLOCK_H"
  ],
  "ApplyMLARotaryEmbQ": {
    "forward": [
      "ctx",
      "q",
      "cos",
      "sin",
      "qk_head_dim",
      "emb_dim",
      "cu_seqlens_q",
      "cp_rank",
      "cp_size",
      "rotary_interleaved"
    ],
    "backward": [
      "ctx",
      "grad"
    ]
  },
  "fused_apply_mla_rope_for_q": [
    "t",
    "cos",
    "sin",
    "qk_head_dim",
    "emb_dim",
    "cu_seqlens_q",
    "cp_rank",
    "cp_size",
    "rotary_interleaved"
  ],
  "rotary_fwd_kv_kernel": [
    "KV",
    "K_POS_EMB",
    "O_KEY",
    "O_VALUE",
    "COS",
    "SIN",
    "emb_dim",
    "k_dim",
    "v_dim",
    "head_num",
    "batch_size",
    "seq_num",
    "cu_seqlens_kv",
    "stride_kv_seq",
    "stride_kv_nheads",
    "stride_emb_seq",
    "stride_k_seq",
    "stride_k_nheads",
    "stride_v_seq",
    "stride_v_nheads",
    "cp_rank",
    "cp_size",
    "BLOCK_H"
  ],
  "rotary_bwd_kv_kernel": [
    "dK",
    "dV",
    "dKV",
    "dEMB",
    "COS",
    "SIN",
    "emb_dim",
    "k_dim",
    "v_dim",
    "head_num",
    "batch_size",
    "seq_num",
    "cu_seqlens_kv",
    "stride_dk_seq",
    "stride_dk_nheads",
    "stride_dv_seq",
    "stride_dv_nheads",
    "stride_dkv_seq",
    "stride_dkv_nheads",
    "stride_demb_seq",
    "cp_rank",
    "cp_size",
    "BLOCK_H"
  ],
  "ApplyMLARotaryEmbKV": {
    "forward": [
      "ctx",
      "kv",
      "k_pos_emb",
      "cos",
      "sin",
      "emb_dim",
      "k_dim",
      "v_dim",
      "cu_seqlens_kv",
      "cp_rank",
      "cp_size",
      "rotary_interleaved"
    ],
    "backward": [
      "ctx",
      "dk",
      "dv"
    ]
  },
  "fused_apply_mla_rope_for_kv": [
    "kv",
    "k_pos_emb",
    "cos",
    "sin",
    "emb_dim",
    "k_dim",
    "v_dim",
    "cu_seqlens_kv",
    "cp_rank",
    "cp_size",
    "rotary_interleaved"
  ],
  "calculate_logits_max": [
    "vocab_parallel_logits"
  ],
  "calculate_predicted_logits": [
    "vocab_parallel_logits",
    "target",
    "logits_max",
    "vocab_start_index",
    "vocab_end_index"
  ],
  "calculate_cross_entropy_loss": [
    "exp_logits",
    "predicted_logits_sum_exp_logits"
  ],
  "calculate_gradients": [
    "softmax",
    "grad_output",
    "target_mask",
    "masked_target_1d"
  ],
  "_VocabParallelCrossEntropy": {
    "forward": [
      "ctx",
      "vocab_parallel_logits",
      "target",
      "tp_group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "fused_vocab_parallel_cross_entropy": [
    "vocab_parallel_logits",
    "target",
    "tp_group"
  ],
  "weighted_squared_relu": [
    "x",
    "weights"
  ],
  "_squared_relu_back": [
    "g",
    "x"
  ],
  "weighted_squared_relu_back": [
    "g",
    "x",
    "weights"
  ],
  "WeightedSquaredReLUFunction": {
    "forward": [
      "ctx",
      "input",
      "weights"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "weighted_squared_relu_impl": [
    "input",
    "weights"
  ],
  "_pad_routing_map_kernel": [
    "routing_map_ptr",
    "output_ptr",
    "num_tokens",
    "pad_multiple",
    "BLOCK_SIZE"
  ],
  "fused_pad_routing_map": [
    "routing_map",
    "pad_multiple"
  ],
  "swiglu": [
    "y"
  ],
  "bias_swiglu": [
    "y",
    "bias"
  ],
  "weighted_swiglu": [
    "y",
    "weights"
  ],
  "swiglu_back": [
    "g",
    "y"
  ],
  "bias_swiglu_back": [
    "g",
    "y",
    "bias"
  ],
  "weighted_swiglu_back": [
    "g",
    "y",
    "weights"
  ],
  "BiasSwiGLUFunction": {
    "forward": [
      "ctx",
      "input",
      "bias",
      "fp8_input_store",
      "cpu_offload_input"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "SwiGLUFunction": {
    "forward": [
      "ctx",
      "input",
      "fp8_input_store",
      "cpu_offload_input"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "WeightedSwiGLUFunction": {
    "forward": [
      "ctx",
      "input",
      "weights",
      "fp8_input_store"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "bias_swiglu_impl": [
    "input",
    "bias",
    "fp8_input_store",
    "cpu_offload_input"
  ],
  "weighted_bias_swiglu_impl": [
    "input",
    "bias",
    "weights",
    "fp8_input_store"
  ],
  "ScaledUpperTriangMaskedSoftmax": {
    "forward": [
      "ctx",
      "inputs",
      "scale"
    ],
    "backward": [
      "ctx",
      "output_grads"
    ]
  },
  "ScaledMaskedSoftmax": {
    "forward": [
      "ctx",
      "inputs",
      "mask",
      "scale"
    ],
    "backward": [
      "ctx",
      "output_grads"
    ]
  },
  "ScaledSoftmax": {
    "forward": [
      "ctx",
      "inputs",
      "scale"
    ],
    "backward": [
      "ctx",
      "output_grads"
    ]
  },
  "SoftmaxOne": {
    "__init__": [
      "self",
      "dim",
      "denominator_offset"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FusedScaleMaskSoftmax": {
    "__init__": [
      "self",
      "input_in_fp16",
      "input_in_bf16",
      "attn_mask_type",
      "scaled_masked_softmax_fusion",
      "mask_func",
      "softmax_in_fp32",
      "scale",
      "window_size"
    ],
    "forward": [
      "self",
      "input",
      "mask",
      "softmax_offset"
    ],
    "is_kernel_available": [
      "self",
      "mask",
      "b",
      "np",
      "sq",
      "sk"
    ],
    "forward_fused_softmax": [
      "self",
      "input",
      "mask"
    ],
    "forward_torch_softmax": [
      "self",
      "input",
      "mask",
      "softmax_offset"
    ],
    "get_batch_per_block": [
      "sq",
      "sk",
      "b",
      "np"
    ]
  },
  "geglu": [
    "y"
  ],
  "bias_geglu": [
    "bias",
    "y"
  ],
  "geglu_back": [
    "g",
    "y"
  ],
  "bias_geglu_back": [
    "g",
    "y",
    "bias"
  ],
  "BiasGeGLUFunction": {
    "forward": [
      "ctx",
      "input",
      "bias"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "GeGLUFunction": {
    "forward": [
      "ctx",
      "input"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "bias_geglu_impl": [
    "input",
    "bias"
  ],
  "quick_geglu": [
    "y",
    "linear_offset"
  ],
  "weighted_quick_geglu": [
    "y",
    "weights",
    "linear_offset"
  ],
  "quick_geglu_back": [
    "g",
    "y",
    "linear_offset"
  ],
  "weighted_quick_geglu_back": [
    "g",
    "y",
    "weights",
    "linear_offset"
  ],
  "weighted_bias_quick_geglu": [
    "y",
    "bias",
    "weights",
    "linear_offset"
  ],
  "weighted_bias_quick_geglu_back": [
    "g",
    "y",
    "bias",
    "weights",
    "linear_offset"
  ],
  "WeightedQuickGeGLUFunction": {
    "forward": [
      "ctx",
      "input",
      "weights",
      "fp8_input_store",
      "linear_offset"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "WeightedBiasQuickGeGLUFunction": {
    "forward": [
      "ctx",
      "input",
      "bias",
      "weights",
      "fp8_input_store",
      "linear_offset"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "weighted_bias_quick_geglu_impl": [
    "input",
    "bias",
    "weights",
    "fp8_input_store",
    "linear_offset",
    "clamp_value"
  ],
  "MegatronTokenizerBase": {
    "__init__": [
      "self",
      "path",
      "config"
    ],
    "tokenize": [
      "self"
    ],
    "detokenize": [
      "self"
    ],
    "vocab": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "apply_chat_template": [
      "self"
    ]
  },
  "TOKENIZER_MAPPING_NAMES": [],
  "TOKENIZER_LIBRARIES": [],
  "MegatronTokenizer": {
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "tokenizer_path",
      "metadata_path"
    ],
    "write_metadata": [
      "tokenizer_path",
      "tokenizer_library",
      "model_type",
      "tokenizer_class",
      "chat_template",
      "overwrite",
      "metadata_path"
    ]
  },
  "_get_metadata_path": [
    "tokenizer_path"
  ],
  "TOKENIZER_MAPPING_LIBRARIES": [],
  "MegatronTokenizerText": {
    "__init__": [
      "self",
      "path",
      "config"
    ],
    "_restore_model": [
      "self"
    ],
    "tokenize": [
      "self",
      "text"
    ],
    "detokenize": [
      "self",
      "ids"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template"
    ],
    "save_pretrained": [
      "self",
      "path"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens"
    ],
    "additional_special_tokens_ids": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "vocab": [
      "self"
    ],
    "unique_identifiers": [
      "self"
    ],
    "pad": [
      "self"
    ],
    "pad_id": [
      "self"
    ],
    "eod": [
      "self"
    ],
    "bos": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "eos": [
      "self"
    ],
    "unk": [
      "self"
    ],
    "unk_id": [
      "self"
    ],
    "mask": [
      "self"
    ],
    "mask_id": [
      "self"
    ],
    "cls": [
      "self"
    ],
    "cls_id": [
      "self"
    ],
    "sep": [
      "self"
    ],
    "sep_id": [
      "self"
    ],
    "vocab_file": [
      "self"
    ],
    "merges_file": [
      "self"
    ],
    "inv_vocab": [
      "self"
    ]
  },
  "ByteLevelTokenizer": {
    "__init__": [
      "self",
      "special_tokens",
      "vocab_size",
      "_eos_id",
      "_pad_id",
      "_bos_id"
    ],
    "text_to_tokens": [
      "self",
      "text"
    ],
    "tokens_to_text": [
      "self",
      "tokens"
    ],
    "text_to_ids": [
      "self",
      "text"
    ],
    "ids_to_text": [
      "self",
      "ids"
    ],
    "tokens_to_ids": [
      "self",
      "tokens"
    ],
    "ids_to_tokens": [
      "self",
      "ids"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens"
    ],
    "pad_id": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "eod": [
      "self"
    ],
    "unk_id": [
      "self"
    ]
  },
  "MegatronTokenizerTextAbstract": {
    "text_to_tokens": [
      "self",
      "text"
    ],
    "tokens_to_text": [
      "self",
      "tokens"
    ],
    "tokens_to_ids": [
      "self",
      "tokens"
    ],
    "ids_to_tokens": [
      "self",
      "ids"
    ],
    "text_to_ids": [
      "self",
      "text"
    ],
    "ids_to_text": [
      "self",
      "ids"
    ],
    "add_special_tokens": [
      "self"
    ],
    "cls_id": [
      "self"
    ],
    "sep_id": [
      "self"
    ],
    "pad_id": [
      "self"
    ],
    "eod": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "mask_id": [
      "self"
    ]
  },
  "MegatronTokenizerChatTemplate": {
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template",
      "tokenize",
      "truncation",
      "max_length",
      "add_generation_prompt"
    ]
  },
  "NullTokenizer": {
    "__init__": [
      "self",
      "vocab_size"
    ],
    "text_to_ids": [
      "self",
      "text"
    ],
    "ids_to_text": [
      "self",
      "ids"
    ],
    "offsets": [
      "self",
      "ids",
      "text"
    ],
    "unique_identifiers": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "vocab": [
      "self"
    ],
    "inv_vocab": [
      "self"
    ],
    "cls": [
      "self"
    ],
    "sep": [
      "self"
    ],
    "mask": [
      "self"
    ],
    "eod": [
      "self"
    ],
    "additional_special_tokens_ids": [
      "self"
    ]
  },
  "PATTERN_TIKTOKEN_V1": [],
  "PATTERN_TIKTOKEN_V2": [],
  "DEFAULT_TIKTOKEN_MAX_VOCAB": [],
  "SPECIAL_TOKENS": [],
  "SPECIAL_TOKEN_TEMPLATE": [],
  "reload_mergeable_ranks": [
    "path",
    "max_vocab",
    "num_special_tokens"
  ],
  "TikTokenTokenizer": {
    "__init__": [
      "self",
      "tokenizer_path",
      "special_tokens",
      "num_special_tokens",
      "chat_template",
      "pattern",
      "vocab_size"
    ],
    "text_to_tokens": [
      "self",
      "text"
    ],
    "tokens_to_text": [
      "self",
      "tokens"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "tokens_to_ids": [
      "self",
      "tokens"
    ],
    "id_to_token": [
      "self",
      "token_id"
    ],
    "ids_to_tokens": [
      "self",
      "token_ids"
    ],
    "text_to_ids": [
      "self",
      "text"
    ],
    "ids_to_text": [
      "self",
      "tokens",
      "remove_special_tokens"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens_dict"
    ],
    "additional_special_tokens_ids": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "eod": [
      "self"
    ],
    "unk_id": [
      "self"
    ],
    "mask_id": [
      "self"
    ],
    "pad_id": [
      "self"
    ],
    "cls_id": [
      "self"
    ],
    "sep_id": [
      "self"
    ],
    "vocab": [
      "self"
    ],
    "decoder": [
      "self"
    ],
    "encoder": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "inv_vocab": [
      "self"
    ]
  },
  "torch_home": [],
  "MEGATRON_CACHE": [],
  "MEGATRON_CONFIG_MAP": [],
  "MegatronHFTokenizer": {
    "__init__": [
      "self",
      "tokenizer_path",
      "vocab_file",
      "merges_file"
    ],
    "_get_vocab_file": [
      "self",
      "tokenizer_name",
      "vocab_file"
    ],
    "_get_merges_file": [
      "self",
      "tokenizer_name",
      "merges_file"
    ],
    "_get_available_models_list": [
      "self"
    ],
    "_download": [
      "self",
      "path",
      "url"
    ]
  },
  "HuggingFaceTokenizer": {
    "__init__": [
      "self",
      "tokenizer_path",
      "vocab_file",
      "merges_file",
      "mask_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "sep_token",
      "cls_token",
      "unk_token",
      "additional_special_tokens",
      "use_fast",
      "trust_remote_code",
      "include_special_tokens",
      "chat_template"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens_dict"
    ],
    "additional_special_tokens_ids": [
      "self"
    ],
    "text_to_tokens": [
      "self",
      "text"
    ],
    "tokens_to_text": [
      "self",
      "tokens"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "tokens_to_ids": [
      "self",
      "tokens"
    ],
    "ids_to_tokens": [
      "self",
      "ids"
    ],
    "text_to_ids": [
      "self",
      "text"
    ],
    "ids_to_text": [
      "self",
      "ids",
      "remove_special_tokens"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template"
    ],
    "vocab": [
      "self"
    ],
    "inv_vocab": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "pad_id": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "eod": [
      "self"
    ],
    "sep_id": [
      "self"
    ],
    "cls_id": [
      "self"
    ],
    "unk_id": [
      "self"
    ],
    "mask_id": [
      "self"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ]
  },
  "SentencePieceTokenizer": {
    "__init__": [
      "self",
      "tokenizer_path",
      "special_tokens",
      "legacy",
      "ignore_extra_whitespaces",
      "chat_template",
      "trim_spm_separator_after_special_token",
      "spm_separator"
    ],
    "text_to_tokens": [
      "self",
      "text"
    ],
    "text_to_ids": [
      "self",
      "text",
      "sample_alpha"
    ],
    "_text_to_ids": [
      "self",
      "text",
      "sample_alpha"
    ],
    "_text_to_ids_extra_space": [
      "self",
      "text",
      "sample_alpha"
    ],
    "tokens_to_text": [
      "self",
      "tokens"
    ],
    "ids_to_text": [
      "self",
      "ids"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "ids_to_tokens": [
      "self",
      "ids"
    ],
    "tokens_to_ids": [
      "self",
      "tokens",
      "tokens_to_skip"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens"
    ],
    "pad_id": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "sep_id": [
      "self"
    ],
    "cls_id": [
      "self"
    ],
    "mask_id": [
      "self"
    ],
    "unk_id": [
      "self"
    ],
    "additional_special_tokens_ids": [
      "self"
    ],
    "vocab": [
      "self"
    ],
    "inv_vocab": [
      "self"
    ]
  },
  "MEGATRON_TOKENIZERS": [],
  "SP_TOKENIZERS": [],
  "build_tokenizer": [
    "args"
  ],
  "T5Tokenizer": {
    "__init__": [
      "self",
      "path",
      "config"
    ]
  },
  "GPTTokenizer": {
    "__init__": [
      "self",
      "path",
      "config"
    ]
  },
  "RetroTokenizer": {
    "__init__": [
      "self",
      "path",
      "config"
    ]
  },
  "MambaTokenizer": {
    "__init__": [
      "self",
      "path",
      "config"
    ]
  },
  "BertTokenizer": {
    "__init__": [
      "self",
      "path",
      "config"
    ]
  },
  "DefaultTokenizerText": {
    "__init__": [
      "self",
      "path",
      "config"
    ]
  },
  "MegatronGradScaler": {
    "__init__": [
      "self",
      "initial_scale"
    ],
    "scale": [
      "self"
    ],
    "inv_scale": [
      "self"
    ],
    "update": [
      "self",
      "found_inf"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "ConstantGradScaler": {
    "update": [
      "self",
      "found_inf"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "DynamicGradScaler": {
    "__init__": [
      "self",
      "initial_scale",
      "min_scale",
      "growth_factor",
      "backoff_factor",
      "growth_interval",
      "hysteresis"
    ],
    "update": [
      "self",
      "found_inf"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "_get_param_groups": [
    "model_chunks",
    "no_weight_decay_cond",
    "scale_lr_cond",
    "lr_mult",
    "lr",
    "min_lr",
    "decoupled_lr",
    "decoupled_min_lr",
    "default_skip_embedding_weight_decay"
  ],
  "_update_min_and_max_lr_in_param_groups": [
    "param_groups",
    "lr",
    "min_lr",
    "decoupled_lr",
    "decoupled_min_lr"
  ],
  "_get_param_groups_and_buffers": [
    "model_chunks",
    "model_chunk_offset",
    "config",
    "no_weight_decay_cond",
    "scale_lr_cond",
    "lr_mult",
    "filter_fn",
    "buffer_name",
    "default_skip_embedding_weight_decay"
  ],
  "_get_megatron_optimizer_based_on_param_groups": [
    "config",
    "model_chunks",
    "param_groups",
    "per_model_buffers",
    "model_parallel_group",
    "data_parallel_group",
    "data_parallel_group_gloo",
    "data_parallel_group_idx",
    "intra_dist_opt_group",
    "distributed_optimizer_instance_id"
  ],
  "get_megatron_optimizer": [
    "config",
    "model_chunks",
    "no_weight_decay_cond",
    "scale_lr_cond",
    "lr_mult",
    "use_gloo_process_groups",
    "default_skip_embedding_weight_decay",
    "pg_collection"
  ],
  "get_grad_norm_fp32": [
    "grads_for_norm",
    "norm_type",
    "grad_stats_parallel_group"
  ],
  "clip_grad_by_total_norm_fp32": [
    "parameters",
    "max_norm",
    "total_norm",
    "use_decoupled_grad"
  ],
  "count_zeros_fp32": [
    "parameters",
    "grad_stats_parallel_group",
    "use_decoupled_grad"
  ],
  "OptimizerConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "HAVE_APEX_OR_TE": [],
  "USING_TE_OPTIMIZER": [],
  "USING_APEX_OPTIMIZER": [],
  "Range": {
    "__init__": [
      "self",
      "start",
      "end"
    ],
    "normalize": [
      "self",
      "start"
    ],
    "__str__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__len__": [
      "self"
    ]
  },
  "DistributedOptimizer": {
    "_build_model_gbuf_param_range_map": [
      "cls",
      "param_world_index_map",
      "gbuf_world_range",
      "bucket_offset"
    ],
    "_build_model_gbuf_range": [
      "cls",
      "param_and_grad_buffer",
      "bucket_index"
    ],
    "_build_gbuf_range_map": [
      "cls",
      "param_and_grad_buffer"
    ],
    "_build_model_param_gbuf_map": [
      "cls",
      "gbuf_ranges"
    ],
    "_build_optimizer_group_ranges": [
      "cls",
      "param_groups",
      "gbuf_ranges"
    ],
    "_build_model_and_main_param_groups": [
      "cls",
      "gbuf_ranges",
      "param_gbuf_map",
      "opt_group_ranges",
      "config"
    ],
    "__init__": [
      "self",
      "optimizer",
      "config",
      "grad_scaler",
      "init_state_fn",
      "model_chunks",
      "per_model_buffers",
      "data_parallel_group",
      "data_parallel_group_gloo",
      "data_parallel_group_idx",
      "distributed_optimizer_instance_id"
    ],
    "_get_model_param_range_map": [
      "self",
      "param"
    ],
    "get_grad_stats_parallel_group": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "_get_main_param_and_optimizer_states": [
      "self",
      "model_param"
    ],
    "_set_main_param_and_optimizer_states": [
      "self",
      "model_param",
      "tensors"
    ],
    "get_parameter_state_dp_reshardable": [
      "self"
    ],
    "get_parameter_state_dp_zero": [
      "self",
      "use_gloo_comm",
      "empty_data",
      "return_on_all_ranks"
    ],
    "save_parameter_state": [
      "self",
      "filename"
    ],
    "_init_optimizer_states_with_dummy_values": [
      "self"
    ],
    "_param_name": [
      "self",
      "param"
    ],
    "sharded_state_dict": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "sharding_type",
      "metadata"
    ],
    "_param_groups_to_param2group_meta": [
      "self",
      "param_groups"
    ],
    "_param2group_meta_to_param_groups": [
      "self",
      "param_to_group_meta",
      "param_groups",
      "strict"
    ],
    "sharded_param_state_fsdp_dtensor": [
      "self",
      "is_loading"
    ],
    "sharded_param_state_dp_zero": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "metadata"
    ],
    "sharded_param_state_fully_reshardable": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "metadata"
    ],
    "sharded_param_state_dp_reshardable": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "metadata"
    ],
    "sharded_param_state_fs_model_space": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "metadata"
    ],
    "load_parameter_state_from_dp_reshardable": [
      "self",
      "state_dict"
    ],
    "load_parameter_state_from_fs_model_space": [
      "self",
      "state_dict"
    ],
    "_update_legacy_world_tensors": [
      "cls",
      "old_tensors",
      "new_numels"
    ],
    "load_parameter_state_from_dp_zero_legacy": [
      "self",
      "state_dict"
    ],
    "load_parameter_state_from_dp_zero": [
      "self",
      "state_dict"
    ],
    "load_parameter_state_from_fully_reshardable": [
      "self",
      "state_dict"
    ],
    "split_state_dict_if_needed": [
      "self",
      "state_dict"
    ],
    "load_parameter_state": [
      "self",
      "filename"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "_collect_main_grad_data_for_unscaling": [
      "self"
    ],
    "_get_model_and_main_params_data_float16": [
      "self"
    ],
    "_get_fp8_params_and_shard_fp32_from_fp8": [
      "self"
    ],
    "_copy_model_grads_to_main_grads": [
      "self"
    ],
    "_copy_main_params_to_model_params": [
      "self"
    ],
    "_copy_main_params_to_param_buffer": [
      "self"
    ],
    "_build_model_param_to_state_dict_param_map": [
      "self",
      "state_dict"
    ],
    "_copy_model_params_to_main_params": [
      "self",
      "state_dict"
    ],
    "step_with_ready_grads": [
      "self"
    ]
  },
  "_zero_grad_group_helper": [
    "group",
    "set_to_none",
    "use_decoupled_grad"
  ],
  "_multi_tensor_copy_this_to_that": [
    "this",
    "that",
    "overflow_buf"
  ],
  "param_group_identifier_keys": [],
  "MegatronOptimizer": {
    "__init__": [
      "self",
      "optimizer",
      "config",
      "init_state_fn"
    ],
    "get_parameters": [
      "self"
    ],
    "get_main_grads_for_grad_norm": [
      "self"
    ],
    "get_grad_stats_parallel_group": [
      "self"
    ],
    "prepare_grads": [
      "self"
    ],
    "step_with_ready_grads": [
      "self"
    ],
    "get_grad_norm": [
      "self"
    ],
    "clip_grad_norm": [
      "self",
      "clip_grad"
    ],
    "count_zeros": [
      "self"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "get_loss_scale": [
      "self"
    ],
    "scale_loss": [
      "self",
      "loss"
    ],
    "reload_model_params": [
      "self",
      "state_dict"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "_get_state": [
      "self"
    ],
    "_set_state": [
      "self",
      "value"
    ],
    "state": [],
    "_get_param_groups": [
      "self"
    ],
    "_set_param_groups": [
      "self",
      "value"
    ],
    "param_groups": [],
    "step": [
      "self"
    ],
    "sharded_state_dict": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "metadata"
    ],
    "_extract_common_per_param_step": [
      "state_dict"
    ],
    "_restore_common_per_param_step": [
      "state_dict",
      "step"
    ],
    "offload_to_cpu": [
      "self"
    ],
    "restore_from_cpu": [
      "self"
    ],
    "_filter_and_reorder_param_groups": [
      "current_groups",
      "state_dict_groups"
    ]
  },
  "MixedPrecisionOptimizer": {
    "__init__": [
      "self",
      "optimizer",
      "config",
      "grad_scaler",
      "init_state_fn"
    ],
    "get_loss_scale": [
      "self"
    ],
    "reload_model_params": [
      "self",
      "state_dict"
    ],
    "_unscale_main_grads_and_check_for_nan": [
      "self"
    ],
    "prepare_grads": [
      "self"
    ],
    "step_with_ready_grads": [
      "self"
    ],
    "step": [
      "self"
    ]
  },
  "Float16OptimizerWithFloat16Params": {
    "__init__": [
      "self",
      "optimizer",
      "config",
      "grad_scaler",
      "init_state_fn"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "_collect_main_grad_data_for_unscaling": [
      "self"
    ],
    "_get_model_and_main_params_data_float16": [
      "self"
    ],
    "_copy_model_grads_to_main_grads": [
      "self"
    ],
    "_copy_main_params_to_model_params": [
      "self"
    ],
    "_copy_model_params_to_main_params": [
      "self",
      "state_dict"
    ],
    "state_dict": [
      "self",
      "is_loading"
    ],
    "sharded_state_dict": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "metadata"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "FP32Optimizer": {
    "__init__": [
      "self",
      "optimizer",
      "config",
      "init_state_fn"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "get_loss_scale": [
      "self"
    ],
    "prepare_grads": [
      "self"
    ],
    "step_with_ready_grads": [
      "self"
    ],
    "step": [
      "self"
    ],
    "reload_model_params": [
      "self",
      "state_dict"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "sharded_state_dict": [
      "self",
      "model_sharded_state_dict",
      "is_loading",
      "metadata"
    ]
  },
  "ProxyDict": {
    "__init__": [
      "self",
      "inner_dicts"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "items": [
      "self"
    ]
  },
  "ChainedOptimizer": {
    "__init__": [
      "self",
      "chained_optimizers"
    ],
    "optimizer": [
      "self"
    ],
    "param_groups": [
      "self"
    ],
    "state": [
      "self"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "get_loss_scale": [
      "self"
    ],
    "_split_state_dict": [
      "self",
      "state_dict"
    ],
    "reload_model_params": [
      "self",
      "state_dict"
    ],
    "state_dict": [
      "self"
    ],
    "sharded_state_dict": [
      "self",
      "model_sharded_state_dict",
      "is_loading"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "prepare_grads": [
      "self"
    ],
    "step_with_ready_grads": [
      "self"
    ],
    "grads_states_parallel_group_is_shared": [
      "self"
    ],
    "get_grad_stats_parallel_group": [
      "self"
    ],
    "get_grad_norm": [
      "self"
    ],
    "count_zeros": [
      "self"
    ],
    "step": [
      "self"
    ],
    "save_parameter_state": [
      "self",
      "filename"
    ],
    "load_parameter_state": [
      "self",
      "filename"
    ],
    "_synchronize_steps": [
      "self"
    ],
    "offload_to_cpu": [
      "self"
    ],
    "restore_from_cpu": [
      "self"
    ]
  },
  "_param_generator": [
    "cpu_optimizer"
  ],
  "HybridDeviceOptimizer": {
    "__init__": [
      "self",
      "params",
      "offload_fraction",
      "cpu_optimizer_cls",
      "gpu_optimizer_cls",
      "param_update_in_fp32",
      "pin_cpu_grads",
      "pin_cpu_params",
      "overlap_cpu_optimizer_d2h_h2d"
    ],
    "_set_sub_optimizer_grads": [
      "self"
    ],
    "_register_param_copy_back_gpu_hook": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ],
    "_init_sub_optimizers": [
      "self"
    ],
    "build_cpu_optimizer_list": [
      "cpu_optimizer_cls",
      "cpu_param_groups"
    ],
    "_get_sub_optimizer_param_groups": [
      "self",
      "offload_fraction"
    ],
    "_sync_sub_optimizers_state_to_hdo": [
      "self"
    ],
    "_sync_hdo_state_to_sub_optimizers": [
      "self"
    ],
    "_sync_hdo_param_groups_to_sub_optimizers": [
      "self"
    ],
    "_move_new_state_to_right_device": [
      "self"
    ],
    "_update_fp32_params_by_new_state": [
      "self"
    ],
    "update_fp32_param_by_new_param": [
      "self"
    ],
    "_register_load_state_dict_hooks": [
      "self"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "dummy_step": [
      "self"
    ],
    "sub_optimizers": [
      "self"
    ]
  },
  "DataType": [],
  "ExportConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "TRT_MODEL_CONFIG": [],
  "TRT_MODEL_TYPE_STRING": [],
  "TRTLLMHelper": {
    "__init__": [
      "self"
    ],
    "_get_trtllm_config": [
      "self",
      "export_config",
      "world_size",
      "gpus_per_node",
      "vocab_size_padded",
      "dtype",
      "fp8_quantized",
      "fp8_kvcache"
    ],
    "_load_scaling_factors": [
      "self",
      "model_state_dict"
    ],
    "get_trtllm_pretrained_config_and_model_weights": [
      "self",
      "model_state_dict",
      "dtype",
      "export_config",
      "on_device_distributed_conversion",
      "vocab_size",
      "gpus_per_node",
      "state_dict_split_by_layer_numbers",
      "fp8_quantized",
      "fp8_kvcache"
    ],
    "_add_scales_to_converter": [
      "self",
      "converter",
      "scales",
      "fp8_kvcache"
    ],
    "_get_trtllm_pretrained_config_and_model_weights_in_distributed_setting": [
      "self",
      "model_state_dict",
      "dtype",
      "vocab_size",
      "gpus_per_node",
      "scales",
      "fp8_quantized",
      "fp8_kvcache"
    ],
    "_get_trtllm_pretrained_config_and_model_weights_list_on_single_device": [
      "self",
      "export_config",
      "model_state_dict",
      "dtype",
      "gpus_per_node",
      "state_dict_split_by_layer_numbers",
      "scales",
      "fp8_quantized",
      "fp8_kvcache"
    ],
    "build_and_save_engine": [
      "self",
      "engine_dir",
      "trtllm_model_weights",
      "trtllm_model_config",
      "max_input_len",
      "max_output_len",
      "max_batch_size",
      "lora_ckpt_list",
      "use_lora_plugin",
      "max_lora_rank",
      "lora_target_modules",
      "max_prompt_embedding_table_size",
      "paged_kv_cache",
      "remove_input_padding",
      "paged_context_fmha",
      "use_refit",
      "max_num_tokens",
      "max_seq_len",
      "opt_num_tokens",
      "max_beam_width",
      "tokens_per_block",
      "multiple_profiles",
      "gpt_attention_plugin",
      "gemm_plugin"
    ]
  },
  "TRTLLMLayers": {
    "position_embedding": [],
    "vocab_embedding": [],
    "lm_head": [],
    "final_layernorm_weight": [],
    "final_layernorm_bias": [],
    "input_layernorm_weight": [],
    "input_layernorm_bias": [],
    "attention_qkv_weight": [],
    "attention_qkv_bias": [],
    "attention_dense_weight": [],
    "attention_dense_bias": [],
    "attention_linear_weight": [],
    "mlp_fc_weight": [],
    "mlp_fc_bias": [],
    "post_layernorm_weight": [],
    "post_layernorm_bias": [],
    "mlp_projection_weight": [],
    "mlp_projection_bias": [],
    "ffn_fc_weight": [],
    "ffn_projection_weight": [],
    "ffn_linear_weight": [],
    "mlp_router_weight": [],
    "mlp_fc_weight_mixture_of_experts": [],
    "mlp_projection_weight_mixture_of_experts": [],
    "return_layer_name_and_number": [
      "layer_name"
    ],
    "rename_input_layer_names_to_trtllm_layer_names": [
      "model_state_dict",
      "trtllm_conversion_dict",
      "state_dict_split_by_layer_numbers"
    ]
  },
  "NON_TRANSFORMER_LAYERS_NAMES": [],
  "get_layer_name_without_prefix": [
    "layer"
  ],
  "TRTLLMEngineBuilder": {
    "build_and_save_engine": [
      "engine_dir",
      "trtllm_model_weights",
      "trtllm_model_config",
      "max_input_len",
      "max_output_len",
      "max_batch_size",
      "lora_ckpt_list",
      "use_lora_plugin",
      "max_lora_rank",
      "lora_target_modules",
      "max_prompt_embedding_table_size",
      "paged_kv_cache",
      "remove_input_padding",
      "paged_context_fmha",
      "use_refit",
      "max_num_tokens",
      "max_seq_len",
      "opt_num_tokens",
      "max_beam_width",
      "tokens_per_block",
      "multiple_profiles",
      "gpt_attention_plugin",
      "gemm_plugin",
      "reduce_fusion"
    ]
  },
  "GATED_ACTIVATION": [],
  "is_gated_activation": [
    "helper"
  ],
  "pad_vocab_size": [
    "vocab_size",
    "tp_size"
  ],
  "str_dtype_to_torch": [
    "dtype"
  ],
  "SingleDeviceTRTLLMModelWeightsConverter": {
    "__init__": [
      "self",
      "export_config",
      "transformer_config",
      "dtype",
      "multi_query_mode",
      "activation",
      "scales"
    ],
    "_convert_non_transformer_layer": [
      "self",
      "model_state_dict",
      "layer_name"
    ],
    "_cast_value": [
      "self",
      "val",
      "layer_name"
    ],
    "_convert_transformer_layer": [
      "self",
      "layer_name",
      "val"
    ],
    "convert": [
      "self",
      "model_state_dict",
      "trtllm_conversion_dict",
      "state_dict_split_by_layer_numbers"
    ],
    "get_padded_vocab_size": [
      "self"
    ],
    "get_local_model_weights_per_gpu": [
      "self",
      "mapping",
      "trtllm_model_config"
    ]
  },
  "DistributedTRTLLMModelWeightsConverter": {
    "__init__": [
      "self",
      "transformer_config",
      "dtype",
      "multi_query_mode",
      "activation",
      "scales"
    ],
    "_add_to_trtllm_model_weights": [
      "self",
      "val",
      "layer_name"
    ],
    "_convert_transformer_layer": [
      "self",
      "layer_name",
      "val"
    ],
    "_convert_non_transformer_layer": [
      "self",
      "model_state_dict",
      "layer_name"
    ],
    "_get_remove_vocab_padding": [
      "self",
      "layer_name",
      "model_state_dict",
      "tokenizer_vocab_size"
    ],
    "convert": [
      "self",
      "model_state_dict",
      "trtllm_conversion_dict",
      "tokenizer_vocab_size"
    ]
  },
  "DEFAULT_CONVERSION_DICT": [],
  "NEMOTRON_NAS_CONVERSION_DICT": [],
  "_BaseDataParallel": {
    "__init__": [
      "self",
      "config",
      "module"
    ],
    "forward": [
      "self"
    ],
    "no_sync": [
      "self"
    ],
    "start_grad_sync": [
      "self"
    ],
    "scale_gradients": [
      "self",
      "scaling_factor"
    ],
    "finish_grad_sync": [
      "self"
    ],
    "zero_grad_buffer": [
      "self"
    ],
    "broadcast_params": [
      "self"
    ],
    "state_dict": [
      "self",
      "prefix",
      "keep_vars",
      "destination"
    ],
    "state_dict_for_save_checkpoint": [
      "self",
      "prefix",
      "keep_vars"
    ],
    "load_state_dict": [
      "self",
      "state_dict",
      "strict"
    ]
  },
  "TorchFullyShardedDataParallel": {
    "__init__": [
      "self",
      "config",
      "ddp_config",
      "module",
      "sub_modules_to_wrap",
      "disable_bucketing",
      "process_group"
    ],
    "load_state_dict": [
      "self",
      "state_dict",
      "strict"
    ]
  },
  "_get_main_grad_attr": [
    "param"
  ],
  "_unshard_if_dtensor": [
    "tensor"
  ],
  "_reshard_if_dtensor": [
    "tensor_to_shard",
    "reference_tensor"
  ],
  "_allreduce_conditional_embedding_grads": [
    "model",
    "config",
    "pp_group"
  ],
  "_get_shared_word_embedding_weight": [
    "model_module",
    "config"
  ],
  "_get_position_embedding_weight": [
    "model_module"
  ],
  "_allreduce_word_embedding_grads": [
    "model",
    "config",
    "embd_group",
    "pp_group"
  ],
  "_allreduce_embedding_grad": [
    "model",
    "embd_group",
    "pp_group",
    "weight_getter",
    "skip_if_none"
  ],
  "_allreduce_position_embedding_grads": [
    "model",
    "config",
    "pos_emb_group",
    "pp_group"
  ],
  "reset_model_temporary_tensors": [
    "config",
    "model"
  ],
  "_update_router_expert_bias": [
    "model",
    "config"
  ],
  "_allreduce_non_tensor_model_parallel_grads": [
    "model",
    "config",
    "tp_group"
  ],
  "_allreduce_layernorm_grads": [],
  "finalize_model_grads": [
    "model",
    "num_tokens",
    "pg_collection"
  ],
  "DistributedDataParallel": {
    "__init__": [
      "self",
      "config",
      "ddp_config",
      "module",
      "disable_bucketing",
      "pg_collection"
    ],
    "enable_forward_pre_hook": [
      "self"
    ],
    "disable_forward_pre_hook": [
      "self",
      "param_sync"
    ],
    "_make_forward_pre_hook": [
      "self"
    ],
    "_make_backward_post_hook": [
      "self",
      "param"
    ],
    "no_sync": [
      "self"
    ],
    "start_param_sync": [
      "self"
    ],
    "start_grad_sync": [
      "self"
    ],
    "finish_grad_sync": [
      "self"
    ],
    "scale_gradients": [
      "self",
      "scaling_factor"
    ],
    "zero_grad_buffer": [
      "self"
    ],
    "broadcast_params": [
      "self"
    ]
  },
  "TorchFullyShardedDataParallelConfig": {},
  "BufferType": {
    "PARAM": [],
    "GRAD": []
  },
  "shard_buffer": [
    "buffer",
    "data_parallel_world_size"
  ],
  "_ParamAndGradBucket": {
    "__init__": [
      "self",
      "params",
      "param_data",
      "grad_data",
      "offset",
      "numel_unpadded",
      "gradient_scaling_factor",
      "bucket_id"
    ]
  },
  "_ParamAndGradBucketGroup": {
    "__init__": [
      "self",
      "buckets",
      "ddp_config",
      "collective_group",
      "collective_group_size"
    ],
    "reset": [
      "self"
    ],
    "check_grads": [
      "self",
      "check_for_nan_or_inf",
      "check_for_large"
    ],
    "start_param_sync": [
      "self",
      "force_sync"
    ],
    "finish_param_sync": [
      "self",
      "skip_next_bucket_dispatch"
    ],
    "start_grad_sync": [
      "self"
    ],
    "finish_grad_sync": [
      "self"
    ],
    "register_grad_ready": [
      "self",
      "param"
    ]
  },
  "_ParamAndGradBuffer": {
    "__init__": [
      "self",
      "ddp_config",
      "param_dtype",
      "grad_dtype",
      "params",
      "data_parallel_group",
      "bucket_size",
      "param_to_name",
      "gradient_scaling_factor",
      "param_indices",
      "nccl_ub",
      "pg_collection"
    ],
    "scale_gradients": [
      "self",
      "scaling_factor"
    ],
    "_get": [
      "self",
      "shape",
      "start_index",
      "buffer_type"
    ],
    "_new_bucket": [
      "self",
      "bucket_params",
      "start_index",
      "end_index",
      "numel_unpadded",
      "bucket_id"
    ],
    "reset": [
      "self"
    ]
  },
  "partition_buckets": [
    "buffers",
    "force_single_bucket_group"
  ],
  "DistributedDataParallelConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "FullyShardedDataParallel": {
    "__init__": [
      "self",
      "config",
      "ddp_config",
      "module",
      "fsdp_unit_modules",
      "disable_bucketing",
      "device",
      "pg_collection"
    ],
    "load_state_dict": [
      "self",
      "state_dict",
      "strict"
    ],
    "_init_dist_index": [
      "self",
      "pg_collection"
    ],
    "stop_communication": [
      "self"
    ]
  },
  "_get_hsdp_tp_mesh": [
    "outer_fsdp_dp_group",
    "dp_cp_group",
    "tp_group"
  ],
  "_get_dp_tp_mesh": [
    "dp_cp_group",
    "tp_group"
  ],
  "_check_mesh_ranks_and_group_ranks_are_consistent": [
    "mesh_ranks",
    "group_ranks"
  ],
  "_MODEL_PARALLEL_RNG_TRACKER_NAME": [],
  "get_mesh_names": [
    "device_mesh",
    "only_submesh_dims"
  ],
  "contains_submesh": [
    "device_mesh",
    "submesh_names"
  ],
  "_get_cuda_rng_state": [
    "device",
    "clone",
    "graph_safe"
  ],
  "_set_cuda_rng_state": [
    "new_state",
    "device",
    "graph_safe"
  ],
  "initialize_rng_tracker": [
    "use_te_rng_tracker",
    "inference_rng_tracker",
    "use_cudagraphable_rng",
    "force_reset"
  ],
  "get_cuda_rng_tracker": [
    "use_te_rng_tracker",
    "inference_rng_tracker",
    "use_cudagraphable_rng"
  ],
  "FSDPDistributedIndex": {
    "__init__": [
      "self",
      "device_mesh",
      "dp_shard_dim",
      "dp_outer_dim",
      "tp_dim",
      "hybrid_fsdp_group",
      "hsdp_outer_dp_shard"
    ],
    "get_submesh": [
      "self",
      "mesh_dim_names"
    ],
    "get_dp_group": [
      "self",
      "is_expert_parallel"
    ],
    "get_fsdp_group": [
      "self",
      "is_expert_parallel"
    ],
    "get_outer_fsdp_group": [
      "self"
    ],
    "get_root_mesh": [
      "self",
      "is_expert_parallel"
    ],
    "get_logical_hybrid_fsdp_rank": [
      "self"
    ]
  },
  "create_updated_function_signature": [
    "original_function"
  ],
  "TrainingState": {
    "FORWARD": [],
    "PRE_BACKWARD": [],
    "POST_BACKWARD": [],
    "IDLE": []
  },
  "MegatronFSDP": {
    "__init__": [
      "self",
      "module",
      "dist_index",
      "ddp_config",
      "fsdp_unit_modules",
      "disable_bucketing",
      "device",
      "calculate_per_token_loss",
      "init_model_with_meta_device",
      "sync_model_each_microbatch",
      "keep_fp8_transpose_cache",
      "nccl_ub",
      "fsdp_double_buffer",
      "disable_symmetric_registration"
    ],
    "_check_module_parameter_types": [
      "self"
    ],
    "_init_fsdp_param_and_grad_buffer": [
      "self"
    ],
    "_import_class_from_path": [
      "self",
      "class_path"
    ],
    "all_gather_and_wait_parameters_ready": [
      "self",
      "params",
      "prefetch",
      "prefetch_order",
      "wait_bucket_ready"
    ],
    "_register_fsdp_hooks": [
      "self",
      "root_module"
    ],
    "no_sync": [
      "self"
    ],
    "sync": [
      "self"
    ],
    "set_model_auto_sync": [
      "self",
      "sync_model"
    ],
    "get_distributed_index": [
      "self"
    ],
    "start_param_sync": [
      "self"
    ],
    "start_grad_sync": [
      "self"
    ],
    "synchronize_param_gather": [
      "self"
    ],
    "synchronize_gradient_reduce": [
      "self"
    ],
    "attach_grad_to_optimizer_state": [
      "self"
    ],
    "finish_grad_sync": [
      "self"
    ],
    "_replace_param_with_distributed_if_needed": [
      "self"
    ],
    "_replace_param_with_raw_if_needed": [
      "self"
    ],
    "_reestablish_shared_weights": [
      "self",
      "old_params",
      "new_params"
    ],
    "scale_gradients": [
      "self",
      "scaling_factor"
    ],
    "zero_grad_buffer": [
      "self"
    ],
    "install_optimized_model_weights": [
      "self"
    ],
    "broadcast_params": [
      "self"
    ],
    "forward": [
      "self"
    ]
  },
  "RegisterFSDPBackwardFunction": {
    "forward": [
      "ctx",
      "post_backward"
    ],
    "backward": [
      "ctx"
    ]
  },
  "_replace_module_parameter": [
    "module",
    "name",
    "new_param"
  ],
  "ShardingStrategy": {
    "NO_SHARD": [],
    "OPTIM": [],
    "OPTIM_GRADS": [],
    "OPTIM_GRADS_PARAMS": []
  },
  "experimental_api": [
    "func"
  ],
  "fully_shard_model": [
    "module",
    "device_mesh",
    "dp_shard_dim",
    "dp_outer_dim",
    "tp_dim",
    "hybrid_fsdp_group",
    "fsdp_unit_modules",
    "zero_dp_strategy",
    "outer_dp_sharding_strategy",
    "device",
    "init_model_with_meta_device",
    "grad_reduce_in_fp32",
    "preserve_fp32_weights",
    "overlap_grad_reduce",
    "overlap_param_gather",
    "sync_model_each_microbatch",
    "preproc_state_dict_for_dcp_ckpt",
    "check_for_nan_in_grad",
    "average_in_collective",
    "disable_bucketing",
    "calculate_per_token_loss",
    "keep_fp8_transpose_cache",
    "nccl_ub",
    "fsdp_double_buffer",
    "disable_symmetric_registration"
  ],
  "fully_shard_optimizer": [
    "optimizer",
    "preproc_state_dict_for_dcp_ckpt"
  ],
  "fully_shard": [
    "module",
    "optimizer",
    "device_mesh",
    "dp_shard_dim",
    "dp_outer_dim",
    "tp_dim",
    "hybrid_fsdp_group",
    "fsdp_unit_modules",
    "zero_dp_strategy",
    "outer_dp_sharding_strategy",
    "device",
    "init_model_with_meta_device",
    "grad_reduce_in_fp32",
    "preserve_fp32_weights",
    "overlap_grad_reduce",
    "overlap_param_gather",
    "sync_model_each_microbatch",
    "preproc_state_dict_for_dcp_ckpt",
    "check_for_nan_in_grad",
    "average_in_collective",
    "disable_bucketing",
    "calculate_per_token_loss",
    "keep_fp8_transpose_cache",
    "nccl_ub",
    "fsdp_double_buffer",
    "disable_symmetric_registration"
  ],
  "gather_and_compute_chunk_metadata": [
    "dtensor"
  ],
  "update_uneven_dtensor_chunk_metadata": [
    "dtensor"
  ],
  "validate_uneven_dtensor": [
    "dtensor"
  ],
  "filter_unflattened_state_dict": [
    "state_dict",
    "key_chain",
    "visit_condition"
  ],
  "get_unflattened_state_dict": [
    "state_dict",
    "key_chain"
  ],
  "preprocess_state_dict_for_uneven_dtensor": [
    "state_dict"
  ],
  "gather_uneven_dtensor_to_full_tensor": [
    "dtensor",
    "target_device"
  ],
  "_assemble_full_tensor_from_uneven_chunks": [
    "dtensor",
    "all_chunk_info",
    "process_group",
    "target_device"
  ],
  "_intersection": [
    "s1",
    "s2"
  ],
  "_offset_slice": [
    "s",
    "offset"
  ],
  "split_dtensor": [
    "dtensor",
    "split_size_or_sections",
    "dim",
    "update_uneven_dtensor_chunk_meta"
  ],
  "NCCL_MEMORY_POOL": [],
  "_p_assert": [
    "cond",
    "s",
    "raise_assertion_error"
  ],
  "_alloc_storage": [
    "tensor",
    "size"
  ],
  "_free_storage": [
    "tensor"
  ],
  "TensorItemIndex": [],
  "BucketIndex": [],
  "ShardBucketIndex": [],
  "MultiGroupUBRAllocator": {
    "__init__": [
      "self",
      "pool",
      "groups"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "BucketingPolicy": {},
  "_pad": [
    "number_to_be_padded",
    "divisor"
  ],
  "build_data_parallel_buffer_index": [
    "elements",
    "data_parallel_rank",
    "data_parallel_world_size",
    "is_data_distributed",
    "ddp_config",
    "bucket_id",
    "chunk_size_factor"
  ],
  "_get_dp_buffer_shard_bucket_index": [
    "bucket_index",
    "is_data_distributed",
    "data_parallel_world_size",
    "data_parallel_rank"
  ],
  "Bucket": {},
  "TemporaryBucketAllocator": {
    "__init__": [
      "self"
    ],
    "allocate": [
      "self",
      "bucket_id",
      "size",
      "dtype",
      "device",
      "mem_alloc_context"
    ],
    "free": [
      "self",
      "bucket_id"
    ]
  },
  "StorageResizeBasedBucketAllocator": {
    "__init__": [
      "self"
    ],
    "allocate": [
      "self",
      "bucket_id",
      "size",
      "dtype",
      "device",
      "mem_alloc_context"
    ],
    "free": [
      "self",
      "bucket_id"
    ]
  },
  "RotaryBucketAllocator": {
    "__init__": [
      "self",
      "name"
    ],
    "allocate": [
      "self",
      "bucket_id",
      "size",
      "dtype",
      "device",
      "mem_alloc_context"
    ],
    "_get_gbuf_name": [
      "self",
      "buffer_id"
    ],
    "free": [
      "self",
      "bucket_id"
    ]
  },
  "FixedPoolAllocator": {
    "__init__": [
      "self",
      "name",
      "fsdp_param_groups",
      "size"
    ],
    "_is_two_bucket_group_equal": [
      "self",
      "group_a",
      "group_b"
    ],
    "allocate": [
      "self",
      "bucket_id",
      "size",
      "dtype",
      "device",
      "mem_alloc_context"
    ],
    "_get_gbuf_name": [
      "self",
      "buf_group_id",
      "bucket_index"
    ],
    "free": [
      "self",
      "bucket_id"
    ]
  },
  "DataParallelBuffer": {
    "__init__": [
      "self",
      "ddp_config",
      "params",
      "is_data_distributed",
      "bucket_id",
      "dtype",
      "device",
      "data_parallel_group",
      "dp_rank",
      "temporary_bucket_allocator",
      "is_dtype_float8",
      "gradient_scaling_factor",
      "chunk_size_factor",
      "mem_alloc_context",
      "item_index_map",
      "bucket_index",
      "shard_bucket_index"
    ],
    "init_data": [
      "self",
      "data"
    ],
    "fetch_bucket": [
      "self",
      "dtype",
      "set_param_data"
    ],
    "free_bucket_storage": [
      "self"
    ],
    "_get_item_slice_in_shard": [
      "self",
      "item_id"
    ],
    "locate_item_in_global_item": [
      "self",
      "item_id"
    ],
    "_get_item_local_shard_index": [
      "self",
      "item_id"
    ],
    "_get_item_local_index": [
      "self",
      "item_id"
    ],
    "set_item": [
      "self",
      "item_id",
      "item_data"
    ],
    "get_item": [
      "self",
      "item_id",
      "only_shard"
    ],
    "get_item_from_bucket": [
      "self",
      "bucket",
      "item_id"
    ],
    "get_shard_from_bucket": [
      "self",
      "bucket"
    ],
    "get_shard_from_local_buffer": [
      "self"
    ]
  },
  "ParameterGroup": {},
  "_get_parameter_groups": [
    "module",
    "policy",
    "meta_device_init_fp8_params",
    "bucket_group_by_fsdp_unit"
  ],
  "ParamAndGradBuffer": {
    "__init__": [
      "self",
      "ddp_config",
      "module",
      "bucketing_policy",
      "dist_index",
      "preserve_fp32_weights",
      "grad_reduce_in_fp32",
      "gradient_scaling_factor",
      "expert_gradient_scaling_factor",
      "device",
      "only_create_grad_buffer_and_main_weight_buffer_for_param_requires_grad",
      "reset_parameters_for_meta_device_init_module"
    ],
    "get_mem_alloc_context": [
      "self",
      "groups"
    ],
    "_log_parameter_groups": [
      "self"
    ],
    "_init_each_parameter_group_buffers": [
      "self",
      "meta_device_init_fp8_params"
    ],
    "_reset_parameters": [
      "self",
      "old_params",
      "new_params"
    ],
    "scale_gradients": [
      "self",
      "scaling_factor"
    ],
    "zero_grad": [
      "self"
    ],
    "_init_distributed_params": [
      "self"
    ],
    "_init_optimizer_named_parameters": [
      "self"
    ],
    "update_main_grads": [
      "self"
    ],
    "num_buckets": [
      "self"
    ],
    "copy_main_weights_to_model_weights": [
      "self"
    ],
    "copy_model_weights_to_main_weights": [
      "self"
    ],
    "all_gather_parameters": [
      "self",
      "async_op"
    ],
    "reduce_scatter_gradients": [
      "self",
      "async_op"
    ],
    "all_reduce_gradients": [
      "self",
      "async_op"
    ]
  },
  "BucketStatus": {
    "EMPTY": [],
    "COMMUNICATING": [],
    "READY_TO_USE": []
  },
  "GradReducePipeline": {
    "__init__": [
      "self",
      "param_and_grad_buffer",
      "rs_stream",
      "check_nans"
    ],
    "num_buckets": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "reduce_gradients": [
      "self",
      "params",
      "suggested_queue_capacity",
      "outer_fsdp_group_grad_reduce"
    ],
    "wait_for_previous_grad_reduce": [
      "self",
      "suggested_queue_size",
      "suggested_queue_capacity"
    ],
    "_enforce_double_buffer_limit": [
      "self",
      "add_buckets"
    ],
    "get_ready_bucket_group_for_reduction": [
      "self",
      "bucket_id"
    ],
    "get_fsdp_buffer": [
      "self",
      "bucket_id"
    ],
    "_bucket_group_gradient_reduce": [
      "self",
      "bucket_group",
      "async_op",
      "outer_fsdp_group_grad_reduce"
    ]
  },
  "PrefetchOrder": {
    "FORWARD_PASS_ORDER": [],
    "BACKWARD_PASS_ORDER": []
  },
  "AllGatherPipeline": {
    "__init__": [
      "self",
      "param_and_grad_buffer",
      "ag_stream"
    ],
    "num_buckets": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "all_gather_params": [
      "self",
      "params",
      "prefetch",
      "prefetch_order",
      "suggested_AG_prefetch_size",
      "async_param_gather",
      "outer_fsdp_group_param_gather"
    ],
    "wait_bucket_ready": [
      "self",
      "bucket_id",
      "empty_ok"
    ],
    "release_bucket": [
      "self",
      "bucket_id"
    ],
    "recycle_unused_buckets": [
      "self"
    ],
    "get_fsdp_buffer": [
      "self",
      "bucket_id"
    ],
    "async_bucket_gather": [
      "self",
      "bucket_id"
    ]
  },
  "gradient_reduce_preprocessing": [
    "grad_data",
    "scaling_factor",
    "ddp_config"
  ],
  "check_gpu_memory": [
    "threshold"
  ],
  "ResetParametersContext": {
    "__init__": [
      "self",
      "init_param_with_fp8",
      "with_cuda_rng_tracker"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "override_sharded_param_methods_with_safety_checks": [
    "params",
    "all_gather_pipeline"
  ],
  "_dtype_size": [
    "dtype"
  ],
  "_get_fsdp_tensor_spec": [
    "param",
    "dist_index",
    "is_sharded_param"
  ],
  "make_fsdp_dtensor": [
    "local_tensor",
    "param",
    "dist_index",
    "is_sharded_param",
    "is_expert_param",
    "run_check",
    "update_uneven_dtensor_chunk_meta"
  ],
  "CONFIG_FNAME": [],
  "CheckpointingException": {},
  "CheckpointingConfig": {},
  "check_is_distributed_checkpoint": [
    "checkpoint_dir"
  ],
  "maybe_load_config": [
    "checkpoint_dir"
  ],
  "save_config": [
    "config",
    "checkpoint_dir"
  ],
  "_ShardId": [],
  "zip_strict": [],
  "_sharded_tensor_shard_id": [
    "sharded_tensor"
  ],
  "_sharded_object_id": [
    "sharded_object"
  ],
  "extract_sharded_tensors": [
    "sharded_state_dict"
  ],
  "extract_sharded_tensors_and_factories": [
    "sharded_state_dict"
  ],
  "extract_sharded_tensors_or_nonpersistent": [
    "sharded_state_dict"
  ],
  "extract_sharded_base": [
    "sharded_state_dict"
  ],
  "extract_nonpersistent": [
    "sharded_state_dict"
  ],
  "add_prefix_for_sharding": [
    "sharded_state_dict",
    "prefix"
  ],
  "replace_prefix_for_sharding": [
    "sharded_state_dict",
    "old_prefix",
    "new_prefix"
  ],
  "apply_prefix_mapping": [
    "sharded_state_dict",
    "prefix_map"
  ],
  "force_all_tensors_to_non_fp8": [
    "sharded_state_dict"
  ],
  "fallback_logger": [],
  "__LOGGER_NAME_STACK": [],
  "__LOGGER_STACK": [],
  "logger_stack": [
    "name",
    "current_logger"
  ],
  "debug_time": [
    "name",
    "logger",
    "threshold",
    "level"
  ],
  "debug_msg": [
    "msg"
  ],
  "MCoreTensorAwareStateDict": {
    "_validate_params": [
      "algo"
    ],
    "_get_distribution": [
      "fully_parallel",
      "sharded_part",
      "parallelization_group",
      "cached_distribution"
    ],
    "_remove_redundant_data": [
      "fully_parallel",
      "sharded_part",
      "shard_to_saving_rank",
      "parallelization_group"
    ],
    "from_state_dict": [
      "cls",
      "sharded_state_dict",
      "algo",
      "parallelization_group",
      "cached_metadata"
    ],
    "is_hollow": [
      "self"
    ],
    "_sharded_tensors": [
      "self"
    ],
    "tensors": [
      "self"
    ],
    "common_state_dict": [
      "self"
    ],
    "pop_tensors": [
      "self"
    ],
    "insert_tensors": [
      "self",
      "tensor_data"
    ],
    "init_tensors": [
      "self"
    ],
    "copy_tensors_to_cpu": [
      "self",
      "non_blocking"
    ],
    "restore_tensor_device": [
      "self",
      "non_blocking"
    ],
    "_insert_sharded_data": [
      "self",
      "fully_parallel",
      "sharded_part",
      "parallelization_group",
      "exchange_algo"
    ],
    "to_state_dict": [
      "self",
      "sharded_state_dict",
      "algo",
      "exchange_algo",
      "validate_access_integrity",
      "parallelization_group",
      "strict",
      "return_mismatch_keys"
    ]
  },
  "extract_matching_values": [
    "x",
    "predicate",
    "return_lists_as_dicts"
  ],
  "diff": [
    "x1",
    "x2",
    "prefix"
  ],
  "inspect_types": [
    "x",
    "prefix",
    "indent"
  ],
  "nested_values": [
    "x"
  ],
  "nested_items_iter": [
    "x"
  ],
  "dict_map": [
    "f",
    "d"
  ],
  "dict_map_with_key": [
    "f",
    "d"
  ],
  "dict_list_map_inplace": [
    "f",
    "x"
  ],
  "dict_list_map_outplace": [
    "f",
    "x"
  ],
  "merge": [
    "x1",
    "x2",
    "key"
  ],
  "map_reduce": [
    "xs",
    "key_fn",
    "value_fn",
    "reduce_fn"
  ],
  "save_preprocess": [
    "sharded_state_dict",
    "validate_access_integrity",
    "preprocess_common_before_consistancy_check"
  ],
  "load_preprocess": [
    "sharded_state_dict"
  ],
  "filter_out_empty_flatten_tensor": [
    "sharded_state_dict"
  ],
  "_LocalMetadata": [],
  "_GlobalMetadata": [],
  "StrictHandling": {
    "ASSUME_OK_UNEXPECTED": [],
    "LOG_UNEXPECTED": [],
    "LOG_ALL": [],
    "RAISE_UNEXPECTED": [],
    "RAISE_ALL": [],
    "RETURN_UNEXPECTED": [],
    "RETURN_ALL": [],
    "IGNORE_ALL": [],
    "requires_explicit_ckpt_mismatch_check": [
      "val"
    ],
    "requires_global_app_metadata": [
      "val"
    ],
    "requires_returning_mismatch_keys": [
      "val"
    ]
  },
  "parse_strict_flag": [
    "strict"
  ],
  "validate_integrity_and_strict_load": [
    "sharded_state_dict",
    "strict",
    "validate_access_integrity",
    "local_metadata",
    "global_metadata",
    "ckpt_sharded_metadata"
  ],
  "verify_checkpoint_and_load_strategy": [
    "checkpoint_dir",
    "sharded_strategy",
    "common_strategy"
  ],
  "adjust_non_strict_load": [
    "sharded_state_dict",
    "sharded_keys_to_remove"
  ],
  "_determine_missing_and_unexpected_keys": [
    "ckpt_sharded_metadata",
    "local_metadata",
    "global_metadata"
  ],
  "maybe_report_missing_and_unexpected_keys": [
    "missing_keys",
    "unexpected_keys",
    "raise_error"
  ],
  "_validate_common_state_dict": [
    "common_state_dict"
  ],
  "validate_sharding_integrity": [
    "global_metadata",
    "common_state_dict"
  ],
  "_validate_sharding_for_key": [
    "rank_sharding"
  ],
  "_compute_shards_access": [
    "rank_sharding"
  ],
  "_validate_sharding_for_key_flattened": [
    "tensors_by_shard"
  ],
  "_validate_objects_for_key": [
    "sharded_objects"
  ],
  "determine_global_metadata": [
    "sharded_state_dict"
  ],
  "validate_sharded_objects_handling": [
    "sharded_strategy",
    "common_strategy"
  ],
  "KEEP_VARS_HINT": [],
  "get_optim_param_to_id_map": [
    "optim_params_iter"
  ],
  "get_param_id_to_sharded_param_map": [
    "model_sharded_state_dict",
    "optim_params_iter"
  ],
  "make_sharded_optimizer_tensor": [
    "model_param",
    "optim_param",
    "prefix"
  ],
  "optim_state_to_sharding_state": [
    "optim_state_dict",
    "id_to_sharded_param_map",
    "exclude_keys"
  ],
  "CkptShardedMetadata": [],
  "_CONTENT_METADATA_KEY": [],
  "load": [
    "sharded_state_dict",
    "checkpoint_dir",
    "sharded_strategy",
    "common_strategy",
    "validate_access_integrity",
    "strict"
  ],
  "load_common_state_dict": [
    "checkpoint_dir"
  ],
  "load_tensors_metadata": [
    "checkpoint_dir",
    "sharded_strategy"
  ],
  "load_sharded_metadata": [
    "checkpoint_dir",
    "sharded_strategy",
    "common_strategy"
  ],
  "load_plain_tensors": [
    "checkpoint_dir"
  ],
  "load_content_metadata": [
    "checkpoint_dir"
  ],
  "remove_sharded_tensors": [
    "checkpoint_dir",
    "key_prefix"
  ],
  "save": [
    "sharded_state_dict",
    "checkpoint_dir",
    "sharded_strategy",
    "common_strategy",
    "validate_access_integrity",
    "async_sharded_save",
    "preprocess_common_before_consistancy_check",
    "content_metadata"
  ],
  "get_default_save_sharded_strategy": [
    "backend",
    "version"
  ],
  "get_default_save_common_strategy": [
    "backend",
    "version"
  ],
  "get_default_load_sharded_strategy": [
    "checkpoint_dir"
  ],
  "StateDict": [],
  "CommonStateDict": [],
  "ShardedStateDict": [],
  "ReplicaId": [],
  "_logged_deprecations": [],
  "ShardedBase": {
    "validate_metadata_integrity": [
      "self"
    ],
    "without_data": [
      "self"
    ]
  },
  "ShardedTensor": {
    "__post_init__": [
      "self"
    ],
    "validate_metadata_integrity": [
      "self"
    ],
    "has_regular_grid": [
      "self"
    ],
    "global_slice": [
      "self"
    ],
    "global_coordinates": [
      "self"
    ],
    "local_coordinates": [
      "self"
    ],
    "local_chunk_offset_in_global": [
      "self"
    ],
    "max_allowed_chunks": [
      "self"
    ],
    "without_data": [
      "self"
    ],
    "from_rank_offsets": [
      "cls",
      "key",
      "data"
    ],
    "from_rank_offsets_flat": [
      "cls",
      "key",
      "data",
      "non_flat_local_shape"
    ],
    "init_data": [
      "self",
      "device",
      "init_fn"
    ],
    "narrow": [
      "self",
      "dim",
      "start",
      "length"
    ]
  },
  "is_main_replica": [
    "replica_id"
  ],
  "LocalNonpersistentObject": {
    "__init__": [
      "self",
      "obj"
    ],
    "unwrap": [
      "self"
    ]
  },
  "ShardedObject": {
    "__post_init__": [
      "self"
    ],
    "validate_metadata_integrity": [
      "self"
    ],
    "without_data": [
      "self"
    ],
    "unique_key": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "empty_from_unique_key": [
      "cls",
      "unique_key",
      "replica_id"
    ]
  },
  "FactoryBuildFn": [],
  "FactoryMergeFn": [],
  "ShardedTensorFactory": {
    "build": [
      "self"
    ],
    "validate_metadata_integrity": [
      "self"
    ],
    "without_data": [
      "self"
    ]
  },
  "apply_factories": [
    "sharded_state_dict"
  ],
  "apply_factory_merges": [
    "x1",
    "x2",
    "key"
  ],
  "ShardDistribution": {},
  "_shard_size": [
    "sh_ten"
  ],
  "_get_empty_tensor_for_exchange": [
    "shard_id",
    "needed_shards",
    "unneeded_shards",
    "loaded_tensors"
  ],
  "T": [],
  "distribute_shards_to_ranks": [
    "shard_to_ranks",
    "shard_to_size",
    "num_ranks",
    "cross_parallelization_group_loads"
  ],
  "determine_main_replica_uniform_distribution": [
    "sharded_state_dict",
    "parallelization_group",
    "ignore_groups"
  ],
  "exchange_loaded_tensors_gather_rounds": [
    "loaded_tensors",
    "unloaded_shards",
    "shard_distribution",
    "parallelization_group"
  ],
  "exchange_loaded_tensors_gather_object": [
    "loaded_tensors",
    "unloaded_shards",
    "shard_distribution",
    "parallelization_group"
  ],
  "exchange_loaded_objects_gather_object": [
    "loaded_objects"
  ],
  "exchange_loaded_tensors_broadcast": [
    "loaded_tensors",
    "unloaded_shards",
    "shard_distribution",
    "parallelization_group"
  ],
  "exchange_by_distribution": [
    "loaded_tensors",
    "unloaded_shards",
    "shard_distribution",
    "parallelization_group",
    "exchange_algo"
  ],
  "register_default_torch_strategies": [],
  "flatten_state_dict": [
    "state_dict"
  ],
  "sharded_tensor_to_torch_sharded_tensor": [
    "sh_tens",
    "rank",
    "load_legacy_1d_flatten_tensors"
  ],
  "mcore_to_pyt_state_dict": [
    "state_dict",
    "is_loading",
    "init_device",
    "load_legacy_1d_flatten_tensors"
  ],
  "_unwrap_pyt_sharded_tensor": [
    "sh_ten"
  ],
  "_replace_state_dict_keys_with_sharded_keys": [
    "sharded_state_dict",
    "keep_only_main_replica"
  ],
  "_replace_sharded_keys_with_state_dict_keys": [
    "state_dict",
    "flat_mapping",
    "rename_mapping"
  ],
  "_restore_dict_types": [
    "x",
    "keys_template"
  ],
  "MCoreMetadata": {},
  "MCoreSavePlan": {},
  "MCoreSavePlanner": {
    "__init__": [
      "self"
    ],
    "create_local_plan": [
      "self"
    ],
    "create_global_plan": [
      "self",
      "all_plans"
    ],
    "create_decentralized_global_plan": [
      "self",
      "local_plan"
    ],
    "transform_object": [
      "self",
      "write_item",
      "object"
    ]
  },
  "MCoreLoadPlanner": {
    "__init__": [
      "self"
    ],
    "_expected_shape": [
      "sh_ten"
    ],
    "_validate_global_shapes": [
      "self",
      "metadata",
      "sharded_tensors"
    ],
    "_temporarily_bypass_shape_validation": [
      "self"
    ],
    "create_local_plan": [
      "self"
    ],
    "resolve_tensor": [
      "self",
      "read_item"
    ],
    "commit_tensor": [
      "self",
      "read_item",
      "tensor"
    ]
  },
  "TorchDistSaveShardedStrategy": {
    "__init__": [
      "self",
      "backend",
      "version",
      "keep_only_main_replica",
      "thread_count",
      "cached_metadata",
      "separation_hint"
    ],
    "async_save": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "_get_save_and_finalize_callbacks": [
      "self",
      "writer",
      "save_state_dict_ret"
    ],
    "can_handle_sharded_objects": [
      "self"
    ]
  },
  "_get_filesystem_reader": [
    "checkpoint_dir",
    "cache_metadata"
  ],
  "get_reformulation_metadata": [
    "sharded_state_dict",
    "checkpoint_dir"
  ],
  "TorchDistLoadShardedStrategy": {
    "__init__": [
      "self"
    ],
    "load": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "load_tensors_metadata": [
      "self",
      "checkpoint_dir",
      "metadata"
    ],
    "load_sharded_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "remove_sharded_tensors": [
      "self",
      "checkpoint_dir",
      "key_prefix"
    ],
    "can_handle_sharded_objects": [
      "self"
    ],
    "check_backend_compatibility": [
      "self",
      "loaded_version"
    ],
    "check_version_compatibility": [
      "self",
      "loaded_version"
    ]
  },
  "COMMON_STATE_FNAME": [],
  "register_default_common_strategies": [],
  "TorchCommonSaveStrategy": {
    "save_common": [
      "self",
      "common_state_dict",
      "checkpoint_dir"
    ],
    "save_sharded_objects": [
      "self",
      "sharded_objects_state_dict",
      "checkpoint_dir"
    ],
    "can_handle_sharded_objects": [
      "self"
    ]
  },
  "TorchCommonLoadStrategy": {
    "load_common": [
      "self",
      "checkpoint_dir"
    ],
    "load_sharded_objects": [
      "self",
      "sharded_objects_state_dict",
      "checkpoint_dir"
    ],
    "load_sharded_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "can_handle_sharded_objects": [
      "self"
    ],
    "check_backend_compatibility": [
      "self",
      "loaded_version"
    ],
    "check_version_compatibility": [
      "self",
      "loaded_version"
    ]
  },
  "FullyParallelSaveStrategyWrapper": {
    "__init__": [
      "self",
      "strategy",
      "parallelization_group",
      "do_cache_distribution"
    ],
    "async_save": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "save": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "apply_saving_parallelization": [
      "self",
      "sharded_state_dict"
    ],
    "can_handle_sharded_objects": [
      "self"
    ]
  },
  "FullyParallelLoadStrategyWrapper": {
    "__init__": [
      "self",
      "strategy",
      "parallelization_group",
      "do_cache_distribution",
      "exchange_algo"
    ],
    "load": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "_defer_loading_sharded_objects": [
      "sharded_state_dict"
    ],
    "_defer_loading_sharded_tensors": [
      "sharded_state_dict"
    ],
    "fill_in_deferred_sharded_objects": [
      "sharded_state_dict",
      "loaded_objects"
    ],
    "fill_in_deferred_sharded_tensors": [
      "sharded_state_dict",
      "loaded_tensors"
    ],
    "apply_loading_parallelization": [
      "self",
      "sharded_state_dict"
    ],
    "can_handle_sharded_objects": [
      "self"
    ],
    "load_tensors_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "load_sharded_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "check_backend_compatibility": [
      "self",
      "loaded_version"
    ],
    "check_version_compatibility": [
      "self",
      "loaded_version"
    ]
  },
  "distribute_main_replicas_with_precomputed_distribution": [
    "sharded_state_dict",
    "parallelization_group",
    "precomputed_distribution"
  ],
  "_defer_loading_sharded_items": [
    "sharded_state_dict",
    "item_type",
    "shard_id_func"
  ],
  "_fill_in_deferred_sharded_items": [
    "sharded_state_dict",
    "loaded_items",
    "item_type",
    "shard_id_func"
  ],
  "TensorReformulationMetadata": {
    "__post_init__": [
      "self"
    ]
  },
  "nd_flattened_tensor_reformulated_global_shape": [
    "sh_ten"
  ],
  "is_nd_flattened_tensor": [
    "sh_ten"
  ],
  "ReformulationRestoreMetadata": [],
  "apply_nd_flattened_tensors_reformulation": [
    "sharded_state_dict",
    "reformulation_metadata"
  ],
  "restore_nd_flattened_tensors_formulation": [
    "state_dict",
    "formulation_restore_metadata"
  ],
  "reformulate_single_nd_flattened_tensor": [
    "sh_ten",
    "reformulation_metadata"
  ],
  "StrategyAction": {
    "LOAD_COMMON": [],
    "LOAD_SHARDED": [],
    "SAVE_COMMON": [],
    "SAVE_SHARDED": []
  },
  "async_calls": [],
  "get_default_strategy": [
    "action",
    "backend",
    "version"
  ],
  "register_default_strategy": [
    "action",
    "backend",
    "version",
    "strategy"
  ],
  "LoadStrategyBase": {
    "check_backend_compatibility": [
      "self",
      "loaded_backend"
    ],
    "check_version_compatibility": [
      "self",
      "loaded_version"
    ],
    "can_handle_sharded_objects": [
      "self"
    ]
  },
  "SaveStrategyBase": {
    "__init__": [
      "self",
      "backend",
      "version"
    ],
    "can_handle_sharded_objects": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "LoadCommonStrategy": {
    "load_common": [
      "self",
      "checkpoint_dir"
    ],
    "load_sharded_objects": [
      "self",
      "sharded_objects_state_dict",
      "checkpoint_dir"
    ],
    "load_sharded_metadata": [
      "self",
      "checkpoint_dir"
    ]
  },
  "LoadShardedStrategy": {
    "load": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "load_tensors_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "load_sharded_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "remove_sharded_tensors": [
      "self",
      "checkpoint_dir",
      "key_prefix"
    ]
  },
  "SaveCommonStrategy": {
    "save_common": [
      "self",
      "common_state_dict",
      "checkpoint_dir"
    ],
    "save_sharded_objects": [
      "self",
      "sharded_objects_state_dict",
      "checkpoint_dir"
    ]
  },
  "SaveShardedStrategy": {
    "save": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ]
  },
  "AsyncSaveShardedStrategy": {
    "async_save": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "save": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ]
  },
  "_disable_gc": [],
  "AsyncRequest": {
    "add_finalize_fn": [
      "self",
      "fn"
    ],
    "execute_sync": [
      "self"
    ],
    "freeze": [
      "self"
    ]
  },
  "AsyncCaller": {
    "schedule_async_call": [
      "self",
      "async_req"
    ],
    "is_current_async_call_done": [
      "self",
      "blocking",
      "no_dist"
    ],
    "sync_all_async_calls": [
      "self",
      "is_alive"
    ],
    "close": [
      "self",
      "abort"
    ],
    "__del__": [
      "self"
    ]
  },
  "TemporalAsyncCaller": {
    "__init__": [
      "self"
    ],
    "schedule_async_call": [
      "self",
      "async_req"
    ],
    "is_current_async_call_done": [
      "self",
      "blocking",
      "no_dist"
    ],
    "close": [
      "self",
      "abort"
    ],
    "__del__": [
      "self"
    ]
  },
  "PersistentAsyncCaller": {
    "__init__": [
      "self"
    ],
    "schedule_async_call": [
      "self",
      "async_req"
    ],
    "is_current_async_call_done": [
      "self",
      "blocking",
      "no_dist"
    ],
    "close": [
      "self",
      "abort"
    ],
    "__del__": [
      "self"
    ],
    "async_loop": [
      "rank",
      "queue",
      "preload_q",
      "comp_q",
      "log_level"
    ]
  },
  "_ActiveAsyncRequest": {},
  "AsyncCallsQueue": {
    "__init__": [
      "self",
      "persistent"
    ],
    "_get_async_caller": [
      "self"
    ],
    "schedule_async_request": [
      "self",
      "async_request"
    ],
    "maybe_finalize_async_calls": [
      "self",
      "blocking",
      "no_dist"
    ],
    "get_num_unfinalized_calls": [
      "self"
    ],
    "close": [
      "self",
      "abort"
    ]
  },
  "_import_trigger": [],
  "timers": [],
  "timed": [
    "verbose"
  ],
  "_ShardedTensorMetadata": {},
  "sharded_tensor_chunk_id": [
    "sharded_tensor"
  ],
  "TwoStageDataParallelLoadShardedStrategy": {
    "__init__": [
      "self",
      "data_parallel_group",
      "cpu_transfer"
    ],
    "load": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "summarize_load_times": [
      "self"
    ],
    "load_tensor_from_storage": [
      "self",
      "checkpoint_dir",
      "ten_meta"
    ],
    "maybe_init_gloo_group": [
      "self"
    ],
    "check_backend_compatibility": [
      "self",
      "loaded_version"
    ],
    "check_version_compatibility": [
      "self",
      "loaded_version"
    ],
    "_build_load_plan": [
      "self",
      "sharded_state_dict"
    ],
    "deduplicate_chunks": [
      "self",
      "ten_metas"
    ],
    "_exchange_loaded_tensors": [
      "self",
      "ten_metas",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "_distribute_data_to_state_dict": [
      "self",
      "ten_meta",
      "loaded_ten",
      "sharded_state_dict"
    ],
    "load_tensors_metadata": [
      "self",
      "checkpoint_dir"
    ]
  },
  "CachedMetadataFileSystemReader": {
    "__init__": [
      "self",
      "path"
    ],
    "read_metadata": [
      "self"
    ]
  },
  "register_default_tensorstore_strategies": [],
  "TensorStoreLoadShardedStrategy": {
    "__init__": [
      "self",
      "load_directly_on_device"
    ],
    "load": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "load_tensors_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "check_backend_compatibility": [
      "self",
      "loaded_version"
    ],
    "check_version_compatibility": [
      "self",
      "loaded_version"
    ]
  },
  "merge_global_slice_with_shape": [
    "global_slice",
    "actual_shape",
    "key"
  ],
  "_load_from_array": [
    "sharded_tensor",
    "checkpoint_dir",
    "load_directly_on_device",
    "apply_flattened_range"
  ],
  "_load_regular_chunk": [
    "sharded_tensor",
    "checkpoint_dir"
  ],
  "open_ts_array": [
    "arr_path"
  ],
  "_compare_dataclasses": [
    "obj1",
    "obj2"
  ],
  "save_state_dict_async_plan": [
    "state_dict",
    "storage_writer",
    "process_group",
    "coordinator_rank",
    "planner",
    "cached_ckpt_structure",
    "loaded_all_plans"
  ],
  "verify_global_md_reuse": [
    "loaded_all_plans",
    "local_plan",
    "rank",
    "dist_wrapper"
  ],
  "save_state_dict_async_finalize": [
    "storage_writer",
    "global_metadata",
    "dist_wrapper"
  ],
  "CheckpointableShardedTensor": {
    "__new__": [
      "cls",
      "data",
      "sh_ten"
    ],
    "__init__": [
      "self",
      "data",
      "sh_ten"
    ],
    "__create_write_items__": [
      "self",
      "fqn",
      "sh_ten",
      "index"
    ],
    "__create_chunk_list__": [
      "self"
    ],
    "__get_tensor_shard__": [
      "self",
      "index"
    ],
    "from_sh_ten": [
      "cls",
      "sh_ten"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__repr__": [
      "self"
    ]
  },
  "LocalShardsContainer": {
    "__new__": [
      "cls",
      "local_shards"
    ],
    "__init__": [
      "self",
      "local_shards"
    ],
    "__torch_dispatch__": [
      "cls",
      "func",
      "types",
      "args",
      "kwargs"
    ],
    "__create_write_items__": [
      "self",
      "fqn",
      "local_shards_cont"
    ],
    "__create_chunk_list__": [
      "self"
    ],
    "__get_tensor_shard__": [
      "self",
      "index"
    ],
    "__repr__": [
      "self"
    ]
  },
  "WriteBucket": [],
  "_results_queue": [],
  "_get_write_results_queue": [],
  "FileSystemWriterAsync": {
    "__init__": [
      "self",
      "path"
    ],
    "prepare_write_data": [
      "self",
      "plan",
      "planner"
    ],
    "get_save_function_and_args": [
      "self"
    ],
    "preload_tensors": [
      "write_buckets",
      "non_blocking"
    ],
    "write_preloaded_data_multiproc": [
      "transform_list",
      "use_msc",
      "rank",
      "write_buckets",
      "global_results_queue"
    ],
    "write_preloaded_data": [
      "transform_list",
      "local_proc_idx",
      "write_bucket",
      "results_queue",
      "count_queue",
      "use_fsync"
    ],
    "write_data": [
      "self",
      "plan",
      "planner"
    ],
    "retrieve_write_results": [
      "self"
    ],
    "prepare_decentralized_global_plan": [
      "self",
      "local_plan"
    ],
    "finish": [
      "self",
      "metadata",
      "results"
    ],
    "prepare_local_plan": [
      "self",
      "plan"
    ],
    "checkpoint_id": [
      "self"
    ],
    "validate_checkpoint_id": [
      "cls",
      "checkpoint_id"
    ]
  },
  "_split_by_size_and_type": [
    "bins",
    "items"
  ],
  "_split_by_separation_hint": [
    "buckets",
    "separation_hint"
  ],
  "_item_size": [
    "item"
  ],
  "_process_memory": [],
  "numpy_to_torch_dtype_dict": [],
  "torch_to_numpy_dtype_dict": [],
  "register_default_zarr_strategies": [],
  "ZarrSaveShardedStrategy": {
    "__init__": [
      "self",
      "backend",
      "version"
    ],
    "save": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ]
  },
  "_create_or_open_zarr_arrays": [
    "sharded_tensors",
    "checkpoint_dir"
  ],
  "_should_create_array": [
    "ten"
  ],
  "_save_to_existing_array": [
    "sharded_tensor",
    "arr"
  ],
  "_create_zarr_array": [
    "sharded_tensor",
    "checkpoint_dir"
  ],
  "ZarrLoadShardedStrategy": {
    "load": [
      "self",
      "sharded_state_dict",
      "checkpoint_dir"
    ],
    "load_tensors_metadata": [
      "self",
      "checkpoint_dir"
    ],
    "check_backend_compatibility": [
      "self",
      "loaded_version"
    ],
    "check_version_compatibility": [
      "self",
      "loaded_version"
    ]
  },
  "_open_zarr_array_verbose": [
    "path",
    "mode"
  ],
  "postprocess_numpy_array": [
    "loaded_array",
    "sharded_tensor",
    "apply_flattened_range"
  ],
  "flatten_range": [
    "sharded_tensor",
    "x"
  ],
  "pad_to_expected_shape": [
    "x",
    "expected_sharded_ten"
  ],
  "load_zarr_based_sharded_metadata": [
    "checkpoint_dir",
    "get_shape_dtype_fn"
  ],
  "MLASelfAttentionSubmodules": {},
  "MultiLatentAttention": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "attn_mask_type",
      "attention_type",
      "cp_comm_type",
      "pg_collection"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "key_value_states",
      "inference_context",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "rotary_pos_cos_sin",
      "attention_bias",
      "packed_seq_params",
      "position_ids",
      "sequence_len_offset"
    ]
  },
  "MLASelfAttention": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "attn_mask_type",
      "cp_comm_type",
      "pg_collection"
    ],
    "get_query_key_value_tensors": [
      "self",
      "hidden_states",
      "key_value_states",
      "position_ids",
      "packed_seq_params",
      "inference_context"
    ],
    "uncompress_kv_from_cache": [
      "self",
      "kv_cached"
    ],
    "prepare_for_absorption": [
      "self"
    ],
    "backward_dw": [
      "self"
    ],
    "_backward_kv_proj": [
      "self"
    ],
    "_backward_q_proj": [
      "self"
    ],
    "_backward_output_proj": [
      "self"
    ],
    "set_for_recompute_input_layernorm": [
      "self"
    ]
  },
  "IdentityOp": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "IdentityFuncOp": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self"
    ]
  },
  "WrappedTorchNorm": {
    "__new__": [
      "cls",
      "config",
      "hidden_size",
      "eps",
      "persist_layer_norm",
      "zero_centered_gamma",
      "normalization"
    ]
  },
  "L2Norm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_IS_GRAPH_CAPTURING": [],
  "is_graph_capturing": [],
  "_set_capture_start": [],
  "_set_capture_end": [],
  "ArgMetadata": {
    "__init__": [
      "self",
      "arg"
    ]
  },
  "_check_supported_type": [
    "meta"
  ],
  "_determine_if_transformer_decoder_layer": [
    "base_module"
  ],
  "_determine_if_first_last_layer_of_this_vp_chunk": [
    "base_module"
  ],
  "_CudagraphGlobalRecord": {
    "cudagraph_created": [],
    "cudagraph_record": [],
    "cudagraph_inference_record": [],
    "record_fwd_graph": [
      "cls",
      "runner",
      "args",
      "kwargs"
    ],
    "record_bwd_graph": [
      "cls",
      "runner"
    ],
    "create_cudagraphs": [
      "cls"
    ]
  },
  "create_cudagraphs": [],
  "delete_cuda_graphs": [],
  "_GraphStatus": {
    "FWD_READY": [],
    "BWD_READY": []
  },
  "_CudagraphRecordNode": {
    "forward": [
      "ctx",
      "runner",
      "inputs"
    ],
    "backward": [
      "ctx",
      "grads"
    ]
  },
  "_CudagraphReplayNode": {
    "forward": [
      "ctx",
      "runner",
      "is_first_microbatch"
    ],
    "backward": [
      "ctx"
    ]
  },
  "_CudaGraphRunner": {
    "__init__": [
      "self",
      "base_module",
      "fwd_mempool",
      "bwd_mempool",
      "fwd_graph_input_args",
      "fwd_graph_input_kwargs",
      "share_cudagraph_io_buffers"
    ],
    "__str__": [
      "self"
    ],
    "get_fp8_context": [
      "self"
    ],
    "get_fp4_context": [
      "self"
    ],
    "get_quantization_context": [
      "self"
    ],
    "create_fwd_graph": [
      "self",
      "args",
      "kwargs",
      "clone_inputs"
    ],
    "create_bwd_graph": [
      "self",
      "static_grad_outputs"
    ],
    "get_input_grads_with_dummy_flags": [
      "self"
    ],
    "record_graph_capture": [
      "self",
      "args",
      "kwargs"
    ],
    "replay_graph_capture": [
      "self",
      "is_first_microbatch",
      "args",
      "kwargs"
    ],
    "get_mismatch_errors": [
      "self",
      "args",
      "kwargs"
    ],
    "zero_out_tensors": [
      "self",
      "args",
      "kwargs"
    ],
    "get_tensors": [
      "cls",
      "args",
      "kwargs"
    ]
  },
  "CudaGraphManager": {
    "global_mempool": [],
    "fwd_mempools": [],
    "bwd_mempool": [],
    "__init__": [
      "self",
      "config",
      "share_cudagraph_io_buffers",
      "vp_stage"
    ],
    "set_is_first_microbatch": [
      "self",
      "is_first_microbatch"
    ],
    "call_ddp_preforward_hook": [
      "self",
      "module"
    ],
    "get_cudagraph_runner": [
      "self",
      "megatron_module",
      "args",
      "kwargs"
    ],
    "__call__": [
      "self",
      "megatron_module",
      "args",
      "kwargs"
    ]
  },
  "_layer_is_graphable": [
    "layer",
    "config"
  ],
  "TECudaGraphHelper": {
    "__init__": [
      "self",
      "model",
      "config",
      "seq_length",
      "micro_batch_size",
      "optimizers"
    ],
    "_get_cuda_graph_input_data": [
      "self"
    ],
    "_start_capturing": [
      "self"
    ],
    "_finish_capturing": [
      "self",
      "start_time"
    ],
    "create_cudagraphs": [
      "self"
    ],
    "cuda_graph_set_manual_hooks": [
      "self"
    ]
  },
  "WrappedTorchLayerNorm": [],
  "TransformerConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "MLATransformerConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "ModuleSpec": {},
  "import_module": [
    "module_path"
  ],
  "get_module": [
    "spec_or_module"
  ],
  "build_module": [
    "spec_or_module"
  ],
  "get_transformer_layer_offset": [
    "config",
    "vp_stage",
    "pp_rank"
  ],
  "TransformerLayerSubmodules": {},
  "BaseTransformerLayer": {
    "__init__": [
      "self"
    ]
  },
  "TransformerLayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "hidden_dropout",
      "pg_collection",
      "vp_stage"
    ],
    "_get_layer_offset": [
      "config"
    ],
    "forward": [
      "self"
    ],
    "_forward_attention": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "rotary_pos_cos_sin",
      "attention_bias",
      "inference_context",
      "packed_seq_params",
      "sequence_len_offset"
    ],
    "_forward_mlp": [
      "self",
      "hidden_states",
      "inference_context"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "get_layer_static_inputs": [
      "self",
      "seq_length",
      "micro_batch_size"
    ],
    "_get_submodules_under_cudagraphs": [
      "self"
    ],
    "_te_cuda_graph_capture": [
      "self"
    ],
    "_te_cuda_graph_replay": [
      "self"
    ],
    "_get_te_cuda_graph_replay_args": [
      "self"
    ],
    "_should_call_local_cudagraph": [
      "self"
    ],
    "__call__": [
      "self"
    ]
  },
  "get_linear_layer": [
    "rows",
    "columns",
    "init_method",
    "perform_initialization"
  ],
  "get_default_causal_mask": [
    "sq"
  ],
  "get_sliding_window_causal_mask": [
    "sq",
    "skv",
    "window_size"
  ],
  "attention_mask_func": [
    "attention_scores",
    "attention_mask"
  ],
  "gelu_impl": [
    "x"
  ],
  "openai_gelu": [
    "x"
  ],
  "erf_gelu": [
    "x"
  ],
  "make_sharded_tensors_for_checkpoint": [
    "state_dict",
    "prefix",
    "tensor_parallel_layers_axis_map",
    "sharded_offsets",
    "extra_state_suffix"
  ],
  "make_sharded_object_for_checkpoint": [
    "obj",
    "key",
    "sharded_offsets",
    "replica_id"
  ],
  "_get_extra_state_offsets": [
    "sharded_offsets"
  ],
  "sharded_state_dict_default": [
    "module",
    "prefix",
    "sharded_offsets",
    "metadata"
  ],
  "_sequence_parallel_attr_cache": [],
  "_init_sequence_parallel_cache": [
    "model",
    "exclude_modules"
  ],
  "set_model_to_sequence_parallel": [
    "model",
    "set_to",
    "exclude_modules"
  ],
  "cuda_graph_attr_cache": [],
  "init_cuda_graph_cache": [
    "model"
  ],
  "toggle_cuda_graphs": [
    "model",
    "set_to",
    "reset_cuda_graphs"
  ],
  "is_layer_window_attention": [
    "window_size",
    "window_attn_skip_freq",
    "layer_number"
  ],
  "MLPSubmodules": {},
  "MLP": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "is_expert",
      "input_size",
      "ffn_hidden_size",
      "tp_group"
    ],
    "forward": [
      "self",
      "hidden_states",
      "per_token_scale"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "apply_swiglu_sharded_factory": [
    "original_sh_ten",
    "sharded_offsets",
    "singleton_local_shards"
  ],
  "_FLOAT_TYPES": [],
  "_HALF_TYPES": [],
  "_BF16_TYPES": [],
  "param_is_not_shared": [
    "param"
  ],
  "MegatronModule": {
    "__init__": [
      "self",
      "config"
    ],
    "state_dict_for_save_checkpoint": [
      "self",
      "prefix",
      "keep_vars"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "set_is_first_microbatch": [
      "self"
    ],
    "set_symmetric_ar": [
      "self",
      "set_to"
    ]
  },
  "GraphableMegatronModule": {
    "__init__": [
      "self",
      "config",
      "vp_stage"
    ],
    "get_layer_static_inputs": [
      "self",
      "seq_length",
      "micro_batch_size"
    ],
    "setup_manual_hooks": [
      "self",
      "make_hook_func"
    ],
    "_get_submodules_under_cudagraphs": [
      "self"
    ],
    "_te_cuda_graph_capture": [
      "self"
    ],
    "_te_cuda_graph_replay": [
      "self"
    ],
    "_get_te_cuda_graph_replay_args": [
      "self"
    ],
    "_should_call_local_cudagraph": [
      "self"
    ],
    "_should_call_te_cudagraph": [
      "self"
    ],
    "__call__": [
      "self"
    ]
  },
  "conversion_helper": [
    "val",
    "conversion"
  ],
  "fp32_to_float16": [
    "val",
    "float16_convertor"
  ],
  "float16_to_fp32": [
    "val"
  ],
  "Float16Module": {
    "__init__": [
      "self",
      "config",
      "module"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "forward": [
      "self"
    ],
    "state_dict": [
      "self",
      "destination",
      "prefix",
      "keep_vars"
    ],
    "state_dict_for_save_checkpoint": [
      "self",
      "prefix",
      "keep_vars"
    ],
    "sharded_state_dict": [
      "self",
      "prefix"
    ],
    "load_state_dict": [
      "self",
      "state_dict",
      "strict"
    ]
  },
  "SelfAttentionSubmodules": {},
  "CrossAttentionSubmodules": {},
  "Attention": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "attn_mask_type",
      "attention_type",
      "cp_comm_type",
      "pg_collection"
    ],
    "_checkpointed_attention_forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "rotary_pos_emb",
      "attn_mask_type",
      "attention_bias",
      "packed_seq_params"
    ],
    "_allocate_memory": [
      "self",
      "inference_max_sequence_length",
      "batch_size",
      "dim",
      "dtype"
    ],
    "_adjust_key_value_for_inference": [
      "self",
      "inference_context",
      "query",
      "key",
      "value",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "rotary_pos_cos_sin",
      "sequence_len_offset"
    ],
    "get_query_key_value_tensors": [
      "self",
      "hidden_states",
      "key_value_states",
      "split_qkv"
    ],
    "flash_decode": [
      "self",
      "sequence_len_offset",
      "query_layer",
      "key_layer",
      "value_layer",
      "inference_key_memory",
      "inference_value_memory",
      "rotary_cos",
      "rotary_sin",
      "rotary_interleaved"
    ],
    "flash_decode_and_prefill": [
      "self",
      "q",
      "k",
      "v",
      "max_seqlen_q",
      "max_seqlen_k",
      "cu_seqlens_q",
      "cu_seqlens_k",
      "seqlens_k",
      "block_table"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "key_value_states",
      "inference_context",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "rotary_pos_cos_sin",
      "attention_bias",
      "packed_seq_params",
      "sequence_len_offset"
    ],
    "set_for_recompute_input_layernorm": [
      "self"
    ]
  },
  "SelfAttention": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "attn_mask_type",
      "cp_comm_type",
      "pg_collection"
    ],
    "run_realtime_tests": [
      "self"
    ],
    "get_query_key_value_tensors": [
      "self",
      "hidden_states",
      "key_value_states",
      "split_qkv"
    ],
    "backward_dw": [
      "self"
    ],
    "_backward_qkv_proj": [
      "self"
    ],
    "_backward_output_proj": [
      "self"
    ],
    "set_for_recompute_input_layernorm": [
      "self"
    ]
  },
  "CrossAttention": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "attn_mask_type",
      "cp_comm_type",
      "pg_collection"
    ],
    "get_query_key_value_tensors": [
      "self",
      "hidden_states",
      "key_value_states",
      "split_qkv"
    ]
  },
  "SUPPORTED_ATTN_MASK": [],
  "tie_word_embeddings_state_dict": [
    "sharded_state_dict",
    "word_emb_weight",
    "word_emb_weight_key"
  ],
  "tie_output_layer_state_dict": [
    "sharded_state_dict",
    "output_layer_weight",
    "output_layer_weight_key"
  ],
  "roll_tensor": [
    "tensor",
    "shifts",
    "dims",
    "cp_group"
  ],
  "MTPLossLoggingHelper": {
    "tracker": [],
    "save_loss_to_tracker": [
      "loss",
      "layer_number",
      "num_layers",
      "reduce_group",
      "avg_group"
    ],
    "clean_loss_in_tracker": [],
    "reduce_loss_in_tracker": [],
    "track_mtp_metrics": [
      "loss_scale",
      "iteration",
      "writer",
      "wandb_writer",
      "total_loss_dict"
    ]
  },
  "MultiTokenPredictionLayerSubmodules": {},
  "get_mtp_layer_spec": [
    "transformer_layer_spec",
    "use_transformer_engine"
  ],
  "get_mtp_layer_spec_for_backend": [
    "transformer_layer_spec",
    "backend"
  ],
  "get_mtp_layer_offset": [
    "config"
  ],
  "get_mtp_num_layers_to_build": [
    "config",
    "vp_stage",
    "pp_rank"
  ],
  "MTPLossAutoScaler": {
    "forward": [
      "ctx",
      "output",
      "mtp_loss"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ],
    "set_loss_scale": [
      "scale"
    ]
  },
  "MultiTokenPredictionLayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "vp_stage",
      "pg_collection"
    ],
    "_get_embeddings": [
      "self",
      "input_ids",
      "position_ids",
      "embedding",
      "hidden_states"
    ],
    "_concat_embeddings": [
      "self",
      "hidden_states",
      "decoder_input"
    ],
    "_proj_and_transformer_layer": [
      "self",
      "hidden_states",
      "decoder_input",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "inference_params",
      "packed_seq_params",
      "sequence_len_offset"
    ],
    "_postprocess": [
      "self",
      "hidden_states"
    ],
    "_checkpointed_forward": [
      "self",
      "forward_func"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "inference_params",
      "packed_seq_params",
      "sequence_len_offset",
      "embedding"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "MultiTokenPredictionBlockSubmodules": {},
  "_get_mtp_block_submodules": [
    "config",
    "spec"
  ],
  "MultiTokenPredictionBlock": {
    "__init__": [
      "self",
      "config",
      "spec",
      "vp_stage",
      "pg_collection"
    ],
    "_build_layers": [
      "self",
      "pg_collection"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "inference_params",
      "packed_seq_params",
      "sequence_len_offset",
      "extra_block_kwargs",
      "embedding"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "PipelineParallelLayerLayout": {
    "__repr__": [
      "self"
    ],
    "__init__": [
      "self",
      "layout",
      "pipeline_model_parallel_size"
    ],
    "validate_layer_layout": [
      "self",
      "num_layers",
      "mtp_num_layers"
    ],
    "get_num_layers_to_build": [
      "self",
      "layer_type",
      "vp_stage",
      "pp_rank"
    ],
    "get_layer_offset": [
      "self",
      "layer_type",
      "vp_stage",
      "pp_rank"
    ],
    "get_layer_id_list": [
      "self",
      "layer_type",
      "vp_stage",
      "pp_rank"
    ],
    "pretty_repr": [
      "self"
    ],
    "from_str": [
      "layout",
      "pipeline_model_parallel_size"
    ],
    "get_num_stages_from_str": [
      "layout"
    ],
    "parse_str_to_list": [
      "layout_str"
    ]
  },
  "handle_swiglu_in_state_dict": [
    "model",
    "model_state_dict",
    "optimizer_state_dict"
  ],
  "handle_fp8_extra_state_case": [
    "model_state_dict"
  ],
  "print_diff_in_state_dicts": [
    "state_dict_metadata",
    "load_state_dict"
  ],
  "LayerType": {
    "embedding": [],
    "loss": [],
    "encoder": [],
    "decoder": [],
    "mtp": []
  },
  "AttnType": {
    "self_attn": [],
    "cross_attn": []
  },
  "AttnMaskType": {
    "padding": [],
    "causal": [],
    "no_mask": [],
    "padding_causal": [],
    "arbitrary": [],
    "causal_bottom_right": []
  },
  "AttnBackend": {
    "flash": [],
    "fused": [],
    "unfused": [],
    "local": [],
    "auto": []
  },
  "DotProductAttention": {
    "__init__": [
      "self",
      "config",
      "layer_number",
      "attn_mask_type",
      "attention_type",
      "attention_dropout",
      "softmax_scale",
      "cp_comm_type",
      "pg_collection"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "attn_mask_type",
      "attention_bias",
      "packed_seq_params"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "get_cpu_offload_context": [],
  "get_num_layers_to_build": [
    "config",
    "vp_stage",
    "pp_rank"
  ],
  "TransformerBlockSubmodules": {},
  "_get_block_submodules": [
    "config",
    "spec",
    "vp_stage",
    "pp_rank"
  ],
  "TransformerBlock": {
    "__init__": [
      "self",
      "config",
      "spec",
      "post_layer_norm",
      "pre_process",
      "post_process",
      "pg_collection",
      "vp_stage"
    ],
    "_build_layers": [
      "self"
    ],
    "_get_layer": [
      "self",
      "layer_number"
    ],
    "_checkpointed_forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "attention_bias",
      "packed_seq_params",
      "use_inner_quantization_context"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "_should_call_local_cudagraph": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "rotary_pos_cos_sin",
      "attention_bias",
      "inference_context",
      "packed_seq_params",
      "sequence_len_offset"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "AttentionConfig": {
    "build_config_from_dict": [
      "cls",
      "block_config_dict",
      "num_attention_heads"
    ]
  },
  "MLPConfig": {
    "build_config_from_dict": [
      "cls",
      "block_config_dict",
      "hidden_size"
    ],
    "ffn_mult_to_intermediate_size": [
      "ffn_mult",
      "hidden_size"
    ],
    "find_multiple": [
      "n",
      "k"
    ]
  },
  "TransformerBlockConfig": {},
  "HeterogeneousTransformerConfig": {
    "__post_init__": [
      "self"
    ],
    "get_config_for_layer": [
      "self",
      "layer_number"
    ]
  },
  "_gather_from_tensor_parallel_region": [
    "x",
    "config"
  ],
  "ColumnParallelLinearGathered": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "input_",
      "weight",
      "runtime_gather_output"
    ]
  },
  "grouped_gemm_is_available": [],
  "assert_grouped_gemm_is_available": [],
  "ops": [],
  "_MOE_LAYER_WISE_LOGGING_TRACKER": [],
  "switch_load_balancing_loss_func": [
    "probs",
    "tokens_per_expert",
    "total_num_tokens",
    "topk",
    "num_experts",
    "moe_aux_loss_coeff",
    "fused"
  ],
  "z_loss_func": [
    "logits",
    "z_loss_coeff"
  ],
  "sinkhorn": [
    "cost",
    "tol"
  ],
  "get_capacity": [
    "num_tokens",
    "num_experts",
    "capacity_factor",
    "min_capacity"
  ],
  "MoEAuxLossAutoScaler": {
    "forward": [
      "ctx",
      "output",
      "aux_loss"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ],
    "set_loss_scale": [
      "scale"
    ]
  },
  "permute": [
    "tokens",
    "routing_map",
    "probs",
    "num_out_tokens",
    "fused",
    "drop_and_pad"
  ],
  "unpermute": [
    "permuted_tokens",
    "sorted_indices",
    "restore_shape",
    "probs",
    "routing_map",
    "fused",
    "drop_and_pad"
  ],
  "sort_chunks_by_idxs": [
    "input",
    "split_sizes",
    "sorted_idxs",
    "probs",
    "fused"
  ],
  "group_limited_topk": [
    "scores",
    "topk",
    "num_tokens",
    "num_experts",
    "num_groups",
    "group_topk"
  ],
  "pad_routing_map": [
    "routing_map",
    "pad_multiple"
  ],
  "topk_routing_with_score_function": [
    "logits",
    "topk",
    "use_pre_softmax",
    "num_groups",
    "group_topk",
    "scaling_factor",
    "score_function",
    "expert_bias",
    "fused"
  ],
  "compute_routing_scores_for_aux_loss": [
    "logits",
    "topk",
    "score_function",
    "fused"
  ],
  "apply_router_token_dropping": [
    "routing_probs",
    "routing_map",
    "router_topk",
    "capacity_factor",
    "drop_policy",
    "pad_to_capacity"
  ],
  "save_to_aux_losses_tracker": [
    "name",
    "loss",
    "layer_number",
    "num_layers",
    "reduce_group",
    "avg_group"
  ],
  "clear_aux_losses_tracker": [],
  "reduce_aux_losses_tracker_across_ranks": [
    "track_names"
  ],
  "track_moe_metrics": [
    "loss_scale",
    "iteration",
    "writer",
    "wandb_writer",
    "total_loss_dict",
    "per_layer_logging",
    "force_initialize",
    "track_names",
    "num_layers",
    "moe_layer_freq",
    "mtp_num_layers"
  ],
  "get_updated_expert_bias": [
    "tokens_per_expert",
    "expert_bias",
    "expert_bias_update_rate"
  ],
  "maybe_move_tensor_to_cpu": [
    "tensor",
    "as_numpy",
    "record_stream"
  ],
  "get_moe_layer_wise_logging_tracker": [],
  "RandomSTE": {
    "generator": [],
    "forward": [
      "ctx",
      "logits"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "apply_random_logits": [
    "logits"
  ],
  "RouterGatingLinearFunction": {
    "forward": [
      "ctx",
      "inp",
      "weight",
      "bias",
      "router_dtype"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "router_gating_linear": [
    "inp",
    "weight",
    "bias",
    "router_dtype"
  ],
  "get_default_pg_collection": [],
  "_buffer": [],
  "get_hidden_bytes": [
    "x"
  ],
  "get_buffer": [
    "group",
    "hidden_bytes"
  ],
  "FusedDispatch": {
    "forward": [
      "ctx",
      "x",
      "token_indices",
      "token_probs",
      "num_experts",
      "group",
      "async_finish",
      "allocate_on_comm_stream"
    ],
    "backward": [
      "ctx",
      "grad_output",
      "grad_token_indices",
      "grad_token_probs",
      "grad_tokens_per_expert",
      "grad_handle"
    ]
  },
  "FusedCombine": {
    "forward": [
      "ctx",
      "x",
      "group",
      "handle",
      "async_finish",
      "allocate_on_comm_stream"
    ],
    "backward": [
      "ctx",
      "grad_output",
      "previous_event"
    ]
  },
  "SharedExpertMLP": {
    "stream": [],
    "__init__": [
      "self",
      "config",
      "submodules",
      "gate",
      "pg_collection"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "pre_forward_comm": [
      "self",
      "input"
    ],
    "linear_fc1_forward_and_act": [
      "self",
      "overlapped_comm_output"
    ],
    "linear_fc2_forward": [
      "self",
      "overlapped_comm_output"
    ],
    "post_forward_comm": [
      "self"
    ],
    "get_output": [
      "self"
    ]
  },
  "set_tensor_grad_fn_sequence_sr": [
    "tensor",
    "value"
  ],
  "MoESubmodules": {},
  "BaseMoELayer": {
    "__init__": [
      "self",
      "config",
      "layer_number",
      "pg_collection"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "set_layer_number": [
      "self",
      "layer_number"
    ]
  },
  "MoELayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "pg_collection"
    ],
    "router_and_preprocess": [
      "self",
      "hidden_states"
    ],
    "dispatch": [
      "self",
      "hidden_states",
      "probs"
    ],
    "shared_experts_compute": [
      "self",
      "hidden_states"
    ],
    "routed_experts_compute": [
      "self",
      "hidden_states",
      "probs",
      "residual"
    ],
    "combine": [
      "self",
      "output",
      "shared_expert_output"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "backward_dw": [
      "self"
    ],
    "set_for_recompute_pre_mlp_layernorm": [
      "self"
    ]
  },
  "ExpertsType": [],
  "ActivationFuncName": [],
  "_get_keys_endswith": [
    "model",
    "suffix"
  ],
  "_find_submodule": [
    "model",
    "submodule_name"
  ],
  "_get_config": [
    "moe_model",
    "dense_model"
  ],
  "_convert_to_moe_state_dict": [
    "moe_model",
    "dense_model"
  ],
  "upcycle_state_dict": [
    "moe_model",
    "dense_model"
  ],
  "load_and_upcycle_model": [
    "load_dense_ckpt_func",
    "moe_model",
    "dense_model",
    "strict",
    "load_args",
    "load_kwargs"
  ],
  "MoETokenDispatcher": {
    "__init__": [
      "self",
      "config",
      "pg_collection"
    ],
    "dispatch_preprocess": [
      "self",
      "tokens",
      "routing_map",
      "probs"
    ],
    "token_dispatch": [
      "self",
      "hidden_states",
      "probs"
    ],
    "dispatch_postprocess": [
      "self",
      "hidden_states",
      "probs"
    ],
    "combine_preprocess": [
      "self",
      "hidden_states"
    ],
    "token_combine": [
      "self",
      "hidden_states"
    ],
    "combine_postprocess": [
      "self",
      "hidden_states"
    ],
    "set_shared_experts": [
      "self",
      "shared_experts"
    ]
  },
  "MoEAllGatherTokenDispatcher": {
    "__init__": [
      "self",
      "num_local_experts",
      "local_expert_indices",
      "config",
      "pg_collection"
    ],
    "dispatch_preprocess": [
      "self",
      "hidden_states",
      "routing_map",
      "probs"
    ],
    "token_dispatch": [
      "self",
      "hidden_states",
      "probs"
    ],
    "dispatch_postprocess": [
      "self",
      "hidden_states",
      "probs"
    ],
    "combine_preprocess": [
      "self",
      "hidden_states"
    ],
    "token_combine": [
      "self",
      "hidden_states"
    ],
    "combine_postprocess": [
      "self",
      "hidden_states"
    ]
  },
  "MoEAlltoAllTokenDispatcher": {
    "cuda_dtoh_stream": [],
    "__init__": [
      "self",
      "num_local_experts",
      "local_expert_indices",
      "config",
      "pg_collection"
    ],
    "preprocess": [
      "self",
      "routing_map"
    ],
    "dispatch_preprocess": [
      "self",
      "hidden_states",
      "routing_map",
      "probs"
    ],
    "token_dispatch": [
      "self",
      "permutated_local_input_tokens",
      "permuted_probs"
    ],
    "dispatch_postprocess": [
      "self",
      "global_input_tokens",
      "global_probs"
    ],
    "combine_preprocess": [
      "self",
      "hidden_states"
    ],
    "token_combine": [
      "self",
      "hidden_states",
      "async_finish",
      "allocate_on_comm_stream"
    ],
    "combine_postprocess": [
      "self",
      "permutated_local_input_tokens"
    ],
    "_maybe_update_cuda_sync_point": [
      "self",
      "point"
    ],
    "_maybe_dtoh_and_synchronize": [
      "self",
      "point",
      "tokens_per_expert"
    ]
  },
  "_DispatchManager": {
    "setup_metadata": [
      "self",
      "routing_map",
      "probs"
    ],
    "dispatch": [
      "self",
      "hidden_states"
    ],
    "combine": [
      "self",
      "hidden_states"
    ],
    "get_dispached_metadata": [
      "self"
    ],
    "get_permuted_hidden_states_by_experts": [
      "self",
      "hidden_states"
    ],
    "get_restored_hidden_states_by_experts": [
      "self",
      "hidden_states"
    ]
  },
  "_DeepepManager": {
    "__init__": [
      "self",
      "group",
      "num_local_experts",
      "router_topk",
      "num_experts",
      "config"
    ],
    "setup_metadata": [
      "self",
      "routing_map",
      "probs"
    ],
    "dispatch": [
      "self",
      "hidden_states",
      "async_finish",
      "allocate_on_comm_stream"
    ],
    "_indices_to_multihot": [
      "self",
      "indices",
      "probs"
    ],
    "get_dispached_metadata": [
      "self"
    ],
    "get_number_of_tokens_per_expert": [
      "self"
    ],
    "combine": [
      "self",
      "hidden_states",
      "async_finish",
      "allocate_on_comm_stream"
    ],
    "_pad_routing_map": [
      "self",
      "routing_map",
      "tokens_per_expert"
    ],
    "get_permuted_hidden_states_by_experts": [
      "self",
      "hidden_states"
    ],
    "get_restored_hidden_states_by_experts": [
      "self",
      "hidden_states"
    ]
  },
  "MoEFlexTokenDispatcher": {
    "__init__": [
      "self",
      "num_local_experts",
      "local_expert_indices",
      "config",
      "pg_collection"
    ],
    "set_shared_experts": [
      "self",
      "shared_experts"
    ],
    "_initialize_metadata": [
      "self",
      "routing_map",
      "probs"
    ],
    "dispatch_preprocess": [
      "self",
      "hidden_states",
      "routing_map",
      "probs"
    ],
    "token_dispatch": [
      "self",
      "hidden_states",
      "probs",
      "async_finish",
      "allocate_on_comm_stream"
    ],
    "dispatch_postprocess": [
      "self",
      "hidden_states",
      "probs"
    ],
    "combine_preprocess": [
      "self",
      "hidden_states"
    ],
    "token_combine": [
      "self",
      "hidden_states",
      "async_finish",
      "allocate_on_comm_stream"
    ],
    "combine_postprocess": [
      "self",
      "hidden_states"
    ]
  },
  "Router": {
    "__init__": [
      "self",
      "config",
      "pg_collection"
    ],
    "reset_parameters": [
      "self"
    ],
    "gating": [
      "self",
      "input"
    ],
    "routing": [
      "self",
      "logits"
    ],
    "forward": [
      "self",
      "input"
    ],
    "set_layer_number": [
      "self",
      "layer_number"
    ]
  },
  "TopKRouter": {
    "__init__": [
      "self",
      "config",
      "pg_collection"
    ],
    "_maintain_float32_expert_bias": [
      "self"
    ],
    "sinkhorn_load_balancing": [
      "self",
      "logits"
    ],
    "get_aux_loss_coeff": [
      "self",
      "aux_loss_type"
    ],
    "is_aux_loss_enabled": [
      "self"
    ],
    "_apply_aux_loss": [
      "self",
      "probs",
      "scores_for_aux_loss",
      "routing_map"
    ],
    "_apply_seq_aux_loss": [
      "self",
      "probs",
      "scores_for_aux_loss",
      "routing_map",
      "seq_length",
      "bsz"
    ],
    "_apply_global_aux_loss": [
      "self",
      "probs",
      "scores_for_aux_loss",
      "routing_map"
    ],
    "attach_and_log_load_balancing_loss": [
      "self",
      "activation",
      "aux_loss_coeff",
      "aux_loss",
      "aux_loss_name",
      "reduce_group"
    ],
    "apply_z_loss": [
      "self",
      "logits"
    ],
    "apply_input_jitter": [
      "self",
      "input"
    ],
    "routing": [
      "self",
      "logits"
    ],
    "reset_global_aux_loss_tracker": [
      "self"
    ],
    "forward": [
      "self",
      "input"
    ],
    "_load_from_state_dict": [
      "self"
    ],
    "_save_to_state_dict": [
      "self"
    ]
  },
  "expert_dist_ckpt_decorator": [
    "func"
  ],
  "GroupedMLP": {
    "__init__": [
      "self",
      "num_local_experts",
      "config",
      "pg_collection"
    ],
    "forward": [
      "self",
      "permuted_local_hidden_states",
      "tokens_per_expert",
      "permuted_probs"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "TEGroupedMLP": {
    "__init__": [
      "self",
      "num_local_experts",
      "config",
      "submodules",
      "pg_collection"
    ],
    "_apply_bias": [
      "intermediate_parallel",
      "bias_parallel",
      "tokens_per_expert",
      "permuted_probs"
    ],
    "forward": [
      "self",
      "permuted_local_hidden_states",
      "tokens_per_expert",
      "permuted_probs"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "backward_dw": [
      "self"
    ]
  },
  "SequentialMLP": {
    "__init__": [
      "self",
      "num_local_experts",
      "config",
      "submodules",
      "pg_collection"
    ],
    "_pad_tensor_for_fp8": [
      "self",
      "hidden",
      "probs"
    ],
    "forward": [
      "self",
      "permuted_local_hidden_states",
      "tokens_per_expert",
      "permuted_probs"
    ],
    "backward_dw": [
      "self"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "FP8_PER_TENSOR_REAL_QUANT_CFG": [],
  "FP8_2D_BLOCKWISE_REAL_QUANT_CFG": [],
  "Norm": {
    "__new__": [
      "cls",
      "config",
      "hidden_size",
      "eps"
    ]
  },
  "Linear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "RealQuantTransformerLayer": {
    "__init__": [
      "self"
    ],
    "_collect_original_tensor_info": [
      "self"
    ],
    "_report_quantize_tensor_info": [
      "self"
    ]
  },
  "FP8WeightTransformerLayer": {},
  "BlockwiseFP8WeightTransformerLayer": {},
  "get_mamba_stack_modelopt_spec": [
    "local_core_attention",
    "remap_te_layernorm"
  ],
  "mcore_gpt_load_te_state_dict_pre_hook": [
    "state_dict",
    "prefix",
    "local_metadata",
    "strict",
    "missing_keys",
    "unexpected_keys",
    "error_msgs"
  ],
  "get_gpt_modelopt_spec": [
    "config",
    "local_core_attention",
    "remap_te_layernorm",
    "real_quant_cfg",
    "qk_l2_norm",
    "use_arbitrary_attention_mask"
  ],
  "STOP_ITERATION": [],
  "AsyncStream": {
    "__init__": [
      "self",
      "request_id",
      "cancel"
    ],
    "put": [
      "self",
      "item"
    ],
    "finish": [
      "self",
      "exception"
    ],
    "finished": [
      "self"
    ],
    "generator": [
      "self"
    ],
    "_is_raisable": [
      "value"
    ]
  },
  "DataParallelInferenceCoordinator": {
    "__init__": [
      "self",
      "tokenizer",
      "inference_coordinator_port",
      "data_parallel_size"
    ],
    "get_next_data_parallel_rank": [
      "self"
    ],
    "tokenize_prompt": [
      "self",
      "prompt",
      "add_BOS"
    ],
    "postprocess": [
      "self",
      "request_ids",
      "finished_request_ids",
      "generated_tokens",
      "log_probs",
      "chunked_prefill_request_id",
      "materialize_only_last_token_logits"
    ],
    "start": [
      "self"
    ],
    "entrypoint": [
      "cls",
      "ready_event",
      "tokenizer",
      "inference_coordinator_port",
      "data_parallel_size"
    ],
    "stop": [
      "self"
    ]
  },
  "Scheduler": {
    "__init__": [
      "self",
      "max_batch_size"
    ],
    "get_new_request_id": [
      "self"
    ],
    "add_request": [
      "self",
      "prompt",
      "prompt_tokens",
      "encoder_prompt",
      "sampling_params",
      "arrival_time",
      "streaming",
      "inference_request"
    ],
    "num_requests_pending": [
      "self"
    ],
    "have_requests_pending": [
      "self"
    ],
    "add_earliest_waiting_request_to_active_pool": [
      "self"
    ],
    "update_requests_pools": [
      "self",
      "result_dict"
    ],
    "abort_request": [
      "self",
      "request_id"
    ]
  },
  "Counter": {
    "__init__": [
      "self",
      "start"
    ],
    "__next__": [
      "self"
    ],
    "reset": [
      "self"
    ]
  },
  "get_attention_mask": [
    "seq_length"
  ],
  "moe_layer_cache": [],
  "_init_moe_expert_cache": [
    "model"
  ],
  "set_decode_expert_padding": [
    "model",
    "set_to",
    "capacity_factor"
  ],
  "tensor_swap": [
    "x",
    "src_idxs",
    "dst_idxs"
  ],
  "Status": {
    "WAITING_IN_QUEUE": [],
    "ACTIVE_AND_GENERATING_TOKENS": [],
    "ACTIVE_BUT_NOT_GENERATING_TOKENS": [],
    "COMPLETED": [],
    "FAILED": []
  },
  "InferenceRequest": {
    "__post_init__": [
      "self"
    ],
    "serializable": [
      "self"
    ]
  },
  "DynamicInferenceEventType": {
    "ADD": [],
    "PAUSE": [],
    "FINISH": [],
    "FAIL": [],
    "ERROR_TRANSIENT": [],
    "ERROR_NONTRANSIENT": []
  },
  "DynamicInferenceEvent": {
    "__post_init__": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "DynamicInferenceRequest": {
    "finished_chunk_token_count": [],
    "__post_init__": [
      "self"
    ],
    "remaining_prompt_length": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "add_event": [
      "self",
      "type",
      "payload"
    ],
    "add_event_add": [
      "self"
    ],
    "add_event_pause": [
      "self"
    ],
    "add_event_finish": [
      "self"
    ],
    "add_event_fail": [
      "self"
    ],
    "add_event_error_transient": [
      "self",
      "error"
    ],
    "add_event_error_nontransient": [
      "self",
      "error"
    ],
    "succeeded": [
      "self"
    ],
    "failed": [
      "self"
    ]
  },
  "VLMInferenceRequest": {},
  "_mempool_c_src": [],
  "has_unified_memory": [],
  "_alloc": [],
  "create_unified_mempool": [],
  "InferenceClient": {
    "__init__": [
      "self",
      "inference_coordinator_port"
    ],
    "add_request": [
      "self",
      "prompt",
      "sampling_params"
    ],
    "_listen_for_completed_requests": [
      "self"
    ],
    "_connect_with_inference_coordinator": [
      "self"
    ],
    "start": [
      "self"
    ],
    "_send_signal_to_engines": [
      "self",
      "signal"
    ],
    "pause_engines": [
      "self"
    ],
    "unpause_engines": [
      "self"
    ],
    "stop_engines": [
      "self"
    ],
    "stop": [
      "self"
    ]
  },
  "Headers": {
    "CONNECT": [],
    "ACK": [],
    "SUBMIT_REQUEST": [],
    "ENGINE_REPLY": [],
    "PAUSE": [],
    "UNPAUSE": [],
    "STOP": []
  },
  "_is_cuda": [
    "tensor"
  ],
  "_is_cuda_contiguous": [
    "tensor"
  ],
  "broadcast_from_last_pipeline_stage": [
    "size",
    "dtype",
    "tensor",
    "pp_group"
  ],
  "recv_from_prev_pipeline_rank_": [
    "recv_buffer",
    "pp_group"
  ],
  "send_to_next_pipeline_rank": [
    "tensor",
    "pp_group"
  ],
  "broadcast_tensor": [
    "size",
    "dtype",
    "tensor",
    "rank",
    "data_parallel"
  ],
  "broadcast_list": [
    "size",
    "dtype",
    "list_values",
    "rank",
    "data_parallel"
  ],
  "broadcast_int_list": [
    "size",
    "int_list",
    "rank",
    "data_parallel"
  ],
  "broadcast_float_list": [
    "size",
    "float_list",
    "rank",
    "data_parallel"
  ],
  "SamplingParams": {
    "add_attributes": [
      "self",
      "attribute_value_pair"
    ],
    "serializable": [
      "self"
    ],
    "deserialize": [
      "cls",
      "data"
    ]
  },
  "run_mcore_engine": [
    "engine",
    "prompts",
    "temperature",
    "top_k",
    "top_p",
    "logprobs",
    "tokens_to_generate",
    "top_n_logprobs",
    "random_seed"
  ],
  "MegatronGenerate": {
    "__init__": [
      "self",
      "engine",
      "args"
    ],
    "put": [
      "self"
    ]
  },
  "MegatronServer": {
    "__init__": [
      "self",
      "model",
      "args"
    ],
    "run": [
      "self",
      "url",
      "port"
    ]
  },
  "tokenize_prompts": [
    "tokenizer",
    "prompts",
    "tokens_to_generate",
    "add_BOS",
    "rank",
    "data_parallel"
  ],
  "_tokenize_prompts_and_batch": [
    "tokenizer",
    "prompts",
    "tokens_to_generate",
    "add_BOS"
  ],
  "GENERATE_NUM": [],
  "LOCK": [],
  "send_do_generate": [],
  "detokenize": [
    "prompt",
    "tok"
  ],
  "MegatronCompletions": {
    "__init__": [
      "self",
      "engine",
      "args"
    ],
    "post": [
      "self"
    ]
  },
  "StaticInferenceEngine": {
    "__init__": [
      "self",
      "text_generation_controller",
      "max_batch_size",
      "random_seed"
    ],
    "get_new_request_id": [
      "self"
    ],
    "add_request": [
      "self",
      "prompt",
      "add_BOS",
      "encoder_prompt",
      "sampling_params",
      "streaming",
      "inference_request"
    ],
    "get_stream_generator": [
      "self",
      "request_id"
    ],
    "generate": [
      "self",
      "prompts",
      "add_BOS",
      "encoder_prompts",
      "common_inference_params",
      "sampling_params",
      "inference_requests"
    ],
    "run_engine": [
      "self"
    ],
    "_wrapped_run_engine": [
      "self",
      "cuda_device"
    ],
    "run_engine_async": [
      "self"
    ]
  },
  "AbstractEngine": {
    "generate": [
      "self"
    ]
  },
  "format_mem_bytes": [
    "mem_bytes"
  ],
  "DynamicInferenceEngine": {
    "__init__": [
      "self",
      "controller",
      "context",
      "termination_id",
      "enable_cuda_graph",
      "random_seed"
    ],
    "start_listening_to_data_parallel_coordinator": [
      "self",
      "sampling_params",
      "inference_coordinator_port",
      "launch_inference_coordinator"
    ],
    "_notify_cond_for_new_request": [
      "self"
    ],
    "has_unfinished_requests": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "_add_request": [
      "self",
      "request"
    ],
    "add_request": [
      "self",
      "request_id",
      "prompt",
      "num_tokens_to_generate",
      "num_tokens_total"
    ],
    "post_process_requests": [
      "self",
      "request_ids",
      "finished_request_ids",
      "step_time",
      "sample",
      "log_probs"
    ],
    "schedule_waiting_requests": [
      "self"
    ],
    "schedule_non_chunked_prefill": [
      "self"
    ],
    "schedule_chunked_prefill": [
      "self"
    ],
    "async_step": [
      "self",
      "sampling_params"
    ],
    "step_modern": [
      "self",
      "sampling_params"
    ],
    "step_legacy": [
      "self",
      "sampling_params"
    ],
    "step": [],
    "generate": [
      "self",
      "prompts",
      "sampling_params"
    ],
    "schedule_requests": [
      "self"
    ],
    "stop": [
      "self"
    ],
    "run_engine": [
      "self",
      "sampling_params"
    ],
    "run_engine_with_coordinator": [
      "self",
      "sampling_params"
    ]
  },
  "ContextOverflowError": {
    "__init__": [
      "self",
      "request_id",
      "message"
    ]
  },
  "RequestOverflowError": {},
  "TokenOverflowError": {},
  "MaxSequenceLengthOverflowError": {
    "__init__": [
      "self",
      "request_id",
      "message"
    ]
  },
  "BlockOverflowError": {},
  "ActiveRequestCountOverflowError": {
    "__init__": [
      "self",
      "max_request_count",
      "active_request_count"
    ]
  },
  "WarmupEngineMode": {
    "DECODE": [],
    "NON_DECODE": []
  },
  "DynamicInferenceContext": {
    "__init__": [
      "self"
    ],
    "TOKEN_ROUNDER": [],
    "REQUEST_ROUNDER": [],
    "round_up_tokens": [
      "cls",
      "value",
      "tp_size"
    ],
    "round_up_requests": [
      "cls",
      "value",
      "tp_size"
    ],
    "round_up": [
      "cls",
      "value"
    ],
    "is_static_batching": [
      "self"
    ],
    "is_decode_only": [
      "self"
    ],
    "has_unfinished_requests": [
      "self"
    ],
    "cu_query_lengths": [
      "self"
    ],
    "cu_kv_lengths": [
      "self"
    ],
    "get_active_sequence_lengths": [
      "self"
    ],
    "get_max_sequence_lengths": [
      "self"
    ],
    "get_active_request_count": [
      "self"
    ],
    "append_key_value_cache": [
      "self",
      "layer_number",
      "key",
      "value"
    ],
    "key_value_cache": [
      "self",
      "layer_number"
    ],
    "apply_fused_qk_rotary_emb": [
      "self",
      "query",
      "key",
      "cos_sin_emb",
      "config"
    ],
    "apply_rotary_emb_query": [
      "self",
      "query",
      "query_emb",
      "config",
      "cu_seqlens_q",
      "cp_group",
      "mscale"
    ],
    "apply_rotary_emb_key": [
      "self",
      "key",
      "key_emb",
      "config",
      "cp_group",
      "mscale"
    ],
    "reset_attention_state": [
      "self"
    ],
    "using_cuda_graph_this_step": [
      "self"
    ],
    "initialize_attention_state": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "current_input_and_position_ids": [
      "self"
    ],
    "last_token_logits": [
      "self",
      "logits"
    ],
    "check_availability": [
      "self",
      "req",
      "safe"
    ],
    "add_request": [
      "self",
      "req",
      "chunk_length"
    ],
    "_move_book_keeping_tensors": [
      "self",
      "src_idxs",
      "dst_idxs",
      "next_tokens"
    ],
    "_swap_book_keeping_tensors": [
      "self",
      "src_idxs",
      "dst_idxs",
      "next_tokens"
    ],
    "update_requests": [
      "self",
      "active_requests_mask",
      "new_tokens"
    ],
    "calculate_log_probs": [
      "self",
      "logits",
      "new_tokens",
      "only_last_token_logits"
    ]
  },
  "StaticInferenceContext": {
    "__init__": [
      "self",
      "max_batch_size",
      "max_sequence_length",
      "use_flashinfer_fused_rope"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "swap_key_value_dict": [
      "self",
      "batch_idx"
    ],
    "enable_prefill_mode": [
      "self"
    ],
    "enable_decode_mode": [
      "self"
    ],
    "is_decode_only": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "is_static_batching": [
      "self"
    ]
  },
  "_append_kv_cache_kernel": [
    "key_ptr",
    "value_ptr",
    "key_cache_ptr",
    "value_cache_ptr",
    "block_idx_ptr",
    "local_kv_seq_idx_ptr",
    "stride_key_token",
    "stride_key_head",
    "stride_key_hdim",
    "stride_value_token",
    "stride_value_head",
    "stride_value_hdim",
    "stride_cache_block",
    "stride_cache_pos",
    "stride_cache_head",
    "stride_cache_hdim",
    "n_tokens",
    "num_heads",
    "H_DIM",
    "BLOCK_SIZE_H"
  ],
  "triton_append_key_value_cache": [
    "layer_number",
    "key",
    "value",
    "memory_buffer",
    "padded_active_token_count",
    "token_to_block_idx",
    "token_to_local_position_within_kv_block"
  ],
  "BaseInferenceContext": {
    "__init__": [
      "self",
      "materialize_only_last_token_logits"
    ],
    "is_static_batching": [
      "self"
    ],
    "is_dynamic_batching": [
      "self"
    ],
    "increment_sequence_len_offset": [
      "self",
      "increment"
    ],
    "increment_batch_size_offset": [
      "self",
      "increment"
    ],
    "reset_batch_size_offset": [
      "self"
    ]
  },
  "BlockAllocator": {
    "__init__": [
      "self",
      "block_count_total",
      "gtd_block_count"
    ],
    "is_memory_available": [
      "self",
      "num_blocks",
      "safe"
    ],
    "allocate_memory_blocks": [
      "self",
      "num_blocks",
      "safe"
    ],
    "release_memory_blocks": [
      "self",
      "blocks"
    ],
    "reset": [
      "self"
    ]
  },
  "InferenceWrapperConfig": {
    "add_attributes": [
      "self",
      "attribute_value_pair"
    ]
  },
  "AbstractModelInferenceWrapper": {
    "__init__": [
      "self",
      "model",
      "inference_wrapper_config",
      "inference_context",
      "pg_collection"
    ],
    "inference_params": [
      "self",
      "value"
    ],
    "prep_model_for_inference": [
      "self",
      "prompts_tokens"
    ],
    "prep_inference_input": [
      "self",
      "prompt_tokens"
    ],
    "get_batch_for_context_window": [
      "self"
    ],
    "_forward": [
      "self",
      "inference_input"
    ],
    "_get_batch_size_and_seq_len": [
      "self",
      "tokens",
      "recv_buffer_seq_len"
    ],
    "_allocate_recv_buffer": [
      "self",
      "batch_size",
      "seq_len"
    ],
    "forward_pass_without_pipeline_parallel": [
      "self",
      "inference_input"
    ],
    "forward_pass_with_pipeline_parallel_small_input_batch": [
      "self",
      "inference_input",
      "recv_buffer_seq_len"
    ],
    "forward_pass_with_pipeline_parallel_large_input_batch": [
      "self",
      "inference_input",
      "recv_buffer_seq_len"
    ],
    "run_one_forward_step": [
      "self",
      "inference_input",
      "recv_buffer_seq_len"
    ]
  },
  "T5InferenceWrapper": {
    "__init__": [
      "self",
      "model",
      "inference_wrapper_config",
      "inference_context",
      "use_local"
    ],
    "prep_inference_input": [
      "self",
      "prompts_tokens",
      "encoder_prompts",
      "tokenizer"
    ],
    "tokenize_encoder_prompt": [
      "self",
      "encoder_prompt",
      "tokenizer"
    ],
    "pad_encoder_prompts_tokens": [
      "self",
      "encoder_prompts_tokens_list",
      "max_sequence_length",
      "tokenizer"
    ],
    "get_batch_for_context_window": [
      "self",
      "inference_input",
      "context_start_position",
      "context_end_position"
    ],
    "forward_pass_without_pipeline_parallel": [
      "self",
      "inference_input"
    ]
  },
  "GPTInferenceWrapper": {
    "__init__": [
      "self",
      "model",
      "inference_wrapper_config",
      "inference_context"
    ],
    "prep_inference_input": [
      "self",
      "prompts_tokens"
    ],
    "_build_attention_mask_and_position_ids": [
      "self",
      "prompts_tokens"
    ],
    "get_batch_for_context_window": [
      "self",
      "inference_input",
      "context_start_position",
      "context_end_position"
    ]
  },
  "VLMInferenceWrapper": {
    "prep_model_for_inference": [
      "self",
      "prompts_tokens"
    ],
    "prep_inference_input": [
      "self",
      "prompts_tokens",
      "num_img_embeddings_per_tile",
      "images",
      "num_tiles",
      "decoder_seq_length"
    ],
    "get_batch_for_context_window": [
      "self",
      "inference_input",
      "context_start_position",
      "context_end_position"
    ],
    "_forward": [
      "self",
      "inference_input"
    ],
    "run_one_forward_step": [
      "self",
      "inference_input"
    ]
  },
  "EncoderDecoderTextGenerationController": {
    "prep_inference_input": [
      "self",
      "prompts_tokens",
      "active_requests",
      "use_attention_mask"
    ]
  },
  "TextGenerationController": {
    "__init__": [
      "self",
      "inference_wrapped_model",
      "tokenizer",
      "pp_group"
    ],
    "tokenize_prompt": [
      "self",
      "prompt",
      "add_BOS"
    ],
    "_detokenize": [
      "self",
      "tokens",
      "skip_special_tokens"
    ],
    "detokenize_generations": [
      "self",
      "tokens_gpu_tensor",
      "lengths_gpu_tensor",
      "detokenize_segments",
      "skip_special_tokens"
    ],
    "sample_from_logits": [
      "self",
      "last_token_logits",
      "sampling_params",
      "vocab_size",
      "generation_started",
      "top_n_logprobs_dict",
      "logits"
    ],
    "update_generation_status": [
      "self",
      "updated_prompts_tokens",
      "generation_started",
      "current_context_end_position",
      "is_generation_done_tensor",
      "generated_sequence_lengths",
      "termination_id"
    ],
    "pad_input_prompt_tokens": [
      "self",
      "batch_prompt_tokens_list",
      "padded_batch_size",
      "padded_sequence_length"
    ],
    "unpad_input_prompt_tokens": [
      "self",
      "padded_batch_prompt_tokens",
      "original_batch_size"
    ],
    "async_generate_output_tokens_dynamic_batch": [
      "self",
      "sampling_params",
      "termination_id"
    ],
    "generate_output_tokens_dynamic_batch": [
      "self",
      "sampling_params",
      "termination_id"
    ],
    "_update_top_n_logprobs_dict": [
      "self",
      "top_n_logprobs_this_step",
      "top_n_logprobs_indices",
      "mask",
      "top_n_logprobs_dict"
    ],
    "generate_all_output_tokens_static_batch": [
      "self",
      "active_requests",
      "active_streams"
    ],
    "prep_inference_input": [
      "self",
      "prompts_tokens",
      "active_requests",
      "use_attention_mask"
    ],
    "stream_tokens": [
      "self",
      "sampling_params",
      "request_ids",
      "requests",
      "streams",
      "generation_started",
      "is_generation_done",
      "tokens",
      "prompt_lengths",
      "generated_lengths",
      "output_log_probs"
    ]
  },
  "VLMTextGenerationController": {
    "prep_inference_input": [
      "self",
      "prompts_tokens",
      "active_requests",
      "use_attention_mask"
    ]
  },
  "get_quant_config_or_none": [
    "module_path",
    "recipe"
  ],
  "load_quantization_recipe": [
    "recipe_path"
  ],
  "kitchen_quantization_recipe_config": [
    "recipe_idx"
  ],
  "MatchContext": {},
  "QuantizationConfig": {
    "__init__": [
      "self",
      "config",
      "match_input",
      "config_key"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Matcher": {
    "match": [
      "self",
      "context"
    ]
  },
  "GlobMatcher": {
    "__init__": [
      "self",
      "pattern",
      "config_key"
    ],
    "match": [
      "self",
      "context"
    ],
    "__repr__": [
      "self"
    ]
  },
  "RecipeConfig": {
    "__init__": [
      "self",
      "matchers",
      "config_dict"
    ],
    "_build_matchers": [
      "matchers_dict"
    ],
    "from_yaml_file": [
      "recipe_yaml_path"
    ],
    "from_config_dict": [
      "config"
    ],
    "match_to_config_key": [
      "self",
      "operator_context"
    ],
    "match": [
      "self",
      "operator_context"
    ],
    "__repr__": [
      "self"
    ]
  },
  "CommRole": {
    "SENDER": [],
    "RECEIVER": [],
    "MEMBER": []
  },
  "RankCommInfo": {},
  "BridgeCommunicator": {
    "__init__": [
      "self",
      "src_grid",
      "dest_grid",
      "dim_mapping",
      "comm_dtype",
      "src_module_name",
      "dest_module_name"
    ],
    "get_leader_rank": [
      "self",
      "grid",
      "is_src"
    ],
    "get_boundary_pp_stage_ranks": [
      "self",
      "grid",
      "is_src"
    ],
    "is_current_rank_in_grid": [
      "self",
      "grid"
    ],
    "build_comm_map": [
      "self",
      "src_tp_leaders",
      "dest_tp_leaders"
    ],
    "send_forward": [
      "self",
      "tensor_to_send"
    ],
    "recv_forward": [
      "self"
    ],
    "send_backward": [
      "self",
      "grad_tensor"
    ],
    "recv_backward": [
      "self"
    ],
    "send_forward_recv_backward": [
      "self",
      "input_tensor",
      "grad_shape"
    ],
    "send_backward_recv_forward": [
      "self",
      "grad_tensor",
      "forward_shape"
    ],
    "_communicate_shapes": [
      "self",
      "tensor_to_send_next",
      "recv_next",
      "recv_prev",
      "tensor_to_send_prev"
    ],
    "_split_tensor_at_batch_dim": [
      "self",
      "aggregated_tensor",
      "num_splits"
    ]
  },
  "Shape": [],
  "get_forward_backward_func": [],
  "deallocate_output_tensor": [
    "out",
    "deallocate_pipeline_outputs"
  ],
  "custom_backward": [
    "output",
    "grad_output"
  ],
  "set_current_microbatch": [
    "model",
    "microbatch_id"
  ],
  "forward_step_calc_loss": [
    "model",
    "output_tensor",
    "loss_func",
    "config",
    "vp_stage",
    "collect_non_loss_data",
    "num_microbatches",
    "forward_data_store",
    "cp_group_size",
    "is_last_stage"
  ],
  "forward_step": [
    "forward_step_func",
    "data_iterator",
    "model",
    "num_microbatches",
    "input_tensor",
    "forward_data_store",
    "config",
    "cp_group_size",
    "collect_non_loss_data",
    "checkpoint_activations_microbatch",
    "is_first_microbatch",
    "current_microbatch",
    "vp_stage",
    "is_last_stage"
  ],
  "backward_step": [
    "input_tensor",
    "output_tensor",
    "output_tensor_grad",
    "model_type",
    "config"
  ],
  "check_first_val_step": [
    "first_val_step",
    "forward_only",
    "cond"
  ],
  "forward_backward_no_pipelining": [],
  "clear_embedding_activation_buffer": [
    "config",
    "model",
    "is_last_stage"
  ],
  "finish_embedding_wgrad_compute": [
    "config",
    "embedding_module",
    "is_last_stage",
    "tp_group"
  ],
  "get_pp_rank_microbatches": [
    "num_microbatches",
    "num_model_chunks",
    "microbatch_group_size_per_vp_stage",
    "forward_only",
    "overlap_moe_expert_parallel_comm",
    "p2p_communicator"
  ],
  "get_schedule_table": [
    "num_microbatches",
    "num_model_chunks",
    "microbatch_group_size_per_vp_stage"
  ],
  "convert_schedule_table_to_order": [
    "num_warmup_microbatches",
    "num_model_chunks",
    "schedule_table"
  ],
  "forward_backward_pipelining_with_interleaving": [],
  "get_tensor_shapes": [],
  "forward_backward_pipelining_without_interleaving": [],
  "combined_1f1b_schedule_for_no_pipelining": [
    "forward_step_func",
    "data_iterator",
    "model",
    "num_microbatches",
    "input_tensor",
    "output_tensor_grad",
    "forward_data_store",
    "config",
    "collect_non_loss_data",
    "first_val_step",
    "forward_only",
    "no_sync_func",
    "total_num_tokens",
    "check_first_val_step"
  ],
  "combined_1f1b_schedule_for_interleaved_pipelining": [
    "config",
    "forward_step_func",
    "data_iterator",
    "model",
    "num_microbatches",
    "forward_data_store",
    "forward_step_helper_preprocess",
    "forward_step_helper_postprocess",
    "backward_step_helper_preprocess",
    "backward_step_helper_postprocess",
    "get_microbatch_id_in_model_chunk",
    "get_model_chunk_id",
    "check_first_val_step",
    "is_first_microbatch_for_model_chunk",
    "collect_non_loss_data",
    "f_virtual_microbatch_id",
    "b_virtual_microbatch_id",
    "pre_forward",
    "pre_backward",
    "post_forward",
    "post_backward"
  ],
  "combined_forward_backward_step": [
    "forward_step_func",
    "data_iterator",
    "f_model",
    "num_microbatches",
    "input_tensor",
    "forward_data_store",
    "b_model",
    "b_input_tensor",
    "b_output_tensor",
    "b_output_tensor_grad",
    "config",
    "f_model_chunk_id",
    "pre_forward",
    "pre_backward",
    "post_forward",
    "post_backward",
    "collect_non_loss_data",
    "checkpoint_activations_microbatch",
    "is_first_microbatch",
    "current_microbatch",
    "encoder_decoder_xattn"
  ],
  "is_pp_first_stage": [
    "pp_group"
  ],
  "is_pp_last_stage": [
    "pp_group"
  ],
  "is_vp_first_stage": [
    "vp_stage",
    "vp_size"
  ],
  "is_vp_last_stage": [
    "vp_stage",
    "vp_size"
  ],
  "get_pp_first_rank": [
    "pp_group"
  ],
  "get_pp_last_rank": [
    "pp_group"
  ],
  "get_pp_next_rank": [
    "pp_group"
  ],
  "get_pp_prev_rank": [
    "pp_group"
  ],
  "make_viewless": [
    "e"
  ],
  "stream_acquire_context": [
    "stream",
    "event"
  ],
  "NoopScheduleNode": {
    "forward": [
      "self",
      "inputs"
    ],
    "backward": [
      "self",
      "outgrads"
    ]
  },
  "ScheduleNode": {
    "__init__": [
      "self",
      "forward_func",
      "stream",
      "event",
      "backward_func",
      "free_input",
      "name"
    ],
    "default_backward_func": [
      "self",
      "outputs",
      "output_grad"
    ],
    "forward": [
      "self",
      "inputs"
    ],
    "_forward": [
      "self"
    ],
    "get_output": [
      "self"
    ],
    "backward": [
      "self",
      "output_grad"
    ],
    "_backward": [
      "self"
    ],
    "get_grad": [
      "self"
    ],
    "_release_state": [
      "self"
    ]
  },
  "AbstractSchedulePlan": {
    "run": [
      "f_schedule_plan",
      "b_schedule_plan",
      "grad",
      "pre_forward",
      "pre_backward",
      "post_forward",
      "post_backward"
    ]
  },
  "_COMP_STREAM": [],
  "_COMM_STREAM": [],
  "set_streams": [
    "comp_stream",
    "comm_stream"
  ],
  "get_comp_stream": [],
  "get_comm_stream": [],
  "_batched_p2p_ops": [],
  "_p2p_ops": [],
  "is_single_shape": [
    "x"
  ],
  "P2PCommunicator": {
    "__init__": [
      "self",
      "pp_group",
      "config"
    ],
    "_communicate_shapes": [
      "self",
      "tensor_send_next",
      "tensor_send_prev",
      "recv_prev",
      "recv_next"
    ],
    "_communicate": [
      "self"
    ],
    "recv_forward": [
      "self",
      "tensor_shapes",
      "is_first_stage"
    ],
    "recv_backward": [
      "self",
      "tensor_shapes",
      "is_last_stage"
    ],
    "send_forward": [
      "self",
      "output_tensors",
      "is_last_stage"
    ],
    "send_backward": [
      "self",
      "input_tensor_grads",
      "is_first_stage"
    ],
    "send_forward_recv_backward": [
      "self",
      "output_tensors",
      "tensor_shapes",
      "is_last_stage"
    ],
    "send_backward_recv_forward": [
      "self",
      "input_tensor_grads",
      "tensor_shapes",
      "is_first_stage"
    ],
    "send_forward_recv_forward": [
      "self",
      "output_tensor",
      "recv_prev",
      "tensor_shape",
      "overlap_p2p_comm"
    ],
    "send_backward_recv_backward": [
      "self",
      "input_tensor_grad",
      "recv_next",
      "tensor_shape",
      "overlap_p2p_comm"
    ],
    "send_forward_backward_recv_forward_backward": [
      "self",
      "output_tensor",
      "input_tensor_grad",
      "recv_prev",
      "recv_next",
      "tensor_shape"
    ]
  },
  "BackendSpecProvider": {
    "column_parallel_linear": [
      "self"
    ],
    "row_parallel_linear": [
      "self"
    ],
    "fuse_layernorm_and_linear": [
      "self"
    ],
    "column_parallel_layer_norm_linear": [
      "self"
    ],
    "layer_norm": [
      "self",
      "rms_norm",
      "for_qk"
    ],
    "core_attention": [
      "self"
    ],
    "grouped_mlp_modules": [
      "self",
      "moe_use_grouped_gemm",
      "moe_use_legacy_grouped_gemm"
    ],
    "activation_func": [
      "self"
    ]
  },
  "LocalSpecProvider": {
    "column_parallel_linear": [
      "self"
    ],
    "row_parallel_linear": [
      "self"
    ],
    "fuse_layernorm_and_linear": [
      "self"
    ],
    "column_parallel_layer_norm_linear": [
      "self"
    ],
    "layer_norm": [
      "self",
      "rms_norm",
      "for_qk"
    ],
    "core_attention": [
      "self"
    ],
    "grouped_mlp_modules": [
      "self",
      "moe_use_grouped_gemm",
      "moe_use_legacy_grouped_gemm"
    ],
    "activation_func": [
      "self"
    ]
  },
  "ModelChunkState": {},
  "TransformerLayerSchedulePlan": {
    "attn": [],
    "post_attn": [],
    "moe_dispatch": [],
    "mlp": [],
    "moe_combine": [],
    "mtp_post_process": [],
    "__init__": [
      "self",
      "layer",
      "event",
      "chunk_state",
      "comp_stream",
      "comm_stream",
      "extra_args"
    ],
    "_build_callable_nodes": [
      "self",
      "event",
      "comp_stream",
      "comm_stream",
      "extra_args"
    ],
    "get_fp8_context": [
      "self"
    ],
    "run": [
      "f_layer",
      "b_layer",
      "f_input",
      "b_grad",
      "is_last_layer_in_bwd"
    ]
  },
  "TransformerModelChunkSchedulePlan": {
    "__init__": [
      "self",
      "model",
      "input_ids",
      "position_ids",
      "attention_mask",
      "decoder_input",
      "labels",
      "packed_seq_params",
      "extra_block_kwargs",
      "runtime_gather_output",
      "loss_mask"
    ],
    "event": [
      "self"
    ],
    "record_current_stream": [
      "self"
    ],
    "wait_current_stream": [
      "self"
    ],
    "get_layer": [
      "self",
      "i"
    ],
    "num_layers": [
      "self"
    ],
    "state": [
      "self"
    ],
    "release_state": [
      "self"
    ],
    "run": [
      "f_schedule_plan",
      "b_schedule_plan",
      "b_grad",
      "pre_forward",
      "pre_backward",
      "post_forward",
      "post_backward"
    ]
  },
  "VisionModule": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "RotaryEmbedding": {
    "__init__": [
      "self",
      "kv_channels",
      "rotary_percent",
      "rotary_interleaved",
      "seq_len_interpolation_factor",
      "rotary_base",
      "rope_scaling",
      "rope_scaling_factor",
      "use_cpu_initialization",
      "cp_group"
    ],
    "_apply_scaling": [
      "self",
      "freqs",
      "factor",
      "low_freq_factor",
      "high_freq_factor",
      "original_max_position_embeddings"
    ],
    "get_freqs_non_repeated": [
      "self",
      "max_seq_len",
      "offset"
    ],
    "get_cos_sin": [
      "self",
      "max_seq_len",
      "offset"
    ],
    "forward": [
      "self",
      "max_seq_len",
      "offset",
      "packed_seq"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix"
    ],
    "get_rotary_seq_len": [
      "self",
      "inference_context",
      "transformer",
      "transformer_input",
      "transformer_config",
      "packed_seq_params"
    ]
  },
  "MultimodalRotaryEmbedding": {
    "__init__": [
      "self",
      "kv_channels",
      "rotary_percent",
      "rotary_interleaved",
      "seq_len_interpolation_factor",
      "rotary_base",
      "cp_group"
    ],
    "forward": [
      "self",
      "position_ids",
      "mrope_section"
    ]
  },
  "LanguageModelEmbedding": {
    "__init__": [
      "self",
      "config",
      "vocab_size",
      "max_sequence_length",
      "position_embedding_type",
      "num_tokentypes",
      "scatter_to_sequence_parallel",
      "tp_group"
    ],
    "zero_parameters": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "tokentype_ids"
    ]
  },
  "YarnRotaryEmbedding": {
    "__init__": [
      "self",
      "kv_channels",
      "rotary_percent",
      "rotary_interleaved",
      "seq_len_interpolation_factor",
      "rotary_base",
      "use_cpu_initialization",
      "scaling_factor",
      "original_max_position_embeddings",
      "beta_fast",
      "beta_slow",
      "mscale",
      "mscale_all_dim",
      "correction_range_round_to_int",
      "cp_group"
    ],
    "forward": [
      "self",
      "max_seq_len",
      "offset",
      "packed_seq"
    ],
    "_set_cos_sin_cache": [
      "self",
      "seq_len",
      "offset",
      "dtype",
      "packed_seq"
    ],
    "get_cached_cos_sin": [
      "self",
      "seq_len",
      "offset",
      "dtype",
      "packed_seq"
    ]
  },
  "_yarn_find_correction_dim": [
    "num_rotations",
    "dim",
    "rotary_base",
    "max_position_embeddings"
  ],
  "_yarn_find_correction_range": [
    "low_rot",
    "high_rot",
    "dim",
    "rotary_base",
    "max_position_embeddings",
    "round_to_int"
  ],
  "_yarn_linear_ramp_mask": [
    "min",
    "max",
    "dim"
  ],
  "_yarn_get_mscale": [
    "scale",
    "mscale"
  ],
  "_yarn_get_concentration_factor": [
    "scaling_factor",
    "mscale",
    "mscale_all_dim"
  ],
  "_yarn_get_concentration_factor_from_config": [
    "config"
  ],
  "RelativePositionEmbedding": {
    "__init__": [
      "self",
      "bidirectional",
      "init_method",
      "num_attention_heads",
      "relative_attention_num_buckets",
      "relative_attention_max_distance"
    ],
    "_relative_position_bucket": [
      "self",
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "_compute_bias": [
      "self",
      "query_length",
      "key_length"
    ],
    "get_relative_seq_len": [
      "inference_context",
      "transformer",
      "transformer_input",
      "transformer_config"
    ],
    "forward": [
      "self",
      "query_seq_length",
      "key_seq_length"
    ]
  },
  "get_pos_emb_on_this_cp_rank": [
    "pos_emb",
    "seq_dim",
    "cp_group"
  ],
  "_rotate_half": [
    "x",
    "rotary_interleaved"
  ],
  "_apply_rotary_pos_emb_bshd": [
    "t",
    "freqs",
    "rotary_interleaved",
    "multi_latent_attention",
    "mscale"
  ],
  "_get_thd_freqs_on_this_cp_rank": [
    "cp_rank",
    "cp_size",
    "x",
    "freqs",
    "offset"
  ],
  "_apply_rotary_pos_emb_thd": [
    "t",
    "cu_seqlens",
    "freqs",
    "rotary_interleaved",
    "multi_latent_attention",
    "mscale",
    "cp_group"
  ],
  "apply_rotary_pos_emb": [
    "t",
    "freqs",
    "config",
    "cu_seqlens",
    "mscale",
    "cp_group"
  ],
  "apply_rotary_pos_emb_with_cos_sin": [
    "t",
    "cos",
    "sin",
    "rotary_interleaved"
  ],
  "LanguageModule": {
    "__init__": [
      "self",
      "config",
      "pg_collection"
    ],
    "_is_in_embd_group": [
      "self"
    ],
    "_set_attention_backend": [
      "self"
    ],
    "compute_language_model_loss": [
      "self",
      "labels",
      "logits"
    ],
    "setup_embeddings_and_output_layer": [
      "self"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "tie_embeddings_and_output_weights_state_dict": [
      "self",
      "sharded_state_dict",
      "output_layer_weight_key",
      "first_stage_word_emb_key"
    ]
  },
  "HuggingFaceModule": {
    "__init__": [
      "self",
      "config"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ]
  },
  "AutoHuggingFaceModel": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "get_hf_model_type": [
    "model_path"
  ],
  "build_hf_model": [
    "config",
    "model_path"
  ],
  "SiglipHuggingFaceModel": {
    "_fsdp_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ]
  },
  "QwenHuggingFaceModel": {
    "_fsdp_modules": [],
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self"
    ],
    "embedding": [
      "self",
      "input_ids",
      "position_ids"
    ]
  },
  "mamba_stack_spec": [],
  "MambaModel": {
    "__init__": [
      "self",
      "config",
      "mamba_stack_spec",
      "vocab_size",
      "max_sequence_length",
      "pre_process",
      "hybrid_attention_ratio",
      "hybrid_mlp_ratio",
      "hybrid_override_pattern",
      "post_process",
      "fp16_lm_cross_entropy",
      "parallel_output",
      "share_embeddings_and_output_weights",
      "position_embedding_type",
      "rotary_percent",
      "rotary_base",
      "scatter_embedding_sequence_parallel",
      "seq_len_interpolation_factor",
      "pg_collection"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "decoder_input",
      "labels",
      "inference_context",
      "runtime_gather_output"
    ]
  },
  "AudioModalitySubmodules": {
    "__init__": [
      "self",
      "encoders",
      "decoders",
      "input_projections",
      "output_projections"
    ],
    "encode": [
      "self",
      "encoders_data_batch"
    ],
    "decode": [
      "self",
      "embeddings",
      "data_batch"
    ],
    "combine_embeddings": [
      "self",
      "embeddings"
    ],
    "project_embeddings": [
      "self",
      "embeddings",
      "is_input"
    ],
    "forward": [
      "self",
      "encoder_inputs"
    ]
  },
  "ModalitySubmodules": {
    "__init__": [
      "self",
      "encoders",
      "decoders",
      "input_projections",
      "output_projections"
    ],
    "from_spec": [
      "cls",
      "module_spec"
    ],
    "combine_embeddings": [
      "self",
      "embeddings"
    ],
    "encode": [
      "self",
      "data_batch"
    ],
    "decode": [
      "self",
      "embeddings",
      "data_batch"
    ],
    "project_embeddings": [
      "self",
      "embeddings",
      "is_input"
    ],
    "forward": [
      "self",
      "encoder_inputs"
    ]
  },
  "VisionModalitySubmodules": {
    "__init__": [
      "self",
      "encoders",
      "decoders",
      "input_projections",
      "output_projections"
    ],
    "encode": [
      "self",
      "encoders_data_batch"
    ],
    "decode": [
      "self",
      "embeddings",
      "data_batch"
    ],
    "combine_embeddings": [
      "self",
      "embeddings"
    ],
    "project_embeddings": [
      "self",
      "embeddings",
      "is_input"
    ],
    "forward": [
      "self",
      "encoder_inputs"
    ]
  },
  "MimoModelConfig": {},
  "MimoModel": {
    "__init__": [
      "self",
      "mimo_config"
    ],
    "align_embeddings_by_token_positions": [
      "self",
      "modality_embeddings",
      "input_ids",
      "special_token_ids"
    ],
    "_initialize_submodules": [
      "self"
    ],
    "_initialize_language_model": [
      "self"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "get_text_embeddings": [
      "self",
      "input_ids",
      "position_ids",
      "special_token_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "loss_mask",
      "labels",
      "modality_inputs"
    ]
  },
  "BertModel": {
    "__init__": [
      "self",
      "config",
      "num_tokentypes",
      "transformer_layer_spec",
      "vocab_size",
      "max_sequence_length",
      "pre_process",
      "post_process",
      "fp16_lm_cross_entropy",
      "parallel_output",
      "share_embeddings_and_output_weights",
      "position_embedding_type",
      "rotary_percent",
      "seq_len_interpolation_factor",
      "add_binary_head",
      "return_embeddings",
      "vp_stage"
    ],
    "_sanity_check_attention_and_get_attn_mask_dimension": [
      "self"
    ],
    "bert_extended_attention_mask": [
      "self",
      "attention_mask"
    ],
    "bert_position_ids": [
      "self",
      "token_ids"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "tokentype_ids",
      "lm_labels",
      "inference_context"
    ]
  },
  "get_bert_layer_with_transformer_engine_spec": [],
  "__getattr__": [
    "name"
  ],
  "bert_layer_local_spec": [],
  "BertLMHead": {
    "__init__": [
      "self",
      "hidden_size",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Pooler": {
    "__init__": [
      "self",
      "hidden_size",
      "init_method",
      "config",
      "sequence_parallel"
    ],
    "forward": [
      "self",
      "hidden_states",
      "sequence_index"
    ]
  },
  "GPTModel": {
    "__init__": [
      "self",
      "config",
      "transformer_layer_spec",
      "vocab_size",
      "max_sequence_length",
      "pre_process",
      "post_process",
      "fp16_lm_cross_entropy",
      "parallel_output",
      "share_embeddings_and_output_weights",
      "position_embedding_type",
      "rotary_percent",
      "rotary_base",
      "rope_scaling",
      "rope_scaling_factor",
      "scatter_embedding_sequence_parallel",
      "seq_len_interpolation_factor",
      "mtp_block_spec",
      "pg_collection",
      "vp_stage"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "_preprocess": [
      "self",
      "input_ids",
      "position_ids",
      "decoder_input",
      "inference_context",
      "packed_seq_params"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "decoder_input",
      "labels",
      "inference_context",
      "packed_seq_params",
      "extra_block_kwargs",
      "runtime_gather_output"
    ],
    "_postprocess": [
      "self",
      "hidden_states",
      "input_ids",
      "position_ids",
      "labels",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "mtp_in_postprocess",
      "loss_mask",
      "decoder_input",
      "attention_mask",
      "inference_params",
      "packed_seq_params",
      "sequence_len_offset",
      "runtime_gather_output",
      "extra_block_kwargs",
      "inference_context"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "build_schedule_plan": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "decoder_input",
      "labels",
      "inference_context",
      "packed_seq_params",
      "extra_block_kwargs",
      "runtime_gather_output",
      "inference_params",
      "loss_mask"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "get_moe_module_spec": [
    "use_te",
    "num_experts",
    "moe_grouped_gemm",
    "moe_use_legacy_grouped_gemm"
  ],
  "get_moe_module_spec_for_backend": [
    "backend",
    "num_experts",
    "moe_grouped_gemm",
    "moe_use_legacy_grouped_gemm",
    "use_te_activation_func"
  ],
  "get_gpt_layer_with_transformer_engine_spec": [
    "num_experts",
    "moe_grouped_gemm",
    "qk_layernorm",
    "multi_latent_attention",
    "fp8",
    "moe_use_legacy_grouped_gemm",
    "qk_l2_norm",
    "use_te_op_fuser",
    "use_kitchen",
    "use_te_activation_func"
  ],
  "get_gpt_layer_local_spec": [
    "num_experts",
    "moe_grouped_gemm",
    "qk_layernorm",
    "multi_latent_attention",
    "fp8",
    "moe_use_legacy_grouped_gemm",
    "normalization",
    "qk_l2_norm",
    "use_kitchen"
  ],
  "_get_mlp_module_spec": [
    "use_te",
    "num_experts",
    "moe_grouped_gemm",
    "fp8",
    "moe_use_legacy_grouped_gemm"
  ],
  "get_mlp_module_spec": [
    "use_te",
    "num_experts",
    "moe_grouped_gemm",
    "fp8",
    "moe_use_legacy_grouped_gemm",
    "use_te_op_fuser"
  ],
  "get_mlp_module_spec_for_backend": [
    "backend",
    "num_experts",
    "moe_grouped_gemm",
    "moe_use_legacy_grouped_gemm",
    "use_te_op_fuser",
    "use_te_activation_func"
  ],
  "get_gpt_decoder_block_spec": [
    "config",
    "use_transformer_engine",
    "normalization",
    "qk_l2_norm",
    "vp_stage",
    "pp_rank"
  ],
  "get_gpt_mtp_block_spec": [
    "config",
    "spec",
    "use_transformer_engine",
    "vp_stage",
    "pp_rank"
  ],
  "get_gpt_mtp_block_spec_for_backend": [
    "config",
    "spec",
    "backend",
    "vp_stage",
    "pp_rank"
  ],
  "weak_method": [
    "method"
  ],
  "should_free_input": [
    "name",
    "is_moe",
    "is_deepep"
  ],
  "TransformerLayerState": {},
  "PreProcessNode": {
    "__init__": [
      "self",
      "gpt_model",
      "chunk_state",
      "event",
      "stream"
    ],
    "forward_impl": [
      "self"
    ]
  },
  "PostProcessNode": {
    "__init__": [
      "self",
      "gpt_model",
      "chunk_state",
      "event",
      "stream"
    ],
    "forward_impl": [
      "self",
      "hidden_states"
    ]
  },
  "TransformerLayerNode": {
    "__init__": [
      "self",
      "stream",
      "event",
      "layer_state",
      "chunk_state",
      "submodule",
      "name",
      "bwd_dw_callables",
      "extra_args"
    ],
    "detach": [
      "self",
      "t"
    ],
    "forward_impl": [
      "self"
    ],
    "backward_impl": [
      "self",
      "outputs",
      "output_grad"
    ],
    "backward_dw": [
      "self"
    ],
    "_release_state": [
      "self"
    ]
  },
  "build_transformer_layer_callables": [
    "layer"
  ],
  "build_mtp_layer_callables": [
    "layer"
  ],
  "build_layer_callables": [
    "layer"
  ],
  "_get_layer_norm": [
    "config",
    "use_te",
    "normalization"
  ],
  "_get_qk_layernorm": [
    "use_te",
    "normalization"
  ],
  "_get_heterogenous_attention_spec": [
    "attn_config",
    "use_te",
    "qk_layernorm",
    "normalization"
  ],
  "_get_heterogenous_mlp_spec": [
    "mlp_config",
    "use_te"
  ],
  "_get_sharded_state_dict_keys_map": [
    "block_config",
    "use_te"
  ],
  "get_gpt_heterogeneous_layer_spec": [
    "config",
    "use_te",
    "vp_stage",
    "pp_rank"
  ],
  "RetroDecoderCrossAttention": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "attn_mask_type",
      "encoder_block_spec",
      "pg_collection"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "key_value_states",
      "inference_context"
    ]
  },
  "RetroDecoderBiasDropoutAdd": {
    "__init__": [
      "self",
      "config"
    ],
    "_forward": [
      "cls",
      "x_with_bias",
      "residual",
      "prob",
      "retro_chunk_length",
      "bias_dropout_add"
    ],
    "forward": [
      "self",
      "training",
      "fused"
    ]
  },
  "get_retro_decoder_layer_te_spec": [
    "encoder_block_spec"
  ],
  "get_retro_decoder_layer_local_spec": [
    "encoder_block_spec"
  ],
  "get_retro_decoder_block_spec": [
    "config",
    "use_transformer_engine",
    "vp_stage",
    "pp_rank"
  ],
  "get_config_path": [
    "project_dir"
  ],
  "get_gpt_data_dir": [
    "project_dir"
  ],
  "get_all_true_mask": [
    "size",
    "device"
  ],
  "RetroConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "RetroModel": {
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "context_input_ids",
      "context_position_ids",
      "context_mask",
      "decoder_input",
      "labels",
      "inference_context"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "BaseRetroCrossAttention": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "attn_mask_type",
      "pg_collection"
    ]
  },
  "get_retro_encoder_layer_te_spec": [],
  "get_retro_encoder_layer_local_spec": [],
  "get_retro_encoder_block_spec": [
    "config",
    "use_transformer_engine"
  ],
  "RetroEncoderCrossAttention": {
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "key_value_states",
      "inference_context"
    ]
  },
  "RetroEncoderBiasDropoutAdd": {
    "__init__": [
      "self",
      "config"
    ],
    "_forward": [
      "cls",
      "x_with_bias",
      "residual",
      "prob",
      "retro_num_neighbors",
      "bias_dropout_add"
    ],
    "forward": [
      "self",
      "training",
      "fused"
    ]
  },
  "RetroEncoderLayerNorm": {
    "__init__": [
      "self",
      "config",
      "submodules"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "decoder_model_with_transformer_engine_default_spec": [
    "num_experts",
    "moe_grouped_gemm",
    "qk_layernorm"
  ],
  "decoder_model_with_local_default_spec": [
    "num_experts",
    "moe_grouped_gemm",
    "qk_layernorm"
  ],
  "IGNORE_INDEX": [],
  "DEFAULT_IMAGE_TOKEN_INDEX": [],
  "IMAGE_TOKEN": [],
  "VIDEO_TOKEN": [],
  "LLaVAModel": {
    "__init__": [
      "self",
      "language_transformer_config",
      "language_transformer_layer_spec",
      "language_vocab_size",
      "language_max_sequence_length",
      "vision_transformer_config",
      "vision_transformer_layer_spec",
      "drop_vision_class_token",
      "vision_projection_config",
      "vision_projection_layer_spec",
      "vision_projection_type",
      "allow_missing_vision_projection_checkpoint",
      "parallel_output",
      "share_embeddings_and_output_weights",
      "language_position_embedding_type",
      "language_rotary_percent",
      "pre_process",
      "post_process",
      "add_encoder",
      "add_decoder",
      "img_h",
      "img_w",
      "patch_dim",
      "language_rotary_base",
      "language_rope_scaling",
      "language_rope_scaling_factor",
      "hybrid_attention_ratio",
      "hybrid_mlp_ratio",
      "hybrid_override_pattern",
      "fp16_lm_cross_entropy",
      "image_token_index",
      "pixel_shuffle",
      "tile_tags",
      "pg_collection",
      "max_num_tiles",
      "tokenizer_type",
      "vp_stage"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "freeze": [
      "self",
      "freeze_language_model",
      "freeze_vision_model",
      "freeze_vision_projection"
    ],
    "_preprocess_data": [
      "self",
      "image_embeddings",
      "language_embeddings",
      "input_ids",
      "loss_mask",
      "labels",
      "use_inference_kv_cache",
      "inference_context",
      "image_token_index",
      "num_image_tiles"
    ],
    "_process_embedding_token_parallel": [
      "self",
      "combined_embeddings",
      "new_labels",
      "new_loss_mask",
      "packed_seq_params"
    ],
    "_apply_tile_tagging": [
      "self",
      "image_embeddings",
      "num_image_tiles"
    ],
    "forward": [
      "self",
      "images",
      "input_ids",
      "position_ids",
      "attention_mask",
      "labels",
      "loss_mask",
      "inference_context",
      "num_image_tiles",
      "image_token_index",
      "runtime_gather_output",
      "packed_seq_params"
    ]
  },
  "_load_state_dict_hook_ignore_param_names": [
    "param_names",
    "module",
    "incompatible_keys"
  ],
  "_load_state_dict_hook_ignore_extra_state": [
    "module",
    "incompatible_keys"
  ],
  "pixel_shuffle": [
    "x",
    "scale_factor",
    "version"
  ],
  "get_padding": [
    "seq_len",
    "cp_size",
    "tp_size",
    "has_sp",
    "decoder_tp_comm_overlap",
    "decoder_seq_len",
    "fp8_enabled",
    "fp8_recipe"
  ],
  "get_packed_seq_params": [
    "tokens",
    "img_seq_len",
    "padding_needed",
    "cp_size",
    "use_packed_sequence"
  ],
  "MultimodalProjector": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "projector_type",
      "input_size",
      "tp_group"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "get_vit_layer_with_transformer_engine_spec": [],
  "get_vit_layer_with_local_spec": [],
  "RADIOViTModel": {
    "__init__": [
      "self",
      "transformer_config",
      "transformer_layer_spec",
      "ln_pre_impl",
      "ln_post_impl",
      "use_mask_token",
      "add_class_token",
      "class_token_len",
      "patch_dim",
      "img_h",
      "img_w",
      "max_img_h",
      "max_img_w",
      "pos_dropout",
      "has_cpe",
      "embedder_bias",
      "pg_collection",
      "vp_stage"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "forward": [
      "self",
      "x",
      "attention_mask"
    ],
    "apply_pos_enc": [
      "self",
      "patches",
      "patch_idxs",
      "input_size"
    ],
    "get_pos_enc": [
      "self",
      "batch_size",
      "patch_idxs",
      "input_size"
    ],
    "_get_pos_embeddings": [
      "self",
      "batch_size",
      "input_dims"
    ]
  },
  "fp8_pad_hook": [
    "module",
    "state_dict",
    "prefix",
    "local_metadata",
    "strict",
    "missing_keys",
    "unexpected_keys",
    "error_msgs"
  ],
  "CLIPViTModel": {
    "__init__": [
      "self",
      "transformer_config",
      "transformer_layer_spec",
      "ln_pre_impl",
      "ln_post_impl",
      "add_class_token",
      "class_token_len",
      "patch_dim",
      "img_h",
      "img_w",
      "model_subtype",
      "pg_collection",
      "vp_stage"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "forward": [
      "self",
      "x",
      "attention_mask"
    ]
  },
  "get_num_image_embeddings": [
    "img_h",
    "img_w",
    "patch_dim",
    "vision_model_type",
    "disable_vision_class_token",
    "class_token_len",
    "pixel_shuffle",
    "use_tile_tags",
    "max_num_tiles",
    "tokenizer_type"
  ],
  "encoder_model_with_transformer_engine_default_spec": [],
  "encoder_model_with_local_spec": [],
  "decoder_model_with_local_spec": [],
  "get_t5_encoder_with_transformer_engine_block_spec": [
    "num_layers"
  ],
  "get_t5_decoder_with_transformer_engine_block_spec": [
    "num_layers"
  ],
  "get_t5_encoder_with_local_block_spec": [
    "num_layers"
  ],
  "get_t5_decoder_with_local_block_spec": [
    "num_layers"
  ],
  "T5LMHead": {
    "__init__": [
      "self",
      "config",
      "parallel_output",
      "vocab_size",
      "pre_process",
      "share_embeddings_and_output_weights",
      "tp_group"
    ],
    "forward": [
      "self",
      "hidden_states",
      "word_embeddings_weight"
    ]
  },
  "T5Model": {
    "__init__": [
      "self",
      "config",
      "encoder_config",
      "transformer_encoder_layer_spec",
      "transformer_decoder_layer_spec",
      "vocab_size",
      "max_sequence_length",
      "pre_process",
      "post_process",
      "fp16_lm_cross_entropy",
      "parallel_output",
      "share_embeddings_and_output_weights",
      "position_embedding_type",
      "rotary_percent",
      "seq_len_interpolation_factor",
      "relative_attention_num_buckets",
      "relative_attention_max_distance",
      "add_encoder",
      "add_decoder",
      "pg_collection"
    ],
    "forward": [
      "self",
      "encoder_input_ids",
      "decoder_input_ids",
      "encoder_attn_mask",
      "decoder_attn_mask",
      "encoder_decoder_attn_mask",
      "lm_labels",
      "encoder_hidden_states",
      "output_encoder_hidden_only",
      "inference_context",
      "packed_seq_params"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "t5_extended_attention_mask": [
    "attention_mask_list"
  ],
  "t5_position_ids": [
    "token_ids"
  ],
  "VocabParallelCrossEntropy": {
    "calculate_logits_max": [
      "vocab_parallel_logits"
    ],
    "calculate_predicted_logits": [
      "vocab_parallel_logits",
      "target",
      "logits_max",
      "vocab_start_index",
      "vocab_end_index"
    ],
    "calculate_cross_entropy_loss": [
      "exp_logits",
      "predicted_logits",
      "sum_exp_logits"
    ],
    "prepare_gradient_calculation_operands": [
      "softmax",
      "target_mask"
    ],
    "calculate_gradients": [
      "grad_2d",
      "arange_1d",
      "masked_target_1d",
      "softmax_update",
      "grad_input",
      "grad_output"
    ]
  },
  "vocab_parallel_cross_entropy": [
    "vocab_parallel_logits",
    "target",
    "label_smoothing"
  ],
  "_reduce": [
    "input_",
    "group"
  ],
  "_split_along_last_dim": [
    "input_",
    "group"
  ],
  "_split_along_first_dim": [
    "input_",
    "group"
  ],
  "_gather_along_last_dim": [
    "input_",
    "group"
  ],
  "_reduce_scatter_along_last_dim": [
    "input_",
    "group"
  ],
  "_gather_along_first_dim": [
    "input_",
    "group",
    "output_split_sizes",
    "use_global_buffer"
  ],
  "_reduce_scatter_along_first_dim": [
    "input_",
    "group",
    "input_split_sizes",
    "use_global_buffer"
  ],
  "_CopyToModelParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group"
    ],
    "forward": [
      "ctx",
      "input_",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_ReduceFromModelParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group"
    ],
    "forward": [
      "ctx",
      "input_",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_ScatterToModelParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group"
    ],
    "forward": [
      "ctx",
      "input_",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_GatherFromModelParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group"
    ],
    "forward": [
      "ctx",
      "input_",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_ScatterToSequenceParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group"
    ],
    "forward": [
      "ctx",
      "input_",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_GatherFromSequenceParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group",
      "tensor_parallel_output_grad",
      "output_split_sizes",
      "use_global_buffer"
    ],
    "forward": [
      "ctx",
      "input_",
      "group",
      "tensor_parallel_output_grad",
      "output_split_sizes",
      "use_global_buffer"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_ReduceScatterToSequenceParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group",
      "input_split_sizes",
      "use_global_buffer"
    ],
    "forward": [
      "ctx",
      "input_",
      "group",
      "input_split_sizes",
      "use_global_buffer"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_AllGatherFromTensorParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group"
    ],
    "forward": [
      "ctx",
      "input_",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_ReduceScatterToTensorParallelRegion": {
    "symbolic": [
      "graph",
      "input_",
      "group"
    ],
    "forward": [
      "ctx",
      "input_",
      "group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "_AllToAll": {
    "forward": [
      "ctx",
      "group",
      "input",
      "output_split_sizes",
      "input_split_sizes"
    ],
    "backward": [
      "ctx"
    ]
  },
  "copy_to_tensor_model_parallel_region": [
    "input_",
    "group"
  ],
  "reduce_from_tensor_model_parallel_region": [
    "input_",
    "group"
  ],
  "scatter_to_tensor_model_parallel_region": [
    "input_",
    "group"
  ],
  "gather_from_tensor_model_parallel_region": [
    "input_",
    "group"
  ],
  "scatter_to_sequence_parallel_region": [
    "input_",
    "group"
  ],
  "gather_from_sequence_parallel_region": [
    "input_",
    "tensor_parallel_output_grad",
    "group",
    "output_split_sizes",
    "use_global_buffer"
  ],
  "reduce_scatter_to_sequence_parallel_region": [
    "input_",
    "group",
    "input_split_sizes",
    "use_global_buffer"
  ],
  "all_gather_last_dim_from_tensor_parallel_region": [
    "input_",
    "group"
  ],
  "reduce_scatter_last_dim_to_tensor_parallel_region": [
    "input_",
    "group"
  ],
  "all_to_all": [
    "group",
    "input_",
    "output_split_sizes_",
    "input_split_sizes"
  ],
  "all_to_all_sp2hp": [
    "input_",
    "group"
  ],
  "all_to_all_hp2sp": [
    "input_",
    "group"
  ],
  "split_tensor_along_last_dim": [
    "tensor",
    "num_partitions",
    "contiguous_split_chunks"
  ],
  "split_tensor_into_1d_equal_chunks": [
    "tensor",
    "new_buffer",
    "tp_group"
  ],
  "gather_split_1d_tensor": [
    "tensor",
    "tp_group"
  ],
  "VocabUtility": {
    "vocab_range_from_per_partition_vocab_size": [
      "per_partition_vocab_size",
      "rank",
      "world_size"
    ],
    "vocab_range_from_global_vocab_size": [
      "global_vocab_size",
      "rank",
      "world_size"
    ]
  },
  "_grad_accum_fusion_available": [],
  "_MODEL_PARALLEL_ATTRIBUTE_DEFAULTS": [],
  "param_is_not_tensor_parallel_duplicate": [
    "param"
  ],
  "set_tensor_model_parallel_attributes": [
    "tensor",
    "is_parallel",
    "dim",
    "stride"
  ],
  "set_defaults_if_not_set_tensor_model_parallel_attributes": [
    "tensor"
  ],
  "copy_tensor_model_parallel_attributes": [
    "destination_tensor",
    "source_tensor"
  ],
  "_initialize_affine_weight_gpu": [
    "weight",
    "init_method",
    "partition_dim",
    "stride",
    "is_expert"
  ],
  "_initialize_affine_weight_cpu": [
    "weight",
    "output_size",
    "input_size",
    "per_partition_size",
    "partition_dim",
    "init_method",
    "stride",
    "return_master_weight"
  ],
  "VocabParallelEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "input_"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ]
  },
  "LinearWithFrozenWeight": {
    "forward": [
      "ctx",
      "input",
      "weight",
      "bias",
      "allreduce_dgrad",
      "tp_group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "linear_with_frozen_weight": [
    "input",
    "weight",
    "bias",
    "gradient_accumulation_fusion",
    "allreduce_dgrad",
    "sequence_parallel",
    "tp_group",
    "grad_output_buffer",
    "wgrad_deferral_limit",
    "async_grad_allreduce"
  ],
  "LinearWithGradAccumulationAndAsyncCommunication": {
    "forward": [
      "ctx",
      "input",
      "weight",
      "bias",
      "gradient_accumulation_fusion",
      "allreduce_dgrad",
      "sequence_parallel",
      "grad_output_buffer",
      "wgrad_deferral_limit",
      "tp_group"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "linear_with_grad_accumulation_and_async_allreduce": [
    "input",
    "weight",
    "bias",
    "gradient_accumulation_fusion",
    "allreduce_dgrad",
    "sequence_parallel",
    "grad_output_buffer",
    "wgrad_deferral_limit",
    "async_grad_allreduce",
    "tp_group"
  ],
  "ColumnParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "_forward_impl": [
      "self",
      "input",
      "weight"
    ],
    "forward": [
      "self",
      "input_",
      "weight",
      "runtime_gather_output"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "set_extra_state": [
      "self",
      "state"
    ],
    "get_extra_state": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "RowParallelLinear": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "_forward_impl": [
      "self",
      "input",
      "weight"
    ],
    "forward": [
      "self",
      "input_"
    ],
    "sharded_state_dict": [
      "self",
      "prefix",
      "sharded_offsets",
      "metadata"
    ],
    "set_extra_state": [
      "self",
      "state"
    ],
    "get_extra_state": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_MAX_DATA_DIM": [],
  "_check_data_types": [
    "keys",
    "data",
    "target_dtype"
  ],
  "_build_key_size_numel_dictionaries": [
    "keys",
    "data",
    "tp_group"
  ],
  "broadcast_data": [
    "keys",
    "data",
    "datatype",
    "tp_group"
  ],
  "_EXPERT_PARALLEL_RNG_TRACKER_NAME": [],
  "_DATA_PARALLEL_RNG_TRACKER_NAME": [],
  "get_expert_parallel_rng_tracker_name": [],
  "get_data_parallel_rng_tracker_name": [],
  "CudaRNGStatesTracker": {
    "__init__": [
      "self",
      "use_cudagraphable_rng",
      "is_inference_rng_tracker"
    ],
    "is_initialized": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "get_states": [
      "self"
    ],
    "set_states": [
      "self",
      "states"
    ],
    "add": [
      "self",
      "name",
      "seed"
    ],
    "fork": [
      "self",
      "name"
    ]
  },
  "_CUDA_RNG_STATE_TRACKER": [],
  "_CUDA_RNG_STATE_TRACKER_INITIALIZED": [],
  "get_all_rng_states": [],
  "model_parallel_cuda_manual_seed": [
    "seed",
    "te_rng_tracker",
    "inference_rng_tracker",
    "use_cudagraphable_rng",
    "tp_rank",
    "ep_rank",
    "etp_rank",
    "force_reset_rng"
  ],
  "_get_all_rng_states": [],
  "_set_all_rng_states": [
    "cpu_rng_state",
    "cuda_rng_state",
    "cuda_rng_state_tracker"
  ],
  "_fork_rng": [],
  "CheckpointFunction": {
    "forward": [
      "ctx",
      "run_function",
      "distribute_saved_activations"
    ],
    "backward": [
      "ctx"
    ]
  },
  "checkpoint": [
    "function",
    "distribute_saved_activations"
  ],
  "CheckpointWithoutOutputFunction": {
    "forward": [
      "ctx",
      "run_function",
      "checkpoint_without_output_obj"
    ],
    "backward": [
      "ctx"
    ]
  },
  "CheckpointWithoutOutput": {
    "__init__": [
      "self",
      "fp8"
    ],
    "checkpoint": [
      "self",
      "run_function"
    ],
    "_recompute": [
      "self",
      "_"
    ],
    "discard_output_and_register_recompute": [
      "self",
      "hook_tensor"
    ]
  }
}