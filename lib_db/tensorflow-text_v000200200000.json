{
  "_allowed_symbols": [],
  "__version__": [],
  "gen_phrase_tokenizer": [],
  "_tf_text_phrase_tokenizer_op_create_counter": [],
  "PhraseTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "token_out_type",
      "unknown_token",
      "support_detokenization",
      "prob",
      "split_end_punctuation",
      "model_buffer"
    ],
    "tokenize": [
      "self",
      "input"
    ],
    "detokenize": [
      "self",
      "input_t"
    ]
  },
  "_tf_text_fast_bert_tokenizer_op_create_counter": [],
  "FastBertTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "suffix_indicator",
      "max_bytes_per_word",
      "token_out_type",
      "unknown_token",
      "no_pretokenization",
      "support_detokenization",
      "fast_wordpiece_model_buffer",
      "lower_case_nfd_strip_accents",
      "fast_bert_normalizer_model_buffer"
    ],
    "tokenize_with_offsets": [
      "self",
      "text_input"
    ],
    "tokenize": [
      "self",
      "text_input"
    ],
    "detokenize": [
      "self",
      "token_ids"
    ]
  },
  "pad_model_inputs": [
    "input",
    "max_seq_length",
    "pad_value"
  ],
  "_unichr": [
    "codepoint"
  ],
  "coerce_to_structurally_valid_utf8": [
    "input",
    "replacement_char",
    "name"
  ],
  "_ragged_tensor_scatter_nd_update": [
    "params",
    "indices",
    "updates"
  ],
  "_get_random": [
    "positions"
  ],
  "_get_selected_item_positions": [
    "item_selector",
    "input_ids",
    "axis"
  ],
  "mask_language_model": [
    "input_ids",
    "item_selector",
    "mask_values_chooser",
    "axis"
  ],
  "MaskValuesChooser": {
    "__init__": [
      "self",
      "vocab_size",
      "mask_token",
      "mask_token_rate",
      "random_token_rate"
    ],
    "mask_token": [
      "self"
    ],
    "random_token_rate": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "get_mask_values": [
      "self",
      "masked_lm_ids"
    ]
  },
  "_emoticon_regex": [],
  "_emoji_regex": [],
  "_begins_with_open_quote_regex": [],
  "_ends_with_close_quote_regex": [],
  "WordShape": {
    "HAS_PUNCTUATION_DASH": [],
    "HAS_NO_DIGITS": [],
    "HAS_SOME_DIGITS": [],
    "HAS_ONLY_DIGITS": [],
    "IS_NUMERIC_VALUE": [],
    "HAS_NO_PUNCT_OR_SYMBOL": [],
    "HAS_SOME_PUNCT_OR_SYMBOL": [],
    "IS_PUNCT_OR_SYMBOL": [],
    "BEGINS_WITH_PUNCT_OR_SYMBOL": [],
    "ENDS_WITH_PUNCT_OR_SYMBOL": [],
    "ENDS_WITH_ELLIPSIS": [],
    "IS_EMOTICON": [],
    "ENDS_WITH_EMOTICON": [],
    "HAS_EMOJI": [],
    "IS_ACRONYM_WITH_PERIODS": [],
    "IS_UPPERCASE": [],
    "IS_LOWERCASE": [],
    "HAS_MIXED_CASE": [],
    "IS_MIXED_CASE_LETTERS": [],
    "HAS_TITLE_CASE": [],
    "BEGINS_WITH_OPEN_QUOTE": [],
    "ENDS_WITH_CLOSE_QUOTE": [],
    "HAS_MATH_SYMBOL": [],
    "HAS_CURRENCY_SYMBOL": [],
    "HAS_NON_LETTER": []
  },
  "_wordshape_doc": [],
  "_add_identifier_list_to_docstring": [
    "func"
  ],
  "wordshape": [
    "input_tensor",
    "pattern",
    "name"
  ],
  "gen_normalize_ops": [],
  "case_fold_utf8": [
    "input",
    "name"
  ],
  "normalize_utf8": [
    "input",
    "normalization_form",
    "name"
  ],
  "normalize_utf8_with_offsets_map": [
    "input",
    "normalization_form",
    "name"
  ],
  "find_source_offsets": [
    "offsets_map",
    "input_offsets",
    "name"
  ],
  "_max_bits": [],
  "create_feature_bitmask": [
    "tensor",
    "dtype",
    "name"
  ],
  "gen_fast_sentencepiece_tokenizer": [],
  "FastSentencepieceTokenizer": {
    "__init__": [
      "self",
      "model",
      "reverse",
      "add_bos",
      "add_eos"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "input"
    ],
    "vocab_size": [
      "self"
    ]
  },
  "pad_along_dimension": [
    "data",
    "axis",
    "left_pad",
    "right_pad",
    "name"
  ],
  "_get_positive_axis": [
    "axis",
    "ndims"
  ],
  "_padding_for_dimension": [
    "data",
    "axis",
    "pad_value"
  ],
  "gen_ngrams_op": [],
  "Reduction": {
    "SUM": [],
    "MEAN": [],
    "STRING_JOIN": []
  },
  "ngrams": [
    "data",
    "width",
    "axis",
    "reduction_type",
    "string_separator",
    "name"
  ],
  "gen_fast_bert_normalizer": [],
  "_tf_text_fast_bert_normalizer_op_create_counter": [],
  "FastBertNormalizer": {
    "__init__": [
      "self",
      "lower_case_nfd_strip_accents",
      "model_buffer"
    ],
    "normalize": [
      "self",
      "input"
    ],
    "normalize_with_offsets": [
      "self",
      "input"
    ],
    "_normalize_with_offsets_helper": [
      "self",
      "input",
      "get_offsets"
    ]
  },
  "combine_segments": [
    "segments",
    "start_of_sequence_id",
    "end_of_segment_id"
  ],
  "concatenate_segments": [
    "segments"
  ],
  "_tf_text_unicode_char_tokenizer_create_counter": [],
  "UnicodeCharTokenizer": {
    "__init__": [
      "self"
    ],
    "tokenize": [
      "self",
      "input"
    ],
    "tokenize_with_offsets": [
      "self",
      "input"
    ],
    "detokenize": [
      "self",
      "input",
      "name"
    ]
  },
  "ItemSelector": {
    "__init__": [
      "self",
      "unselectable_ids"
    ],
    "unselectable_ids": [
      "self"
    ],
    "get_selectable": [
      "self",
      "input_ids",
      "axis"
    ],
    "get_selection_mask": [
      "self",
      "input_ids",
      "axis"
    ]
  },
  "RandomItemSelector": {
    "__init__": [
      "self",
      "max_selections_per_batch",
      "selection_rate",
      "unselectable_ids",
      "shuffle_fn"
    ],
    "shuffle_fn": [
      "self"
    ],
    "max_selections_per_batch": [
      "self"
    ],
    "selection_rate": [
      "self"
    ],
    "get_selection_mask": [
      "self",
      "input_ids",
      "axis"
    ]
  },
  "_get_row_lengths_merged_to_axis": [
    "segments",
    "axis"
  ],
  "_get_selection_mask": [
    "original",
    "num_to_select",
    "axis",
    "reverse"
  ],
  "_get_first_or_last_n_item_selectable": [
    "input_ids",
    "axis",
    "all_selectable",
    "num_to_select",
    "reverse"
  ],
  "FirstNItemSelector": {
    "__init__": [
      "self",
      "num_to_select",
      "unselectable_ids"
    ],
    "get_selectable": [
      "self",
      "input_ids",
      "axis"
    ]
  },
  "LastNItemSelector": {
    "__init__": [
      "self",
      "num_to_select",
      "unselectable_ids"
    ],
    "get_selectable": [
      "self",
      "input_ids",
      "axis"
    ]
  },
  "NothingSelector": {
    "__init__": [
      "self"
    ],
    "get_selectable": [
      "self",
      "input_ids",
      "axis"
    ]
  },
  "gen_constrained_sequence_op": [],
  "viterbi_constrained_sequence": [
    "scores",
    "sequence_length",
    "allowed_transitions",
    "transition_weights",
    "use_log_space",
    "use_start_and_end_states",
    "name"
  ],
  "gather_with_default": [
    "params",
    "indices",
    "default",
    "name",
    "axis"
  ],
  "span_overlaps": [
    "source_start",
    "source_limit",
    "target_start",
    "target_limit",
    "contains",
    "contained_by",
    "partial_overlap",
    "name"
  ],
  "_span_overlaps": [
    "source_start",
    "source_limit",
    "target_start",
    "target_limit",
    "contains",
    "contained_by",
    "partial_overlap"
  ],
  "_broadcast_ragged_targets_for_overlap": [
    "target_start",
    "target_limit",
    "source_splits"
  ],
  "_broadcast_ragged_sources_for_overlap": [
    "source_start",
    "source_limit",
    "target_splits"
  ],
  "span_alignment": [
    "source_start",
    "source_limit",
    "target_start",
    "target_limit",
    "contains",
    "contained_by",
    "partial_overlap",
    "multivalent_result",
    "name"
  ],
  "_multivalent_span_alignment": [
    "overlaps"
  ],
  "_check_type": [
    "value",
    "name",
    "expected_type"
  ],
  "sliding_window": [
    "data",
    "width",
    "axis",
    "name"
  ],
  "Splitter": {
    "__metaclass__": [],
    "split": [
      "self",
      "input"
    ]
  },
  "SplitterWithOffsets": {
    "split_with_offsets": [
      "self",
      "input"
    ]
  },
  "gen_fast_wordpiece_tokenizer": [],
  "_tf_text_fast_wordpiece_tokenizer_op_create_counter": [],
  "FastWordpieceTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "suffix_indicator",
      "max_bytes_per_word",
      "token_out_type",
      "unknown_token",
      "no_pretokenization",
      "support_detokenization",
      "model_buffer"
    ],
    "tokenize": [
      "self",
      "input"
    ],
    "tokenize_with_offsets": [
      "self",
      "input"
    ],
    "detokenize": [
      "self",
      "input"
    ]
  },
  "gen_sentence_breaking_ops": [],
  "SentenceBreaker": {
    "break_sentences": [
      "self",
      "input"
    ]
  },
  "SentenceBreakerWithOffsets": {
    "break_sentences_with_offsets": [
      "self",
      "input"
    ]
  },
  "sentence_fragments": [
    "token_word",
    "token_starts",
    "token_ends",
    "token_properties",
    "input_encoding",
    "errors",
    "replacement_char",
    "replace_control_characters"
  ],
  "gen_byte_splitter": [],
  "_tf_text_byte_splitter_op_create_counter": [],
  "ByteSplitter": {
    "__init__": [
      "self"
    ],
    "split": [
      "self",
      "input"
    ],
    "split_with_offsets": [
      "self",
      "input"
    ],
    "_byte_split_with_offsets": [
      "self",
      "input_tensor"
    ],
    "split_by_offsets": [
      "self",
      "input",
      "start_offsets",
      "end_offsets"
    ]
  },
  "_tf_text_hub_module_splitter_create_counter": [],
  "HubModuleSplitter": {
    "__init__": [
      "self",
      "hub_module_handle"
    ],
    "_predict_pieces": [
      "self",
      "input_strs"
    ],
    "split_with_offsets": [
      "self",
      "input_strs"
    ],
    "split": [
      "self",
      "input_strs"
    ]
  },
  "gen_unicode_script_tokenizer": [],
  "_tf_text_unicode_script_tokenizer_create_counter": [],
  "UnicodeScriptTokenizer": {
    "__init__": [
      "self",
      "keep_whitespace"
    ],
    "tokenize": [
      "self",
      "input"
    ],
    "tokenize_with_offsets": [
      "self",
      "input"
    ],
    "_tokenize_with_offsets_encode_decode_wrapper": [
      "self",
      "input_tensor"
    ],
    "_tokenize_codepoints_with_offsets": [
      "self",
      "codepoints_tensor"
    ]
  },
  "gen_whitespace_tokenizer": [],
  "gen_whitespace_tokenizer_v2": [],
  "_tf_text_whitespace_tokenizer_op_create_counter": [],
  "WhitespaceTokenizer": {
    "__init__": [
      "self"
    ],
    "tokenize": [
      "self",
      "input"
    ],
    "tokenize_with_offsets": [
      "self",
      "input"
    ],
    "_whitespace_tokenize_with_offsets": [
      "self",
      "input_tensor"
    ]
  },
  "gen_state_based_sentence_breaker_op": [],
  "StateBasedSentenceBreaker": {
    "break_sentences": [
      "self",
      "doc"
    ],
    "break_sentences_with_offsets": [
      "self",
      "doc"
    ]
  },
  "gen_wordpiece_tokenizer": [],
  "_tf_text_wordpiece_tokenizer_op_create_counter": [],
  "WordpieceTokenizer": {
    "__init__": [
      "self",
      "vocab_lookup_table",
      "suffix_indicator",
      "max_bytes_per_word",
      "max_chars_per_token",
      "token_out_type",
      "unknown_token",
      "split_unknown_characters"
    ],
    "_get_vocab_and_ids": [
      "self"
    ],
    "vocab_size": [
      "self",
      "name"
    ],
    "tokenize": [
      "self",
      "input"
    ],
    "tokenize_with_offsets": [
      "self",
      "input"
    ],
    "detokenize": [
      "self",
      "token_ids"
    ]
  },
  "gen_split_merge_tokenizer": [],
  "_tf_text_split_merge_tokenizer_op_create_counter": [],
  "SplitMergeTokenizer": {
    "__init__": [
      "self"
    ],
    "tokenize": [
      "self",
      "input",
      "labels",
      "force_split_at_break_character"
    ],
    "tokenize_with_offsets": [
      "self",
      "input",
      "labels",
      "force_split_at_break_character"
    ]
  },
  "gen_utf8_binarize_op": [],
  "utf8_binarize": [
    "tokens",
    "word_length",
    "bits_per_char",
    "replacement_char",
    "name"
  ],
  "gen_split_merge_from_logits_tokenizer": [],
  "_tf_text_split_merge_from_logits_tokenizer_op_create_counter": [],
  "SplitMergeFromLogitsTokenizer": {
    "__init__": [
      "self",
      "force_split_at_break_character"
    ],
    "tokenize": [
      "self",
      "strings",
      "logits"
    ],
    "tokenize_with_offsets": [
      "self",
      "strings",
      "logits"
    ]
  },
  "_tf_text_bert_tokenizer_op_create_counter": [],
  "_DELIM_REGEX": [],
  "_DELIM_REGEX_PATTERN": [],
  "_KEEP_DELIM_NO_WHITESPACE": [],
  "_UNUSED_TOKEN_REGEX": [],
  "_KEEP_DELIM_NO_WHITESPACE_PATTERN": [],
  "BasicTokenizer": {
    "__init__": [
      "self",
      "lower_case",
      "keep_whitespace",
      "normalization_form",
      "preserve_unused_token"
    ],
    "tokenize": [
      "self",
      "text_input"
    ],
    "tokenize_with_offsets": [
      "self",
      "text_input"
    ],
    "lower_case": [
      "self",
      "text_input"
    ]
  },
  "AccentPreservingBasicTokenizer": {
    "__init__": [
      "self"
    ],
    "lower_case": [
      "self",
      "text_input"
    ]
  },
  "BertTokenizer": {
    "__init__": [
      "self",
      "vocab_lookup_table",
      "suffix_indicator",
      "max_bytes_per_word",
      "max_chars_per_token",
      "token_out_type",
      "unknown_token",
      "split_unknown_characters",
      "lower_case",
      "keep_whitespace",
      "normalization_form",
      "preserve_unused_token",
      "basic_tokenizer_class"
    ],
    "tokenize_with_offsets": [
      "self",
      "text_input"
    ],
    "tokenize": [
      "self",
      "text_input"
    ],
    "detokenize": [
      "self",
      "token_ids"
    ]
  },
  "gen_boise_offset_converter": [],
  "_validate_input_has_same_type": [],
  "offsets_to_boise_tags": [
    "token_begin_offsets",
    "token_end_offsets",
    "span_begin_offsets",
    "span_end_offsets",
    "span_type",
    "use_strict_boundary_mode"
  ],
  "boise_tags_to_offsets": [
    "token_begin_offsets",
    "token_end_offsets",
    "boise_tags"
  ],
  "_tf_text_hub_module_tokenizer_create_counter": [],
  "HubModuleTokenizer": {
    "__init__": [
      "self",
      "hub_module_handle"
    ],
    "tokenize_with_offsets": [
      "self",
      "input_strs"
    ],
    "tokenize": [
      "self",
      "input_strs"
    ]
  },
  "gen_sentencepiece_tokenizer": [],
  "_tf_text_sentencepiece_tokenizer_op_create_counter": [],
  "_SentencepieceModelResource": {
    "__init__": [
      "self",
      "model",
      "name"
    ],
    "_create_resource": [
      "self"
    ]
  },
  "SentencepieceTokenizer": {
    "__init__": [
      "self",
      "model",
      "out_type",
      "nbest_size",
      "alpha",
      "reverse",
      "add_bos",
      "add_eos",
      "return_nbest",
      "name"
    ],
    "tokenize": [
      "self",
      "input",
      "name"
    ],
    "tokenize_with_offsets": [
      "self",
      "input",
      "name"
    ],
    "detokenize": [
      "self",
      "input",
      "name"
    ],
    "vocab_size": [
      "self",
      "name"
    ],
    "id_to_string": [
      "self",
      "input",
      "name"
    ],
    "string_to_id": [
      "self",
      "input",
      "name"
    ]
  },
  "greedy_constrained_sequence": [
    "scores",
    "sequence_length",
    "allowed_transitions",
    "transition_weights",
    "use_log_space",
    "use_start_and_end_states",
    "name"
  ],
  "gen_mst_ops": [],
  "max_spanning_tree": [],
  "max_spanning_tree_gradient": [
    "mst_op",
    "d_loss_d_max_scores"
  ],
  "gen_trimmer_ops": [],
  "Trimmer": {
    "trim": [
      "self",
      "segments"
    ],
    "generate_mask": [
      "self",
      "segments"
    ]
  },
  "_get_row_lengths": [
    "segments",
    "axis"
  ],
  "WaterfallTrimmer": {
    "__init__": [
      "self",
      "max_seq_length",
      "axis"
    ],
    "generate_mask": [
      "self",
      "segments"
    ]
  },
  "_round_robin_allocation": [
    "row_lengths",
    "max_seq_length"
  ],
  "RoundRobinTrimmer": {
    "__init__": [
      "self",
      "max_seq_length",
      "axis"
    ],
    "generate_mask": [
      "self",
      "segments"
    ],
    "trim": [
      "self",
      "segments"
    ]
  },
  "_shrink_longest_allocation": [
    "segment_lengths",
    "max_row_length"
  ],
  "ShrinkLongestTrimmer": {
    "__init__": [
      "self",
      "max_seq_length",
      "axis"
    ],
    "generate_mask": [
      "self",
      "segments"
    ]
  },
  "Tokenizer": {
    "tokenize": [
      "self",
      "input"
    ],
    "split": [
      "self",
      "input"
    ]
  },
  "TokenizerWithOffsets": {
    "tokenize_with_offsets": [
      "self",
      "input"
    ],
    "split_with_offsets": [
      "self",
      "input"
    ]
  },
  "Detokenizer": {
    "__metaclass__": [],
    "detokenize": [
      "self",
      "input"
    ]
  },
  "gen_regex_split_ops": [],
  "regex_split_with_offsets": [
    "input",
    "delim_regex_pattern",
    "keep_delim_regex_pattern",
    "name"
  ],
  "regex_split": [
    "input",
    "delim_regex_pattern",
    "keep_delim_regex_pattern",
    "name"
  ],
  "RegexSplitter": {
    "__init__": [
      "self",
      "split_regex"
    ],
    "split": [
      "self",
      "input"
    ],
    "split_with_offsets": [
      "self",
      "input"
    ]
  },
  "gen_text_similarity_metric_ops": [],
  "rouge_l": [
    "hypotheses",
    "references",
    "alpha"
  ],
  "decode": [
    "score",
    "transition_params",
    "allowed_transitions",
    "use_log_space",
    "use_start_and_end_states"
  ],
  "_decode_in_log_space": [
    "score",
    "transition_params",
    "use_start_and_end_states"
  ],
  "_decode_in_exp_space": [
    "score",
    "transition_params",
    "use_start_and_end_states"
  ],
  "TokenizerBase": {
    "__init__": [
      "self",
      "tokenizer_instance",
      "pad_value",
      "squeeze_token_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_set_tokenizer": [
      "self",
      "tokenizer"
    ],
    "call": [
      "self",
      "text_to_be_tokenized"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "compute_output_signature": [
      "self",
      "input_signature"
    ],
    "get_config": [
      "self"
    ]
  },
  "_GetVocabularyFromFile": [
    "vocabulary_path"
  ],
  "ToDense": {
    "__init__": [
      "self",
      "pad_value",
      "mask",
      "shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "bert_vocab_from_dataset": [
    "dataset",
    "vocab_size",
    "reserved_tokens",
    "bert_tokenizer_params",
    "learn_params"
  ],
  "Params": [],
  "extract_char_tokens": [
    "word_counts"
  ],
  "ensure_all_tokens_exist": [
    "input_tokens",
    "output_tokens",
    "include_joiner_token",
    "joiner"
  ],
  "get_split_indices": [
    "word",
    "curr_tokens",
    "include_joiner_token",
    "joiner"
  ],
  "get_search_threshs": [
    "word_counts",
    "upper_thresh",
    "lower_thresh"
  ],
  "get_input_words": [
    "word_counts",
    "reserved_tokens",
    "max_token_length"
  ],
  "get_allowed_chars": [
    "all_counts",
    "max_unique_chars"
  ],
  "filter_input_words": [
    "all_counts",
    "allowed_chars",
    "max_input_tokens"
  ],
  "generate_final_vocabulary": [
    "reserved_tokens",
    "char_tokens",
    "curr_tokens"
  ],
  "learn_with_thresh": [
    "word_counts",
    "thresh",
    "params"
  ],
  "learn_binary_search": [
    "word_counts",
    "lower",
    "upper",
    "params"
  ],
  "count_words": [
    "iterable"
  ],
  "learn": [
    "word_counts",
    "vocab_size",
    "reserved_tokens",
    "upper_thresh",
    "lower_thresh",
    "num_iterations",
    "max_input_tokens",
    "max_token_length",
    "max_unique_chars",
    "slack_ratio",
    "include_joiner_token",
    "joiner"
  ]
}