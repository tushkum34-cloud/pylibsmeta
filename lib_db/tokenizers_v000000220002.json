{
  "Offsets": [],
  "TextInputSequence": [],
  "PreTokenizedInputSequence": [],
  "TextEncodeInput": [],
  "PreTokenizedEncodeInput": [],
  "InputSequence": [],
  "EncodeInput": [],
  "OffsetReferential": {
    "ORIGINAL": [],
    "NORMALIZED": []
  },
  "OffsetType": {
    "BYTE": [],
    "CHAR": []
  },
  "SplitDelimiterBehavior": {
    "REMOVED": [],
    "ISOLATED": [],
    "MERGED_WITH_PREVIOUS": [],
    "MERGED_WITH_NEXT": [],
    "CONTIGUOUS": []
  },
  "PreTokenizer": [],
  "BertPreTokenizer": [],
  "ByteLevel": [],
  "CharDelimiterSplit": [],
  "Digits": [],
  "FixedLength": [],
  "Metaspace": [],
  "Punctuation": [],
  "Sequence": [],
  "Split": [],
  "UnicodeScripts": [],
  "Whitespace": [],
  "WhitespaceSplit": [],
  "CharBPETokenizer": {
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token",
      "suffix",
      "dropout",
      "lowercase",
      "unicode_normalizer",
      "bert_normalizer",
      "split_on_whitespace_only"
    ],
    "from_file": [
      "vocab_filename",
      "merges_filename"
    ],
    "train": [
      "self",
      "files",
      "vocab_size",
      "min_frequency",
      "special_tokens",
      "limit_alphabet",
      "initial_alphabet",
      "suffix",
      "show_progress"
    ],
    "train_from_iterator": [
      "self",
      "iterator",
      "vocab_size",
      "min_frequency",
      "special_tokens",
      "limit_alphabet",
      "initial_alphabet",
      "suffix",
      "show_progress",
      "length"
    ]
  },
  "SentencePieceUnigramTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "replacement",
      "add_prefix_space"
    ],
    "train": [
      "self",
      "files",
      "vocab_size",
      "show_progress",
      "special_tokens",
      "initial_alphabet",
      "unk_token"
    ],
    "train_from_iterator": [
      "self",
      "iterator",
      "vocab_size",
      "show_progress",
      "special_tokens",
      "initial_alphabet",
      "unk_token",
      "length"
    ],
    "from_spm": [
      "filename"
    ]
  },
  "SentencePieceBPETokenizer": {
    "__init__": [
      "self",
      "vocab",
      "merges",
      "unk_token",
      "replacement",
      "add_prefix_space",
      "dropout",
      "fuse_unk"
    ],
    "from_file": [
      "vocab_filename",
      "merges_filename"
    ],
    "train": [
      "self",
      "files",
      "vocab_size",
      "min_frequency",
      "special_tokens",
      "limit_alphabet",
      "initial_alphabet",
      "show_progress"
    ],
    "train_from_iterator": [
      "self",
      "iterator",
      "vocab_size",
      "min_frequency",
      "special_tokens",
      "limit_alphabet",
      "initial_alphabet",
      "show_progress",
      "length"
    ]
  },
  "BaseTokenizer": {
    "__init__": [
      "self",
      "tokenizer",
      "parameters"
    ],
    "__repr__": [
      "self"
    ],
    "num_special_tokens_to_add": [
      "self",
      "is_pair"
    ],
    "get_vocab": [
      "self",
      "with_added_tokens"
    ],
    "get_added_tokens_decoder": [
      "self"
    ],
    "get_vocab_size": [
      "self",
      "with_added_tokens"
    ],
    "enable_padding": [
      "self",
      "direction",
      "pad_to_multiple_of",
      "pad_id",
      "pad_type_id",
      "pad_token",
      "length"
    ],
    "no_padding": [
      "self"
    ],
    "padding": [
      "self"
    ],
    "enable_truncation": [
      "self",
      "max_length",
      "stride",
      "strategy"
    ],
    "no_truncation": [
      "self"
    ],
    "truncation": [
      "self"
    ],
    "add_tokens": [
      "self",
      "tokens"
    ],
    "add_special_tokens": [
      "self",
      "special_tokens"
    ],
    "normalize": [
      "self",
      "sequence"
    ],
    "encode": [
      "self",
      "sequence",
      "pair",
      "is_pretokenized",
      "add_special_tokens"
    ],
    "encode_batch": [
      "self",
      "inputs",
      "is_pretokenized",
      "add_special_tokens"
    ],
    "async_encode_batch": [
      "self",
      "inputs",
      "is_pretokenized",
      "add_special_tokens"
    ],
    "async_encode_batch_fast": [
      "self",
      "inputs",
      "is_pretokenized",
      "add_special_tokens"
    ],
    "decode": [
      "self",
      "ids",
      "skip_special_tokens"
    ],
    "decode_batch": [
      "self",
      "sequences",
      "skip_special_tokens"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "save_model": [
      "self",
      "directory",
      "prefix"
    ],
    "save": [
      "self",
      "path",
      "pretty"
    ],
    "to_str": [
      "self",
      "pretty"
    ],
    "post_process": [
      "self",
      "encoding",
      "pair",
      "add_special_tokens"
    ],
    "model": [
      "self",
      "model"
    ],
    "normalizer": [
      "self",
      "normalizer"
    ],
    "pre_tokenizer": [
      "self",
      "pre_tokenizer"
    ],
    "post_processor": [
      "self",
      "post_processor"
    ],
    "decoder": [
      "self",
      "decoder"
    ]
  },
  "ByteLevelBPETokenizer": {
    "__init__": [
      "self",
      "vocab",
      "merges",
      "add_prefix_space",
      "lowercase",
      "dropout",
      "unicode_normalizer",
      "continuing_subword_prefix",
      "end_of_word_suffix",
      "trim_offsets"
    ],
    "from_file": [
      "vocab_filename",
      "merges_filename"
    ],
    "train": [
      "self",
      "files",
      "vocab_size",
      "min_frequency",
      "show_progress",
      "special_tokens"
    ],
    "train_from_iterator": [
      "self",
      "iterator",
      "vocab_size",
      "min_frequency",
      "show_progress",
      "special_tokens",
      "length"
    ]
  },
  "BertWordPieceTokenizer": {
    "__init__": [
      "self",
      "vocab",
      "unk_token",
      "sep_token",
      "cls_token",
      "pad_token",
      "mask_token",
      "clean_text",
      "handle_chinese_chars",
      "strip_accents",
      "lowercase",
      "wordpieces_prefix"
    ],
    "from_file": [
      "vocab"
    ],
    "train": [
      "self",
      "files",
      "vocab_size",
      "min_frequency",
      "limit_alphabet",
      "initial_alphabet",
      "special_tokens",
      "show_progress",
      "wordpieces_prefix"
    ],
    "train_from_iterator": [
      "self",
      "iterator",
      "vocab_size",
      "min_frequency",
      "limit_alphabet",
      "initial_alphabet",
      "special_tokens",
      "show_progress",
      "wordpieces_prefix",
      "length"
    ]
  },
  "Normalizer": [],
  "BertNormalizer": [],
  "NFD": [],
  "NFKD": [],
  "NFC": [],
  "NFKC": [],
  "Lowercase": [],
  "Prepend": [],
  "Strip": [],
  "StripAccents": [],
  "Nmt": [],
  "Precompiled": [],
  "Replace": [],
  "NORMALIZERS": [],
  "unicode_normalizer_from_str": [
    "normalizer"
  ],
  "PostProcessor": [],
  "BertProcessing": [],
  "RobertaProcessing": [],
  "TemplateProcessing": [],
  "Trainer": [],
  "BpeTrainer": [],
  "UnigramTrainer": [],
  "WordLevelTrainer": [],
  "WordPieceTrainer": [],
  "Model": [],
  "BPE": [],
  "Unigram": [],
  "WordLevel": [],
  "WordPiece": [],
  "Decoder": [],
  "ByteFallback": [],
  "Fuse": [],
  "BPEDecoder": [],
  "CTC": [],
  "DecodeStream": [],
  "dirname": [],
  "css_filename": [],
  "Annotation": {
    "__init__": [
      "self",
      "start",
      "end",
      "label"
    ]
  },
  "AnnotationList": [],
  "PartialIntList": [],
  "CharStateKey": {},
  "CharState": {
    "__init__": [
      "self",
      "char_ix"
    ],
    "token_ix": [
      "self"
    ],
    "is_multitoken": [
      "self"
    ],
    "partition_key": [
      "self"
    ]
  },
  "Aligned": {},
  "EncodingVisualizer": {
    "unk_token_regex": [],
    "__init__": [
      "self",
      "tokenizer",
      "default_to_notebook",
      "annotation_converter"
    ],
    "__call__": [
      "self",
      "text",
      "annotations",
      "default_to_notebook"
    ],
    "calculate_label_colors": [
      "annotations"
    ],
    "consecutive_chars_to_html": [
      "consecutive_chars_list",
      "text",
      "encoding"
    ],
    "__make_html": [
      "text",
      "encoding",
      "annotations"
    ],
    "__make_anno_map": [
      "text",
      "annotations"
    ],
    "__make_char_states": [
      "text",
      "encoding",
      "annotations"
    ]
  },
  "HTMLBody": [
    "children",
    "css_styles"
  ]
}