{
  "USE_NPU": [],
  "BIGDL_IMPORT_IPEX": [],
  "BIGDL_COMPATIBLE_MODE": [],
  "bigdl_patched": [],
  "attrs": [],
  "replace_attr": [
    "obj",
    "name",
    "value"
  ],
  "llm_patch": [
    "train"
  ],
  "llm_unpatch": [],
  "_special_kwarg_check": [
    "kwargs",
    "check_args"
  ],
  "llm_convert": [
    "model",
    "outfile",
    "model_family",
    "outtype",
    "model_format"
  ],
  "main": [],
  "PYTORCH_MODEL_NAME": [],
  "CONFIG_NAME": [],
  "_save_low_bit": [
    "self",
    "save_dir"
  ],
  "DisableTorchAllocTensor": {
    "__init__": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "ContextManagers": {
    "__init__": [
      "self",
      "context_managers"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "low_bit_sanity_check": [
    "model_path"
  ],
  "low_memory_init": [],
  "load_low_bit": [
    "model",
    "model_path"
  ],
  "optimize_model": [
    "model",
    "low_bit",
    "optimize_llm",
    "modules_to_not_convert",
    "cpu_embedding"
  ],
  "create_json_logits_processor": [
    "tokenizer",
    "vocab_size",
    "schema"
  ],
  "reset_json_logits_processor": [
    "processor"
  ],
  "LOG": [],
  "LoraLowBitLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "qa_lora",
      "fan_in_fan_out",
      "is_target_conv_1d_layer",
      "init_lora_weights",
      "use_rslora",
      "use_dora"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LoraBF16Linear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out",
      "is_target_conv_1d_layer",
      "init_lora_weights",
      "use_rslora",
      "use_dora"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "_create_new_module": [
    "create_new_module_func",
    "lora_config",
    "adapter_name",
    "target"
  ],
  "LoraConfig": {
    "__init__": [
      "self"
    ]
  },
  "supported_optim": [],
  "TrainingArguments": {
    "__init__": [
      "self"
    ]
  },
  "get_peft_model": [],
  "prepare_model_for_kbit_training": [
    "model",
    "use_gradient_checkpointing"
  ],
  "PeftModel": {
    "from_pretrained": []
  },
  "cast_lora_weight": [
    "model",
    "dtype"
  ],
  "BatchTextIteratorStreamer": {
    "__init__": [
      "self",
      "batch_size",
      "tokenizer",
      "skip_prompt",
      "timeout"
    ],
    "put": [
      "self",
      "value"
    ],
    "end": [
      "self"
    ],
    "on_finalized_text": [
      "self",
      "texts",
      "stream_end"
    ]
  },
  "T": [],
  "TORCH_LINEAR_THRESHOLD": [],
  "SYM_INT4": [],
  "ASYM_INT4": [],
  "SYM_INT8": [],
  "NF4": [],
  "NF3": [],
  "FP8E4": [],
  "FP4": [],
  "MOFQ4": [],
  "MOFQ8": [],
  "FP8E5": [],
  "FP6": [],
  "FP16": [],
  "IQ2_XXS": [],
  "IQ2_XS": [],
  "Q2_K": [],
  "IQ1_S": [],
  "Q4_K": [],
  "Q6_K": [],
  "Q5_K": [],
  "FP6_K": [],
  "SYM_INT4_RTN": [],
  "SYM_INT8_RTN": [],
  "ASYM_INT4_RTN": [],
  "WOQ_INT4": [],
  "RTN_DTYPE": [],
  "get_block_size": [
    "qtype"
  ],
  "get_qk_size": [
    "qtype"
  ],
  "ggml_convert_qtype": [
    "tensor",
    "qtype",
    "device",
    "convert_shape_only",
    "imatrix",
    "in_features",
    "enable_scale_search"
  ],
  "ggml_q_format_convet_cpu2xpu": [
    "tensor",
    "num_elem",
    "qtype"
  ],
  "ggml_q_format_convet_xpu2cpu": [
    "tensor",
    "num_elem",
    "qtype"
  ],
  "ggml_int4_convert_fp32": [
    "tensor",
    "weight_shape",
    "k"
  ],
  "ggml_convert_fp32": [
    "tensor",
    "weight_shape",
    "k",
    "qtype"
  ],
  "reshape_lm_head_input": [
    "x"
  ],
  "use_batch_forward": [
    "x",
    "qtype",
    "output_len"
  ],
  "FP4Params": {
    "__new__": [
      "cls",
      "data",
      "requires_grad",
      "quantized",
      "_shape",
      "convert_shape_only",
      "qtype",
      "imatrix",
      "in_features",
      "enable_scale_search"
    ],
    "ggml_mse": [
      "self",
      "w",
      "ggml_qtype",
      "device"
    ],
    "quantize": [
      "self",
      "device"
    ],
    "get_shape": [
      "self"
    ],
    "to": [
      "self"
    ]
  },
  "ggml_matmul_src1_x_src0_t": [
    "src0",
    "src1",
    "src0_shape",
    "src0_qtype"
  ],
  "MatMulLowBit": {
    "forward": [
      "ctx",
      "A",
      "weight",
      "output_size"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "MatMulLowBitCPU": {
    "forward": [
      "ctx",
      "A",
      "weight"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "LowBitLinear": {
    "__init__": [
      "self",
      "input_features",
      "output_features",
      "qtype",
      "bias",
      "conver_to_half",
      "mp_group",
      "optimize_lm_head",
      "act_order",
      "enable_scale_search"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FP16Linear": {
    "__init__": [
      "self",
      "input_features",
      "output_features",
      "bias",
      "mp_group",
      "weight_type",
      "optimize_lm_head"
    ],
    "forward": [
      "self",
      "x"
    ],
    "use_esimd_kernel": [
      "self",
      "x"
    ],
    "convert_weight_for_esimd_kernel": [
      "self"
    ]
  },
  "BF16Linear": {
    "__init__": [
      "self",
      "input_features",
      "output_features",
      "bias",
      "mp_group",
      "compute_dtype",
      "optimize_lm_head"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "vLLMLowBitLinear": {
    "__init__": [
      "self",
      "input_features",
      "output_features",
      "qtype",
      "bias",
      "conver_to_half",
      "mp_group",
      "optimize_lm_head",
      "act_order",
      "enable_scale_search"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "vLLMFP16Linear": {
    "__init__": [
      "self",
      "input_features",
      "output_features",
      "bias",
      "mp_group",
      "weight_type",
      "optimize_lm_head"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "vLLMBF16Linear": {
    "__init__": [
      "self",
      "input_features",
      "output_features",
      "bias",
      "mp_group",
      "compute_dtype",
      "optimize_lm_head"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "patch_prepare_ipex": [
    "self"
  ],
  "_setup_devices": [
    "self"
  ],
  "from_pretrained": [
    "cls",
    "pretrained_model_name_or_path",
    "subfolder"
  ],
  "CPUPinnedParam": {
    "device": [
      "self",
      "to_device"
    ],
    "to": [
      "self"
    ]
  },
  "CPUEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "max_norm",
      "norm_type",
      "scale_grad_by_freq",
      "sparse",
      "_weight",
      "_freeze",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ],
    "from_embedding": [
      "cls",
      "embedding"
    ]
  },
  "DiskEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "max_norm",
      "norm_type",
      "scale_grad_by_freq",
      "sparse",
      "_weight",
      "_freeze",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "input_ids"
    ],
    "from_embedding": [
      "cls",
      "embedding"
    ],
    "to_embedding": [
      "self"
    ],
    "replace_normal_embedding": [
      "m"
    ],
    "restore_normal_embedding": [
      "m"
    ]
  },
  "LowBitEmbedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "padding_idx",
      "max_norm",
      "norm_type",
      "scale_grad_by_freq",
      "sparse",
      "_weight",
      "_freeze",
      "device",
      "dtype",
      "convert_shape_only",
      "qtype"
    ],
    "forward": [
      "self",
      "x"
    ],
    "from_embedding": [
      "cls",
      "embedding",
      "convert_shape_only",
      "qtype"
    ]
  },
  "DynamicLayerActivationCallback": {
    "__init__": [
      "self",
      "n_layers",
      "interval_steps",
      "model"
    ],
    "freeze_all_layers": [
      "self"
    ],
    "on_step_begin": [
      "self",
      "args",
      "state",
      "control"
    ],
    "switch_active_layers": [
      "self"
    ]
  },
  "logger": [],
  "WEIGHTS_NAME": [],
  "WEIGHTS_INDEX_NAME": [],
  "extract_local_archive_file": [
    "pretrained_model_name_or_path",
    "subfolder",
    "variant"
  ],
  "load_state_dict": [
    "checkpoint_file"
  ],
  "load": [
    "module",
    "state_dict",
    "prefix"
  ],
  "get_local_shard_files": [
    "pretrained_model_name_or_path",
    "index_filename",
    "subfolder"
  ],
  "fix_key": [
    "key"
  ],
  "is_autocast_enabled": [
    "device_type"
  ],
  "get_autocast_dtype": [
    "device_type"
  ],
  "get_xpu_device_name": [
    "device"
  ],
  "load_imatrix_data": [
    "imatrix_file"
  ],
  "module_name_process": [
    "full_module_name"
  ],
  "get_cur_qtype_and_imatrix": [
    "qtype",
    "full_module_name",
    "imatrix_data",
    "model_config"
  ],
  "get_modelscope_hf_config": [
    "model_id_or_path",
    "revision"
  ],
  "is_torch_bf16_gpu_available": [],
  "check_hidden_size": [
    "qtype",
    "hidden_size"
  ],
  "_constant_buffered_norm2": [
    "self",
    "input",
    "buffer_size"
  ],
  "LLAMA_IDS": [],
  "get_tokenizer_cls": [
    "model_path"
  ],
  "get_model_cls": [
    "model_path",
    "low_bit"
  ],
  "load_model": [
    "model_path",
    "device",
    "low_bit",
    "trust_remote_code",
    "speculative",
    "load_low_bit_model"
  ],
  "try_run_test_generation": [
    "local_model_hub",
    "model_path",
    "device",
    "low_bit",
    "load_low_bit_model"
  ],
  "get_model_path": [
    "repo_id",
    "local_model_hub"
  ],
  "run_test_generation": [
    "model_path",
    "device",
    "low_bit",
    "load_low_bit_model"
  ],
  "original_generate": [],
  "DummyLayer": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Dummy_MLPLayer": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Dummy_DecoderLayer": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "Dummy_GLMBlock": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "rotary_pos_emb",
      "kv_cache",
      "use_cache"
    ]
  },
  "init_pipeline_parallel": [],
  "low_mem_convert": [
    "model"
  ],
  "_check_quantize_kv_cache": [
    "model",
    "idx",
    "batch_size"
  ],
  "pipeline_parallel": [
    "model",
    "pipeline_parallel_stages",
    "torch_dtype"
  ],
  "generate": [
    "self",
    "inputs",
    "generation_config",
    "logits_processor",
    "stopping_criteria",
    "prefix_allowed_tokens_fn",
    "synced_gpus",
    "assistant_model",
    "streamer",
    "negative_prompt_ids",
    "negative_prompt_attention_mask"
  ],
  "pipeline_parallel_generate": [
    "self",
    "inputs",
    "max_new_tokens",
    "generation_config"
  ],
  "PPConfig": {
    "__init__": [
      "self",
      "pp_rank",
      "pp_world_size"
    ]
  },
  "BatchTask": {},
  "make_attention_mask": [
    "prompt_lengths",
    "device"
  ],
  "PPModelWorker": {
    "__init__": [
      "self",
      "checkpoint",
      "rank",
      "world_size",
      "low_bit",
      "max_num_seqs",
      "max_prefilled_seqs",
      "torch_dtype"
    ],
    "load_model": [
      "self",
      "model_path",
      "world_size",
      "low_bit"
    ],
    "prepare_batch": [
      "self",
      "cur_batch"
    ],
    "cat_kv_cache": [
      "self",
      "model_type",
      "kv_cache_1",
      "kv_cache_2"
    ],
    "update_kv_cache": [
      "self",
      "kv_cache",
      "prefill"
    ],
    "model_step": [
      "self",
      "input",
      "cur_batch"
    ],
    "is_initialized": [
      "self"
    ],
    "add_request": [
      "self",
      "tokenizer"
    ],
    "clear_batch": [
      "self",
      "cur_id"
    ],
    "wait_stream_output": [
      "self",
      "cur_id"
    ],
    "get_printable_text": [
      "self",
      "cur_text",
      "request_id"
    ],
    "stream_output": [
      "self",
      "cur_batch",
      "tokenizer",
      "next_ids"
    ],
    "process_step": [
      "self",
      "tokenizer",
      "result_dict",
      "processor"
    ]
  },
  "_is_chinese_char": [
    "cp"
  ],
  "llama_causallm_forward_4_37_lowmem": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "chatglm3_conditional_generation_forward_lowmem": [
    "self",
    "input_ids",
    "position_ids",
    "attention_mask",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "return_last_logit"
  ],
  "glm4_conditional_generation_forward_lowmem": [
    "self",
    "input_ids",
    "position_ids",
    "attention_mask",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "return_last_logit"
  ],
  "patch_flash_attn_import": [
    "filename"
  ],
  "_IS_VLLM_AVAILABLE": [],
  "_USE_VLLM": [],
  "_USE_VLLM_AWQ": [],
  "_USE_VLLM_GPTQ": [],
  "_VLLM_VERSION": [],
  "is_auto_gptq_available": [],
  "is_auto_awq_available": [],
  "is_vllm_available": [],
  "get_package_version": [
    "package_name"
  ],
  "get_use_vllm": [],
  "is_torch_distributed_initialized": [],
  "is_module_in_classes": [
    "module",
    "classes"
  ],
  "is_deepspeed_available": [],
  "is_lm_head": [
    "name",
    "model_config",
    "out_features"
  ],
  "is_gptq_linear": [
    "module"
  ],
  "is_linear_module": [
    "module"
  ],
  "convert_vllm": [
    "module",
    "qtype",
    "in_features",
    "out_features",
    "mp_group",
    "cur_qtype",
    "optimize_lm_head",
    "enable_scale_search"
  ],
  "convert_vllm_awq_or_gptq": [
    "module",
    "gptq",
    "act_order"
  ],
  "convert_gptq": [
    "module",
    "awq",
    "llm_awq",
    "act_order"
  ],
  "use_scale_search": [
    "model_config",
    "qtype"
  ],
  "_replace_with_low_bit_linear": [
    "model",
    "qtype",
    "modules_to_not_convert",
    "convert_shape_only",
    "cpu_embedding",
    "prefix_name",
    "imatrix_data",
    "embedding_qtype",
    "model_config",
    "torch_dtype",
    "mixed_precision",
    "act_order",
    "enable_scale_search"
  ],
  "replace_with_low_bit_linear_for_module": [
    "model",
    "qtype",
    "module_name",
    "modules_to_not_convert",
    "current_key_name",
    "convert_shape_only",
    "torch_dtype"
  ],
  "_optimize_pre": [
    "model",
    "qtype"
  ],
  "ggml_convert_low_bit": [
    "model",
    "qtype",
    "optimize_model",
    "convert_shape_only",
    "device",
    "modules_to_not_convert",
    "cpu_embedding",
    "torch_dtype",
    "imatrix_data",
    "embedding_qtype",
    "mixed_precision",
    "disable_optimize_pre"
  ],
  "convert_bigdl_other_module": [
    "model",
    "dtype"
  ],
  "convert_forward": [
    "m",
    "target_m",
    "new_forward"
  ],
  "replace_RotaryEmbed": [
    "m",
    "target_m",
    "replace_embed"
  ],
  "replace_func": [
    "m",
    "target_m",
    "func_name",
    "new_func"
  ],
  "get_enable_ipex": [],
  "_optimize_ipex": [
    "model",
    "qtype"
  ],
  "_optimize_post": [
    "model"
  ],
  "convert_forward_to_xpu": [
    "m",
    "target_m",
    "new_forward"
  ],
  "convert_model_hybrid": [
    "model"
  ],
  "patched_training_mode": [],
  "save_low_bit": [
    "self"
  ],
  "_BaseAutoModelClass": {
    "HF_MODEL": [],
    "from_pretrained": [
      "cls"
    ],
    "from_gguf": [
      "fpath",
      "optimize_model",
      "cpu_embedding",
      "low_bit"
    ],
    "load_convert": [
      "cls",
      "q_k",
      "optimize_model"
    ],
    "load_low_bit": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "AutoModelForCausalLM": {
    "HF_Model": []
  },
  "AutoModel": {
    "HF_Model": []
  },
  "AutoModelForSpeechSeq2Seq": {
    "HF_Model": []
  },
  "AutoModelForSeq2SeqLM": {
    "HF_Model": []
  },
  "AutoModelForSequenceClassification": {
    "HF_Model": []
  },
  "AutoModelForMaskedLM": {
    "HF_Model": []
  },
  "AutoModelForQuestionAnswering": {
    "HF_Model": []
  },
  "AutoModelForNextSentencePrediction": {
    "HF_Model": []
  },
  "AutoModelForMultipleChoice": {
    "HF_Model": []
  },
  "AutoModelForTokenClassification": {
    "HF_Model": []
  },
  "DynamicFp8Cache": {
    "__init__": [
      "self",
      "num_hidden_layers"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ]
  },
  "DynamicNormalCache": {
    "KV_ALLOC_BLOCK_LENGTH": [],
    "__init__": [
      "self",
      "num_hidden_layers"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "from_reserved": [
      "cls",
      "layers",
      "bsz",
      "n_head",
      "length",
      "head_dim",
      "dtype",
      "device"
    ]
  },
  "repeat_kv": [
    "hidden_states",
    "n_rep"
  ],
  "compress_kv": [
    "attn_config",
    "key_states",
    "query_states",
    "value_states",
    "attention_mask",
    "num_key_value_groups"
  ],
  "DynamicCompressCache": {
    "__init__": [
      "self",
      "quant_kv"
    ],
    "update_seen_tokens": [
      "self",
      "layer_idx",
      "q_len"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "query_states",
      "attention_mask",
      "num_key_value_groups",
      "attn_config",
      "enough_kv_room",
      "KV_CACHE_ALLOC_BLOCK_LENGTH",
      "cache_kwargs"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "from_legacy_cache": [
      "cls",
      "past_key_values",
      "num_hidden_layers"
    ]
  },
  "DynamicCompressFp8Cache": {
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "query_states",
      "attention_mask",
      "num_key_value_groups",
      "attn_config",
      "enough_kv_room",
      "KV_CACHE_ALLOC_BLOCK_LENGTH",
      "cache_kwargs"
    ]
  },
  "query_group_size": [],
  "PERFORMANCE_MODE_LOOKUP_INPUT_THRESHOLD": [],
  "tensor2key": [
    "key_tensor"
  ],
  "PromptLookupCandidateGenerator": {
    "__init__": [
      "self",
      "num_output_tokens",
      "max_matching_ngram_size",
      "device"
    ],
    "init_look_up_table": [
      "self",
      "input_ids"
    ],
    "update_look_up_table": [
      "self",
      "new_input_ids"
    ],
    "get_n_gram_idx": [
      "self",
      "ngram_tensor"
    ],
    "get_candidates": [
      "self",
      "input_ids"
    ],
    "update_candidate_strategy": [
      "self",
      "candidate_num",
      "num_matches",
      "accept_rate"
    ]
  },
  "lookup_generate": [
    "self",
    "inputs",
    "max_new_tokens",
    "num_output_tokens",
    "max_matching_ngram_size",
    "generation_config",
    "streamer",
    "attention_mask"
  ],
  "_ipex_optimize_rmsnorm": [
    "_model",
    "supported_classes",
    "is_tpp",
    "is_woq"
  ],
  "_ipex_optimize_decoder": [
    "model",
    "is_tpp",
    "is_woq"
  ],
  "_ipex_optimize_attention": [
    "model",
    "is_tpp",
    "is_woq"
  ],
  "_ipex_optimize_model": [
    "model",
    "rms_classes",
    "qtype"
  ],
  "_ipex_jit": [
    "model"
  ],
  "convert_function": [
    "m",
    "func_name",
    "new_function"
  ],
  "GLM_get_masks": [
    "self",
    "input_ids",
    "past_key_values",
    "padding_mask"
  ],
  "_make_causal_mask": [
    "input_ids_shape",
    "dtype",
    "device",
    "past_key_values_length",
    "sliding_window"
  ],
  "_llama_model_forward_4_35": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "trans_version": [],
  "greedy": [
    "logits",
    "return_probs"
  ],
  "deepmind_sample": [
    "logits",
    "return_probs",
    "top_k",
    "top_p",
    "temperature"
  ],
  "logits_to_probs": [
    "logits",
    "top_k",
    "top_p",
    "temperature"
  ],
  "multinomial_sample_one_no_sync": [
    "probs_sort"
  ],
  "clear_benchmarks": [
    "self"
  ],
  "_prepare_past_key_values_storage_cpu": [
    "self",
    "past_key_values",
    "max_new_tokens",
    "_enable_ipex"
  ],
  "_prepare_draft_past_key_values_cpu": [
    "self",
    "past_key_values",
    "past_key_values_storage",
    "_enable_ipex"
  ],
  "_update_past_key_values_storage_cpu": [
    "self",
    "past_key_values",
    "past_key_values_storage",
    "original_draft_past_key_values",
    "_enable_ipex"
  ],
  "_check_and_extend_kv_cache": [
    "past_key_values",
    "max_step_draft",
    "kv_alloc_block_len",
    "model_type"
  ],
  "_crop_past_key_values": [
    "self",
    "past_key_values",
    "new_cache_size",
    "_enable_ipex"
  ],
  "_prepare_generate_args": [
    "self",
    "inputs",
    "generation_config",
    "streamer"
  ],
  "_prepare_generate_args_4_45": [
    "self",
    "inputs",
    "generation_config",
    "streamer"
  ],
  "_non_cpu_ipex_verify": [
    "self",
    "verify_input_ids",
    "past_key_values",
    "cur_attention_mask",
    "return_dict",
    "use_cache"
  ],
  "speculative_generate": [
    "self",
    "inputs",
    "draft_model",
    "max_new_tokens",
    "max_step_draft",
    "th_stop_draft",
    "auto_th_stop_draft",
    "auto_parameters",
    "hf_adjust",
    "min_step_draft",
    "generation_config",
    "attention_mask",
    "streamer"
  ],
  "BigdlNativeForCausalLM": {
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "model_family",
      "dtype"
    ]
  },
  "_BaseGGMLClass": {
    "GGML_Model": [],
    "HF_Class": [],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "native",
      "dtype"
    ]
  },
  "LlamaForCausalLM": {
    "GGML_Module": [],
    "GGML_Model": [],
    "HF_Class": []
  },
  "ChatGLMForCausalLM": {
    "GGML_Module": [],
    "GGML_Model": [],
    "HF_Class": []
  },
  "GptneoxForCausalLM": {
    "GGML_Module": [],
    "GGML_Model": [],
    "HF_Class": []
  },
  "BloomForCausalLM": {
    "GGML_Module": [],
    "GGML_Model": [],
    "HF_Class": []
  },
  "StarcoderForCausalLM": {
    "GGML_Module": [],
    "GGML_Model": [],
    "HF_Class": []
  },
  "ReLoRATrainer": {
    "__init__": [
      "self"
    ],
    "create_scheduler": [
      "self",
      "num_training_steps",
      "optimizer"
    ]
  },
  "is_distributed": [],
  "is_main_process": [],
  "reset_optimizer": [
    "optimizer"
  ],
  "ReLoRACallback": {
    "__init__": [
      "self",
      "relora_steps",
      "relora_cpu_offload",
      "base_model",
      "resume_from_checkpoint"
    ],
    "on_train_begin": [
      "self",
      "_args",
      "_state",
      "control",
      "model"
    ],
    "on_step_begin": [
      "self",
      "args",
      "state",
      "control",
      "model",
      "optimizer"
    ],
    "on_save": [
      "self",
      "args",
      "state",
      "control",
      "model"
    ],
    "on_log": [
      "self",
      "_args",
      "_state",
      "control",
      "logs"
    ],
    "on_train_end": [
      "self",
      "args",
      "_state",
      "control",
      "model"
    ]
  },
  "ReLoRAScheduler": {
    "__init__": [
      "self",
      "optimizer",
      "inner_schedule",
      "relora_steps",
      "warmup_steps",
      "min_lr_scale"
    ],
    "get_lr": [
      "self"
    ]
  },
  "sharded_paths": [
    "path",
    "module_names"
  ],
  "lora_delta_weight": [
    "layer",
    "device"
  ],
  "find_lora_modules": [
    "model"
  ],
  "update_weights": [
    "target",
    "new_weight",
    "reinit",
    "device"
  ],
  "merge_and_save": [
    "model",
    "model_src",
    "model_dst",
    "reinit",
    "cpu_offload",
    "actually_save"
  ],
  "load_weight_checkpoint": [
    "model",
    "checkpoint_path"
  ],
  "_": [
    "attn"
  ],
  "_cast": [
    "value",
    "dtype"
  ],
  "custom_fwd": [
    "fwd"
  ],
  "custom_bwd": [
    "bwd"
  ],
  "ignore_argument": [
    "kwargs",
    "key"
  ],
  "FunAsrAutoModel": {
    "__init__": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "get_cls_model": [
      "cls"
    ],
    "optimize_npu_model": [
      "cls"
    ]
  },
  "EmbeddingModel": {
    "__init__": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "get_cls_model": [
      "cls"
    ],
    "optimize_npu_model": [
      "cls"
    ],
    "load_convert_fp16": [
      "cls",
      "q_k",
      "optimize_model",
      "device",
      "modules_to_not_convert",
      "group_size",
      "imatrix_data"
    ],
    "encode": [
      "self",
      "sentences",
      "batch_size",
      "max_length",
      "normalize_to_unit",
      "return_numpy",
      "enable_tqdm",
      "query_instruction"
    ]
  },
  "layer_type_dict": [],
  "set_op_by_name": [
    "layer",
    "name",
    "new_module"
  ],
  "_load_config": [
    "model_path",
    "model_filename",
    "safetensors",
    "trust_remote_code",
    "max_new_tokens"
  ],
  "get_named_linears": [
    "module"
  ],
  "get_blocks": [
    "model"
  ],
  "get_layer_type": [
    "config"
  ],
  "scale_activations": [
    "module"
  ],
  "_replace_with_awq_layers": [
    "model",
    "awq_config"
  ],
  "AwqConfig": {
    "__init__": [
      "self",
      "bits",
      "group_size",
      "zero_point",
      "version",
      "backend",
      "modules_to_not_convert"
    ],
    "post_init": [
      "self"
    ]
  },
  "ScaledActivation": {
    "__init__": [
      "self",
      "module",
      "scales"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "make_divisible": [
    "c",
    "divisor"
  ],
  "calculate_zeros_width": [
    "in_features",
    "group_size",
    "pack_num"
  ],
  "WQLinear_GEMM": {
    "__init__": [
      "self",
      "bits",
      "group_size",
      "in_features",
      "out_features",
      "bias",
      "dev",
      "backend"
    ],
    "from_linear": [
      "cls",
      "linear",
      "bits",
      "group_size",
      "backend",
      "init_only",
      "scales",
      "zeros"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "WQLinear_GEMV": {
    "__init__": [
      "self",
      "bits",
      "group_size",
      "in_features",
      "out_features",
      "bias",
      "dev"
    ],
    "from_linear": [
      "cls",
      "linear",
      "bits",
      "group_size",
      "backend",
      "init_only",
      "scales",
      "zeros"
    ],
    "forward": [
      "self",
      "x"
    ],
    "extra_repr": [
      "self"
    ]
  },
  "GGUFReader": {
    "__init__": [
      "self",
      "f"
    ],
    "read_value": [
      "self"
    ],
    "read_bool": [
      "self"
    ],
    "read_i8": [
      "self"
    ],
    "read_u8": [
      "self"
    ],
    "read_i16": [
      "self"
    ],
    "read_u16": [
      "self"
    ],
    "read_i32": [
      "self"
    ],
    "read_u32": [
      "self"
    ],
    "read_i64": [
      "self"
    ],
    "read_u64": [
      "self"
    ],
    "read_f32": [
      "self"
    ],
    "read_f64": [
      "self"
    ],
    "read_str": [
      "self"
    ],
    "read_array": [
      "self"
    ]
  },
  "GGUFHeader": {
    "size": [],
    "__init__": [
      "self",
      "f"
    ]
  },
  "GGUFConfig": {
    "__init__": [
      "self",
      "f",
      "header"
    ]
  },
  "GGUFTensorInfos": {
    "__init__": [
      "self",
      "f",
      "header",
      "config"
    ]
  },
  "GGUFTensorLoader": {
    "__init__": [
      "self",
      "fpath",
      "tensor_infos"
    ],
    "__iter__": [
      "self"
    ],
    "load_while_process": [
      "self",
      "process"
    ],
    "convert_f32_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_f16_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_q4_0_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_q4_1_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_q5_0_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_q5_1_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_q8_0_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_q6_k_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ],
    "convert_unknown_tensor": [
      "self",
      "tensor",
      "size",
      "ndims",
      "dims"
    ]
  },
  "GGUFFileLoader": {
    "__init__": [
      "self",
      "fpath"
    ],
    "tensors": [
      "self",
      "dtype"
    ],
    "tensors_iter": [
      "self"
    ],
    "tokenizer_pieces": [
      "self"
    ]
  },
  "qtype_map": [],
  "load_gguf_model": [
    "fpath",
    "dtype",
    "low_bit"
  ],
  "load_gguf_mistral": [
    "loader",
    "dtype",
    "low_bit"
  ],
  "get_mistral_module_name": [
    "name"
  ],
  "load_gguf_baichuan": [
    "loader",
    "dtype",
    "low_bit"
  ],
  "restore_baichuan_weight": [
    "name",
    "weight",
    "n_head",
    "n_head_kv"
  ],
  "get_baichuan_module_name": [
    "name"
  ],
  "load_gguf_mixtral": [
    "loader",
    "dtype",
    "low_bit"
  ],
  "get_mixtral_module_name": [
    "name"
  ],
  "load_gguf_yuan": [
    "loader",
    "dtype",
    "low_bit"
  ],
  "load_gguf_llama": [
    "loader",
    "dtype",
    "low_bit"
  ],
  "get_llama_module_name": [
    "name"
  ],
  "load_gguf_bloom": [
    "loader",
    "dtype"
  ],
  "restore_bloom_weight": [
    "ckpt",
    "n_head",
    "n_embed"
  ],
  "load_gguf_falcon": [
    "loader",
    "dtype"
  ],
  "restore_falcon_weight": [
    "ckpt",
    "n_head",
    "n_head_kv",
    "head_dim"
  ],
  "load_gguf_mpt": [
    "loader",
    "dtype"
  ],
  "YuanConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_act",
      "model_max_length",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "_CONFIG_FOR_DOC": [],
  "LocalizedFiltering": {
    "__init__": [
      "self",
      "hidden_size"
    ],
    "_train_forward": [
      "self",
      "inputs"
    ],
    "_inference_forward": [
      "self",
      "inputs",
      "before_hidden_states"
    ],
    "forward": [
      "self",
      "inputs",
      "before_hidden_states"
    ]
  },
  "_expand_mask": [
    "mask",
    "dtype",
    "tgt_len"
  ],
  "rotate_half": [
    "x"
  ],
  "apply_rotary_pos_emb": [
    "q",
    "k",
    "cos",
    "sin",
    "position_ids"
  ],
  "YuanMLP": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "hidden_act"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "YuanAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ]
  },
  "YuanDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ]
  },
  "YUAN_START_DOCSTRING": [],
  "YuanPreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_skip_keys_device_placement": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "module",
      "value"
    ]
  },
  "YUAN_INPUTS_DOCSTRING": [],
  "YuanModel": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "_prepare_decoder_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "_prepare_decoder_attention_mask_training": [
      "self",
      "input_id",
      "inputs_embeds",
      "eod_token",
      "reset_mask_flag",
      "reset_attention_mask",
      "reset_position_ids"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "YuanForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "get_loss_mask": [
      "self",
      "input_ids",
      "labels",
      "eod_token",
      "sep_token"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds"
    ],
    "_reorder_cache": [
      "past_key_values",
      "beam_idx"
    ]
  },
  "YuanForSequenceClassification": {
    "_keys_to_ignore_on_load_missing": [],
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "RotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "device"
    ],
    "forward": [
      "self",
      "x",
      "seq_len"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "hidden_act"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "_shape": [
      "self",
      "tensor",
      "seq_len",
      "bsz"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ]
  },
  "DecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ]
  },
  "PreTrainedModel": {
    "config_class": [],
    "base_model_prefix": [],
    "supports_gradient_checkpointing": [],
    "_no_split_modules": [],
    "_keys_to_ignore_on_load_unexpected": [],
    "_init_weights": [
      "self",
      "module"
    ],
    "_set_gradient_checkpointing": [
      "self",
      "module",
      "value"
    ]
  },
  "Model": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "_prepare_decoder_attention_mask": [
      "self",
      "attention_mask",
      "input_shape",
      "inputs_embeds",
      "past_key_values_length"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "BaiChuanForCausalLM": {
    "__init__": [
      "self",
      "config"
    ],
    "get_input_embeddings": [
      "self"
    ],
    "set_input_embeddings": [
      "self",
      "value"
    ],
    "get_output_embeddings": [
      "self"
    ],
    "set_output_embeddings": [
      "self",
      "new_embeddings"
    ],
    "set_decoder": [
      "self",
      "decoder"
    ],
    "get_decoder": [
      "self"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_key_values",
      "attention_mask",
      "inputs_embeds"
    ],
    "_reorder_cache": [
      "past_key_values",
      "beam_idx"
    ]
  },
  "BaiChuanConfig": {
    "model_type": [],
    "keys_to_ignore_at_inference": [],
    "__init__": [
      "self",
      "vocab_size",
      "hidden_size",
      "intermediate_size",
      "num_hidden_layers",
      "num_attention_heads",
      "hidden_act",
      "max_position_embeddings",
      "initializer_range",
      "rms_norm_eps",
      "use_cache",
      "pad_token_id",
      "bos_token_id",
      "eos_token_id",
      "tie_word_embeddings"
    ]
  },
  "VOCAB_FILES_NAMES": [],
  "PRETRAINED_VOCAB_FILES_MAP": [],
  "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES": [],
  "BaiChuanTokenizer": {
    "vocab_files_names": [],
    "pretrained_vocab_files_map": [],
    "max_model_input_sizes": [],
    "model_input_names": [],
    "__init__": [
      "self",
      "vocab_file",
      "unk_token",
      "bos_token",
      "eos_token",
      "pad_token",
      "sp_model_kwargs",
      "add_bos_token",
      "add_eos_token",
      "clean_up_tokenization_spaces"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "d"
    ],
    "vocab_size": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "index"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "get_special_tokens_mask": [
      "self",
      "token_ids_0",
      "token_ids_1",
      "already_has_special_tokens"
    ],
    "create_token_type_ids_from_sequences": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ]
  },
  "merge_qkv": [
    "module"
  ],
  "merge_mlp": [
    "module"
  ],
  "qwen2_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "qwen2_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "qwen2_mlp_forward": [
    "self",
    "x"
  ],
  "SlicedLMHead": {
    "__init__": [
      "self",
      "weight",
      "bias",
      "split_num",
      "use_split",
      "group_size",
      "asym"
    ],
    "forward": [
      "self",
      "hidden_states"
    ],
    "get_weight_dtype": [
      "self"
    ],
    "get_fused_lm_head": [
      "self"
    ]
  },
  "mistral_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "mistral_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position"
  ],
  "mistral_mlp_forward": [
    "self",
    "x"
  ],
  "merge_linear": [
    "linears"
  ],
  "split_linear": [
    "module",
    "module_name",
    "n_splits",
    "load"
  ],
  "split_linears": [
    "module",
    "n_splits_hidden_size",
    "n_splits_down_proj",
    "load"
  ],
  "VIT_KEY": [],
  "VIT_FILE": [],
  "load_weights": [
    "input_dir"
  ],
  "convert_state_dict": [
    "original_state_dict",
    "config",
    "partial_rotary_factor",
    "decouple_tied_embeddings"
  ],
  "convert_config": [
    "original_config",
    "decouple_tied_embeddings"
  ],
  "convert_glm_tokenizer": [
    "input_dir"
  ],
  "baichuan_mlp_forward": [
    "self",
    "x"
  ],
  "baichuan_attention_fwd": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "split_mlp_down_proj": [
    "module"
  ],
  "split_mlp_forward": [
    "self",
    "x"
  ],
  "LowBitQwenMultiDecoderlayer": {
    "__init__": [
      "self",
      "hidden_shape"
    ],
    "build_decoder": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "input_layernorm_weight",
      "post_attention_layernorm_weight",
      "q_bias",
      "k_bias",
      "v_bias",
      "past_key",
      "past_value"
    ]
  },
  "FusedQwenLowBitMultiDecoderlayer": {
    "__init__": [
      "self",
      "parameters",
      "input_laynorm_weights",
      "post_attn_layernorm_weights",
      "q_biases",
      "k_biases",
      "v_biases",
      "layer_indexes",
      "intra_stages",
      "cached_cos",
      "cached_sin",
      "num_heads",
      "head_dim",
      "num_key_value_heads",
      "rms_norm_eps",
      "intermediate_size",
      "max_seq_len",
      "transpose_value",
      "do_print",
      "n_splits_linear",
      "n_splits_down_proj",
      "group_size",
      "asym"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ],
    "post_forward": [
      "self",
      "past_key_value",
      "new_keys",
      "new_values"
    ]
  },
  "FusedQwenLowBitDecoderlayer": {
    "__init__": [
      "self",
      "parameters",
      "cached_cos",
      "cached_sin",
      "layer_norm_0",
      "layer_norm_1",
      "q_bias",
      "k_bias",
      "v_bias",
      "num_heads",
      "num_key_value_heads",
      "layer_idx",
      "rms_norm_eps",
      "intermediate_size",
      "max_seq_len",
      "transpose_value",
      "n_splits_linear",
      "n_splits_down_proj",
      "group_size",
      "asym"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ]
  },
  "run_decode": [
    "model",
    "rank",
    "world_size",
    "port",
    "layer_start",
    "layer_end",
    "intra_stages",
    "max_seq_len",
    "transpose_value_cache",
    "input_queue",
    "result_queue"
  ],
  "DecodeRunner": {
    "__init__": [
      "self",
      "model",
      "max_seq_len",
      "intra_pp",
      "inter_pp",
      "transpose_value_cache"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ],
    "shutdown": [
      "self"
    ],
    "__del__": [
      "self"
    ]
  },
  "run_prefill": [
    "model",
    "max_output_len",
    "max_prompt_len",
    "transpose_value_cache",
    "input_queue",
    "result_queue"
  ],
  "PrefillRunner": {
    "__init__": [
      "self",
      "model",
      "max_output_len",
      "max_prompt_len",
      "transpose_value_cache"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ],
    "shutdown": [
      "self"
    ],
    "__del__": [
      "self"
    ]
  },
  "gen_qwen2_fused_model_forward": [
    "prefill_runner",
    "decode_runner"
  ],
  "qwen2_casullm_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "LowBitMultiEncoderlayer": {
    "__init__": [
      "self",
      "hidden_shape"
    ],
    "build_encoder": [
      "self",
      "hidden_states",
      "attention_mask",
      "attn_output_norm_weight",
      "attn_output_norm_bias",
      "encoder_output_norm_weight",
      "encoder_output_norm_bias",
      "attn_self_query_bias",
      "attn_self_key_bias",
      "attn_self_value_bias",
      "attn_output_dense_bias",
      "encoder_inter_dense_bias",
      "encoder_output_dense_bias"
    ],
    "self_attention": [
      "self",
      "hidden_states",
      "attention_mask",
      "query_bias",
      "key_bias",
      "value_bias"
    ],
    "self_output": [
      "self",
      "input_tensor",
      "hidden_states",
      "output_bias",
      "layer_norm_weight",
      "layer_norm_bias"
    ],
    "self_intermediate": [
      "self",
      "inter_tensor",
      "inter_bias"
    ],
    "encoder_output": [
      "self",
      "input_tensor",
      "hidden_states",
      "output_bias",
      "layer_norm_weight",
      "layer_norm_bias"
    ]
  },
  "FusedLlamaLowBitDecoderlayer": {
    "__init__": [
      "self",
      "parameters",
      "attn_output_norm_weight",
      "attn_output_norm_bias",
      "encoder_output_norm_weight",
      "encoder_output_norm_bias",
      "attn_self_query_bias",
      "attn_self_key_bias",
      "attn_self_value_bias",
      "attn_output_dense_bias",
      "encoder_inter_dense_bias",
      "encoder_output_dense_bias",
      "intermediate_size",
      "num_attention_heads",
      "hidden_size",
      "rms_norm_eps",
      "layer_idx",
      "max_seq_len",
      "transpose_value"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "gen_xlm_fused_encoder_forward": [
    "prefill_runner"
  ],
  "XLMPoolLinear": {
    "__init__": [
      "self",
      "input_shape",
      "output_channels",
      "input_channels",
      "device"
    ]
  },
  "XLMPoolLayer": {
    "__init__": [
      "self",
      "weight",
      "bias",
      "output_channel",
      "input_channel"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "LayerNorm": {
    "__init__": [
      "self",
      "input_shape",
      "weight_shape",
      "bias_shape",
      "eps",
      "device"
    ]
  },
  "XLMLayerNorm": {
    "__init__": [
      "self",
      "weight",
      "bias",
      "eps"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "replace_with_Layernorm": [
    "layer",
    "qtype",
    "device",
    "modules_to_not_convert",
    "group_size"
  ],
  "replace_with_FP16Linear": [
    "layer",
    "qtype",
    "device",
    "modules_to_not_convert",
    "group_size",
    "imatrix"
  ],
  "LowBitLlamaMultiDecoderlayer": {
    "__init__": [
      "self",
      "hidden_shape"
    ],
    "build_decoder": [
      "self",
      "hidden_states",
      "attention_mask",
      "input_layernorm_weight",
      "post_attention_layernorm_weight",
      "position_ids",
      "past_key",
      "past_value",
      "use_prefill_sdp",
      "cos",
      "sin"
    ]
  },
  "FusedLlamaLowBitMultiDecoderlayer": {
    "__init__": [
      "self",
      "parameters",
      "input_laynorm_weights",
      "post_attn_layernorm_weights",
      "layer_indexes",
      "intra_stages",
      "cached_cos",
      "cached_sin",
      "num_heads",
      "head_dim",
      "num_key_value_heads",
      "rms_norm_eps",
      "intermediate_size",
      "max_seq_len",
      "transpose_value",
      "do_print",
      "n_splits_linear",
      "n_splits_down_proj",
      "group_size",
      "asym"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache",
      "cache_position",
      "cos",
      "sin"
    ],
    "post_forward": [
      "self",
      "past_key_value",
      "new_keys",
      "new_values",
      "cache_position"
    ]
  },
  "gen_llama_fused_model_forward": [
    "prefill_runner",
    "decode_runner"
  ],
  "gen_llama_32_fused_model_forward": [
    "prefill_runner",
    "decode_runner"
  ],
  "llama2_casullm_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "MinicpmVConv2d": {
    "__init__": [
      "self",
      "input_shape",
      "weight_shape",
      "bias",
      "strides",
      "padding",
      "dilation",
      "groups",
      "device"
    ]
  },
  "MinicpmVPatchEmbedding": {
    "__init__": [
      "self",
      "weight",
      "bias",
      "strides",
      "padding",
      "dilation",
      "groups"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MinicpmVLayerNorm": {
    "__init__": [
      "self",
      "weight",
      "bias",
      "eps"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "pad_mlp_fc2": [
    "module"
  ],
  "pad_mlp_forward": [
    "self",
    "hidden_states"
  ],
  "pad_lm_head": [
    "module"
  ],
  "lm_head_forward": [
    "self",
    "hidden_states"
  ],
  "encoder_attn_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "output_attentions"
  ],
  "_in_projection_packed": [
    "q",
    "k",
    "v",
    "w",
    "b"
  ],
  "multi_head_attn_forward": [
    "self",
    "query",
    "key",
    "value",
    "embed_dim_to_check",
    "num_heads",
    "in_proj_weight",
    "in_proj_bias",
    "bias_k",
    "bias_v",
    "add_zero_attn",
    "dropout_p",
    "out_proj_weight",
    "out_proj_bias",
    "training",
    "key_padding_mask",
    "need_weights",
    "attn_mask",
    "use_separate_proj_weight",
    "q_proj_weight",
    "k_proj_weight",
    "v_proj_weight",
    "static_k",
    "static_v",
    "average_attn_weights",
    "is_causal"
  ],
  "resampler_forward": [
    "self",
    "x",
    "tgt_sizes"
  ],
  "module_optimization": [
    "func"
  ],
  "replace_with_QuantizedLinear": [
    "layer",
    "qtype",
    "device",
    "modules_to_not_convert",
    "group_size",
    "imatrix"
  ],
  "replace_with_DequantizedLinear": [
    "layer",
    "qtype",
    "device",
    "modules_to_not_convert",
    "group_size",
    "imatrix"
  ],
  "general_convert": [
    "m",
    "target_m",
    "new_func",
    "func_name"
  ],
  "optimize_llm": [
    "model"
  ],
  "optimize_llm_post": [
    "model"
  ],
  "simple_generate": [
    "self",
    "inputs",
    "streamer"
  ],
  "optimize_llm_single_process": [
    "model",
    "kv_len",
    "max_prompt_len",
    "transpose_value_cache",
    "group_size",
    "qtype",
    "save_directory",
    "fuse_layers",
    "has_llm",
    "keep_ir",
    "compile_blob"
  ],
  "prepare_input_ids": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "inputs_embeds"
  ],
  "causal_lm_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "LowBitMultiDecoderlayer": {
    "__init__": [
      "self",
      "hidden_shape",
      "mask_shape",
      "memory_shape",
      "memory_mask_shape"
    ],
    "build_decoder": [
      "self",
      "x",
      "tgt_mask",
      "memory",
      "memory_mask",
      "norm1_weight",
      "norm1_bias",
      "norm2_weight",
      "norm2_bias",
      "norm3_weight",
      "norm3_bias",
      "fsmn_weight",
      "q_bias",
      "kv_bias",
      "out_bias",
      "w1_bias",
      "feed_norm_weight",
      "feed_norm_bias"
    ],
    "run_multi_decoders": [
      "inputs",
      "decoders",
      "models_ptr"
    ]
  },
  "gen_funasr_fused_encoder_forward": [
    "prefill_runner"
  ],
  "gen_funasr_fused_decoder_forward": [
    "decode_runner"
  ],
  "chatglm4_model_forward": [
    "self",
    "input_ids",
    "position_ids",
    "attention_mask",
    "full_attention_mask",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_hidden_states",
    "return_dict"
  ],
  "chatglm4_encoder_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "rotary_pos_emb",
    "kv_caches",
    "use_cache",
    "output_hidden_states"
  ],
  "chatglm4_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "rotary_pos_emb",
    "kv_cache",
    "use_cache"
  ],
  "padding_lm_head": [
    "module"
  ],
  "minicpm_model_causal_lm_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "minicpm_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "minicpm_mlp_forward": [
    "self",
    "x"
  ],
  "init_fused_kv_cache": [
    "batch_size",
    "num_heads",
    "head_dim",
    "current_length",
    "max_length",
    "dtype",
    "device",
    "tranpose_value"
  ],
  "append_fused_kv_cache": [
    "cache_k",
    "cache_v",
    "key_states",
    "value_states",
    "transpose_value"
  ],
  "expand_fused_kv_cache": [
    "cache_k",
    "cache_v",
    "transpose_value"
  ],
  "shrink_fused_kv_cache": [
    "cache_k",
    "cache_v",
    "new_seq_len",
    "transpose_value"
  ],
  "DynamicFusedNormalCache": {
    "__init__": [
      "self",
      "num_hidden_layers"
    ],
    "update": [
      "self",
      "key_states",
      "value_states",
      "layer_idx",
      "cache_kwargs"
    ],
    "get_seq_length": [
      "self",
      "layer_idx"
    ],
    "expand": [
      "self",
      "transpose_value"
    ],
    "shrink": [
      "self",
      "new_seq_len",
      "transpose_value"
    ],
    "_seen_tokens": [
      "self"
    ],
    "seen_tokens": [
      "self"
    ]
  },
  "phi3v_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "pixel_values",
    "image_sizes",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "phi3v_encoder_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "causal_attention_mask",
    "output_attentions"
  ],
  "run_model": [
    "x",
    "weights",
    "backend_cls",
    "op_id",
    "replica"
  ],
  "LLMBaseNNFactory": {
    "__init__": [
      "self",
      "max_seq_len",
      "transpose_value",
      "dtype",
      "profile",
      "device",
      "n_splits_linear",
      "n_splits_down_proj",
      "group_size",
      "asym"
    ],
    "attention": [
      "self"
    ],
    "paraformer_layer_norm": [
      "self",
      "hidden_states",
      "layernorm_weight",
      "layernorm_bias"
    ],
    "self_attn_sanm": [
      "self",
      "x",
      "mask",
      "in_feat",
      "n_feat",
      "n_head",
      "fsmn_weight",
      "qkv_bias",
      "out_bias"
    ],
    "sanm_feed_forward": [
      "self",
      "x",
      "hidden_units",
      "idim",
      "w1_bias",
      "w2_bias"
    ],
    "multihead_attn_sanm_decoder": [
      "self",
      "inputs",
      "mask",
      "fsmn_weight"
    ],
    "sanm_cross_attn": [
      "self",
      "x",
      "memory",
      "mask",
      "q_bias",
      "kv_bias",
      "out_bias",
      "n_feat",
      "n_head"
    ],
    "feed_forward_sanm_decoder": [
      "self",
      "x",
      "w_1_bias",
      "norm_weights",
      "norm_bias"
    ],
    "mlp": [
      "self",
      "hidden_states",
      "seq_len",
      "mode"
    ],
    "layer_norm": [
      "self",
      "hidden_states",
      "layernorm_weight"
    ],
    "rotate_half": [
      "self",
      "x"
    ],
    "apply_rotary_pos_emb": [
      "self"
    ],
    "repeat_kv": [
      "self"
    ],
    "create_cache_op": [
      "self",
      "shape"
    ],
    "create_input_op": [
      "self",
      "shape",
      "dtype"
    ],
    "linear": [
      "self",
      "input_node",
      "output_channels",
      "input_channels",
      "bias",
      "act_dtype",
      "wt_dtype",
      "n_splits",
      "scale_factor",
      "is_prefill",
      "asym"
    ],
    "dq_split_linear": [
      "self",
      "input_node",
      "output_channels",
      "input_channels",
      "n_splits",
      "act_dtype",
      "wt_dtype",
      "scale_factor",
      "is_prefill",
      "asym"
    ],
    "parameter": [
      "self",
      "shape"
    ],
    "update_cache": [
      "self",
      "past_key_value",
      "indexes"
    ],
    "load_cache_async": [
      "self"
    ],
    "set_weights": [
      "self",
      "op_id",
      "weights"
    ],
    "set_weights_async": [
      "self",
      "op_id",
      "weights"
    ],
    "run_decoders": [
      "inputs",
      "decoders",
      "models_ptr"
    ]
  },
  "LowBitBaichuanMultiDecoderlayer": {
    "__init__": [
      "self",
      "hidden_shape"
    ],
    "attention": [
      "self"
    ],
    "build_decoder": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "input_layernorm_weight",
      "post_attention_layernorm_weight",
      "past_key",
      "past_value",
      "use_prefill_sdp"
    ]
  },
  "FusedBaichuanLowBitMultiDecoderlayer": {
    "__init__": [
      "self",
      "parameters",
      "input_laynorm_weights",
      "post_attn_layernorm_weights",
      "layer_indexes",
      "intra_stages",
      "cached_cos",
      "cached_sin",
      "num_heads",
      "head_dim",
      "rms_norm_eps",
      "intermediate_size",
      "max_seq_len",
      "transpose_value",
      "do_print",
      "n_splits_linear",
      "n_splits_down_proj",
      "group_size",
      "asym"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ],
    "post_forward": [
      "self",
      "past_key_value",
      "new_keys",
      "new_values"
    ]
  },
  "FusedBaichuanLowBitDecoderlayer": {
    "__init__": [
      "self",
      "parameters",
      "cached_cos",
      "cached_sin",
      "layer_norm_0",
      "layer_norm_1",
      "num_heads",
      "layer_idx",
      "rms_norm_eps",
      "intermediate_size",
      "max_seq_len",
      "transpose_value",
      "n_splits_linear",
      "n_splits_down_proj",
      "group_size",
      "asym"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_value",
      "output_attentions",
      "use_cache"
    ]
  },
  "gen_baichuan_fused_model_forward": [
    "prefill_runner",
    "decode_runner"
  ],
  "baichuan2_causal_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "LowBitMinicpmMultiDecoderlayer": {
    "__init__": [
      "self",
      "hidden_shape"
    ],
    "build_decoder": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "input_layernorm_weight",
      "post_attention_layernorm_weight",
      "scale_depth",
      "num_hidden_layers",
      "past_key",
      "past_value"
    ]
  },
  "gen_minicpm_fused_model_forward": [
    "prefill_runner",
    "decode_runner"
  ],
  "minicpm_casullm_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "phi3_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "llama_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "llama_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position"
  ],
  "llama_mlp_forward": [
    "self",
    "x"
  ],
  "optimize_llm_pre": [
    "model",
    "qtype",
    "mixed_precision",
    "quantization_group_size",
    "load",
    "max_prompt_len"
  ],
  "convert_llama": [
    "model",
    "max_output_len",
    "max_prompt_len",
    "decoder",
    "inter_pp",
    "intra_pp",
    "transpose_value_cache"
  ],
  "convert_baichuan": [
    "model",
    "max_output_len",
    "max_prompt_len",
    "decoder",
    "inter_pp",
    "intra_pp",
    "transpose_value_cache"
  ],
  "convert_minicpm": [
    "model",
    "max_output_len",
    "max_prompt_len",
    "decoder",
    "inter_pp",
    "intra_pp",
    "transpose_value_cache"
  ],
  "convert_qwen": [
    "model",
    "max_output_len",
    "max_prompt_len",
    "decoder",
    "inter_pp",
    "intra_pp",
    "transpose_value_cache"
  ],
  "convert_bce": [
    "model",
    "max_context_len",
    "max_prompt_len",
    "transpose_value_cache"
  ],
  "optimize_funasr": [
    "model",
    "max_context_len",
    "max_prompt_len",
    "inter_pp",
    "intra_pp",
    "transpose_value_cache"
  ],
  "LMHeadLinear": {
    "__init__": [
      "self",
      "inC",
      "outC",
      "batch",
      "split_num",
      "profile",
      "device",
      "dtype",
      "use_split",
      "group_size",
      "asym"
    ],
    "set_weights": [
      "self",
      "op_id",
      "weights"
    ],
    "set_weights_async": [
      "self",
      "op_id",
      "weights"
    ],
    "run": [
      "self",
      "X"
    ]
  },
  "is_acclib_available": [],
  "Linear": {
    "__init__": [
      "self",
      "weight",
      "bias"
    ],
    "forward": [
      "self",
      "x"
    ],
    "fromTorch": [
      "layer",
      "dtype"
    ],
    "fromTensor": [
      "weight",
      "bias",
      "dtype"
    ]
  },
  "QuantizedLinear": {
    "__init__": [
      "self",
      "weight",
      "scale",
      "zero",
      "bias",
      "qtype",
      "group_size"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DequantizedLinear": {
    "__init__": [
      "self",
      "weight",
      "scale",
      "zero",
      "bias",
      "qtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "get_shared_lib_info": [
    "lib_base_name"
  ],
  "_lib": [],
  "load_model_from_file": [
    "model_dir"
  ],
  "run_prefill_with_logits": [
    "model_ptr",
    "input_ids",
    "logits",
    "vocab_size",
    "inputs_embeds",
    "seq_len"
  ],
  "run_decode_with_logits": [
    "model_ptr",
    "input_id",
    "logits",
    "vocab_size"
  ],
  "reset": [
    "model_ptr"
  ],
  "chatglm2_model_forward": [
    "self",
    "input_ids",
    "position_ids",
    "attention_mask",
    "full_attention_mask",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_hidden_states",
    "return_dict"
  ],
  "chatglm2_encoder_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "rotary_pos_emb",
    "kv_caches",
    "use_cache",
    "output_hidden_states"
  ],
  "rotate_every_two": [
    "x"
  ],
  "chatglm2_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "rotary_pos_emb",
    "kv_cache",
    "use_cache"
  ],
  "stablelm_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "stablelm_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "stablelm_mlp_forward": [
    "self",
    "x"
  ],
  "siglip_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "output_attentions"
  ],
  "minicpmv_decode_stream_wrapper": [
    "origin_decode_stream"
  ],
  "vision_transformer_attention_forward": [
    "self",
    "x"
  ],
  "minicpmv_chat_wrapper": [
    "origin_chat"
  ],
  "minicpmv_get_vision_embedding": [
    "self",
    "pixel_values"
  ],
  "patched_repetition_penalty_call": [
    "self",
    "input_ids",
    "scores"
  ],
  "minicpmv_generate_wrapper": [
    "origin_generate"
  ],
  "whisper_attention_forward": [
    "self",
    "hidden_states",
    "key_value_states",
    "past_key_value",
    "attention_mask",
    "layer_head_mask",
    "output_attentions",
    "cache_position"
  ],
  "qwen2_causal_lm_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "padding_mlp": [
    "module"
  ],
  "mllama_vision_attention_forward": [
    "self",
    "hidden_state",
    "attention_mask",
    "output_attentions"
  ],
  "mllama_text_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "cross_attention_states",
    "cross_attention_mask",
    "full_text_row_masked_out_mask",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "mllama_cross_attention_forward": [
    "self",
    "hidden_states",
    "cross_attention_states",
    "past_key_value",
    "attention_mask",
    "output_attentions",
    "use_cache",
    "cache_position"
  ],
  "KV_CACHE_ALLOC_BLOCK_LENGTH": [],
  "pre_compute_inv_freq": [
    "module"
  ],
  "gemma_rms_norm_forward": [
    "self",
    "hidden_states"
  ],
  "gemma_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "gemma_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position"
  ],
  "merge_qkv_base": [
    "module",
    "attention_class"
  ],
  "padding_linear_hd": [
    "linear",
    "old_head_dim",
    "new_head_dim"
  ],
  "padding_attention_hd_base": [
    "module",
    "attention_class",
    "old_head_dim",
    "new_head_dim"
  ],
  "padding_mla_v_hd_base": [
    "module",
    "attention_class"
  ],
  "padding_states_hd": [
    "states",
    "old_head_dim",
    "new_head_dim"
  ],
  "padding_qkv_hd": [
    "q",
    "k",
    "v",
    "old_head_dim",
    "new_head_dim"
  ],
  "fuse_mlp_base": [
    "module",
    "act",
    "x"
  ],
  "mlp_silu_forward": [
    "self",
    "x"
  ],
  "mlp_gelu_forward": [
    "self",
    "x"
  ],
  "attention_softmax": [
    "attn_weights"
  ],
  "rms_norm_forward": [
    "self",
    "hidden_states"
  ],
  "layer_norm_forward": [
    "self",
    "hidden_states"
  ],
  "prepare_mask": [
    "mask",
    "bsz",
    "n_heads",
    "seq_length",
    "kv_length",
    "is_causal",
    "dtype",
    "device"
  ],
  "scaled_dot_product_attention": [
    "query",
    "key",
    "value",
    "mask",
    "is_causal",
    "scale"
  ],
  "_get_pos_embed": [
    "self",
    "pos_embed",
    "H",
    "W"
  ],
  "internvl_chat": [
    "self",
    "tokenizer",
    "pixel_values",
    "question",
    "generation_config",
    "history",
    "return_history",
    "num_patches_list",
    "IMG_START_TOKEN",
    "IMG_END_TOKEN",
    "IMG_CONTEXT_TOKEN",
    "verbose"
  ],
  "internvl_batch_chat": [
    "self",
    "tokenizer",
    "pixel_values",
    "questions",
    "generation_config",
    "num_patches_list",
    "history",
    "return_history",
    "IMG_START_TOKEN",
    "IMG_END_TOKEN",
    "IMG_CONTEXT_TOKEN",
    "verbose",
    "image_counts"
  ],
  "intern_attention_forward": [
    "self",
    "x"
  ],
  "decilm_attention_forward_4_35_2": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "padding_mask"
  ],
  "vision_attention_forward": [
    "self",
    "x"
  ],
  "extract_key_value": [
    "self",
    "hidden",
    "state"
  ],
  "rwkv_linear_attention_xpu": [
    "time_decay",
    "time_first",
    "key",
    "value",
    "state",
    "return_state"
  ],
  "rwkv_attention_forward": [
    "self",
    "hidden",
    "state",
    "use_cache"
  ],
  "rwkv_ffn_forward": [
    "self",
    "hidden",
    "state"
  ],
  "qwen2moe_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "output_router_logits",
    "return_dict"
  ],
  "qwen2_moe_model_forward_internal": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "output_router_logits",
    "return_dict"
  ],
  "qwen2_moe_causal_lm_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "labels",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "output_router_logits",
    "return_dict"
  ],
  "qwen2moe_moeblock_forward": [
    "self",
    "hidden_states"
  ],
  "baichuan_model_7b_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "baichuan_attention_forward_7b": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "baichuan_attention_forward_13b": [
    "self",
    "hidden_states",
    "attention_mask",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "_get_interleave": [
    "n"
  ],
  "_fill_with_neg_inf": [
    "t"
  ],
  "_buffered_future_mask": [
    "tensor",
    "maxpos",
    "alibi",
    "attn_heads"
  ],
  "baichuan_13b_gen_alibi_mask": [
    "tensor",
    "n_head",
    "max_pos"
  ],
  "MASK_BLOCK_SIZE": [],
  "baichuan_13b_get_alibi_mask": [
    "self",
    "tensor",
    "seq_length_with_past"
  ],
  "get_abs_pos": [
    "abs_pos",
    "tgt_size"
  ],
  "qwen_attention_forward_vl": [
    "self",
    "hidden_states",
    "rotary_pos_emb",
    "registered_causal_mask",
    "layer_past",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "output_attentions",
    "use_cache"
  ],
  "should_use_fuse_rope": [
    "self",
    "query_states"
  ],
  "is_enough_kv_cache_room": [
    "layer_past",
    "kv_seq_len"
  ],
  "qwen_vl_resampler_forward": [
    "self",
    "x",
    "attn_mask"
  ],
  "qwen_vl_vision_transformer_forward": [
    "self",
    "x"
  ],
  "qwen_vl_model_forward": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "token_type_ids",
    "position_ids",
    "head_mask",
    "inputs_embeds",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "padding_mla_v_hd": [
    "module"
  ],
  "deepseek_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "deepseek_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "fuse_gate_forward": [
    "self",
    "x"
  ],
  "moe_infer_decode": [
    "self",
    "x",
    "topk_ids",
    "topk_weight"
  ],
  "deepseek_moe_forward": [
    "self",
    "hidden_states"
  ],
  "is_empty": [
    "images_list"
  ],
  "chatglm4v_model_forward": [
    "self",
    "input_ids",
    "images",
    "position_ids",
    "attention_mask",
    "full_attention_mask",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_hidden_states",
    "return_dict"
  ],
  "chatglm4v_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "rotary_pos_emb",
    "kv_cache",
    "use_cache"
  ],
  "visual_attention_forward": [
    "self",
    "x"
  ],
  "patch_embedding_forward": [
    "self",
    "images"
  ],
  "vision_model_forward": [
    "self",
    "image"
  ],
  "attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "self_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "past_key_value",
    "output_attentions"
  ],
  "encoder_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "past_key_values",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "merge_qk": [
    "module"
  ],
  "yuan_localized_filtering_forward": [
    "self",
    "inputs",
    "before_hidden_states",
    "dtype"
  ],
  "yuan_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "gemma2_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "gemma2_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position"
  ],
  "FP8_KV_ALLOC_LENGTH": [],
  "SILU": [],
  "GELU": [],
  "decoding_fast_path_qtype_check": [
    "proj"
  ],
  "init_kv_cache": [
    "batch_size",
    "num_heads",
    "head_dim",
    "current_length",
    "max_length",
    "dtype",
    "device"
  ],
  "extend_kv_cache": [
    "batch_size",
    "num_heads",
    "head_dim",
    "current_length",
    "max_length",
    "dtype",
    "device"
  ],
  "append_kv_cache": [
    "cache_k",
    "cache_v",
    "key_states",
    "value_states"
  ],
  "use_quantize_kv_cache": [
    "linear",
    "x",
    "num_heads",
    "num_kv_heads"
  ],
  "init_fp8_kv_cache": [
    "batch_size",
    "num_heads",
    "current_length",
    "head_dim",
    "device"
  ],
  "append_fp8_kv_cache": [
    "k_cache",
    "v_cache",
    "key",
    "value"
  ],
  "restore_fp8_kv_cache": [
    "k_cache",
    "v_cache",
    "dtype"
  ],
  "is_enough_kv_cache_room_4_36": [
    "past_key_value",
    "idx",
    "seq_len"
  ],
  "is_enough_kv_cache_room_4_31": [
    "past_key_value",
    "seq_len"
  ],
  "use_sdp": [
    "q_len",
    "kv_len",
    "head_dim",
    "query_states"
  ],
  "use_sdp_causal": [
    "q_len",
    "kv_len",
    "head_dim",
    "query_states",
    "training"
  ],
  "use_sdp_non_causal": [
    "head_dim",
    "device",
    "dtype"
  ],
  "mlp_fusion_check": [
    "x",
    "qtype",
    "training"
  ],
  "update_past_key_value": [
    "past_key_value",
    "key_states",
    "value_states",
    "kv_seq_len",
    "use_quantize_kv",
    "device"
  ],
  "should_use_compresskv": [
    "x",
    "prompt_len"
  ],
  "get_compresskv_attn_mask": [
    "key_states",
    "attention_mask"
  ],
  "get_q_proj_or_qkv_proj": [
    "self"
  ],
  "make_cache_contiguous_inplaced": [
    "cos",
    "sin"
  ],
  "AttnProcessor2_0": {
    "__call__": [
      "self",
      "attn",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "temb"
    ]
  },
  "upcast_vae": [
    "self"
  ],
  "gpt2_attention_attn": [
    "self",
    "query",
    "key",
    "value",
    "attention_mask",
    "head_mask"
  ],
  "qwen_attention_forward": [
    "self",
    "hidden_states",
    "rotary_pos_emb_list",
    "layer_past",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "output_attentions",
    "use_cache"
  ],
  "qwen_attention_forward_registered": [
    "self",
    "hidden_states",
    "rotary_pos_emb_list",
    "registered_causal_mask",
    "layer_past",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "output_attentions",
    "use_cache"
  ],
  "qwen_mlp_forward": [
    "self",
    "x"
  ],
  "qwen_model_forward": [
    "self",
    "input_ids",
    "past_key_values",
    "attention_mask",
    "token_type_ids",
    "position_ids",
    "head_mask",
    "inputs_embeds",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict"
  ],
  "_attn_wrapper": [
    "origin_attn"
  ],
  "gptbigcode_attention_forward": [
    "self",
    "hidden_states",
    "layer_past",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "use_cache",
    "output_attentions"
  ],
  "gptbigcode_sdpa_attention_forward": [
    "self",
    "hidden_states",
    "layer_past",
    "attention_mask",
    "head_mask",
    "encoder_hidden_states",
    "encoder_attention_mask",
    "use_cache",
    "output_attentions"
  ],
  "aquila_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "phixtral_moeblock_forward": [
    "self",
    "hidden_states"
  ],
  "phixtral_mlp_forward": [
    "self",
    "x"
  ],
  "rwkv_linear_attention_cpu": [
    "B",
    "H",
    "S",
    "T",
    "n_head",
    "hidden",
    "time_decay",
    "time_first",
    "receptance",
    "key",
    "value",
    "gate",
    "lxw",
    "lxb",
    "ow",
    "state"
  ],
  "rwkv_ffn_forward_wrapper": [
    "origin_rwkv_ffn_forward"
  ],
  "rwkv_model_forward_wrapper": [
    "origin_rwkv_model_forward"
  ],
  "chatglm4_block_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "rotary_pos_emb",
    "kv_cache",
    "use_cache"
  ],
  "apply_residual_scale": [
    "module"
  ],
  "minicpm_model_forward_wrapper": [
    "origin_forward"
  ],
  "minicpm_decoder_layer_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "padding_v_head_dim": [
    "module"
  ],
  "minicpm3_model_forward_wrapper": [
    "origin_forward"
  ],
  "minicpm3_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "gptneox_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "head_mask",
    "layer_past",
    "use_cache",
    "output_attentions"
  ],
  "split_mlp": [
    "module"
  ],
  "mlp_forward": [
    "self",
    "hidden_states"
  ],
  "phi3_model_forward_wrapper": [
    "origin_model_forward"
  ],
  "phi3v_model_forward_wrapper": [
    "origin_model_forward"
  ],
  "dropout_add": [
    "x",
    "residual",
    "prob",
    "training"
  ],
  "bloom_attention_forward": [
    "self",
    "hidden_states",
    "residual",
    "alibi",
    "attention_mask",
    "layer_past",
    "head_mask",
    "use_cache",
    "output_attentions"
  ],
  "internlm_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "internlm2_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "pre_process_attn_and_mlp": [
    "module"
  ],
  "add_lora": [
    "x",
    "result",
    "im_mask",
    "lora_scaling",
    "Plora_A",
    "Plora_B"
  ],
  "internlm_xcomposser2_model_forward_wrapper": [
    "origin_forward"
  ],
  "internlm_xcomposser2_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "im_mask"
  ],
  "internlm_xcomposser2_mlp_forward": [
    "self",
    "x",
    "im_mask"
  ],
  "internlm_xcomposser2_chat": [
    "self",
    "tokenizer",
    "query",
    "image",
    "history",
    "streamer",
    "max_new_tokens",
    "do_sample",
    "temperature",
    "top_p",
    "repetition_penalty",
    "meta_instruction"
  ],
  "glm_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position",
    "position_embeddings"
  ],
  "glm_model_forward_wrapper": [
    "origin_forward"
  ],
  "qwen2_vl_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "qwen2_vision_get_dtype": [
    "self"
  ],
  "qwen2_vision_attention_forward": [
    "self",
    "hidden_states",
    "cu_seqlens",
    "rotary_pos_emb"
  ],
  "qwen2_vl_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position",
    "position_embeddings"
  ],
  "pre_register_inv_freq": [
    "module"
  ],
  "custom_convolution": [
    "U",
    "K"
  ],
  "eager_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "seqlens",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position",
    "position_embeddings"
  ],
  "apply_rotary_pos_emb_index": [
    "q",
    "k",
    "cos",
    "sin",
    "position_id"
  ],
  "glm_sdpa": [
    "query",
    "key",
    "value",
    "attention_mask",
    "is_causal"
  ],
  "KV_CACHE_ALLOC_MIN_LENGTH": [],
  "attention_fn": [
    "self",
    "query_layer",
    "key_layer",
    "value_layer",
    "attention_mask",
    "hidden_size_per_partition",
    "layer_id",
    "layer_past",
    "scaling_attention_score",
    "use_cache"
  ],
  "chatglm_attention_forward": [
    "self",
    "hidden_states",
    "position_ids",
    "attention_mask",
    "layer_id",
    "layer_past",
    "use_cache",
    "output_attentions"
  ],
  "qwen2_5_omni_attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "cache_position",
    "position_embeddings"
  ],
  "qwen2_5_omni_thinker_model_forward": [
    "self",
    "input_ids",
    "attention_mask",
    "position_ids",
    "past_key_values",
    "inputs_embeds",
    "use_cache",
    "output_attentions",
    "output_hidden_states",
    "return_dict",
    "cache_position"
  ],
  "qwen2_5_omni_vision_attention_forward": [
    "self",
    "hidden_states",
    "cu_seqlens",
    "rotary_pos_emb"
  ],
  "mpt_multihead_attention_forward": [
    "self",
    "x",
    "past_key_value",
    "attn_bias",
    "attention_mask",
    "is_causal",
    "needs_weights",
    "rotary_emb_w_meta_info"
  ],
  "mpt_scaled_multihead_dot_product_attention": [
    "query",
    "key",
    "value",
    "n_heads",
    "past_key_value",
    "softmax_scale",
    "attn_bias",
    "key_padding_mask",
    "is_causal",
    "dropout_p",
    "training",
    "needs_weights",
    "multiquery"
  ],
  "hybrid_DeepseekV3MLP_forward": [
    "self",
    "x"
  ],
  "DeepseekV3Attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache"
  ],
  "hybrid_DeepseekV3Attention_forward": [
    "self",
    "hidden_states",
    "attention_mask",
    "position_ids",
    "past_key_value",
    "output_attentions",
    "use_cache",
    "xpu_device"
  ],
  "update_names_of_IR_and_export_blob": [
    "model",
    "model_name",
    "dir",
    "compile_blob",
    "keep_ir",
    "npu_dpu_groups"
  ],
  "LowBitLLMLMHead": {
    "__init__": [
      "self",
      "hidden_shape",
      "num_heads",
      "rms_norm_eps",
      "model_norm_weight",
      "vocab_size",
      "mode",
      "dtype",
      "max_seq_len",
      "transpose_value",
      "profile",
      "device",
      "n_splits",
      "group_size",
      "asym"
    ]
  },
  "LLMEmbedding": {
    "__init__": [
      "self",
      "vocab_size",
      "embedding_dim",
      "embedding_weight",
      "padding_idx",
      "dtype",
      "input_length",
      "device"
    ]
  },
  "Llama32Embedding": {
    "__init__": [
      "self",
      "vocab_size",
      "embedding_dim",
      "embedding_weight",
      "padding_idx",
      "inv_freq",
      "attention_scaling",
      "dtype",
      "device"
    ]
  },
  "Llama32PostEmbedding": {
    "__init__": [
      "self",
      "inv_freq",
      "attention_scaling",
      "input_len",
      "device"
    ]
  },
  "obtain_weight_from_single_layer": [
    "attn_layer",
    "mlp_layer"
  ],
  "obtain_qkv_bias_from_single_layer": [
    "attn_layer"
  ],
  "obtain_embedding_from_model": [
    "model",
    "convert_model",
    "temp_dir",
    "weight_dir",
    "max_prompt_len",
    "keep_ir",
    "compile_blob"
  ],
  "convert_lm_head_and_embedding": [
    "model",
    "n_splits_linear",
    "temp_dir",
    "weight_dir"
  ],
  "convert_baichuan_layer": [
    "model",
    "layer_idx",
    "n_splits_linear",
    "n_splits_down_proj",
    "temp_dir",
    "weight_dir",
    "transpose_value_cache",
    "kv_len",
    "group_size",
    "layernorm_const"
  ],
  "convert_llm": [
    "model",
    "kv_len",
    "max_prompt_len",
    "transpose_value_cache",
    "group_size",
    "qtype",
    "convert_model",
    "save_directory",
    "fuse_layers",
    "keep_ir",
    "compile_blob"
  ],
  "convert_llm_for_deploy": [
    "model",
    "kv_len",
    "max_prompt_len",
    "transpose_value_cache",
    "n_splits_linear",
    "n_splits_down_proj",
    "group_size",
    "save_directory",
    "fuse_layers",
    "keep_ir",
    "compile_blob"
  ],
  "convert_qwen_layer": [
    "model",
    "layer_idx",
    "n_splits_linear",
    "n_splits_down_proj",
    "temp_dir",
    "weight_dir",
    "transpose_value_cache",
    "kv_len",
    "group_size",
    "const_parameter",
    "mode",
    "keep_ir",
    "compile_blob"
  ],
  "convert_fused_qwen_layer": [
    "model",
    "fused_layers",
    "n_splits_linear",
    "n_splits_down_proj",
    "save_dir",
    "weight_dir",
    "transpose_value_cache",
    "kv_len",
    "group_size",
    "const_parameter",
    "mode",
    "keep_ir",
    "compile_blob"
  ],
  "MiniCPMEmbedding": {
    "__init__": [
      "self",
      "vocab_size",
      "embedding_dim",
      "embedding_weight",
      "padding_idx",
      "dtype",
      "scale_emb",
      "device"
    ]
  },
  "MiniCPMPostEmbedding": {
    "__init__": [
      "self",
      "input_size",
      "embedding_dim",
      "dtype",
      "scale_emb",
      "device"
    ]
  },
  "MiniCPMLMHead": {
    "__init__": [
      "self",
      "hidden_shape",
      "num_heads",
      "rms_norm_eps",
      "model_norm_weight",
      "vocab_size",
      "mode",
      "dtype",
      "max_seq_len",
      "transpose_value",
      "profile",
      "device",
      "n_splits",
      "asym"
    ]
  },
  "convert_minicpm_layer": [
    "model",
    "layer_idx",
    "n_splits_linear",
    "n_splits_down_proj",
    "temp_dir",
    "weight_dir",
    "transpose_value_cache",
    "kv_len",
    "group_size",
    "const_parameter",
    "mode",
    "keep_ir",
    "compile_blob"
  ],
  "convert_fused_minicpm_layer": [
    "model",
    "fused_layers",
    "n_splits_linear",
    "n_splits_down_proj",
    "save_dir",
    "weight_dir",
    "transpose_value_cache",
    "kv_len",
    "group_size",
    "const_parameter",
    "mode",
    "keep_ir",
    "compile_blob"
  ],
  "convert_llama_layer": [
    "model",
    "layer_idx",
    "n_splits_linear",
    "n_splits_down_proj",
    "temp_dir",
    "weight_dir",
    "transpose_value_cache",
    "kv_len",
    "group_size",
    "const_parameter",
    "mode",
    "keep_ir",
    "compile_blob"
  ],
  "convert_fused_llama_layer": [
    "model",
    "fused_layers",
    "n_splits_linear",
    "n_splits_down_proj",
    "save_dir",
    "weight_dir",
    "transpose_value_cache",
    "kv_len",
    "group_size",
    "const_parameter",
    "mode",
    "keep_ir",
    "compile_blob"
  ],
  "InitLLMPipeline": [
    "model_type",
    "kv_len",
    "num_head",
    "head_dim",
    "num_layers",
    "vocab_size",
    "model_weight_dir",
    "model_name",
    "first_blob_name",
    "last_blob_name",
    "rest_blob_name",
    "layernorm_const"
  ],
  "generate_serve": [
    "kv_len",
    "num_head",
    "head_dim",
    "num_layers",
    "vocab_size",
    "transpose_value_cache",
    "param_n_output"
  ],
  "DEFAULT_HUGGINGFACE_MODEL": [],
  "IpexLLM": {
    "__init__": [
      "self",
      "context_window",
      "max_new_tokens",
      "query_wrapper_prompt",
      "tokenizer_name",
      "model_name",
      "model",
      "tokenizer",
      "device_map",
      "stopping_ids",
      "tokenizer_kwargs",
      "tokenizer_outputs_to_remove",
      "model_kwargs",
      "generate_kwargs",
      "is_chat_model",
      "callback_manager",
      "system_prompt",
      "messages_to_prompt",
      "completion_to_prompt",
      "pydantic_program_mode",
      "output_parser",
      "load_low_bit"
    ],
    "from_model_id": [
      "cls",
      "context_window",
      "max_new_tokens",
      "query_wrapper_prompt",
      "tokenizer_name",
      "model_name",
      "model",
      "tokenizer",
      "device_map",
      "stopping_ids",
      "tokenizer_kwargs",
      "tokenizer_outputs_to_remove",
      "model_kwargs",
      "generate_kwargs",
      "is_chat_model",
      "callback_manager",
      "system_prompt",
      "messages_to_prompt",
      "completion_to_prompt",
      "pydantic_program_mode",
      "output_parser"
    ],
    "from_model_id_low_bit": [
      "cls",
      "context_window",
      "max_new_tokens",
      "query_wrapper_prompt",
      "tokenizer_name",
      "model_name",
      "model",
      "tokenizer",
      "device_map",
      "stopping_ids",
      "tokenizer_kwargs",
      "tokenizer_outputs_to_remove",
      "model_kwargs",
      "generate_kwargs",
      "is_chat_model",
      "callback_manager",
      "system_prompt",
      "messages_to_prompt",
      "completion_to_prompt",
      "pydantic_program_mode",
      "output_parser"
    ],
    "class_name": [
      "cls"
    ],
    "metadata": [
      "self"
    ],
    "_tokenizer_messages_to_prompt": [
      "self",
      "messages"
    ],
    "complete": [
      "self",
      "prompt",
      "formatted"
    ],
    "stream_complete": [
      "self",
      "prompt",
      "formatted"
    ],
    "chat": [
      "self",
      "messages"
    ],
    "stream_chat": [
      "self",
      "messages"
    ]
  },
  "__all__": [],
  "write_header": [
    "fout",
    "shape",
    "dst_name",
    "ftype_cur"
  ],
  "convert_non_q4": [
    "src_name",
    "dst_name",
    "model",
    "fout"
  ],
  "expandToInt4": [
    "qweight"
  ],
  "to_ggml_int16": [
    "eweight"
  ],
  "qzeros_to_zeros": [
    "qzeros",
    "bits"
  ],
  "convert_q4": [
    "src_name",
    "dst_name",
    "model",
    "fout",
    "n_head",
    "permute"
  ],
  "find_quantized_model_file": [
    "model_path"
  ],
  "convert_gptq2ggml": [
    "model_path",
    "output_path",
    "tokenizer_path"
  ],
  "BigdlNativeEmbeddings": {
    "family_info": [],
    "validate_environment": [
      "cls",
      "values"
    ],
    "embed_documents": [
      "self",
      "texts"
    ],
    "embed_query": [
      "self",
      "text"
    ]
  },
  "_BaseEmbeddings": {
    "validate_environment": [
      "cls",
      "values"
    ],
    "embed_documents": [
      "self",
      "texts"
    ],
    "embed_query": [
      "self",
      "text"
    ]
  },
  "LlamaEmbeddings": {
    "ggml_model": [],
    "ggml_module": []
  },
  "BloomEmbeddings": {
    "ggml_model": [],
    "ggml_module": []
  },
  "GptneoxEmbeddings": {
    "ggml_model": [],
    "ggml_module": []
  },
  "StarcoderEmbeddings": {
    "ggml_model": [],
    "ggml_module": []
  },
  "DEFAULT_MODEL_NAME": [],
  "TransformersEmbeddings": {
    "from_model_id": [
      "cls",
      "model_id",
      "model_kwargs",
      "device_map"
    ],
    "embed": [
      "self",
      "text"
    ],
    "embed_documents": [
      "self",
      "texts"
    ],
    "embed_query": [
      "self",
      "text"
    ]
  },
  "TransformersBgeEmbeddings": {
    "embed": [
      "self",
      "text"
    ]
  },
  "DEFAULT_MODEL_ID": [],
  "TransformersLLM": {
    "from_model_id": [
      "cls",
      "model_id",
      "model_kwargs",
      "device_map",
      "tokenizer_id"
    ],
    "from_model_id_low_bit": [
      "cls",
      "model_id",
      "model_kwargs",
      "device_map",
      "tokenizer_id"
    ],
    "_identifying_params": [
      "self"
    ],
    "_llm_type": [
      "self"
    ],
    "_call": [
      "self",
      "prompt",
      "stop",
      "run_manager"
    ]
  },
  "BigdlNativeLLM": {
    "family_info": [],
    "validate_environment": [
      "cls",
      "values"
    ],
    "_default_params": [
      "self"
    ],
    "_identifying_params": [
      "self"
    ],
    "_llm_type": [
      "self"
    ],
    "_get_parameters": [
      "self",
      "stop"
    ],
    "_call": [
      "self",
      "prompt",
      "stop",
      "run_manager"
    ],
    "stream": [
      "self",
      "prompt",
      "stop",
      "run_manager"
    ],
    "get_num_tokens": [
      "self",
      "text"
    ]
  },
  "_BaseCausalLM": {
    "validate_environment": [
      "cls",
      "values"
    ],
    "_default_params": [
      "self"
    ],
    "_identifying_params": [
      "self"
    ],
    "_llm_type": [
      "self"
    ],
    "_get_parameters": [
      "self",
      "stop"
    ],
    "_call": [
      "self",
      "prompt",
      "stop",
      "run_manager"
    ],
    "stream": [
      "self",
      "prompt",
      "stop",
      "run_manager"
    ],
    "get_num_tokens": [
      "self",
      "text"
    ]
  },
  "LlamaLLM": {
    "ggml_model": [],
    "ggml_module": []
  },
  "BloomLLM": {
    "ggml_model": [],
    "ggml_module": []
  },
  "GptneoxLLM": {
    "ggml_model": [],
    "ggml_module": []
  },
  "StarcoderLLM": {
    "ggml_model": [],
    "ggml_module": []
  },
  "DEFAULT_TASK": [],
  "VALID_TASKS": [],
  "TransformersPipelineLLM": {
    "from_model_id": [
      "cls",
      "model_id",
      "task",
      "model_kwargs",
      "pipeline_kwargs"
    ],
    "_identifying_params": [
      "self"
    ],
    "_llm_type": [
      "self"
    ],
    "_call": [
      "self",
      "prompt",
      "stop",
      "run_manager"
    ]
  },
  "VLLM": {
    "validate_environment": [
      "cls",
      "values"
    ],
    "_default_params": [
      "self"
    ],
    "_generate": [
      "self",
      "prompts",
      "stop",
      "run_manager"
    ],
    "_llm_type": [
      "self"
    ]
  },
  "VLLMOpenAI": {
    "is_lc_serializable": [
      "cls"
    ],
    "_invocation_params": [
      "self"
    ],
    "_llm_type": [
      "self"
    ]
  },
  "GenerateDecoderOnlyOutput": {},
  "GenerateEncoderDecoderOutput": {},
  "GenerateBeamDecoderOnlyOutput": {},
  "GenerateBeamEncoderDecoderOutput": {},
  "GreedySearchDecoderOnlyOutput": [],
  "ContrastiveSearchDecoderOnlyOutput": [],
  "SampleDecoderOnlyOutput": [],
  "ContrastiveSearchEncoderDecoderOutput": [],
  "GreedySearchEncoderDecoderOutput": [],
  "SampleEncoderDecoderOutput": [],
  "BeamSearchDecoderOnlyOutput": [],
  "BeamSampleDecoderOnlyOutput": [],
  "BeamSearchEncoderDecoderOutput": [],
  "BeamSampleEncoderDecoderOutput": [],
  "GreedySearchOutput": [],
  "SampleOutput": [],
  "BeamSearchOutput": [],
  "BeamSampleOutput": [],
  "ContrastiveSearchOutput": [],
  "GenerateNonBeamOutput": [],
  "GenerateBeamOutput": [],
  "GenerateOutput": [],
  "BenchmarkWrapper": {
    "__init__": [
      "self",
      "model",
      "do_print",
      "verbose"
    ],
    "__getattr__": [
      "self",
      "attr"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "_prepare_model_inputs": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "_maybe_initialize_input_ids_for_generation": [
      "self",
      "inputs",
      "bos_token_id",
      "model_kwargs"
    ],
    "_prepare_attention_mask_for_generation": [
      "self",
      "inputs",
      "pad_token_id",
      "eos_token_id"
    ],
    "_prepare_encoder_decoder_kwargs_for_generation": [
      "self",
      "inputs_tensor",
      "model_kwargs",
      "model_input_name",
      "generation_config"
    ],
    "_prepare_decoder_input_ids_for_generation": [
      "self",
      "batch_size",
      "model_input_name",
      "model_kwargs",
      "decoder_start_token_id",
      "device"
    ],
    "_expand_inputs_for_generation": [
      "expand_size",
      "is_encoder_decoder",
      "input_ids"
    ],
    "_extract_past_from_model_output": [
      "self",
      "outputs"
    ],
    "_update_model_kwargs_for_generation": [
      "self",
      "outputs",
      "model_kwargs",
      "is_encoder_decoder",
      "num_new_tokens"
    ],
    "_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ],
    "_get_candidate_generator": [
      "self",
      "generation_config",
      "input_ids",
      "inputs_tensor",
      "assistant_model",
      "logits_processor",
      "model_kwargs"
    ],
    "_get_logits_processor": [
      "self",
      "generation_config",
      "input_ids_seq_length",
      "encoder_input_ids",
      "prefix_allowed_tokens_fn",
      "logits_processor",
      "device",
      "model_kwargs",
      "negative_prompt_ids",
      "negative_prompt_attention_mask"
    ],
    "_get_stopping_criteria": [
      "self",
      "generation_config",
      "stopping_criteria",
      "tokenizer"
    ],
    "_merge_criteria_processor_list": [
      "self",
      "default_list",
      "custom_list"
    ],
    "compute_transition_scores": [
      "self",
      "sequences",
      "scores",
      "beam_indices",
      "normalize_logits"
    ],
    "_validate_model_class": [
      "self"
    ],
    "_validate_assistant": [
      "self",
      "assistant_model"
    ],
    "_validate_model_kwargs": [
      "self",
      "model_kwargs"
    ],
    "_validate_generated_length": [
      "self",
      "generation_config",
      "input_ids_length",
      "has_default_max_length"
    ],
    "_prepare_generated_length": [
      "self",
      "generation_config",
      "has_default_max_length",
      "has_default_min_length",
      "model_input_name",
      "input_ids_length",
      "inputs_tensor"
    ],
    "_prepare_generation_config": [
      "self",
      "generation_config"
    ],
    "_get_initial_cache_position": [
      "self",
      "input_ids",
      "model_kwargs"
    ],
    "_get_cache": [
      "self",
      "cache_implementation",
      "batch_size",
      "max_cache_len",
      "device",
      "model_kwargs"
    ],
    "_supports_default_dynamic_cache": [
      "self"
    ],
    "_prepare_cache_for_generation": [
      "self",
      "generation_config",
      "model_kwargs",
      "assistant_model",
      "batch_size",
      "max_cache_length",
      "device"
    ],
    "_supports_num_logits_to_keep": [
      "self"
    ],
    "_prepare_special_tokens": [
      "self",
      "generation_config",
      "kwargs_has_attention_mask",
      "device"
    ],
    "generate": [
      "self",
      "inputs",
      "generation_config",
      "logits_processor",
      "stopping_criteria",
      "prefix_allowed_tokens_fn",
      "synced_gpus",
      "assistant_model",
      "streamer",
      "negative_prompt_ids",
      "negative_prompt_attention_mask"
    ],
    "_has_unfinished_sequences": [
      "self",
      "this_peer_finished",
      "synced_gpus",
      "device",
      "cur_len",
      "max_length"
    ],
    "heal_tokens": [
      "self",
      "input_ids",
      "tokenizer"
    ],
    "_dola_decoding": [
      "self",
      "input_ids",
      "dola_layers",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus",
      "streamer"
    ],
    "_contrastive_search": [
      "self",
      "input_ids",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus",
      "streamer"
    ],
    "_sample": [
      "self",
      "input_ids",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus",
      "streamer"
    ],
    "_temporary_reorder_cache": [
      "self",
      "past_key_values",
      "beam_idx"
    ],
    "_beam_search": [
      "self",
      "input_ids",
      "beam_scorer",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus"
    ],
    "_group_beam_search": [
      "self",
      "input_ids",
      "beam_scorer",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus"
    ],
    "_constrained_beam_search": [
      "self",
      "input_ids",
      "constrained_beam_scorer",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus"
    ],
    "_assisted_decoding": [
      "self",
      "input_ids",
      "candidate_generator",
      "logits_processor",
      "stopping_criteria",
      "generation_config",
      "synced_gpus",
      "streamer"
    ]
  },
  "_speculative_sampling": [
    "candidate_input_ids",
    "candidate_logits",
    "candidate_length",
    "new_logits",
    "is_done_candidate"
  ],
  "_split_model_outputs": [
    "outputs",
    "new_outputs",
    "cur_len",
    "added_len",
    "is_decoder_attention"
  ],
  "_ranking_fast": [
    "context_hidden",
    "next_hidden",
    "next_top_k_probs",
    "cosine_matrix_mask",
    "alpha",
    "beam_width"
  ],
  "_split": [
    "data",
    "full_batch_size",
    "num_hidden_layers",
    "split_size"
  ],
  "_split_model_inputs": [
    "model_input",
    "split_size",
    "full_batch_size",
    "config"
  ],
  "stack_model_outputs": [
    "model_outputs",
    "config"
  ],
  "_relative_top_filter": [
    "scores",
    "baseline_scores",
    "relative_top",
    "filter_value",
    "base_filter_value",
    "min_tokens_to_keep"
  ],
  "_dola_select_contrast": [
    "candidate_premature_layers",
    "candidate_premature_logits",
    "final_logits"
  ],
  "GGML_QK8_0": [],
  "GGML_QK4_0": [],
  "GGML_QK4_1": [],
  "GGML_QK5_0": [],
  "GGML_QK5_1": [],
  "GGML_MEM_ALIGN": [],
  "GGMLType": {
    "F32": [],
    "F16": [],
    "Q4_0": [],
    "Q4_1": [],
    "Q5_0": [],
    "Q5_1": [],
    "Q8_0": []
  },
  "ModelType": {
    "CHATGLM": [],
    "CHATGLM2": []
  },
  "quantize_q8_0": [
    "tensor"
  ],
  "quantize_q4_0": [
    "tensor"
  ],
  "quantize_q4_1": [
    "tensor"
  ],
  "quantize_q5_0": [
    "tensor"
  ],
  "quantize_q5_1": [
    "tensor"
  ],
  "dump_tensor": [
    "f",
    "name",
    "tensor",
    "ggml_type"
  ],
  "dump_state_dict": [
    "f",
    "weight_names",
    "state_dict",
    "quantization_bit",
    "ggml_type"
  ],
  "BaseConverter": {
    "convert": [
      "cls",
      "model",
      "tokenizer",
      "ggml_type",
      "save_path"
    ]
  },
  "ChatGLMConverter": {
    "MODEL_TYPE": [],
    "dump_config": [
      "f",
      "config",
      "ggml_type"
    ],
    "dump_tokenizer": [
      "f",
      "tokenizer"
    ],
    "dump_model": [
      "f",
      "model",
      "ggml_type"
    ]
  },
  "ChatGLM2Converter": {
    "MODEL_TYPE": [],
    "dump_config": [
      "f",
      "config",
      "ggml_type"
    ],
    "dump_tokenizer": [
      "f",
      "tokenizer"
    ],
    "dump_model": [
      "f",
      "model",
      "ggml_type"
    ]
  },
  "_convert_chatglm_hf_to_ggml_": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "item_size": [],
  "LazyStorage": {},
  "LazyTensor": {
    "load": [
      "self"
    ],
    "to": [
      "self",
      "data_type"
    ]
  },
  "_load": [
    "pickle_fp",
    "map_location",
    "picklemoudle",
    "pickle_file",
    "zip_file"
  ],
  "lazyload": [
    "f"
  ],
  "LazyLoadTensors": {
    "__init__": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "top_k_top_p_filtering": [
    "logits",
    "top_k",
    "top_p",
    "filter_value",
    "min_tokens_to_keep"
  ],
  "ISAChecker": {
    "__init__": [
      "self"
    ],
    "_avx_vnni": [
      "self",
      "cpuid"
    ],
    "check_avx": [
      "self"
    ],
    "check_avx2": [
      "self"
    ],
    "check_avx_vnni": [
      "self"
    ],
    "check_avx512": [
      "self"
    ],
    "check_avx512_vnni": [
      "self"
    ]
  },
  "isa_checker": [],
  "check_avx": [],
  "check_avx2": [],
  "check_avx_vnni": [],
  "check_avx512": [],
  "check_avx512_vnni": [],
  "is_server": [],
  "is_spr": [],
  "NDArray": [],
  "UnquantizedDataType": {},
  "DT_F16": [],
  "DT_F32": [],
  "DT_I32": [],
  "DT_BF16": [],
  "QuantizedDataType": {},
  "DT_Q4_0": [],
  "DT_Q4_1": [],
  "DataType": [],
  "DATA_TYPE_TO_FTYPE": [],
  "FTYPE_TO_DATA_TYPE": [],
  "DATA_TYPE_TO_NUMPY": [],
  "NUMPY_TYPE_TO_DATA_TYPE": [],
  "GGMLFileType": {
    "AllF32": [],
    "MostlyF16": [],
    "MostlyQ4_0": [],
    "MostlyQ4_1": [],
    "PerLayerIsQ4_1": [],
    "type_for_tensor": [
      "self",
      "name",
      "tensor"
    ]
  },
  "make_tensors_list": [],
  "TENSORS_LIST": [],
  "TENSORS_SET": [],
  "find_n_mult": [
    "n_ff",
    "n_embd"
  ],
  "Params": {
    "guessed": [
      "model"
    ],
    "loadHFTransformerJson": [
      "model",
      "config_path"
    ],
    "loadOriginalParamsJson": [
      "model",
      "config_path"
    ],
    "load": [
      "model_plus"
    ]
  },
  "SentencePieceVocab": {
    "__init__": [
      "self",
      "fname_tokenizer",
      "fname_added_tokens",
      "vocabtype"
    ],
    "sentencepiece_tokens": [
      "self"
    ],
    "added_tokens": [
      "self"
    ],
    "all_tokens": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "GGMLVocab": {
    "__init__": [
      "self",
      "tokens"
    ],
    "all_tokens": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Vocab": [],
  "permute": [
    "weights",
    "n_head",
    "n_kv_head"
  ],
  "dequantize_q4": [
    "qvalues_pack32",
    "scales",
    "addends",
    "g_idx"
  ],
  "Tensor": {
    "astype": [
      "self",
      "data_type"
    ],
    "permute": [
      "self",
      "n_head",
      "n_kv_head"
    ],
    "permute_part": [
      "self",
      "n_part",
      "n_head"
    ],
    "part": [
      "self",
      "n_part"
    ],
    "to_ggml": [
      "self"
    ]
  },
  "bf16_to_fp32": [
    "bf16_arr"
  ],
  "UnquantizedTensor": {
    "__init__": [
      "self",
      "ndarray"
    ],
    "astype": [
      "self",
      "data_type"
    ],
    "to_ggml": [
      "self"
    ],
    "permute_part": [
      "self",
      "n_part",
      "n_head"
    ],
    "part": [
      "self",
      "n_part"
    ],
    "permute": [
      "self",
      "n_head",
      "n_kv_head"
    ]
  },
  "load_unquantized": [
    "lazy_tensor",
    "expected_dtype",
    "convert"
  ],
  "GGMLQuantizedTensor": {
    "__init__": [
      "self",
      "ndarray",
      "shape",
      "data_type"
    ],
    "astype": [
      "self",
      "data_type"
    ],
    "to_ggml": [
      "self"
    ],
    "permute": [
      "self",
      "n_head",
      "n_kv_head"
    ],
    "permute_part": [
      "self",
      "n_part",
      "n_head"
    ],
    "part": [
      "self",
      "n_part"
    ]
  },
  "GGMLCompatibleTensor": [],
  "DeferredPermutedTensor": {
    "__init__": [
      "self",
      "base",
      "n_head",
      "n_kv_head"
    ],
    "astype": [
      "self",
      "data_type"
    ],
    "to_ggml": [
      "self"
    ],
    "permute": [
      "self",
      "n_head",
      "n_kv_head"
    ]
  },
  "GPTQForLLaMaQuantizedTensor": {
    "__init__": [
      "self",
      "model",
      "namebase"
    ],
    "inspect": [
      "self",
      "row",
      "col"
    ],
    "astype": [
      "self",
      "data_type"
    ],
    "groupsize": [
      "self"
    ],
    "regroup": [
      "self",
      "new_groupsize"
    ],
    "permute": [
      "self",
      "n_head",
      "n_kv_head"
    ],
    "to_ggml": [
      "self"
    ]
  },
  "LazyModel": [],
  "ModelPlus": {},
  "merge_sharded": [
    "models"
  ],
  "merge_multifile_models": [
    "models_plus"
  ],
  "permute_lazy": [
    "lazy_tensor",
    "n_head",
    "n_kv_head"
  ],
  "permute_part_lazy": [
    "lazy_tensor",
    "n_part",
    "n_head"
  ],
  "part_lazy": [
    "lazy_tensor",
    "n_part"
  ],
  "convert_transformers_to_orig": [
    "model",
    "params"
  ],
  "handle_quantization": [
    "model"
  ],
  "LazyStorageKind": {},
  "LazyUnpickler": {
    "__init__": [
      "self",
      "fp",
      "data_base_path",
      "zip_file"
    ],
    "persistent_load": [
      "self",
      "pid"
    ],
    "lazy_rebuild_tensor_v2": [
      "storage",
      "storage_offset",
      "size",
      "stride",
      "requires_grad",
      "backward_hooks",
      "metadata"
    ],
    "rebuild_from_type_v2": [
      "func",
      "new_type",
      "args",
      "state"
    ],
    "CLASSES": [],
    "find_class": [
      "self",
      "module",
      "name"
    ]
  },
  "lazy_load_torch_file": [
    "outer_fp",
    "path"
  ],
  "SAFETENSORS_DATA_TYPES": [],
  "lazy_load_safetensors_file": [
    "fp",
    "path"
  ],
  "must_read": [
    "fp",
    "length"
  ],
  "lazy_load_ggml_file": [
    "fp",
    "path"
  ],
  "lazy_load_file": [
    "path"
  ],
  "In": [],
  "Out": [],
  "bounded_parallel_map": [
    "func",
    "iterable",
    "concurrency"
  ],
  "check_vocab_size": [
    "params",
    "vocab"
  ],
  "OutputFile": {
    "__init__": [
      "self",
      "fname_out"
    ],
    "write_file_header": [
      "self",
      "params",
      "file_type"
    ],
    "write_tensor_header": [
      "self",
      "name",
      "shape",
      "data_type"
    ],
    "write_vocab": [
      "self",
      "vocab"
    ],
    "write_vocab_only": [
      "fname_out",
      "vocab"
    ],
    "write_all": [
      "fname_out",
      "params",
      "file_type",
      "model",
      "vocab"
    ]
  },
  "pick_output_type": [
    "model",
    "output_type_str"
  ],
  "do_necessary_conversions": [
    "model",
    "params"
  ],
  "convert_to_output_type": [
    "model",
    "output_type"
  ],
  "nth_multifile_path": [
    "path",
    "n"
  ],
  "find_multifile_paths": [
    "path"
  ],
  "load_some_model": [
    "path"
  ],
  "filter_and_sort_tensors": [
    "model"
  ],
  "load_vocab": [
    "path",
    "vocabtype"
  ],
  "default_outfile": [
    "model_paths",
    "file_type"
  ],
  "bytes_to_unicode": [],
  "_convert_gptneox_hf_to_ggml": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "_convert_bloom_hf_to_ggml": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "_convert_starcoder_hf_to_ggml": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "_convert_chatglm_hf_to_ggml": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "NEED_SETUP_CACHE_CLASSES_MAPPING": [],
  "QUANT_BACKEND_CLASSES_MAPPING": [],
  "GlibcChecker": {
    "__init__": [
      "self",
      "min_glibc_version"
    ],
    "is_linux": [],
    "is_bigdl_core_xe_installed": [],
    "get_glibc_version": [],
    "check_requirements": [
      "self"
    ]
  },
  "glibc_checker": [],
  "check_glibc_version": [],
  "BIGDL_CHECK_DUPLICATE_IMPORT": [],
  "RAW_IMPORT": [],
  "IS_IMPORT_REPLACED": [],
  "ipex_duplicate_import_error": [],
  "replace_import": [],
  "revert_import": [],
  "get_calling_package": [],
  "custom_ipex_import": [
    "name",
    "globals",
    "locals",
    "fromlist",
    "level"
  ],
  "IPEXImporter": {
    "__init__": [
      "self"
    ],
    "is_xpu_version_installed": [],
    "import_ipex": [
      "self"
    ],
    "directly_import_ipex": [
      "self"
    ],
    "get_ipex_version": [
      "self"
    ]
  },
  "ipex_importer": [],
  "insert_fake_module": [
    "name",
    "doc"
  ],
  "LazyImport": {
    "__init__": [
      "self",
      "module_name"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "__call__": [
      "self"
    ]
  },
  "outputUserMessage": [
    "errMsg",
    "fixMsg"
  ],
  "invalidInputError": [
    "condition",
    "errMsg",
    "fixMsg"
  ],
  "invalidOperationError": [
    "condition",
    "errMsg",
    "fixMsg",
    "cause"
  ],
  "MuteHFLogger": {
    "__init__": [
      "self",
      "logger",
      "speak_level"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "convert_model": [],
  "BIGDL_GLIBC_CHECK": [],
  "libs_dirname": [],
  "ggml_tensor_qtype": [],
  "gguf_mixed_qtype": [],
  "_llama_quantize_type": [],
  "_bloom_quantize_type": [],
  "_gptneox_quantize_type": [],
  "_starcoder_quantize_type": [],
  "_quantize_type": [],
  "quantize": [
    "input_path",
    "output_path",
    "model_family",
    "dtype"
  ],
  "_convert_llama": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "_convert_gptneox": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "_convert_bloom": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "_convert_starcoder": [
    "model_path",
    "outfile_dir",
    "outtype"
  ],
  "_convert_to_ggml": [
    "model_path",
    "outfile_dir",
    "model_family",
    "outtype"
  ],
  "GenerationMixin": {
    "tokenize": [
      "self",
      "text",
      "add_bos"
    ],
    "decode": [
      "self",
      "tokens"
    ],
    "batch_decode": [
      "self",
      "tokens"
    ],
    "generate": [
      "self",
      "inputs",
      "max_new_tokens",
      "top_k",
      "top_p",
      "temperature",
      "repetition_penalty",
      "reset",
      "frequency_penalty",
      "presence_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "stop"
    ]
  },
  "_load_shared_library": [
    "lib_base_name"
  ],
  "_lib_base_name": [],
  "c_free": [
    "p"
  ],
  "bloom_load": [
    "fname",
    "n_ctx",
    "n_threads"
  ],
  "bloom_free": [
    "ctx"
  ],
  "bloom_run": [
    "ctx",
    "seed",
    "n_threads",
    "n_batch",
    "n_predict",
    "match_str",
    "prompt",
    "buf"
  ],
  "bloom_tokenize": [
    "ctx",
    "prompt",
    "bos"
  ],
  "bloom_detokenize": [
    "ctx",
    "token_id"
  ],
  "bloom_eval": [
    "ctx",
    "input_ids",
    "seed",
    "n_threads",
    "n_batch"
  ],
  "bloom_embed": [
    "ctx",
    "input_ids",
    "seed",
    "n_threads",
    "n_batch"
  ],
  "bloom_forward": [
    "ctx",
    "input_ids",
    "seed",
    "n_threads",
    "n_batch"
  ],
  "Bloom": {
    "__init__": [
      "self",
      "model_path",
      "n_ctx",
      "n_parts",
      "n_gpu_layers",
      "seed",
      "f16_kv",
      "logits_all",
      "vocab_only",
      "use_mmap",
      "use_mlock",
      "embedding",
      "n_threads",
      "n_batch",
      "last_n_tokens_size",
      "lora_base",
      "lora_path",
      "verbose"
    ],
    "__call__": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "model"
    ],
    "_supported_call": [
      "self",
      "prompt",
      "max_tokens",
      "stream",
      "stop",
      "echo",
      "model"
    ],
    "_eval": [
      "self",
      "prompt",
      "max_tokens",
      "match_str",
      "stop",
      "echo",
      "model"
    ],
    "stream": [
      "self",
      "prompt",
      "max_tokens",
      "stop",
      "echo",
      "model"
    ],
    "_tokenize": [
      "self",
      "text",
      "add_bos"
    ],
    "detokenize": [
      "self",
      "tokens"
    ],
    "forward": [
      "self",
      "input_ids"
    ],
    "eval": [
      "self",
      "input_ids"
    ],
    "_generate": [
      "self",
      "tokens",
      "top_k",
      "top_p",
      "temp",
      "repeat_penalty",
      "reset",
      "frequency_penalty",
      "presence_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "_supported_generate": [
      "self",
      "tokens"
    ],
    "embed": [
      "self",
      "input"
    ],
    "__del__": [
      "self"
    ]
  },
  "c_float_p": [],
  "c_uint8_p": [],
  "c_size_t_p": [],
  "LLAMA_FILE_MAGIC_GGJT": [],
  "LLAMA_FILE_MAGIC_GGLA": [],
  "LLAMA_FILE_MAGIC_GGMF": [],
  "LLAMA_FILE_MAGIC_GGML": [],
  "LLAMA_FILE_MAGIC_GGSN": [],
  "LLAMA_FILE_VERSION": [],
  "LLAMA_FILE_MAGIC": [],
  "LLAMA_FILE_MAGIC_UNVERSIONED": [],
  "LLAMA_SESSION_MAGIC": [],
  "LLAMA_SESSION_VERSION": [],
  "llama_context_p": [],
  "llama_token": [],
  "llama_token_p": [],
  "llama_token_data": {
    "_fields_": []
  },
  "llama_token_data_p": [],
  "llama_token_data_array": {
    "_fields_": []
  },
  "llama_token_data_array_p": [],
  "llama_progress_callback": [],
  "llama_context_params": {
    "_fields_": []
  },
  "llama_context_params_p": [],
  "LLAMA_FTYPE_ALL_F32": [],
  "LLAMA_FTYPE_MOSTLY_F16": [],
  "LLAMA_FTYPE_MOSTLY_Q4_0": [],
  "LLAMA_FTYPE_MOSTLY_Q4_1": [],
  "LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16": [],
  "LLAMA_FTYPE_MOSTLY_Q8_0": [],
  "LLAMA_FTYPE_MOSTLY_Q5_0": [],
  "LLAMA_FTYPE_MOSTLY_Q5_1": [],
  "llama_context_default_params": [],
  "llama_mmap_supported": [],
  "llama_mlock_supported": [],
  "llama_init_backend": [],
  "llama_time_us": [],
  "llama_init_from_file": [
    "path_model",
    "params"
  ],
  "llama_free": [
    "ctx"
  ],
  "llama_model_quantize": [
    "fname_inp",
    "fname_out",
    "ftype",
    "nthread"
  ],
  "llama_apply_lora_from_file": [
    "ctx",
    "path_lora",
    "path_base_model",
    "n_threads"
  ],
  "llama_get_kv_cache_token_count": [
    "ctx"
  ],
  "llama_set_rng_seed": [
    "ctx",
    "seed"
  ],
  "llama_get_state_size": [
    "ctx"
  ],
  "llama_copy_state_data": [
    "ctx",
    "dst"
  ],
  "llama_set_state_data": [
    "ctx",
    "src"
  ],
  "llama_load_session_file": [
    "ctx",
    "path_session",
    "tokens_out",
    "n_token_capacity",
    "n_token_count_out"
  ],
  "llama_save_session_file": [
    "ctx",
    "path_session",
    "tokens",
    "n_token_count"
  ],
  "llama_eval": [
    "ctx",
    "tokens",
    "n_tokens",
    "n_past",
    "n_threads"
  ],
  "llama_tokenize": [
    "ctx",
    "text",
    "tokens",
    "n_max_tokens",
    "add_bos"
  ],
  "llama_n_vocab": [
    "ctx"
  ],
  "llama_n_ctx": [
    "ctx"
  ],
  "llama_n_embd": [
    "ctx"
  ],
  "llama_get_logits": [
    "ctx"
  ],
  "llama_get_embeddings": [
    "ctx"
  ],
  "llama_token_to_str": [
    "ctx",
    "token"
  ],
  "llama_token_bos": [],
  "llama_token_eos": [],
  "llama_token_nl": [],
  "llama_init_candidates": [
    "ctx",
    "candidates"
  ],
  "llama_sample_repetition_penalty": [
    "ctx",
    "candidates",
    "last_tokens_data",
    "last_tokens_size",
    "penalty"
  ],
  "llama_sample_frequency_and_presence_penalties": [
    "ctx",
    "candidates",
    "last_tokens_data",
    "last_tokens_size",
    "alpha_frequency",
    "alpha_presence"
  ],
  "llama_sample_softmax": [
    "ctx",
    "candidates"
  ],
  "llama_sample_top_k": [
    "ctx",
    "candidates",
    "k",
    "min_keep"
  ],
  "llama_sample_top_p": [
    "ctx",
    "candidates",
    "p",
    "min_keep"
  ],
  "llama_sample_tail_free": [
    "ctx",
    "candidates",
    "z",
    "min_keep"
  ],
  "llama_sample_typical": [
    "ctx",
    "candidates",
    "p",
    "min_keep"
  ],
  "llama_sample_temperature": [
    "ctx",
    "candidates",
    "temp"
  ],
  "llama_sample_token_mirostat": [
    "ctx",
    "candidates",
    "tau",
    "eta",
    "m",
    "mu"
  ],
  "llama_sample_token_mirostat_v2": [
    "ctx",
    "candidates",
    "tau",
    "eta",
    "mu"
  ],
  "llama_sample_token_greedy": [
    "ctx",
    "candidates"
  ],
  "llama_sample_token": [
    "ctx",
    "candidates"
  ],
  "llama_print_timings": [
    "ctx"
  ],
  "llama_reset_timings": [
    "ctx"
  ],
  "llama_print_system_info": [],
  "ggml_quantize_tensor": [
    "src",
    "dst",
    "qtype",
    "n",
    "k",
    "hist",
    "scale_search"
  ],
  "ggml_quantize_tensor_with_weights": [
    "src",
    "dst",
    "qtype",
    "nrow",
    "n_per_row",
    "hist",
    "weights"
  ],
  "ggml_quantize_tensor_rtn": [
    "src",
    "dst",
    "scale_ptr",
    "qtype",
    "n",
    "k",
    "hist",
    "scale_search"
  ],
  "ggml_quantize_tensor_rtn_with_weights": [
    "src",
    "dst",
    "scale_ptr",
    "qtype",
    "n",
    "k",
    "hist",
    "scale_search",
    "weights"
  ],
  "ggml_type_size": [
    "qtype"
  ],
  "ggml_qk_size": [
    "qtype"
  ],
  "ggml_dequantize_q4_0": [
    "src",
    "dst",
    "k"
  ],
  "ggml_dequantize": [
    "src",
    "dst",
    "k",
    "qtype"
  ],
  "ggml_compute_forward_mul_mat_q_fp32": [
    "src_0_ne",
    "src_0_data",
    "src_0_qtype",
    "src_1_ne",
    "src_1_data",
    "result"
  ],
  "_llama_initialized": [],
  "EmbeddingUsage": {},
  "EmbeddingData": {},
  "Embedding": {},
  "CompletionLogprobs": {},
  "CompletionChoice": {},
  "CompletionUsage": {},
  "CompletionChunk": {},
  "Completion": {},
  "ChatCompletionMessage": {},
  "ChatCompletionChoice": {},
  "ChatCompletion": {},
  "ChatCompletionChunkDelta": {},
  "ChatCompletionChunkChoice": {},
  "ChatCompletionChunk": {},
  "LlamaCache": {
    "__init__": [
      "self",
      "capacity_bytes"
    ],
    "cache_size": [
      "self"
    ],
    "_find_longest_prefix_key": [
      "self",
      "key"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__contains__": [
      "self",
      "key"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ]
  },
  "LlamaState": {
    "__init__": [
      "self",
      "eval_tokens",
      "eval_logits",
      "llama_state",
      "llama_state_size"
    ]
  },
  "Llama": {
    "__init__": [
      "self",
      "model_path",
      "n_ctx",
      "n_parts",
      "n_gpu_layers",
      "seed",
      "f16_kv",
      "logits_all",
      "vocab_only",
      "use_mmap",
      "use_mlock",
      "embedding",
      "n_threads",
      "n_batch",
      "last_n_tokens_size",
      "lora_base",
      "lora_path",
      "verbose"
    ],
    "_tokenize": [
      "self",
      "text",
      "add_bos"
    ],
    "detokenize": [
      "self",
      "tokens"
    ],
    "set_cache": [
      "self",
      "cache"
    ],
    "reset": [
      "self"
    ],
    "eval": [
      "self",
      "tokens"
    ],
    "_sample": [
      "self",
      "last_n_tokens_data",
      "last_n_tokens_size",
      "top_k",
      "top_p",
      "temp",
      "tfs_z",
      "repeat_penalty",
      "frequency_penalty",
      "presence_penalty",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "penalize_nl"
    ],
    "sample": [
      "self",
      "top_k",
      "top_p",
      "temp",
      "repeat_penalty",
      "frequency_penalty",
      "presence_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_eta",
      "mirostat_tau",
      "penalize_nl"
    ],
    "_generate": [
      "self",
      "tokens",
      "top_k",
      "top_p",
      "temp",
      "repeat_penalty",
      "reset",
      "frequency_penalty",
      "presence_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "create_embedding": [
      "self",
      "input",
      "model"
    ],
    "embed": [
      "self",
      "input"
    ],
    "_create_completion": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "model"
    ],
    "create_completion": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "model"
    ],
    "__call__": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "model"
    ],
    "_convert_text_completion_to_chat": [
      "self",
      "completion"
    ],
    "_convert_text_completion_chunks_to_chat": [
      "self",
      "chunks"
    ],
    "create_chat_completion": [
      "self",
      "messages",
      "temperature",
      "top_p",
      "top_k",
      "stream",
      "stop",
      "max_tokens",
      "presence_penalty",
      "frequency_penalty",
      "repeat_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "model"
    ],
    "__del__": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "save_state": [
      "self"
    ],
    "load_state": [
      "self",
      "state"
    ],
    "n_ctx": [
      "self"
    ],
    "n_embd": [
      "self"
    ],
    "n_vocab": [
      "self"
    ],
    "token_eos": [],
    "token_bos": [],
    "token_nl": [],
    "logits_to_logprobs": [
      "logits"
    ],
    "longest_token_prefix": [
      "a",
      "b"
    ]
  },
  "GPTNEOX_FILE_VERSION": [],
  "GPTNEOX_FILE_MAGIC": [],
  "GPTNEOX_FILE_MAGIC_UNVERSIONED": [],
  "gptneox_context_p": [],
  "gptneox_token": [],
  "gptneox_token_p": [],
  "gptneox_token_data": {
    "_fields_": []
  },
  "gptneox_token_data_p": [],
  "gptneox_token_data_array": {
    "_fields_": []
  },
  "gptneox_token_data_array_p": [],
  "gptneox_progress_callback": [],
  "gptneox_context_params": {
    "_fields_": []
  },
  "gptneox_context_params_p": [],
  "GPTNEOX_FTYPE_ALL_F32": [],
  "GPTNEOX_FTYPE_MOSTLY_F16": [],
  "GPTNEOX_FTYPE_MOSTLY_Q4_0": [],
  "GPTNEOX_FTYPE_MOSTLY_Q4_1": [],
  "GPTNEOX_FTYPE_MOSTLY_Q4_1_SOME_F16": [],
  "GPTNEOX_FTYPE_MOSTLY_Q4_2": [],
  "GPTNEOX_FTYPE_MOSTLY_Q8_0": [],
  "GPTNEOX_FTYPE_MOSTLY_Q5_0": [],
  "GPTNEOX_FTYPE_MOSTLY_Q5_1": [],
  "gptneox_context_default_params": [],
  "gptneox_mmap_supported": [],
  "gptneox_mlock_supported": [],
  "gptneox_init_from_file": [
    "path_model",
    "params"
  ],
  "gptneox_free": [
    "ctx"
  ],
  "gptneox_model_quantize": [
    "fname_inp",
    "fname_out",
    "ftype",
    "nthread"
  ],
  "gptneox_model_copy": [
    "fname_inp",
    "fname_out",
    "ftype"
  ],
  "gptneox_apply_lora_from_file": [
    "ctx",
    "path_lora",
    "path_base_model",
    "n_threads"
  ],
  "gptneox_get_kv_cache_token_count": [
    "ctx"
  ],
  "gptneox_set_rng_seed": [
    "ctx",
    "seed"
  ],
  "gptneox_get_state_size": [
    "ctx"
  ],
  "gptneox_copy_state_data": [
    "ctx",
    "dst"
  ],
  "gptneox_set_state_data": [
    "ctx",
    "src"
  ],
  "gptneox_load_session_file": [
    "ctx",
    "path_session",
    "tokens_out",
    "n_token_capacity",
    "n_token_count_out"
  ],
  "gptneox_save_session_file": [
    "ctx",
    "path_session",
    "tokens",
    "n_token_count"
  ],
  "gptneox_eval": [
    "ctx",
    "tokens",
    "n_tokens",
    "n_past",
    "n_threads"
  ],
  "gptneox_tokenize": [
    "ctx",
    "text",
    "tokens",
    "n_max_tokens",
    "add_bos"
  ],
  "gptneox_n_vocab": [
    "ctx"
  ],
  "gptneox_n_ctx": [
    "ctx"
  ],
  "gptneox_n_embd": [
    "ctx"
  ],
  "gptneox_get_logits": [
    "ctx"
  ],
  "gptneox_get_embeddings": [
    "ctx"
  ],
  "gptneox_token_to_str": [
    "ctx",
    "token"
  ],
  "gptneox_str_to_token": [
    "ctx",
    "input_str"
  ],
  "gptneox_token_bos": [],
  "gptneox_token_eos": [],
  "gptneox_get_candidates": [
    "ctx",
    "n_vocab",
    "logits"
  ],
  "gptneox_sample_repetition_penalty": [
    "ctx",
    "candidates",
    "last_tokens_data",
    "last_tokens_size",
    "penalty"
  ],
  "gptneox_sample_frequency_and_presence_penalties": [
    "ctx",
    "candidates",
    "last_tokens_data",
    "last_tokens_size",
    "alpha_frequency",
    "alpha_presence"
  ],
  "gptneox_sample_softmax": [
    "ctx",
    "candidates"
  ],
  "gptneox_sample_top_k": [
    "ctx",
    "candidates",
    "k",
    "min_keep"
  ],
  "gptneox_sample_top_p": [
    "ctx",
    "candidates",
    "p",
    "min_keep"
  ],
  "gptneox_sample_tail_free": [
    "ctx",
    "candidates",
    "z",
    "min_keep"
  ],
  "gptneox_sample_typical": [
    "ctx",
    "candidates",
    "p",
    "min_keep"
  ],
  "gptneox_sample_temperature": [
    "ctx",
    "candidates",
    "temp"
  ],
  "gptneox_sample_token_mirostat": [
    "ctx",
    "candidates",
    "tau",
    "eta",
    "m",
    "mu"
  ],
  "gptneox_sample_token_mirostat_v2": [
    "ctx",
    "candidates",
    "tau",
    "eta",
    "mu"
  ],
  "gptneox_sample_token_greedy": [
    "ctx",
    "candidates"
  ],
  "gptneox_sample_token": [
    "ctx",
    "candidates"
  ],
  "gptneox_print_timings": [
    "ctx"
  ],
  "gptneox_reset_timings": [
    "ctx"
  ],
  "gptneox_print_system_info": [],
  "GptneoxCache": {
    "__init__": [
      "self",
      "capacity_bytes"
    ],
    "cache_size": [
      "self"
    ],
    "_find_longest_prefix_key": [
      "self",
      "key"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__contains__": [
      "self",
      "key"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ]
  },
  "GptneoxState": {
    "__init__": [
      "self",
      "eval_tokens",
      "eval_logits",
      "gptneox_state",
      "gptneox_state_size"
    ]
  },
  "Gptneox": {
    "__init__": [
      "self",
      "model_path",
      "n_ctx",
      "n_parts",
      "n_gpu_layers",
      "seed",
      "f16_kv",
      "logits_all",
      "vocab_only",
      "use_mmap",
      "use_mlock",
      "embedding",
      "n_threads",
      "n_batch",
      "last_n_tokens_size",
      "lora_base",
      "lora_path",
      "verbose"
    ],
    "_tokenize": [
      "self",
      "text",
      "add_bos"
    ],
    "detokenize": [
      "self",
      "tokens"
    ],
    "set_cache": [
      "self",
      "cache"
    ],
    "reset": [
      "self"
    ],
    "eval": [
      "self",
      "tokens"
    ],
    "_sample": [
      "self",
      "last_n_tokens_data",
      "last_n_tokens_size",
      "top_k",
      "top_p",
      "temp",
      "tfs_z",
      "repeat_penalty",
      "frequency_penalty",
      "presence_penalty",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "sample": [
      "self",
      "top_k",
      "top_p",
      "temp",
      "repeat_penalty",
      "frequency_penalty",
      "presence_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_eta",
      "mirostat_tau"
    ],
    "_generate": [
      "self",
      "tokens",
      "top_k",
      "top_p",
      "temp",
      "repeat_penalty",
      "reset",
      "frequency_penalty",
      "presence_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "create_embedding": [
      "self",
      "input"
    ],
    "embed": [
      "self",
      "input"
    ],
    "_create_completion": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "create_completion": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "__call__": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "_convert_text_completion_to_chat": [
      "self",
      "completion"
    ],
    "_convert_text_completion_chunks_to_chat": [
      "self",
      "chunks"
    ],
    "create_chat_completion": [
      "self",
      "messages",
      "temperature",
      "top_p",
      "top_k",
      "stream",
      "stop",
      "max_tokens",
      "presence_penalty",
      "frequency_penalty",
      "repeat_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "__del__": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "save_state": [
      "self"
    ],
    "load_state": [
      "self",
      "state"
    ],
    "token_eos": [],
    "token_bos": [],
    "logits_to_logprobs": [
      "logits"
    ],
    "longest_token_prefix": [
      "a",
      "b"
    ]
  },
  "starcoder_load": [
    "fname",
    "n_ctx",
    "n_threads"
  ],
  "starcoder_free": [
    "ctx"
  ],
  "starcoder_run": [
    "ctx",
    "seed",
    "n_threads",
    "n_batch",
    "n_predict",
    "match_str",
    "prompt",
    "buf"
  ],
  "starcoder_tokenize": [
    "ctx",
    "prompt",
    "bos"
  ],
  "starcoder_detokenize": [
    "ctx",
    "token_id"
  ],
  "starcoder_eval": [
    "ctx",
    "input_ids",
    "seed",
    "n_threads",
    "n_batch"
  ],
  "starcoder_embed": [
    "ctx",
    "input_ids",
    "seed",
    "n_threads",
    "n_batch"
  ],
  "starcoder_forward": [
    "ctx",
    "input_ids",
    "seed",
    "n_threads",
    "n_batch"
  ],
  "Starcoder": {
    "__init__": [
      "self",
      "model_path",
      "n_ctx",
      "n_parts",
      "n_gpu_layers",
      "seed",
      "f16_kv",
      "logits_all",
      "vocab_only",
      "use_mmap",
      "use_mlock",
      "embedding",
      "n_threads",
      "n_batch",
      "last_n_tokens_size",
      "lora_base",
      "lora_path",
      "verbose"
    ],
    "__call__": [
      "self",
      "prompt",
      "suffix",
      "max_tokens",
      "temperature",
      "top_p",
      "logprobs",
      "echo",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "repeat_penalty",
      "top_k",
      "stream",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta",
      "model"
    ],
    "_supported_call": [
      "self",
      "prompt",
      "max_tokens",
      "stream",
      "stop",
      "echo",
      "model"
    ],
    "_eval": [
      "self",
      "prompt",
      "max_tokens",
      "match_str",
      "stop",
      "echo",
      "model"
    ],
    "stream": [
      "self",
      "prompt",
      "max_tokens",
      "stop",
      "echo",
      "model"
    ],
    "_tokenize": [
      "self",
      "text",
      "add_bos"
    ],
    "detokenize": [
      "self",
      "tokens"
    ],
    "forward": [
      "self",
      "input_ids"
    ],
    "eval": [
      "self",
      "input_ids"
    ],
    "_generate": [
      "self",
      "tokens",
      "top_k",
      "top_p",
      "temp",
      "repeat_penalty",
      "reset",
      "frequency_penalty",
      "presence_penalty",
      "tfs_z",
      "mirostat_mode",
      "mirostat_tau",
      "mirostat_eta"
    ],
    "_supported_generate": [
      "self",
      "tokens"
    ],
    "embed": [
      "self",
      "input"
    ],
    "__del__": [
      "self"
    ]
  },
  "_sample_get_logits": [
    "self",
    "hidden_states",
    "lm_head",
    "embedding_bias"
  ],
  "_model_sample_convert": [],
  "_ipex_llm_convert": [
    "load_in_low_bit"
  ],
  "get_load_function": [
    "low_bit"
  ],
  "IPEXLLMV1Wrapper": {
    "__init__": [
      "self",
      "load_in_low_bit"
    ]
  },
  "get_ipex_llm_v1_wrapper": [
    "load_in_low_bit"
  ],
  "IPEXLLMWrapper": {
    "__init__": [
      "self",
      "load_in_low_bit"
    ]
  },
  "get_ipex_llm_wrapper": [
    "load_in_low_bit"
  ],
  "LoRAParserAction": {
    "__call__": [
      "self",
      "parser",
      "namespace",
      "values",
      "option_string"
    ]
  },
  "PromptAdapterParserAction": {
    "__call__": [
      "self",
      "parser",
      "namespace",
      "values",
      "option_string"
    ]
  },
  "make_arg_parser": [
    "parser"
  ],
  "validate_parsed_serve_args": [
    "args"
  ],
  "create_parser_for_docs": [],
  "TIMEOUT_KEEP_ALIVE": [],
  "lifespan": [
    "app"
  ],
  "build_async_engine_client": [
    "args"
  ],
  "build_async_engine_client_from_engine_args": [
    "engine_args",
    "disable_frontend_multiprocessing",
    "load_in_low_bit"
  ],
  "router": [],
  "mount_metrics": [
    "app"
  ],
  "base": [
    "request"
  ],
  "models": [
    "request"
  ],
  "chat": [
    "request"
  ],
  "completion": [
    "request"
  ],
  "pooling": [
    "request"
  ],
  "embedding": [
    "request"
  ],
  "score": [
    "request"
  ],
  "tokenization": [
    "request"
  ],
  "engine_client": [
    "request"
  ],
  "health": [
    "raw_request"
  ],
  "tokenize": [
    "request",
    "raw_request"
  ],
  "detokenize": [
    "request",
    "raw_request"
  ],
  "show_available_models": [
    "raw_request"
  ],
  "show_version": [],
  "save_dict": [],
  "flag": [],
  "stream_generator": [
    "generator",
    "request",
    "request_id"
  ],
  "create_chat_completion": [
    "request",
    "raw_request"
  ],
  "create_completion": [
    "request",
    "raw_request"
  ],
  "create_embedding": [
    "request",
    "raw_request"
  ],
  "create_pooling": [
    "request",
    "raw_request"
  ],
  "create_score": [
    "request",
    "raw_request"
  ],
  "create_score_v1": [
    "request",
    "raw_request"
  ],
  "build_app": [
    "args"
  ],
  "init_app_state": [
    "engine_client",
    "model_config",
    "state",
    "args"
  ],
  "create_server_socket": [
    "addr"
  ],
  "run_server": [
    "args"
  ],
  "IPEXLLMAsyncLLMEngine": {
    "__init__": [
      "self"
    ],
    "from_engine_args": [
      "cls",
      "engine_args",
      "engine_config",
      "start_engine_loop",
      "usage_context",
      "load_in_low_bit",
      "stat_loggers"
    ]
  },
  "IPEXLLMAsyncV1Engine": {
    "__init__": [
      "self"
    ],
    "from_engine_args": [
      "cls",
      "engine_args",
      "engine_config",
      "start_engine_loop",
      "usage_context",
      "load_in_low_bit",
      "stat_loggers"
    ]
  },
  "IPEXLLMClass": {
    "__init__": [
      "self",
      "model",
      "tokenizer",
      "tokenizer_mode",
      "skip_tokenizer_init",
      "trust_remote_code",
      "allowed_local_media_path",
      "tensor_parallel_size",
      "dtype",
      "quantization",
      "revision",
      "tokenizer_revision",
      "seed",
      "gpu_memory_utilization",
      "swap_space",
      "cpu_offload_gb",
      "enforce_eager",
      "max_seq_len_to_capture",
      "disable_custom_all_reduce",
      "disable_async_output_proc",
      "hf_overrides",
      "mm_processor_kwargs",
      "task",
      "override_pooler_config",
      "compilation_config",
      "load_in_low_bit"
    ],
    "get_engine_class": []
  },
  "IPEXLLMLLMV1Engine": {
    "__init__": [
      "self"
    ],
    "from_engine_args": [
      "cls",
      "engine_args",
      "usage_context",
      "stat_loggers",
      "enable_multiprocessing",
      "load_in_low_bit"
    ]
  },
  "IPEXLLMLLMEngine": {
    "__init__": [
      "self"
    ],
    "from_engine_args": [
      "cls",
      "engine_args",
      "usage_context",
      "stat_loggers",
      "load_in_low_bit"
    ]
  },
  "IPEXLLMMQLLMEngine": {
    "from_engine_args": [
      "cls",
      "engine_args",
      "usage_context",
      "ipc_path",
      "load_in_low_bit"
    ]
  },
  "run_mp_engine": [
    "engine_args",
    "usage_context",
    "ipc_path",
    "load_in_low_bit",
    "engine_alive"
  ],
  "InputsRequest": {},
  "ChatCompletionRequest": {},
  "CompletionRequest": {},
  "app": [],
  "FastApp": {
    "__init__": [
      "self",
      "model",
      "mytokenizer",
      "myprocessor"
    ]
  },
  "get_queue_next_token": [
    "delta_text_queue"
  ],
  "should_return_end_token": [
    "next_token"
  ],
  "chat_stream_generator": [
    "local_model",
    "delta_text_queue",
    "request_id"
  ],
  "completion_stream_generator": [
    "local_model",
    "delta_text_queue",
    "request_id"
  ],
  "generator": [
    "local_model",
    "delta_text_queue",
    "request_id"
  ],
  "generate_stream_api": [
    "inputs_request"
  ],
  "generate_stream": [
    "inputs_request"
  ],
  "get_prompt": [
    "messages"
  ],
  "set_parameters": [
    "req"
  ],
  "transcriptions": [
    "file",
    "model",
    "language",
    "prompt",
    "response_format",
    "temperature",
    "timestamp_granularities"
  ],
  "startup_event": [],
  "process_requests": [
    "local_model",
    "result_dict"
  ],
  "Parameters": {},
  "ModelWorker": {
    "__init__": [
      "self",
      "checkpoint",
      "low_bit",
      "model_type",
      "torch_dtype"
    ],
    "load_model": [
      "self",
      "model_path",
      "low_bit",
      "model_type"
    ],
    "get_local_image_path": [
      "self",
      "image_path"
    ],
    "add_asr_request": [
      "self",
      "processor"
    ],
    "add_request": [
      "self",
      "tokenizer"
    ],
    "process_step": [
      "self",
      "tokenizer",
      "result_dict",
      "processor"
    ]
  },
  "random_uuid": [],
  "TranscriptionRequest": {},
  "TranscriptionResponse": {},
  "OpenAIBaseModel": {
    "model_config": []
  },
  "ErrorResponse": {},
  "ModelPermission": {},
  "ModelCard": {},
  "ModelList": {},
  "UsageInfo": {},
  "ResponseFormat": {},
  "LogProbs": {},
  "CompletionResponseChoice": {},
  "CompletionResponse": {},
  "CompletionResponseStreamChoice": {},
  "CompletionStreamResponse": {},
  "ChatMessage": {},
  "ChatCompletionResponseChoice": {},
  "ChatCompletionResponse": {},
  "DeltaMessage": {},
  "ChatCompletionResponseStreamChoice": {},
  "ChatCompletionStreamResponse": {},
  "pseudo_infinite_int": [],
  "Grammar": {},
  "ChatCompletionParam": {},
  "ChatCompletionDetails": {},
  "ChatCompletionStreamChoice": {},
  "ChatCompletionStreamDetails": {},
  "TokenCheckRequestItem": {},
  "TokenCheckRequest": {},
  "TokenCheckResponseItem": {},
  "TokenCheckResponse": {},
  "EmbeddingsRequest": {},
  "EmbeddingsResponse": {},
  "is_fastchat_patched": [],
  "_mapping_fastchat": [],
  "_get_patch_map": [],
  "load_model_base": [
    "self",
    "model_path",
    "from_pretrained_kwargs"
  ],
  "load_model_chatglm": [
    "self",
    "model_path",
    "from_pretrained_kwargs"
  ],
  "BigDLLLMAdapter": {
    "match": [
      "self",
      "model_path"
    ],
    "load_model": [
      "self",
      "model_path",
      "from_pretrained_kwargs"
    ]
  },
  "BigDLLMLOWBITAdapter": {
    "match": [
      "self",
      "model_path"
    ],
    "load_model": [
      "self",
      "model_path",
      "from_pretrained_kwargs"
    ]
  },
  "patch_fastchat": [],
  "BigDLLLMWorker": {
    "__init__": [
      "self",
      "controller_addr",
      "worker_addr",
      "worker_id",
      "model_path",
      "model_names",
      "limit_worker_concurrency",
      "conv_template",
      "load_in_low_bit",
      "device",
      "no_register",
      "trust_remote_code",
      "embed_in_truncate",
      "speculative",
      "load_low_bit_model",
      "stream_interval",
      "benchmark"
    ],
    "__process_embed_chunk": [
      "self",
      "input_ids",
      "attention_mask"
    ],
    "get_embeddings": [
      "self",
      "params"
    ],
    "generate_stream_gate": [
      "self",
      "params"
    ],
    "generate_gate": [
      "self",
      "params"
    ]
  },
  "api_generate_stream": [
    "request"
  ],
  "api_generate": [
    "request"
  ],
  "api_get_status": [
    "request"
  ],
  "api_get_embeddings": [
    "request"
  ],
  "api_count_token": [
    "request"
  ],
  "api_get_conv": [
    "request"
  ],
  "api_model_details": [
    "request"
  ],
  "worker_id": [],
  "heart_beat_worker": [
    "obj"
  ],
  "BaseModelWorker": {
    "__init__": [
      "self",
      "controller_addr",
      "worker_addr",
      "worker_id",
      "model_path",
      "model_names",
      "limit_worker_concurrency",
      "conv_template"
    ],
    "init_heart_beat": [
      "self"
    ],
    "register_to_controller": [
      "self"
    ],
    "send_heart_beat": [
      "self"
    ],
    "get_queue_length": [
      "self"
    ],
    "get_status": [
      "self"
    ],
    "count_token": [
      "self",
      "params"
    ],
    "get_conv_template": [
      "self"
    ]
  },
  "release_worker_semaphore": [],
  "acquire_worker_semaphore": [],
  "create_background_tasks": [],
  "VLLMWorker": {
    "__init__": [
      "self",
      "controller_addr",
      "worker_addr",
      "worker_id",
      "model_path",
      "model_names",
      "limit_worker_concurrency",
      "no_register",
      "llm_engine",
      "conv_template"
    ],
    "generate_stream": [
      "self",
      "params"
    ],
    "generate": [
      "self",
      "params"
    ]
  },
  "conv_template_map": [],
  "fetch_timeout": [],
  "fetch_remote": [
    "url",
    "pload",
    "name"
  ],
  "AppSettings": {},
  "app_settings": [],
  "headers": [],
  "get_bearer_token": [],
  "check_api_key": [
    "auth"
  ],
  "create_error_response": [
    "code",
    "message"
  ],
  "validation_exception_handler": [
    "request",
    "exc"
  ],
  "check_model": [
    "request"
  ],
  "check_length": [
    "request",
    "prompt",
    "max_tokens",
    "worker_addr"
  ],
  "check_requests": [
    "request"
  ],
  "process_input": [
    "model_name",
    "inp"
  ],
  "create_openai_logprobs": [
    "logprob_dict"
  ],
  "_add_to_set": [
    "s",
    "new_stop"
  ],
  "get_gen_params": [
    "model_name",
    "worker_addr",
    "messages"
  ],
  "get_worker_address": [
    "model_name"
  ],
  "get_conv": [
    "model_name",
    "worker_addr"
  ],
  "get_last_model_name_from_list": [],
  "create_chat_completion_stream": [
    "request"
  ],
  "chat_completion_stream_generator": [
    "model_name",
    "gen_params",
    "n",
    "worker_addr"
  ],
  "generate_completion_stream": [
    "payload",
    "worker_addr"
  ],
  "generate_completion": [
    "payload",
    "worker_addr"
  ],
  "create_openai_api_server": []
}