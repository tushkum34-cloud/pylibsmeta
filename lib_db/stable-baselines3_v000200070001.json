{
  "version_file": [],
  "HER": [],
  "__all__": [],
  "BaseCallback": {
    "__init__": [
      "self",
      "verbose"
    ],
    "training_env": [
      "self"
    ],
    "logger": [
      "self"
    ],
    "init_callback": [
      "self",
      "model"
    ],
    "_init_callback": [
      "self"
    ],
    "on_training_start": [
      "self",
      "locals_",
      "globals_"
    ],
    "_on_training_start": [
      "self"
    ],
    "on_rollout_start": [
      "self"
    ],
    "_on_rollout_start": [
      "self"
    ],
    "_on_step": [
      "self"
    ],
    "on_step": [
      "self"
    ],
    "on_training_end": [
      "self"
    ],
    "_on_training_end": [
      "self"
    ],
    "on_rollout_end": [
      "self"
    ],
    "_on_rollout_end": [
      "self"
    ],
    "update_locals": [
      "self",
      "locals_"
    ],
    "update_child_locals": [
      "self",
      "locals_"
    ]
  },
  "EventCallback": {
    "__init__": [
      "self",
      "callback",
      "verbose"
    ],
    "init_callback": [
      "self",
      "model"
    ],
    "_on_training_start": [
      "self"
    ],
    "_on_event": [
      "self"
    ],
    "_on_step": [
      "self"
    ],
    "update_child_locals": [
      "self",
      "locals_"
    ]
  },
  "CallbackList": {
    "__init__": [
      "self",
      "callbacks"
    ],
    "_init_callback": [
      "self"
    ],
    "_on_training_start": [
      "self"
    ],
    "_on_rollout_start": [
      "self"
    ],
    "_on_step": [
      "self"
    ],
    "_on_rollout_end": [
      "self"
    ],
    "_on_training_end": [
      "self"
    ],
    "update_child_locals": [
      "self",
      "locals_"
    ]
  },
  "CheckpointCallback": {
    "__init__": [
      "self",
      "save_freq",
      "save_path",
      "name_prefix",
      "save_replay_buffer",
      "save_vecnormalize",
      "verbose"
    ],
    "_init_callback": [
      "self"
    ],
    "_checkpoint_path": [
      "self",
      "checkpoint_type",
      "extension"
    ],
    "_on_step": [
      "self"
    ]
  },
  "ConvertCallback": {
    "__init__": [
      "self",
      "callback",
      "verbose"
    ],
    "_on_step": [
      "self"
    ]
  },
  "EvalCallback": {
    "__init__": [
      "self",
      "eval_env",
      "callback_on_new_best",
      "callback_after_eval",
      "n_eval_episodes",
      "eval_freq",
      "log_path",
      "best_model_save_path",
      "deterministic",
      "render",
      "verbose",
      "warn"
    ],
    "_init_callback": [
      "self"
    ],
    "_log_success_callback": [
      "self",
      "locals_",
      "globals_"
    ],
    "_on_step": [
      "self"
    ],
    "update_child_locals": [
      "self",
      "locals_"
    ]
  },
  "StopTrainingOnRewardThreshold": {
    "__init__": [
      "self",
      "reward_threshold",
      "verbose"
    ],
    "_on_step": [
      "self"
    ]
  },
  "EveryNTimesteps": {
    "__init__": [
      "self",
      "n_steps",
      "callback"
    ],
    "_on_step": [
      "self"
    ]
  },
  "LogEveryNTimesteps": {
    "__init__": [
      "self",
      "n_steps"
    ],
    "_log_data": [
      "self",
      "_locals",
      "_globals"
    ]
  },
  "StopTrainingOnMaxEpisodes": {
    "__init__": [
      "self",
      "max_episodes",
      "verbose"
    ],
    "_init_callback": [
      "self"
    ],
    "_on_step": [
      "self"
    ]
  },
  "StopTrainingOnNoModelImprovement": {
    "__init__": [
      "self",
      "max_no_improvement_evals",
      "min_evals",
      "verbose"
    ],
    "_on_step": [
      "self"
    ]
  },
  "ProgressBarCallback": {
    "__init__": [
      "self"
    ],
    "_on_training_start": [
      "self"
    ],
    "_on_step": [
      "self"
    ],
    "_on_training_end": [
      "self"
    ]
  },
  "DEBUG": [],
  "INFO": [],
  "WARN": [],
  "ERROR": [],
  "DISABLED": [],
  "Video": {
    "__init__": [
      "self",
      "frames",
      "fps"
    ]
  },
  "Figure": {
    "__init__": [
      "self",
      "figure",
      "close"
    ]
  },
  "Image": {
    "__init__": [
      "self",
      "image",
      "dataformats"
    ]
  },
  "HParam": {
    "__init__": [
      "self",
      "hparam_dict",
      "metric_dict"
    ]
  },
  "FormatUnsupportedError": {
    "__init__": [
      "self",
      "unsupported_formats",
      "value_description"
    ]
  },
  "KVWriter": {
    "write": [
      "self",
      "key_values",
      "key_excluded",
      "step"
    ],
    "close": [
      "self"
    ]
  },
  "SeqWriter": {
    "write_sequence": [
      "self",
      "sequence"
    ]
  },
  "HumanOutputFormat": {
    "__init__": [
      "self",
      "filename_or_file",
      "max_length"
    ],
    "write": [
      "self",
      "key_values",
      "key_excluded",
      "step"
    ],
    "_truncate": [
      "self",
      "string"
    ],
    "write_sequence": [
      "self",
      "sequence"
    ],
    "close": [
      "self"
    ]
  },
  "filter_excluded_keys": [
    "key_values",
    "key_excluded",
    "_format"
  ],
  "JSONOutputFormat": {
    "__init__": [
      "self",
      "filename"
    ],
    "write": [
      "self",
      "key_values",
      "key_excluded",
      "step"
    ],
    "close": [
      "self"
    ]
  },
  "CSVOutputFormat": {
    "__init__": [
      "self",
      "filename"
    ],
    "write": [
      "self",
      "key_values",
      "key_excluded",
      "step"
    ],
    "close": [
      "self"
    ]
  },
  "TensorBoardOutputFormat": {
    "__init__": [
      "self",
      "folder"
    ],
    "write": [
      "self",
      "key_values",
      "key_excluded",
      "step"
    ],
    "close": [
      "self"
    ]
  },
  "make_output_format": [
    "_format",
    "log_dir",
    "log_suffix"
  ],
  "Logger": {
    "__init__": [
      "self",
      "folder",
      "output_formats"
    ],
    "to_tuple": [
      "string_or_tuple"
    ],
    "record": [
      "self",
      "key",
      "value",
      "exclude"
    ],
    "record_mean": [
      "self",
      "key",
      "value",
      "exclude"
    ],
    "dump": [
      "self",
      "step"
    ],
    "log": [
      "self"
    ],
    "debug": [
      "self"
    ],
    "info": [
      "self"
    ],
    "warn": [
      "self"
    ],
    "error": [
      "self"
    ],
    "set_level": [
      "self",
      "level"
    ],
    "get_dir": [
      "self"
    ],
    "close": [
      "self"
    ],
    "_do_log": [
      "self",
      "args"
    ]
  },
  "configure": [
    "folder",
    "format_strings"
  ],
  "read_json": [
    "filename"
  ],
  "read_csv": [
    "filename"
  ],
  "recursive_getattr": [
    "obj",
    "attr"
  ],
  "recursive_setattr": [
    "obj",
    "attr",
    "val"
  ],
  "is_json_serializable": [
    "item"
  ],
  "data_to_json": [
    "data"
  ],
  "json_to_data": [
    "json_string",
    "custom_objects"
  ],
  "open_path": [
    "path",
    "mode",
    "verbose",
    "suffix"
  ],
  "open_path_str": [
    "path",
    "mode",
    "verbose",
    "suffix"
  ],
  "open_path_pathlib": [
    "path",
    "mode",
    "verbose",
    "suffix"
  ],
  "save_to_zip_file": [
    "save_path",
    "data",
    "params",
    "pytorch_variables",
    "verbose"
  ],
  "save_to_pkl": [
    "path",
    "obj",
    "verbose"
  ],
  "load_from_pkl": [
    "path",
    "verbose"
  ],
  "load_from_zip_file": [
    "load_path",
    "load_data",
    "custom_objects",
    "device",
    "verbose",
    "print_system_info"
  ],
  "evaluate_policy": [
    "model",
    "env",
    "n_eval_episodes",
    "deterministic",
    "render",
    "callback",
    "reward_threshold",
    "return_episode_rewards",
    "warn"
  ],
  "Monitor": {
    "EXT": [],
    "__init__": [
      "self",
      "env",
      "filename",
      "allow_early_resets",
      "reset_keywords",
      "info_keywords",
      "override_existing"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "close": [
      "self"
    ],
    "get_total_steps": [
      "self"
    ],
    "get_episode_rewards": [
      "self"
    ],
    "get_episode_lengths": [
      "self"
    ],
    "get_episode_times": [
      "self"
    ]
  },
  "LoadMonitorResultsError": {},
  "ResultsWriter": {
    "__init__": [
      "self",
      "filename",
      "header",
      "extra_keys",
      "override_existing"
    ],
    "write_row": [
      "self",
      "epinfo"
    ],
    "close": [
      "self"
    ]
  },
  "get_monitor_files": [
    "path"
  ],
  "load_results": [
    "path"
  ],
  "SelfBaseAlgorithm": [],
  "maybe_make_env": [
    "env",
    "verbose"
  ],
  "BaseAlgorithm": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "policy_kwargs",
      "stats_window_size",
      "tensorboard_log",
      "verbose",
      "device",
      "support_multi_env",
      "monitor_wrapper",
      "seed",
      "use_sde",
      "sde_sample_freq",
      "supported_action_spaces"
    ],
    "_wrap_env": [
      "env",
      "verbose",
      "monitor_wrapper"
    ],
    "_setup_model": [
      "self"
    ],
    "set_logger": [
      "self",
      "logger"
    ],
    "logger": [
      "self"
    ],
    "_setup_lr_schedule": [
      "self"
    ],
    "_update_current_progress_remaining": [
      "self",
      "num_timesteps",
      "total_timesteps"
    ],
    "_update_learning_rate": [
      "self",
      "optimizers"
    ],
    "_excluded_save_params": [
      "self"
    ],
    "_get_policy_from_name": [
      "self",
      "policy_name"
    ],
    "_get_torch_save_params": [
      "self"
    ],
    "_init_callback": [
      "self",
      "callback",
      "progress_bar"
    ],
    "_setup_learn": [
      "self",
      "total_timesteps",
      "callback",
      "reset_num_timesteps",
      "tb_log_name",
      "progress_bar"
    ],
    "_update_info_buffer": [
      "self",
      "infos",
      "dones"
    ],
    "get_env": [
      "self"
    ],
    "get_vec_normalize_env": [
      "self"
    ],
    "set_env": [
      "self",
      "env",
      "force_reset"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic"
    ],
    "set_random_seed": [
      "self",
      "seed"
    ],
    "set_parameters": [
      "self",
      "load_path_or_dict",
      "exact_match",
      "device"
    ],
    "load": [
      "cls",
      "path",
      "env",
      "device",
      "custom_objects",
      "print_system_info",
      "force_reset"
    ],
    "get_parameters": [
      "self"
    ],
    "save": [
      "self",
      "path",
      "exclude",
      "include"
    ],
    "dump_logs": [
      "self"
    ],
    "_dump_logs": [
      "self"
    ]
  },
  "SelfDistribution": [],
  "SelfDiagGaussianDistribution": [],
  "SelfSquashedDiagGaussianDistribution": [],
  "SelfCategoricalDistribution": [],
  "SelfMultiCategoricalDistribution": [],
  "SelfBernoulliDistribution": [],
  "SelfStateDependentNoiseDistribution": [],
  "Distribution": {
    "__init__": [
      "self"
    ],
    "proba_distribution_net": [
      "self"
    ],
    "proba_distribution": [
      "self"
    ],
    "log_prob": [
      "self",
      "x"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "get_actions": [
      "self",
      "deterministic"
    ],
    "actions_from_params": [
      "self"
    ],
    "log_prob_from_params": [
      "self"
    ]
  },
  "sum_independent_dims": [
    "tensor"
  ],
  "DiagGaussianDistribution": {
    "__init__": [
      "self",
      "action_dim"
    ],
    "proba_distribution_net": [
      "self",
      "latent_dim",
      "log_std_init"
    ],
    "proba_distribution": [
      "self",
      "mean_actions",
      "log_std"
    ],
    "log_prob": [
      "self",
      "actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "actions_from_params": [
      "self",
      "mean_actions",
      "log_std",
      "deterministic"
    ],
    "log_prob_from_params": [
      "self",
      "mean_actions",
      "log_std"
    ]
  },
  "SquashedDiagGaussianDistribution": {
    "__init__": [
      "self",
      "action_dim",
      "epsilon"
    ],
    "proba_distribution": [
      "self",
      "mean_actions",
      "log_std"
    ],
    "log_prob": [
      "self",
      "actions",
      "gaussian_actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "log_prob_from_params": [
      "self",
      "mean_actions",
      "log_std"
    ]
  },
  "CategoricalDistribution": {
    "__init__": [
      "self",
      "action_dim"
    ],
    "proba_distribution_net": [
      "self",
      "latent_dim"
    ],
    "proba_distribution": [
      "self",
      "action_logits"
    ],
    "log_prob": [
      "self",
      "actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "actions_from_params": [
      "self",
      "action_logits",
      "deterministic"
    ],
    "log_prob_from_params": [
      "self",
      "action_logits"
    ]
  },
  "MultiCategoricalDistribution": {
    "__init__": [
      "self",
      "action_dims"
    ],
    "proba_distribution_net": [
      "self",
      "latent_dim"
    ],
    "proba_distribution": [
      "self",
      "action_logits"
    ],
    "log_prob": [
      "self",
      "actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "actions_from_params": [
      "self",
      "action_logits",
      "deterministic"
    ],
    "log_prob_from_params": [
      "self",
      "action_logits"
    ]
  },
  "BernoulliDistribution": {
    "__init__": [
      "self",
      "action_dims"
    ],
    "proba_distribution_net": [
      "self",
      "latent_dim"
    ],
    "proba_distribution": [
      "self",
      "action_logits"
    ],
    "log_prob": [
      "self",
      "actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "actions_from_params": [
      "self",
      "action_logits",
      "deterministic"
    ],
    "log_prob_from_params": [
      "self",
      "action_logits"
    ]
  },
  "StateDependentNoiseDistribution": {
    "__init__": [
      "self",
      "action_dim",
      "full_std",
      "use_expln",
      "squash_output",
      "learn_features",
      "epsilon"
    ],
    "get_std": [
      "self",
      "log_std"
    ],
    "sample_weights": [
      "self",
      "log_std",
      "batch_size"
    ],
    "proba_distribution_net": [
      "self",
      "latent_dim",
      "log_std_init",
      "latent_sde_dim"
    ],
    "proba_distribution": [
      "self",
      "mean_actions",
      "log_std",
      "latent_sde"
    ],
    "log_prob": [
      "self",
      "actions"
    ],
    "entropy": [
      "self"
    ],
    "sample": [
      "self"
    ],
    "mode": [
      "self"
    ],
    "get_noise": [
      "self",
      "latent_sde"
    ],
    "actions_from_params": [
      "self",
      "mean_actions",
      "log_std",
      "latent_sde",
      "deterministic"
    ],
    "log_prob_from_params": [
      "self",
      "mean_actions",
      "log_std",
      "latent_sde"
    ]
  },
  "TanhBijector": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "forward": [
      "x"
    ],
    "atanh": [
      "x"
    ],
    "inverse": [
      "y"
    ],
    "log_prob_correction": [
      "self",
      "x"
    ]
  },
  "make_proba_distribution": [
    "action_space",
    "use_sde",
    "dist_kwargs"
  ],
  "kl_divergence": [
    "dist_true",
    "dist_pred"
  ],
  "set_random_seed": [
    "seed",
    "using_cuda"
  ],
  "explained_variance": [
    "y_pred",
    "y_true"
  ],
  "update_learning_rate": [
    "optimizer",
    "learning_rate"
  ],
  "FloatSchedule": {
    "__init__": [
      "self",
      "value_schedule"
    ],
    "__call__": [
      "self",
      "progress_remaining"
    ],
    "__repr__": [
      "self"
    ]
  },
  "LinearSchedule": {
    "__init__": [
      "self",
      "start",
      "end",
      "end_fraction"
    ],
    "__call__": [
      "self",
      "progress_remaining"
    ],
    "__repr__": [
      "self"
    ]
  },
  "ConstantSchedule": {
    "__init__": [
      "self",
      "val"
    ],
    "__call__": [
      "self",
      "_"
    ],
    "__repr__": [
      "self"
    ]
  },
  "get_schedule_fn": [
    "value_schedule"
  ],
  "get_linear_fn": [
    "start",
    "end",
    "end_fraction"
  ],
  "constant_fn": [
    "val"
  ],
  "get_device": [
    "device"
  ],
  "get_latest_run_id": [
    "log_path",
    "log_name"
  ],
  "configure_logger": [
    "verbose",
    "tensorboard_log",
    "tb_log_name",
    "reset_num_timesteps"
  ],
  "check_for_correct_spaces": [
    "env",
    "observation_space",
    "action_space"
  ],
  "check_shape_equal": [
    "space1",
    "space2"
  ],
  "is_vectorized_box_observation": [
    "observation",
    "observation_space"
  ],
  "is_vectorized_discrete_observation": [
    "observation",
    "observation_space"
  ],
  "is_vectorized_multidiscrete_observation": [
    "observation",
    "observation_space"
  ],
  "is_vectorized_multibinary_observation": [
    "observation",
    "observation_space"
  ],
  "is_vectorized_dict_observation": [
    "observation",
    "observation_space"
  ],
  "is_vectorized_observation": [
    "observation",
    "observation_space"
  ],
  "safe_mean": [
    "arr"
  ],
  "get_parameters_by_name": [
    "model",
    "included_names"
  ],
  "zip_strict": [],
  "polyak_update": [
    "params",
    "target_params",
    "tau"
  ],
  "obs_as_tensor": [
    "obs",
    "device"
  ],
  "should_collect_more_steps": [
    "train_freq",
    "num_collected_steps",
    "num_collected_episodes"
  ],
  "get_system_info": [
    "print_info"
  ],
  "X_TIMESTEPS": [],
  "X_EPISODES": [],
  "X_WALLTIME": [],
  "POSSIBLE_X_AXES": [],
  "EPISODES_WINDOW": [],
  "rolling_window": [
    "array",
    "window"
  ],
  "window_func": [
    "var_1",
    "var_2",
    "window",
    "func"
  ],
  "ts2xy": [
    "data_frame",
    "x_axis"
  ],
  "plot_curves": [
    "xy_list",
    "x_axis",
    "title",
    "figsize"
  ],
  "plot_results": [
    "dirs",
    "num_timesteps",
    "x_axis",
    "task_name",
    "figsize"
  ],
  "SelfBaseModel": [],
  "BaseModel": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "features_extractor_class",
      "features_extractor_kwargs",
      "features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ],
    "_update_features_extractor": [
      "self",
      "net_kwargs",
      "features_extractor"
    ],
    "make_features_extractor": [
      "self"
    ],
    "extract_features": [
      "self",
      "obs",
      "features_extractor"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "device": [
      "self"
    ],
    "save": [
      "self",
      "path"
    ],
    "load": [
      "cls",
      "path",
      "device"
    ],
    "load_from_vector": [
      "self",
      "vector"
    ],
    "parameters_to_vector": [
      "self"
    ],
    "set_training_mode": [
      "self",
      "mode"
    ],
    "is_vectorized_observation": [
      "self",
      "observation"
    ],
    "obs_to_tensor": [
      "self",
      "observation"
    ]
  },
  "BasePolicy": {
    "__init__": [
      "self"
    ],
    "_dummy_schedule": [
      "progress_remaining"
    ],
    "squash_output": [
      "self"
    ],
    "init_weights": [
      "module",
      "gain"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic"
    ],
    "scale_action": [
      "self",
      "action"
    ],
    "unscale_action": [
      "self",
      "scaled_action"
    ]
  },
  "ActorCriticPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "squash_output",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "reset_noise": [
      "self",
      "n_envs"
    ],
    "_build_mlp_extractor": [
      "self"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "extract_features": [
      "self",
      "obs",
      "features_extractor"
    ],
    "_get_action_dist_from_latent": [
      "self",
      "latent_pi"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "evaluate_actions": [
      "self",
      "obs",
      "actions"
    ],
    "get_distribution": [
      "self",
      "obs"
    ],
    "predict_values": [
      "self",
      "obs"
    ]
  },
  "ActorCriticCnnPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "squash_output",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ]
  },
  "MultiInputActorCriticPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "ortho_init",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "squash_output",
      "features_extractor_class",
      "features_extractor_kwargs",
      "share_features_extractor",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ]
  },
  "ContinuousCritic": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "net_arch",
      "features_extractor",
      "features_dim",
      "activation_fn",
      "normalize_images",
      "n_critics",
      "share_features_extractor"
    ],
    "forward": [
      "self",
      "obs",
      "actions"
    ],
    "q1_forward": [
      "self",
      "obs",
      "actions"
    ]
  },
  "StickyActionEnv": {
    "__init__": [
      "self",
      "env",
      "action_repeat_probability"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ]
  },
  "NoopResetEnv": {
    "__init__": [
      "self",
      "env",
      "noop_max"
    ],
    "reset": [
      "self"
    ]
  },
  "FireResetEnv": {
    "__init__": [
      "self",
      "env"
    ],
    "reset": [
      "self"
    ]
  },
  "EpisodicLifeEnv": {
    "__init__": [
      "self",
      "env"
    ],
    "step": [
      "self",
      "action"
    ],
    "reset": [
      "self"
    ]
  },
  "MaxAndSkipEnv": {
    "__init__": [
      "self",
      "env",
      "skip"
    ],
    "step": [
      "self",
      "action"
    ]
  },
  "ClipRewardEnv": {
    "__init__": [
      "self",
      "env"
    ],
    "reward": [
      "self",
      "reward"
    ]
  },
  "WarpFrame": {
    "__init__": [
      "self",
      "env",
      "width",
      "height"
    ],
    "observation": [
      "self",
      "frame"
    ]
  },
  "AtariWrapper": {
    "__init__": [
      "self",
      "env",
      "noop_max",
      "frame_skip",
      "screen_size",
      "terminal_on_life_loss",
      "clip_reward",
      "action_repeat_probability"
    ]
  },
  "unwrap_wrapper": [
    "env",
    "wrapper_class"
  ],
  "is_wrapped": [
    "env",
    "wrapper_class"
  ],
  "make_vec_env": [
    "env_id",
    "n_envs",
    "seed",
    "start_index",
    "monitor_dir",
    "wrapper_class",
    "env_kwargs",
    "vec_env_cls",
    "vec_env_kwargs",
    "monitor_kwargs",
    "wrapper_kwargs"
  ],
  "make_atari_env": [
    "env_id",
    "n_envs",
    "seed",
    "start_index",
    "monitor_dir",
    "wrapper_kwargs",
    "env_kwargs",
    "vec_env_cls",
    "vec_env_kwargs",
    "monitor_kwargs"
  ],
  "BaseFeaturesExtractor": {
    "__init__": [
      "self",
      "observation_space",
      "features_dim"
    ],
    "features_dim": [
      "self"
    ]
  },
  "FlattenExtractor": {
    "__init__": [
      "self",
      "observation_space"
    ],
    "forward": [
      "self",
      "observations"
    ]
  },
  "NatureCNN": {
    "__init__": [
      "self",
      "observation_space",
      "features_dim",
      "normalized_image"
    ],
    "forward": [
      "self",
      "observations"
    ]
  },
  "create_mlp": [
    "input_dim",
    "output_dim",
    "net_arch",
    "activation_fn",
    "squash_output",
    "with_bias",
    "pre_linear_modules",
    "post_linear_modules"
  ],
  "MlpExtractor": {
    "__init__": [
      "self",
      "feature_dim",
      "net_arch",
      "activation_fn",
      "device"
    ],
    "forward": [
      "self",
      "features"
    ],
    "forward_actor": [
      "self",
      "features"
    ],
    "forward_critic": [
      "self",
      "features"
    ]
  },
  "CombinedExtractor": {
    "__init__": [
      "self",
      "observation_space",
      "cnn_output_dim",
      "normalized_image"
    ],
    "forward": [
      "self",
      "observations"
    ]
  },
  "get_actor_critic_arch": [
    "net_arch"
  ],
  "GymEnv": [],
  "GymObs": [],
  "GymResetReturn": [],
  "AtariResetReturn": [],
  "GymStepReturn": [],
  "AtariStepReturn": [],
  "TensorDict": [],
  "OptimizerStateDict": [],
  "MaybeCallback": [],
  "PyTorchObs": [],
  "Schedule": [],
  "RolloutBufferSamples": {},
  "DictRolloutBufferSamples": {},
  "ReplayBufferSamples": {},
  "DictReplayBufferSamples": {},
  "RolloutReturn": {},
  "TrainFrequencyUnit": {
    "STEP": [],
    "EPISODE": []
  },
  "TrainFreq": {},
  "PolicyPredictor": {
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic"
    ]
  },
  "is_image_space_channels_first": [
    "observation_space"
  ],
  "is_image_space": [
    "observation_space",
    "check_channels",
    "normalized_image"
  ],
  "maybe_transpose": [
    "observation",
    "observation_space"
  ],
  "preprocess_obs": [
    "obs",
    "observation_space",
    "normalize_images"
  ],
  "get_obs_shape": [
    "observation_space"
  ],
  "get_flattened_obs_dim": [
    "observation_space"
  ],
  "get_action_dim": [
    "action_space"
  ],
  "check_for_nested_spaces": [
    "obs_space"
  ],
  "_is_oneof_space": [
    "space"
  ],
  "_is_numpy_array_space": [
    "space"
  ],
  "_starts_at_zero": [
    "space"
  ],
  "_check_non_zero_start": [
    "space",
    "space_type",
    "key"
  ],
  "_check_image_input": [
    "observation_space",
    "key"
  ],
  "_check_unsupported_spaces": [
    "env",
    "observation_space",
    "action_space"
  ],
  "_check_nan": [
    "env"
  ],
  "_is_goal_env": [
    "env"
  ],
  "_check_goal_env_obs": [
    "obs",
    "observation_space",
    "method_name"
  ],
  "_check_goal_env_compute_reward": [
    "obs",
    "env",
    "reward",
    "info"
  ],
  "_check_obs": [
    "obs",
    "observation_space",
    "method_name"
  ],
  "_check_box_obs": [
    "observation_space",
    "key"
  ],
  "_check_returned_values": [
    "env",
    "observation_space",
    "action_space"
  ],
  "_check_spaces": [
    "env"
  ],
  "_check_render": [
    "env",
    "warn"
  ],
  "check_env": [
    "env",
    "warn",
    "skip_render_check"
  ],
  "RunningMeanStd": {
    "__init__": [
      "self",
      "epsilon",
      "shape"
    ],
    "copy": [
      "self"
    ],
    "combine": [
      "self",
      "other"
    ],
    "update": [
      "self",
      "arr"
    ],
    "update_from_moments": [
      "self",
      "batch_mean",
      "batch_var",
      "batch_count"
    ]
  },
  "SelfOnPolicyAlgorithm": [],
  "OnPolicyAlgorithm": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "n_steps",
      "gamma",
      "gae_lambda",
      "ent_coef",
      "vf_coef",
      "max_grad_norm",
      "use_sde",
      "sde_sample_freq",
      "rollout_buffer_class",
      "rollout_buffer_kwargs",
      "stats_window_size",
      "tensorboard_log",
      "monitor_wrapper",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model",
      "supported_action_spaces"
    ],
    "_setup_model": [
      "self"
    ],
    "_maybe_recommend_cpu": [
      "self",
      "mlp_class_name"
    ],
    "collect_rollouts": [
      "self",
      "env",
      "callback",
      "rollout_buffer",
      "n_rollout_steps"
    ],
    "train": [
      "self"
    ],
    "dump_logs": [
      "self",
      "iteration"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_get_torch_save_params": [
      "self"
    ]
  },
  "ActionNoise": {
    "__init__": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "__call__": [
      "self"
    ]
  },
  "NormalActionNoise": {
    "__init__": [
      "self",
      "mean",
      "sigma",
      "dtype"
    ],
    "__call__": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "OrnsteinUhlenbeckActionNoise": {
    "__init__": [
      "self",
      "mean",
      "sigma",
      "theta",
      "dt",
      "initial_noise",
      "dtype"
    ],
    "__call__": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "VectorizedActionNoise": {
    "__init__": [
      "self",
      "base_noise",
      "n_envs"
    ],
    "reset": [
      "self",
      "indices"
    ],
    "__repr__": [
      "self"
    ],
    "__call__": [
      "self"
    ],
    "base_noise": [
      "self",
      "base_noise"
    ],
    "noises": [
      "self",
      "noises"
    ]
  },
  "SelfOffPolicyAlgorithm": [],
  "OffPolicyAlgorithm": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "tau",
      "gamma",
      "train_freq",
      "gradient_steps",
      "action_noise",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "policy_kwargs",
      "stats_window_size",
      "tensorboard_log",
      "verbose",
      "device",
      "support_multi_env",
      "monitor_wrapper",
      "seed",
      "use_sde",
      "sde_sample_freq",
      "use_sde_at_warmup",
      "sde_support",
      "supported_action_spaces"
    ],
    "_convert_train_freq": [
      "self"
    ],
    "_setup_model": [
      "self"
    ],
    "save_replay_buffer": [
      "self",
      "path"
    ],
    "load_replay_buffer": [
      "self",
      "path",
      "truncate_last_traj"
    ],
    "_setup_learn": [
      "self",
      "total_timesteps",
      "callback",
      "reset_num_timesteps",
      "tb_log_name",
      "progress_bar"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "train": [
      "self",
      "gradient_steps",
      "batch_size"
    ],
    "_sample_action": [
      "self",
      "learning_starts",
      "action_noise",
      "n_envs"
    ],
    "dump_logs": [
      "self"
    ],
    "_on_step": [
      "self"
    ],
    "_store_transition": [
      "self",
      "replay_buffer",
      "buffer_action",
      "new_obs",
      "reward",
      "dones",
      "infos"
    ],
    "collect_rollouts": [
      "self",
      "env",
      "callback",
      "train_freq",
      "replay_buffer",
      "action_noise",
      "learning_starts",
      "log_interval"
    ]
  },
  "BaseBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "device",
      "n_envs"
    ],
    "swap_and_flatten": [
      "arr"
    ],
    "size": [
      "self"
    ],
    "add": [
      "self"
    ],
    "extend": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "sample": [
      "self",
      "batch_size",
      "env"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ],
    "to_torch": [
      "self",
      "array",
      "copy"
    ],
    "_normalize_obs": [
      "obs",
      "env"
    ],
    "_normalize_reward": [
      "reward",
      "env"
    ]
  },
  "ReplayBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "device",
      "n_envs",
      "optimize_memory_usage",
      "handle_timeout_termination"
    ],
    "add": [
      "self",
      "obs",
      "next_obs",
      "action",
      "reward",
      "done",
      "infos"
    ],
    "sample": [
      "self",
      "batch_size",
      "env"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ],
    "_maybe_cast_dtype": [
      "dtype"
    ]
  },
  "RolloutBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "device",
      "gae_lambda",
      "gamma",
      "n_envs"
    ],
    "reset": [
      "self"
    ],
    "compute_returns_and_advantage": [
      "self",
      "last_values",
      "dones"
    ],
    "add": [
      "self",
      "obs",
      "action",
      "reward",
      "episode_start",
      "value",
      "log_prob"
    ],
    "get": [
      "self",
      "batch_size"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ]
  },
  "DictReplayBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "device",
      "n_envs",
      "optimize_memory_usage",
      "handle_timeout_termination"
    ],
    "add": [
      "self",
      "obs",
      "next_obs",
      "action",
      "reward",
      "done",
      "infos"
    ],
    "sample": [
      "self",
      "batch_size",
      "env"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ]
  },
  "DictRolloutBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "device",
      "gae_lambda",
      "gamma",
      "n_envs"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self",
      "obs",
      "action",
      "reward",
      "episode_start",
      "value",
      "log_prob"
    ],
    "get": [
      "self",
      "batch_size"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ]
  },
  "NStepReplayBuffer": {
    "__init__": [
      "self"
    ],
    "_get_samples": [
      "self",
      "batch_inds",
      "env"
    ]
  },
  "VecFrameStack": {
    "__init__": [
      "self",
      "venv",
      "n_stack",
      "channels_order"
    ],
    "step_wait": [
      "self"
    ],
    "reset": [
      "self"
    ]
  },
  "TObs": [],
  "StackedObservations": {
    "__init__": [
      "self",
      "num_envs",
      "n_stack",
      "observation_space",
      "channels_order"
    ],
    "compute_stacking": [
      "n_stack",
      "observation_space",
      "channels_order"
    ],
    "reset": [
      "self",
      "observation"
    ],
    "update": [
      "self",
      "observations",
      "dones",
      "infos"
    ]
  },
  "DummyVecEnv": {
    "__init__": [
      "self",
      "env_fns"
    ],
    "step_async": [
      "self",
      "actions"
    ],
    "step_wait": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "close": [
      "self"
    ],
    "get_images": [
      "self"
    ],
    "render": [
      "self",
      "mode"
    ],
    "_save_obs": [
      "self",
      "env_idx",
      "obs"
    ],
    "_obs_from_buf": [
      "self"
    ],
    "get_attr": [
      "self",
      "attr_name",
      "indices"
    ],
    "set_attr": [
      "self",
      "attr_name",
      "value",
      "indices"
    ],
    "env_method": [
      "self",
      "method_name"
    ],
    "env_is_wrapped": [
      "self",
      "wrapper_class",
      "indices"
    ],
    "_get_target_envs": [
      "self",
      "indices"
    ]
  },
  "_worker": [
    "remote",
    "parent_remote",
    "env_fn_wrapper"
  ],
  "SubprocVecEnv": {
    "__init__": [
      "self",
      "env_fns",
      "start_method"
    ],
    "step_async": [
      "self",
      "actions"
    ],
    "step_wait": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "close": [
      "self"
    ],
    "get_images": [
      "self"
    ],
    "has_attr": [
      "self",
      "attr_name"
    ],
    "get_attr": [
      "self",
      "attr_name",
      "indices"
    ],
    "set_attr": [
      "self",
      "attr_name",
      "value",
      "indices"
    ],
    "env_method": [
      "self",
      "method_name"
    ],
    "env_is_wrapped": [
      "self",
      "wrapper_class",
      "indices"
    ],
    "_get_target_remotes": [
      "self",
      "indices"
    ]
  },
  "_stack_obs": [
    "obs_list",
    "space"
  ],
  "VecVideoRecorder": {
    "__init__": [
      "self",
      "venv",
      "video_folder",
      "record_video_trigger",
      "video_length",
      "name_prefix"
    ],
    "reset": [
      "self"
    ],
    "_start_video_recorder": [
      "self"
    ],
    "_video_enabled": [
      "self"
    ],
    "step_wait": [
      "self"
    ],
    "_capture_frame": [
      "self"
    ],
    "close": [
      "self"
    ],
    "_start_recording": [
      "self"
    ],
    "_stop_recording": [
      "self"
    ],
    "__del__": [
      "self"
    ]
  },
  "VecEnvWrapperT": [],
  "unwrap_vec_wrapper": [
    "env",
    "vec_wrapper_class"
  ],
  "unwrap_vec_normalize": [
    "env"
  ],
  "is_vecenv_wrapped": [
    "env",
    "vec_wrapper_class"
  ],
  "sync_envs_normalization": [
    "env",
    "eval_env"
  ],
  "VecExtractDictObs": {
    "__init__": [
      "self",
      "venv",
      "key"
    ],
    "reset": [
      "self"
    ],
    "step_wait": [
      "self"
    ]
  },
  "_patch_env": [
    "env"
  ],
  "_convert_space": [
    "space"
  ],
  "dict_to_obs": [
    "obs_space",
    "obs_dict"
  ],
  "obs_space_info": [
    "obs_space"
  ],
  "VecTransposeImage": {
    "__init__": [
      "self",
      "venv",
      "skip"
    ],
    "transpose_space": [
      "observation_space",
      "key"
    ],
    "transpose_image": [
      "image"
    ],
    "transpose_observations": [
      "self",
      "observations"
    ],
    "step_wait": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "VecNormalize": {
    "__init__": [
      "self",
      "venv",
      "training",
      "norm_obs",
      "norm_reward",
      "clip_obs",
      "clip_reward",
      "gamma",
      "epsilon",
      "norm_obs_keys"
    ],
    "_sanity_checks": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "set_venv": [
      "self",
      "venv"
    ],
    "step_wait": [
      "self"
    ],
    "_update_reward": [
      "self",
      "reward"
    ],
    "_normalize_obs": [
      "self",
      "obs",
      "obs_rms"
    ],
    "_unnormalize_obs": [
      "self",
      "obs",
      "obs_rms"
    ],
    "normalize_obs": [
      "self",
      "obs"
    ],
    "normalize_reward": [
      "self",
      "reward"
    ],
    "unnormalize_obs": [
      "self",
      "obs"
    ],
    "unnormalize_reward": [
      "self",
      "reward"
    ],
    "get_original_obs": [
      "self"
    ],
    "get_original_reward": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "load": [
      "load_path",
      "venv"
    ],
    "save": [
      "self",
      "save_path"
    ]
  },
  "VecCheckNan": {
    "__init__": [
      "self",
      "venv",
      "raise_exception",
      "warn_once",
      "check_inf"
    ],
    "step_async": [
      "self",
      "actions"
    ],
    "step_wait": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "check_array_value": [
      "self",
      "name",
      "value"
    ],
    "_check_val": [
      "self",
      "event"
    ]
  },
  "VecMonitor": {
    "__init__": [
      "self",
      "venv",
      "filename",
      "info_keywords"
    ],
    "reset": [
      "self"
    ],
    "step_wait": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "VecEnvIndices": [],
  "VecEnvObs": [],
  "VecEnvStepReturn": [],
  "tile_images": [
    "images_nhwc"
  ],
  "VecEnv": {
    "__init__": [
      "self",
      "num_envs",
      "observation_space",
      "action_space"
    ],
    "_reset_seeds": [
      "self"
    ],
    "_reset_options": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "step_async": [
      "self",
      "actions"
    ],
    "step_wait": [
      "self"
    ],
    "close": [
      "self"
    ],
    "has_attr": [
      "self",
      "attr_name"
    ],
    "get_attr": [
      "self",
      "attr_name",
      "indices"
    ],
    "set_attr": [
      "self",
      "attr_name",
      "value",
      "indices"
    ],
    "env_method": [
      "self",
      "method_name"
    ],
    "env_is_wrapped": [
      "self",
      "wrapper_class",
      "indices"
    ],
    "step": [
      "self",
      "actions"
    ],
    "get_images": [
      "self"
    ],
    "render": [
      "self",
      "mode"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "set_options": [
      "self",
      "options"
    ],
    "unwrapped": [
      "self"
    ],
    "getattr_depth_check": [
      "self",
      "name",
      "already_found"
    ],
    "_get_indices": [
      "self",
      "indices"
    ]
  },
  "VecEnvWrapper": {
    "__init__": [
      "self",
      "venv",
      "observation_space",
      "action_space"
    ],
    "step_async": [
      "self",
      "actions"
    ],
    "reset": [
      "self"
    ],
    "step_wait": [
      "self"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "set_options": [
      "self",
      "options"
    ],
    "close": [
      "self"
    ],
    "render": [
      "self",
      "mode"
    ],
    "get_images": [
      "self"
    ],
    "has_attr": [
      "self",
      "attr_name"
    ],
    "get_attr": [
      "self",
      "attr_name",
      "indices"
    ],
    "set_attr": [
      "self",
      "attr_name",
      "value",
      "indices"
    ],
    "env_method": [
      "self",
      "method_name"
    ],
    "env_is_wrapped": [
      "self",
      "wrapper_class",
      "indices"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "_get_all_attributes": [
      "self"
    ],
    "getattr_recursive": [
      "self",
      "name"
    ],
    "getattr_depth_check": [
      "self",
      "name",
      "already_found"
    ]
  },
  "CloudpickleWrapper": {
    "__init__": [
      "self",
      "var"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "var"
    ]
  },
  "T": [],
  "IdentityEnv": {
    "__init__": [
      "self",
      "dim",
      "space",
      "ep_length"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "_choose_next_state": [
      "self"
    ],
    "_get_reward": [
      "self",
      "action"
    ],
    "render": [
      "self",
      "mode"
    ]
  },
  "IdentityEnvBox": {
    "__init__": [
      "self",
      "low",
      "high",
      "eps",
      "ep_length"
    ],
    "step": [
      "self",
      "action"
    ],
    "_get_reward": [
      "self",
      "action"
    ]
  },
  "IdentityEnvMultiDiscrete": {
    "__init__": [
      "self",
      "dim",
      "ep_length"
    ]
  },
  "IdentityEnvMultiBinary": {
    "__init__": [
      "self",
      "dim",
      "ep_length"
    ]
  },
  "FakeImageEnv": {
    "__init__": [
      "self",
      "action_dim",
      "screen_height",
      "screen_width",
      "n_channels",
      "discrete",
      "channel_first"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "render": [
      "self",
      "mode"
    ]
  },
  "BitFlippingEnv": {
    "spec": [],
    "__init__": [
      "self",
      "n_bits",
      "continuous",
      "max_steps",
      "discrete_obs_space",
      "image_obs_space",
      "channel_first",
      "render_mode"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "convert_if_needed": [
      "self",
      "state"
    ],
    "convert_to_bit_vector": [
      "self",
      "state",
      "batch_size"
    ],
    "_make_observation_space": [
      "self",
      "discrete_obs_space",
      "image_obs_space",
      "n_bits"
    ],
    "_get_obs": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "compute_reward": [
      "self",
      "achieved_goal",
      "desired_goal",
      "_info"
    ],
    "render": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "SimpleMultiObsEnv": {
    "__init__": [
      "self",
      "num_col",
      "num_row",
      "random_start",
      "discrete_actions",
      "channel_last"
    ],
    "init_state_mapping": [
      "self",
      "num_col",
      "num_row"
    ],
    "get_state_mapping": [
      "self"
    ],
    "init_possible_transitions": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "render": [
      "self",
      "mode"
    ],
    "reset": [
      "self"
    ]
  },
  "RMSpropTFLike": {
    "__init__": [
      "self",
      "params",
      "lr",
      "alpha",
      "eps",
      "weight_decay",
      "momentum",
      "centered"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "GoalSelectionStrategy": {
    "FUTURE": [],
    "FINAL": [],
    "EPISODE": []
  },
  "KEY_TO_GOAL_STRATEGY": [],
  "HerReplayBuffer": {
    "__init__": [
      "self",
      "buffer_size",
      "observation_space",
      "action_space",
      "env",
      "device",
      "n_envs",
      "optimize_memory_usage",
      "handle_timeout_termination",
      "n_sampled_goal",
      "goal_selection_strategy",
      "copy_info_dict"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "set_env": [
      "self",
      "env"
    ],
    "add": [
      "self",
      "obs",
      "next_obs",
      "action",
      "reward",
      "done",
      "infos"
    ],
    "_compute_episode_length": [
      "self",
      "env_idx"
    ],
    "sample": [
      "self",
      "batch_size",
      "env"
    ],
    "_get_real_samples": [
      "self",
      "batch_indices",
      "env_indices",
      "env"
    ],
    "_get_virtual_samples": [
      "self",
      "batch_indices",
      "env_indices",
      "env"
    ],
    "_sample_goals": [
      "self",
      "batch_indices",
      "env_indices"
    ],
    "truncate_last_trajectory": [
      "self"
    ]
  },
  "SelfA2C": [],
  "A2C": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "n_steps",
      "gamma",
      "gae_lambda",
      "ent_coef",
      "vf_coef",
      "max_grad_norm",
      "rms_prop_eps",
      "use_rms_prop",
      "use_sde",
      "sde_sample_freq",
      "rollout_buffer_class",
      "rollout_buffer_kwargs",
      "normalize_advantage",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "train": [
      "self"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ]
  },
  "MlpPolicy": [],
  "CnnPolicy": [],
  "MultiInputPolicy": [],
  "SelfSAC": [],
  "SAC": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "tau",
      "gamma",
      "train_freq",
      "gradient_steps",
      "action_noise",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "ent_coef",
      "target_update_interval",
      "target_entropy",
      "use_sde",
      "sde_sample_freq",
      "use_sde_at_warmup",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "_create_aliases": [
      "self"
    ],
    "train": [
      "self",
      "gradient_steps",
      "batch_size"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_excluded_save_params": [
      "self"
    ],
    "_get_torch_save_params": [
      "self"
    ]
  },
  "LOG_STD_MAX": [],
  "LOG_STD_MIN": [],
  "Actor": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "net_arch",
      "features_extractor",
      "features_dim",
      "activation_fn",
      "use_sde",
      "log_std_init",
      "full_std",
      "use_expln",
      "clip_mean",
      "normalize_images"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "get_std": [
      "self"
    ],
    "reset_noise": [
      "self",
      "batch_size"
    ],
    "get_action_dist_params": [
      "self",
      "obs"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "action_log_prob": [
      "self",
      "obs"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ]
  },
  "SACPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "use_sde",
      "log_std_init",
      "use_expln",
      "clip_mean",
      "features_extractor_class",
      "features_extractor_kwargs",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs",
      "n_critics",
      "share_features_extractor"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "reset_noise": [
      "self",
      "batch_size"
    ],
    "make_actor": [
      "self",
      "features_extractor"
    ],
    "make_critic": [
      "self",
      "features_extractor"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "set_training_mode": [
      "self",
      "mode"
    ]
  },
  "QNetwork": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "features_extractor",
      "features_dim",
      "net_arch",
      "activation_fn",
      "normalize_images"
    ],
    "forward": [
      "self",
      "obs"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "_get_constructor_parameters": [
      "self"
    ]
  },
  "DQNPolicy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "features_extractor_class",
      "features_extractor_kwargs",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "make_q_net": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "deterministic"
    ],
    "_predict": [
      "self",
      "obs",
      "deterministic"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "set_training_mode": [
      "self",
      "mode"
    ]
  },
  "SelfDQN": [],
  "DQN": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "tau",
      "gamma",
      "train_freq",
      "gradient_steps",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "target_update_interval",
      "exploration_fraction",
      "exploration_initial_eps",
      "exploration_final_eps",
      "max_grad_norm",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "_create_aliases": [
      "self"
    ],
    "_on_step": [
      "self"
    ],
    "train": [
      "self",
      "gradient_steps",
      "batch_size"
    ],
    "predict": [
      "self",
      "observation",
      "state",
      "episode_start",
      "deterministic"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_excluded_save_params": [
      "self"
    ],
    "_get_torch_save_params": [
      "self"
    ]
  },
  "TD3Policy": {
    "__init__": [
      "self",
      "observation_space",
      "action_space",
      "lr_schedule",
      "net_arch",
      "activation_fn",
      "features_extractor_class",
      "features_extractor_kwargs",
      "normalize_images",
      "optimizer_class",
      "optimizer_kwargs",
      "n_critics",
      "share_features_extractor"
    ],
    "_build": [
      "self",
      "lr_schedule"
    ],
    "_get_constructor_parameters": [
      "self"
    ],
    "make_actor": [
      "self",
      "features_extractor"
    ],
    "make_critic": [
      "self",
      "features_extractor"
    ],
    "forward": [
      "self",
      "observation",
      "deterministic"
    ],
    "_predict": [
      "self",
      "observation",
      "deterministic"
    ],
    "set_training_mode": [
      "self",
      "mode"
    ]
  },
  "SelfTD3": [],
  "TD3": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "tau",
      "gamma",
      "train_freq",
      "gradient_steps",
      "action_noise",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "policy_delay",
      "target_policy_noise",
      "target_noise_clip",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "_create_aliases": [
      "self"
    ],
    "train": [
      "self",
      "gradient_steps",
      "batch_size"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ],
    "_excluded_save_params": [
      "self"
    ],
    "_get_torch_save_params": [
      "self"
    ]
  },
  "SelfPPO": [],
  "PPO": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "n_steps",
      "batch_size",
      "n_epochs",
      "gamma",
      "gae_lambda",
      "clip_range",
      "clip_range_vf",
      "normalize_advantage",
      "ent_coef",
      "vf_coef",
      "max_grad_norm",
      "use_sde",
      "sde_sample_freq",
      "rollout_buffer_class",
      "rollout_buffer_kwargs",
      "target_kl",
      "stats_window_size",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "_setup_model": [
      "self"
    ],
    "train": [
      "self"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ]
  },
  "SelfDDPG": [],
  "DDPG": {
    "__init__": [
      "self",
      "policy",
      "env",
      "learning_rate",
      "buffer_size",
      "learning_starts",
      "batch_size",
      "tau",
      "gamma",
      "train_freq",
      "gradient_steps",
      "action_noise",
      "replay_buffer_class",
      "replay_buffer_kwargs",
      "optimize_memory_usage",
      "n_steps",
      "tensorboard_log",
      "policy_kwargs",
      "verbose",
      "seed",
      "device",
      "_init_setup_model"
    ],
    "learn": [
      "self",
      "total_timesteps",
      "callback",
      "log_interval",
      "tb_log_name",
      "reset_num_timesteps",
      "progress_bar"
    ]
  }
}