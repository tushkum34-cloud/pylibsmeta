{
  "_prepare_model_for_gradient_checkpointing": [
    "model"
  ],
  "_check_config_compatible": [
    "peft_config"
  ],
  "PeftMixedModel": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "peft_config": [
      "self"
    ],
    "active_adapter": [
      "self"
    ],
    "active_adapters": [
      "self"
    ],
    "get_nb_trainable_parameters": [
      "self"
    ],
    "print_trainable_parameters": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "forward": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "disable_adapter": [
      "self"
    ],
    "add_adapter": [
      "self",
      "adapter_name",
      "peft_config",
      "low_cpu_mem_usage"
    ],
    "set_modules_to_save": [
      "self",
      "peft_config",
      "adapter_name"
    ],
    "set_adapter": [
      "self",
      "adapter_name",
      "inference_mode"
    ],
    "delete_adapter": [
      "self",
      "adapter_name"
    ],
    "merge_and_unload": [
      "self"
    ],
    "unload": [
      "self"
    ],
    "get_layer_status": [
      "self"
    ],
    "get_model_status": [
      "self"
    ],
    "_split_kwargs": [
      "cls",
      "kwargs"
    ],
    "_check_new_adapter_config": [
      "self",
      "peft_config",
      "is_trainable"
    ],
    "load_adapter": [
      "self",
      "model_id",
      "adapter_name"
    ],
    "create_or_update_model_card": [
      "self",
      "output_dir"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "safe_serialization",
      "selected_adapters"
    ],
    "from_pretrained": [
      "cls",
      "model",
      "model_id",
      "adapter_name",
      "is_trainable",
      "config"
    ]
  },
  "update_forward_signature": [
    "model"
  ],
  "update_generate_signature": [
    "model"
  ],
  "update_signature": [
    "model",
    "method"
  ],
  "check_if_peft_model": [
    "model_name_or_path"
  ],
  "rescale_adapter_scale": [
    "model",
    "multiplier"
  ],
  "disable_input_dtype_casting": [
    "model",
    "active"
  ],
  "PeftModel": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name",
      "autocast_adapter_dtype",
      "low_cpu_mem_usage"
    ],
    "peft_config": [
      "self",
      "value"
    ],
    "active_adapters": [
      "self"
    ],
    "has_active_enabled_adapter": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory",
      "safe_serialization",
      "selected_adapters",
      "save_embedding_layers",
      "is_main_process",
      "path_initial_model_for_weight_conversion"
    ],
    "from_pretrained": [
      "cls",
      "model",
      "model_id",
      "adapter_name",
      "is_trainable",
      "config",
      "autocast_adapter_dtype",
      "ephemeral_gpu_offload",
      "low_cpu_mem_usage",
      "key_mapping"
    ],
    "_setup_prompt_encoder": [
      "self",
      "adapter_name"
    ],
    "prepare_model_for_gradient_checkpointing": [
      "self",
      "model"
    ],
    "_prepare_model_for_gradient_checkpointing": [
      "self",
      "model"
    ],
    "get_prompt_embedding_to_save": [
      "self",
      "adapter_name"
    ],
    "get_prompt": [
      "self",
      "batch_size",
      "task_ids",
      "max_cache_len"
    ],
    "get_nb_trainable_parameters": [
      "self"
    ],
    "print_trainable_parameters": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "_enable_peft_forward_hooks": [
      "self"
    ],
    "forward": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "_get_base_model_class": [
      "self",
      "is_prompt_tuning"
    ],
    "disable_adapter": [
      "self"
    ],
    "get_base_model": [
      "self"
    ],
    "add_adapter": [
      "self",
      "adapter_name",
      "peft_config",
      "low_cpu_mem_usage"
    ],
    "delete_adapter": [
      "self",
      "adapter_name"
    ],
    "modules_to_save": [
      "self"
    ],
    "get_layer_status": [
      "self"
    ],
    "get_model_status": [
      "self"
    ],
    "_split_kwargs": [
      "cls",
      "kwargs"
    ],
    "_update_offload": [
      "self",
      "offload_index",
      "adapters_weights"
    ],
    "_check_new_adapter_config": [
      "self",
      "peft_config",
      "is_trainable"
    ],
    "load_adapter": [
      "self",
      "model_id",
      "adapter_name",
      "is_trainable",
      "torch_device",
      "autocast_adapter_dtype",
      "ephemeral_gpu_offload",
      "low_cpu_mem_usage",
      "key_mapping"
    ],
    "set_adapter": [
      "self",
      "adapter_name"
    ],
    "set_requires_grad": [
      "self",
      "adapter_names",
      "requires_grad"
    ],
    "base_model_torch_dtype": [
      "self"
    ],
    "active_peft_config": [
      "self"
    ],
    "_get_peft_specific_model_tags": [
      "self"
    ],
    "create_or_update_model_card": [
      "self",
      "output_dir"
    ]
  },
  "PeftModelForSequenceClassification": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "add_adapter": [
      "self",
      "adapter_name",
      "peft_config",
      "low_cpu_mem_usage"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "task_ids"
    ],
    "_prefix_tuning_forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PeftModelForCausalLM": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "task_ids"
    ],
    "_cpt_forward": [
      "self",
      "input_ids",
      "inputs_embeds",
      "peft_config",
      "task_ids",
      "batch_size"
    ],
    "generate": [
      "self"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ]
  },
  "PeftModelForSeq2SeqLM": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "decoder_input_ids",
      "decoder_attention_mask",
      "decoder_inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "task_ids"
    ],
    "generate": [
      "self"
    ],
    "prepare_inputs_for_generation": [
      "self"
    ]
  },
  "PeftModelForTokenClassification": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "add_adapter": [
      "self",
      "adapter_name",
      "peft_config",
      "low_cpu_mem_usage"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "task_ids"
    ],
    "_prefix_tuning_forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "labels",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PeftModelForQuestionAnswering": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "add_adapter": [
      "self",
      "adapter_name",
      "peft_config",
      "low_cpu_mem_usage"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "token_type_ids",
      "position_ids",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "task_ids"
    ],
    "_prefix_tuning_forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "start_positions",
      "end_positions",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "PeftModelForFeatureExtraction": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "forward": [
      "self",
      "input_ids",
      "attention_mask",
      "inputs_embeds",
      "output_attentions",
      "output_hidden_states",
      "return_dict",
      "task_ids"
    ]
  },
  "TunerLayerStatus": {},
  "get_layer_status": [
    "model"
  ],
  "TunerModelStatus": {},
  "get_model_status": [
    "model"
  ],
  "__getattr__": [
    "name"
  ],
  "_BaseAutoPeftModel": {
    "_target_class": [],
    "_target_peft_class": [],
    "__init__": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "adapter_name",
      "is_trainable",
      "config",
      "revision"
    ]
  },
  "AutoPeftModel": {
    "_target_class": [],
    "_target_peft_class": []
  },
  "AutoPeftModelForCausalLM": {
    "_target_class": [],
    "_target_peft_class": []
  },
  "AutoPeftModelForSeq2SeqLM": {
    "_target_class": [],
    "_target_peft_class": []
  },
  "AutoPeftModelForSequenceClassification": {
    "_target_class": [],
    "_target_peft_class": []
  },
  "AutoPeftModelForTokenClassification": {
    "_target_class": [],
    "_target_peft_class": []
  },
  "AutoPeftModelForQuestionAnswering": {
    "_target_class": [],
    "_target_peft_class": []
  },
  "AutoPeftModelForFeatureExtraction": {
    "_target_class": [],
    "_target_peft_class": []
  },
  "__version__": [],
  "__all__": [],
  "is_bnb_available": [],
  "is_bnb_4bit_available": [],
  "is_auto_gptq_available": [],
  "is_gptqmodel_available": [],
  "is_optimum_available": [],
  "is_torch_tpu_available": [
    "check_device"
  ],
  "is_aqlm_available": [],
  "is_auto_awq_available": [],
  "is_eetq_available": [],
  "is_hqq_available": [],
  "is_inc_available": [],
  "is_torchao_available": [],
  "is_xpu_available": [
    "check_device"
  ],
  "is_diffusers_available": [],
  "MIN_EXPECTED_CONFIG_KEYS": [],
  "_check_and_remove_unused_kwargs": [
    "cls",
    "kwargs"
  ],
  "_is_dev_version": [
    "version"
  ],
  "_get_commit_hash": [
    "pkg_name"
  ],
  "PeftConfigMixin": {
    "__post_init__": [
      "self"
    ],
    "_get_peft_version": [],
    "to_dict": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ],
    "from_peft_type": [
      "cls"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path",
      "subfolder"
    ],
    "from_json_file": [
      "cls",
      "path_json_file"
    ],
    "_split_kwargs": [
      "cls",
      "kwargs"
    ],
    "_get_peft_type": [
      "cls",
      "model_id"
    ],
    "check_kwargs": [
      "cls"
    ],
    "is_prompt_learning": [
      "self"
    ],
    "is_adaption_prompt": [
      "self"
    ]
  },
  "PeftConfig": {},
  "PromptLearningConfig": {
    "is_prompt_learning": [
      "self"
    ]
  },
  "get_peft_config": [
    "config_dict"
  ],
  "inject_adapter_in_model": [
    "peft_config",
    "model",
    "adapter_name",
    "low_cpu_mem_usage",
    "state_dict"
  ],
  "get_peft_model": [
    "model",
    "peft_config",
    "adapter_name",
    "mixed",
    "autocast_adapter_dtype",
    "revision",
    "low_cpu_mem_usage"
  ],
  "is_transformers_ge_v5": [],
  "check_deepspeed_zero3_enabled": [],
  "gather_params_ctx": [
    "param",
    "modifier_rank",
    "fwd_module"
  ],
  "dequantize_module_weight": [
    "module"
  ],
  "dequantize_bnb_weight": [
    "weight",
    "state"
  ],
  "get_bnb_param_type": [
    "param"
  ],
  "get_layer_device_map": [
    "model"
  ],
  "map_cache_to_layer_device_map": [
    "model",
    "cache"
  ],
  "init_empty_weights": [
    "include_buffers"
  ],
  "_init_on_device": [
    "device",
    "include_buffers"
  ],
  "_skip_init_on_device": [],
  "skip_init_on_device": [
    "func"
  ],
  "has_valid_embedding_base_layer": [
    "layer"
  ],
  "get_embedding_layer_name": [
    "model",
    "layer",
    "is_embedding_in_target_modules"
  ],
  "get_peft_model_state_dict": [
    "model",
    "state_dict",
    "adapter_name",
    "unwrap_compiled",
    "save_embedding_layers"
  ],
  "_find_mismatched_keys": [
    "model",
    "peft_model_state_dict",
    "ignore_mismatched_sizes"
  ],
  "_insert_adapter_name_into_state_dict": [
    "state_dict",
    "adapter_name",
    "parameter_prefix"
  ],
  "set_peft_model_state_dict": [
    "model",
    "peft_model_state_dict",
    "adapter_name",
    "ignore_mismatched_sizes",
    "low_cpu_mem_usage"
  ],
  "torch_load": [],
  "load_peft_weights": [
    "model_id",
    "device",
    "key_mapping"
  ],
  "IncrementalPCA": {
    "__init__": [
      "self",
      "n_components",
      "copy",
      "batch_size",
      "svd_driver",
      "lowrank",
      "lowrank_q",
      "lowrank_niter",
      "lowrank_seed"
    ],
    "_validate_lowrank_params": [
      "self"
    ],
    "_svd_fn_full": [
      "self",
      "X"
    ],
    "_svd_fn_lowrank": [
      "self",
      "X"
    ],
    "_validate_data": [
      "self",
      "X"
    ],
    "_incremental_mean_and_var": [
      "X",
      "last_mean",
      "last_variance",
      "last_sample_count"
    ],
    "_svd_flip": [
      "u",
      "v",
      "u_based_decision"
    ],
    "fit": [
      "self",
      "X",
      "check_input"
    ],
    "partial_fit": [
      "self",
      "X",
      "check_input"
    ],
    "transform": [
      "self",
      "X"
    ],
    "gen_batches": [
      "n",
      "batch_size",
      "min_batch_size"
    ]
  },
  "CONFIG_KEYS_TO_CHECK": [],
  "_update_scaling": [
    "lora_module",
    "adapter_name",
    "scaling"
  ],
  "_convert_scalings_to_tensor": [
    "model"
  ],
  "_get_padded_linear": [
    "lora_module",
    "target_rank",
    "is_lora_A"
  ],
  "_get_padded_conv2d": [
    "lora_module",
    "target_rank",
    "is_lora_A"
  ],
  "_pad_lora_weights": [
    "model",
    "target_rank"
  ],
  "prepare_model_for_compiled_hotswap": [
    "model"
  ],
  "hotswap_adapter_from_state_dict": [
    "model",
    "state_dict",
    "adapter_name",
    "config",
    "parameter_prefix"
  ],
  "check_hotswap_configs_compatible": [
    "config0",
    "config1"
  ],
  "hotswap_adapter": [
    "model",
    "model_name_or_path",
    "adapter_name",
    "torch_device"
  ],
  "bloom_model_postprocess_past_key_value": [
    "past_key_values"
  ],
  "starcoder_model_postprocess_past_key_value": [
    "past_key_values"
  ],
  "TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING": [],
  "transformers_le_4_53": [],
  "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_BOFT_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_BONE_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_C3A_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_DELORA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_HRA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_LOHA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_LOKR_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_MISS_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_OFT_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_POLY_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_RANDLORA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_ROAD_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_FOURIERFT_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_SHIRA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_VERA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_LNTUNING_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_IA3_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_IA3_FEEDFORWARD_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_VBLORA_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_OSF_TARGET_MODULES_MAPPING": [],
  "TRANSFORMERS_MODELS_TO_WAVEFT_TARGET_MODULES_MAPPING": [],
  "WEIGHTS_NAME": [],
  "SAFETENSORS_WEIGHTS_NAME": [],
  "CONFIG_NAME": [],
  "EMBEDDING_LAYER_NAMES": [],
  "SEQ_CLS_HEAD_NAMES": [],
  "INCLUDE_LINEAR_LAYERS_SHORTHAND": [],
  "TOKENIZER_CONFIG_NAME": [],
  "DUMMY_TARGET_MODULES": [],
  "DUMMY_MODEL_CONFIG": [],
  "MIN_TARGET_MODULES_FOR_OPTIMIZATION": [],
  "mlu_available": [],
  "infer_device": [],
  "prepare_model_for_kbit_training": [
    "model",
    "use_gradient_checkpointing",
    "gradient_checkpointing_kwargs"
  ],
  "shift_tokens_right": [
    "input_ids",
    "pad_token_id",
    "decoder_start_token_id"
  ],
  "AuxiliaryTrainingWrapper": {
    "__init__": [
      "self",
      "module_to_save",
      "adapter_name"
    ],
    "init_modules": [
      "self",
      "adapter_name"
    ],
    "_get_available_adapters": [
      "self"
    ],
    "_error_message_name": [
      "self"
    ],
    "check_module": [
      "self"
    ],
    "disable_adapters": [
      "self"
    ],
    "active_adapter": [
      "self"
    ],
    "active_adapters": [
      "self"
    ],
    "_hasattr_wrapped": [
      "self",
      "name",
      "modules"
    ],
    "_getattr_wrapped": [
      "self",
      "name",
      "modules"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "update": [
      "self",
      "adapter_name"
    ],
    "_create_new_hook": [
      "self",
      "old_hook"
    ],
    "_check_forward_args": [
      "self",
      "x"
    ],
    "_forward_wrapped": [
      "self",
      "x"
    ],
    "_forward_wrapped_mixed_batch": [
      "self",
      "x",
      "active_adapter"
    ],
    "_forward_wrapped_passthrough": [
      "self",
      "x"
    ],
    "_mixed_batch_forward": [
      "self",
      "input"
    ],
    "forward": [
      "self",
      "x"
    ],
    "enable_adapters": [
      "self",
      "enabled"
    ],
    "check_set_adapter": [
      "self",
      "adapter_name"
    ],
    "set_adapter": [
      "self",
      "adapter_names",
      "inference_mode"
    ],
    "delete_adapter": [
      "self",
      "adapter_name",
      "new_active_adapters"
    ],
    "set_requires_grad": [
      "self",
      "adapter_names",
      "requires_grad"
    ],
    "adapter_state_dict": [
      "self",
      "adapter_name"
    ],
    "adapter_state_dict_load_map": [
      "self",
      "adapter_name"
    ],
    "unload_and_optionally_merge_module": [
      "self",
      "merge",
      "safe_merge",
      "adapter_names"
    ]
  },
  "ModulesToSaveWrapper": {
    "__init__": [
      "self",
      "module_to_save",
      "adapter_name",
      "tied_module"
    ],
    "init_modules": [
      "self",
      "adapter_name"
    ],
    "_error_message_name": [
      "self"
    ],
    "_forward_wrapped": [
      "self",
      "x"
    ],
    "_forward_wrapped_mixed_batch": [
      "self",
      "x",
      "active_adapter"
    ],
    "_forward_wrapped_passthrough": [
      "self",
      "x"
    ],
    "_hasattr_wrapped": [
      "self",
      "name",
      "modules"
    ],
    "_getattr_wrapped": [
      "self",
      "name",
      "modules"
    ],
    "update": [
      "self",
      "adapter_name",
      "tied_module"
    ],
    "enable_adapters": [
      "self",
      "enabled"
    ],
    "check_set_adapter": [
      "self",
      "adapter_name"
    ],
    "set_adapter": [
      "self",
      "adapter_names",
      "inference_mode"
    ],
    "delete_adapter": [
      "self",
      "adapter_name",
      "new_active_adapters"
    ],
    "adapter_state_dict_load_map": [
      "self",
      "adapter_name"
    ],
    "adapter_state_dict": [
      "self",
      "adapter_name",
      "state_dict"
    ],
    "unload_and_optionally_merge_module": [
      "self",
      "merge",
      "safe_merge",
      "adapter_names"
    ],
    "_get_available_adapters": [
      "self"
    ]
  },
  "TrainableTokensWrapper": {
    "__init__": [
      "self",
      "module_to_save",
      "adapter_name",
      "token_indices",
      "tied_adapter"
    ],
    "original_module": [
      "self"
    ],
    "init_modules": [
      "self",
      "adapter_name",
      "token_indices",
      "tied_adapter"
    ],
    "_error_message_name": [
      "self"
    ],
    "_hasattr_wrapped": [
      "self",
      "name",
      "modules"
    ],
    "_getattr_wrapped": [
      "self",
      "name",
      "modules"
    ],
    "_forward_wrapped": [
      "self",
      "x"
    ],
    "_forward_wrapped_mixed_batch": [
      "self",
      "x",
      "active_adapter"
    ],
    "_forward_wrapped_passthrough": [
      "self",
      "x"
    ],
    "update": [
      "self",
      "active_adapter"
    ],
    "adapter_state_dict_load_map": [
      "self",
      "adapter_name"
    ],
    "adapter_state_dict": [
      "self",
      "adapter_name",
      "state_dict"
    ],
    "enable_adapters": [
      "self",
      "enabled"
    ],
    "check_set_adapter": [
      "self",
      "adapter_name"
    ],
    "set_adapter": [
      "self",
      "adapter_names",
      "inference_mode"
    ],
    "delete_adapter": [
      "self",
      "adapter_name",
      "new_active_adapters"
    ],
    "unload_and_optionally_merge_module": [
      "self",
      "merge",
      "safe_merge",
      "adapter_names"
    ],
    "_get_available_adapters": [
      "self"
    ]
  },
  "_get_input_embeddings_name": [
    "model",
    "default"
  ],
  "_get_submodules": [
    "model",
    "key"
  ],
  "_get_submodules_with_grandparent": [
    "model",
    "key"
  ],
  "_freeze_adapter": [
    "model",
    "adapter_name"
  ],
  "_set_trainable": [
    "model",
    "adapter_name",
    "module_names",
    "inference_mode",
    "strict_module_check",
    "wrapper_cls",
    "activate_adapter"
  ],
  "_set_adapter": [
    "model",
    "adapter_name",
    "inference_mode"
  ],
  "_prepare_prompt_learning_config": [
    "peft_config",
    "model_config"
  ],
  "_get_no_split_modules": [
    "model"
  ],
  "fsdp_auto_wrap_policy": [
    "model"
  ],
  "transpose": [
    "weight",
    "fan_in_fan_out"
  ],
  "_is_valid_match": [
    "key",
    "target_key"
  ],
  "_get_batch_size": [
    "input_ids",
    "inputs_embeds"
  ],
  "get_quantization_config": [
    "model",
    "method"
  ],
  "get_auto_gptq_quant_linear": [
    "gptq_quantization_config"
  ],
  "get_gptqmodel_quant_linear": [
    "gptq_quantization_config",
    "device_map"
  ],
  "id_tensor_storage": [
    "tensor"
  ],
  "cast_mixed_precision_params": [
    "model",
    "dtype"
  ],
  "str_to_bool": [
    "value"
  ],
  "check_file_exists_on_hf_hub": [
    "repo_id",
    "filename"
  ],
  "match_target_against_key": [
    "target_pattern",
    "key"
  ],
  "get_pattern_key": [
    "pattern_keys",
    "key_to_match"
  ],
  "set_additional_trainable_modules": [
    "model",
    "peft_config",
    "model_config",
    "adapter_name",
    "activate_adapter"
  ],
  "create_attention_mask": [
    "model"
  ],
  "_get_module_names_tied_with_embedding": [
    "model"
  ],
  "NFQuantizer": {
    "__init__": [
      "self",
      "num_bits",
      "device",
      "method",
      "block_size"
    ],
    "create_uniform_map": [
      "symmetric",
      "num_bits"
    ],
    "create_normal_map": [
      "offset",
      "symmetric",
      "num_bits"
    ],
    "quantize_tensor": [
      "self",
      "weight"
    ],
    "dequantize_tensor": [
      "self",
      "qweight",
      "max_abs"
    ],
    "quantize_block": [
      "self",
      "weight"
    ],
    "dequantize_block": [
      "self",
      "qweight",
      "weight_max",
      "weight_shape"
    ]
  },
  "_low_rank_decomposition": [
    "weight",
    "reduced_rank"
  ],
  "loftq_init": [
    "weight",
    "num_bits",
    "reduced_rank",
    "num_iter"
  ],
  "_loftq_init_new": [
    "qweight",
    "weight",
    "num_bits",
    "reduced_rank"
  ],
  "_SafetensorLoader": {
    "__init__": [
      "self",
      "peft_model",
      "model_path"
    ],
    "get_tensor": [
      "self",
      "name"
    ]
  },
  "replace_lora_weights_loftq": [
    "peft_model",
    "model_path",
    "adapter_name",
    "callback"
  ],
  "PeftType": {
    "PROMPT_TUNING": [],
    "MULTITASK_PROMPT_TUNING": [],
    "P_TUNING": [],
    "PREFIX_TUNING": [],
    "LORA": [],
    "ADALORA": [],
    "BOFT": [],
    "ADAPTION_PROMPT": [],
    "IA3": [],
    "LOHA": [],
    "LOKR": [],
    "OFT": [],
    "POLY": [],
    "LN_TUNING": [],
    "VERA": [],
    "FOURIERFT": [],
    "XLORA": [],
    "HRA": [],
    "VBLORA": [],
    "CPT": [],
    "BONE": [],
    "MISS": [],
    "RANDLORA": [],
    "ROAD": [],
    "TRAINABLE_TOKENS": [],
    "SHIRA": [],
    "C3A": [],
    "WAVEFT": [],
    "OSF": [],
    "DELORA": []
  },
  "TaskType": {
    "SEQ_CLS": [],
    "SEQ_2_SEQ_LM": [],
    "CAUSAL_LM": [],
    "TOKEN_CLS": [],
    "QUESTION_ANS": [],
    "FEATURE_EXTRACTION": []
  },
  "register_peft_method": [],
  "reshape_weight_task_tensors": [
    "task_tensors",
    "weights"
  ],
  "magnitude_based_pruning": [
    "tensor",
    "density"
  ],
  "random_pruning": [
    "tensor",
    "density",
    "rescale"
  ],
  "prune": [
    "tensor",
    "density",
    "method",
    "rescale"
  ],
  "calculate_majority_sign_mask": [
    "tensor",
    "method"
  ],
  "disjoint_merge": [
    "task_tensors",
    "majority_sign_mask"
  ],
  "task_arithmetic": [
    "task_tensors",
    "weights"
  ],
  "magnitude_prune": [
    "task_tensors",
    "weights",
    "density"
  ],
  "ties": [
    "task_tensors",
    "weights",
    "density",
    "majority_sign_method"
  ],
  "dare_linear": [
    "task_tensors",
    "weights",
    "density"
  ],
  "dare_ties": [
    "task_tensors",
    "weights",
    "density",
    "majority_sign_method"
  ],
  "PeftWarning": {},
  "create_loraplus_optimizer": [
    "model",
    "optimizer_cls"
  ],
  "LoraFAOptimizer": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "eps",
      "weight_decay",
      "correct_bias"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "create_lorafa_optimizer": [
    "model",
    "r",
    "lora_alpha",
    "lr",
    "weight_decay",
    "use_rslora"
  ],
  "LycorisConfig": {},
  "LycorisLayer": {
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "_available_adapters": [
      "self"
    ],
    "_init_empty_weights": [
      "self",
      "cls"
    ],
    "create_adapter_parameters": [
      "self",
      "adapter_name",
      "r"
    ],
    "_get_delta_activations": [
      "self",
      "adapter_name",
      "x"
    ],
    "get_delta_weight": [
      "self",
      "adapter_name"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "reset_adapter_parameters": [
      "self",
      "adapter_name"
    ],
    "set_scale": [
      "self",
      "adapter",
      "scale"
    ],
    "scale_layer": [
      "self",
      "scale"
    ],
    "unmerge": [
      "self"
    ],
    "unscale_layer": [
      "self",
      "scale"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "alpha"
    ]
  },
  "LycorisTuner": {
    "tuner_layer_cls": [],
    "_create_and_replace": [
      "self",
      "config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "cls",
      "config",
      "adapter_name",
      "target"
    ]
  },
  "BufferDict": {
    "__init__": [
      "self",
      "buffers",
      "persistent"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__setitem__": [
      "self",
      "key",
      "buffer"
    ],
    "__delitem__": [
      "self",
      "key"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "__contains__": [
      "self",
      "key"
    ],
    "clear": [
      "self"
    ],
    "pop": [
      "self",
      "key"
    ],
    "keys": [
      "self"
    ],
    "items": [
      "self"
    ],
    "values": [
      "self"
    ],
    "update": [
      "self",
      "buffers"
    ],
    "extra_repr": [
      "self"
    ],
    "__call__": [
      "self",
      "input"
    ]
  },
  "_torch_supports_dtensor": [],
  "_torch_supports_distributed": [],
  "onload_layer": [
    "layer"
  ],
  "_check_lora_target_modules_mamba": [
    "peft_config",
    "model",
    "target_name"
  ],
  "_get_in_out_features": [
    "module"
  ],
  "BaseTuner": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name",
      "low_cpu_mem_usage",
      "state_dict"
    ],
    "active_adapters": [
      "self"
    ],
    "forward": [
      "self"
    ],
    "_pre_injection_hook": [
      "self",
      "model",
      "config",
      "adapter_name"
    ],
    "_prepare_adapter_config": [
      "self",
      "peft_config",
      "model_config"
    ],
    "_prepare_model": [
      "self",
      "peft_config",
      "model"
    ],
    "_check_target_module_exists": [
      "peft_config",
      "key"
    ],
    "_create_and_replace": [
      "self",
      "peft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key",
      "parameter_name"
    ],
    "_mark_only_adapters_as_trainable": [
      "self",
      "model"
    ],
    "_set_adapter_layers": [
      "self",
      "enabled"
    ],
    "disable_adapter_layers": [
      "self"
    ],
    "enable_adapter_layers": [
      "self"
    ],
    "delete_adapter": [
      "self",
      "adapter_name"
    ],
    "set_requires_grad": [
      "self",
      "adapter_names",
      "requires_grad"
    ],
    "_check_new_adapter_config": [
      "self",
      "config"
    ],
    "_cast_adapter_dtype": [
      "self",
      "adapter_name",
      "autocast_adapter_dtype"
    ],
    "_check_merge_allowed": [
      "self"
    ],
    "_unload_and_optionally_merge": [
      "self",
      "merge",
      "progressbar",
      "safe_merge",
      "adapter_names"
    ],
    "merge_and_unload": [
      "self",
      "progressbar",
      "safe_merge",
      "adapter_names"
    ],
    "unload": [
      "self"
    ],
    "_check_target_module_compatiblity": [
      "self",
      "peft_config",
      "model",
      "target_name"
    ],
    "_create_and_replace_parameter": [
      "self",
      "peft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "inject_adapter": [
      "self",
      "model",
      "adapter_name",
      "autocast_adapter_dtype",
      "low_cpu_mem_usage",
      "state_dict"
    ],
    "_inject_parameters": [
      "self",
      "peft_config",
      "model",
      "adapter_name",
      "low_cpu_mem_usage"
    ],
    "_replace_module": [
      "self",
      "parent",
      "child_name",
      "new_module",
      "child"
    ],
    "merge_adapter": [
      "self",
      "adapter_names",
      "safe_merge"
    ],
    "unmerge_adapter": [
      "self"
    ],
    "set_auxiliary_adapters": [
      "self",
      "adapter_name",
      "inference_mode"
    ],
    "set_adapter": [
      "self",
      "adapter_name",
      "inference_mode"
    ],
    "get_model_config": [
      "model"
    ],
    "_get_tied_target_modules": [
      "self",
      "model"
    ],
    "_get_module_names_tied_with_embedding": [
      "self"
    ],
    "_add_modules_to_tie": [
      "self",
      "peft_config",
      "tied_weight_keys"
    ],
    "_check_tied_modules": [
      "self",
      "model",
      "peft_config"
    ],
    "__getattr__": [
      "self",
      "name"
    ]
  },
  "BaseTunerLayer": {
    "get_base_layer": [
      "self"
    ],
    "_get_embed_scale": [
      "self"
    ],
    "weight": [
      "self"
    ],
    "bias": [
      "self"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "merged": [
      "self"
    ],
    "disable_adapters": [
      "self"
    ],
    "active_adapter": [
      "self"
    ],
    "_get_available_adapters": [
      "self"
    ],
    "active_adapters": [
      "self"
    ],
    "enable_adapters": [
      "self",
      "enabled"
    ],
    "set_adapter": [
      "self",
      "adapter_names",
      "inference_mode"
    ],
    "_all_available_adapter_names": [
      "self"
    ],
    "delete_adapter": [
      "self",
      "adapter_name"
    ],
    "set_requires_grad": [
      "self",
      "adapter_names",
      "requires_grad"
    ],
    "_move_adapter_to_device_of_base_layer": [
      "self",
      "adapter_name",
      "device"
    ],
    "_cast_input_dtype": [
      "self",
      "x",
      "dtype"
    ]
  },
  "_find_minimal_target_modules": [
    "target_modules",
    "other_module_names"
  ],
  "_ExcludedModule": {
    "__bool__": [
      "self"
    ]
  },
  "check_target_module_exists": [
    "config",
    "key"
  ],
  "inspect_matched_modules": [
    "tuner",
    "adapter_name"
  ],
  "_maybe_include_all_linear_layers": [
    "peft_config",
    "model"
  ],
  "check_adapters_to_merge": [
    "module",
    "adapter_names"
  ],
  "clone_module": [
    "module",
    "share_weights"
  ],
  "replicate_layers": [
    "model",
    "layer_map"
  ],
  "set_adapter": [
    "model",
    "adapter_name",
    "inference_mode",
    "layer_cls"
  ],
  "_delete_auxiliary_adapter": [
    "model",
    "adapter_name",
    "new_active_adapters"
  ],
  "delete_adapter": [
    "model",
    "adapter_name",
    "prefix",
    "layer_cls"
  ],
  "cast_adapter_dtype": [
    "model",
    "adapter_name",
    "autocast_adapter_dtype"
  ],
  "set_requires_grad": [
    "model",
    "adapter_names",
    "requires_grad"
  ],
  "CPTConfig": {
    "is_prompt_learning": [],
    "__post_init__": [
      "self"
    ]
  },
  "CPTEmbedding": {
    "__init__": [
      "self",
      "config",
      "word_embeddings"
    ],
    "forward": [
      "self",
      "indices"
    ],
    "set_updated_tokens": [
      "self"
    ],
    "get_epsilon": [
      "self"
    ],
    "get_projection": [
      "self"
    ],
    "calculate_loss": [
      "base_model_output",
      "labels",
      "cpt_type_mask",
      "config"
    ]
  },
  "PrefixTuningConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "PrefixEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "prefix"
    ]
  },
  "LoHaConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "LoHaModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ]
  },
  "LoHaLayer": {
    "adapter_layer_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "_available_adapters": [
      "self"
    ],
    "create_adapter_parameters": [
      "self",
      "adapter_name",
      "r",
      "shape"
    ],
    "reset_adapter_parameters": [
      "self",
      "adapter_name"
    ],
    "reset_adapter_parameters_random": [
      "self",
      "adapter_name"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "alpha",
      "rank_dropout",
      "module_dropout",
      "init_weights",
      "use_effective_conv2d",
      "inference_mode"
    ],
    "get_delta_weight": [
      "self",
      "adapter_name"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Linear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "alpha",
      "rank_dropout",
      "module_dropout",
      "init_weights"
    ],
    "_get_delta_activations": [
      "self",
      "adapter_name",
      "input"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Conv2d": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "alpha",
      "rank_dropout",
      "module_dropout",
      "use_effective_conv2d",
      "init_weights"
    ],
    "_get_delta_activations": [
      "self",
      "adapter_name",
      "input"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Conv1d": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "alpha",
      "rank_dropout",
      "module_dropout",
      "use_effective_conv2d",
      "init_weights"
    ],
    "_get_delta_activations": [
      "self",
      "adapter_name",
      "input"
    ],
    "__repr__": [
      "self"
    ]
  },
  "HadaWeight": {
    "forward": [
      "ctx",
      "w1a",
      "w1b",
      "w2a",
      "w2b",
      "scale"
    ],
    "backward": [
      "ctx",
      "grad_out"
    ]
  },
  "HadaWeightCP": {
    "forward": [
      "ctx",
      "t1",
      "w1a",
      "w1b",
      "t2",
      "w2a",
      "w2b",
      "scale"
    ],
    "backward": [
      "ctx",
      "grad_out"
    ]
  },
  "make_weight": [
    "w1a",
    "w1b",
    "w2a",
    "w2b",
    "scale"
  ],
  "make_weight_cp": [
    "t1",
    "w1a",
    "w1b",
    "t2",
    "w2a",
    "w2b",
    "scale"
  ],
  "MultitaskPromptTuningInit": {
    "TEXT": [],
    "RANDOM": [],
    "AVERAGE_SOURCE_TASKS": [],
    "EXACT_SOURCE_TASK": [],
    "ONLY_SOURCE_SHARED": []
  },
  "MultitaskPromptTuningConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "MultitaskPromptEmbedding": {
    "__init__": [
      "self",
      "config",
      "word_embeddings"
    ],
    "forward": [
      "self",
      "indices",
      "task_ids"
    ]
  },
  "Number": [],
  "TemperatureScaledSoftmax": {
    "__init__": [
      "self",
      "temperature"
    ],
    "forward": [
      "self",
      "logits"
    ]
  },
  "XLoraClassifier": {
    "__init__": [
      "self",
      "model",
      "config",
      "n_classes",
      "n_layers",
      "device"
    ],
    "make_dummy_scalings": [
      "self",
      "input_ids",
      "inputs_embeds"
    ],
    "forward": [
      "self",
      "result",
      "input_ids",
      "inputs_embeds"
    ],
    "_get_bucketed_scalings": [
      "self"
    ],
    "_set_override_scaling_pass_value": [
      "self",
      "value"
    ]
  },
  "XLoraConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "convert_layers_to_xlora": [
    "base",
    "xloramodel",
    "config"
  ],
  "_load_adapter_into_lora_model": [
    "lora_model",
    "adapter_name",
    "model_id",
    "torch_device",
    "ephemeral_gpu_offload",
    "autocast_adapter_dtype",
    "subfolder"
  ],
  "XLoraModel": {
    "__init__": [
      "self",
      "model",
      "config",
      "adapter_name",
      "torch_device",
      "ephemeral_gpu_offload",
      "autocast_adapter_dtype"
    ],
    "_maybe_freeze_all_adapters": [
      "self"
    ],
    "generate": [
      "self"
    ],
    "_enable_peft_forward_hooks": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "_prepare_adapter_config": [
      "peft_config",
      "_model_config"
    ],
    "_mark_only_adapters_as_trainable": [
      "self"
    ],
    "enable_adapter_layers": [
      "self"
    ],
    "disable_adapter_layers": [
      "self"
    ],
    "_create_and_replace": [
      "self",
      "lora_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_check_target_module_exists": [
      "lora_config",
      "key"
    ],
    "forward": [
      "self"
    ],
    "set_topk_lora": [
      "self",
      "value"
    ],
    "set_global_scaling_weight": [
      "self",
      "weight"
    ],
    "set_scaling_pass_value": [
      "self",
      "value"
    ],
    "get_global_scaling_weight": [
      "self"
    ],
    "get_latest_scalings": [
      "self"
    ],
    "get_scalings_log": [
      "self"
    ],
    "enable_scalings_logging": [
      "self"
    ],
    "disable_scalings_logging": [
      "self"
    ],
    "clear_scalings_log": [
      "self"
    ],
    "get_bucketed_scalings_log": [
      "self"
    ]
  },
  "XLoraLayer": {
    "__init__": [
      "self",
      "model",
      "target",
      "target_forward",
      "layer_number",
      "config"
    ],
    "apply_scalings_to_x": [
      "x",
      "scalings_layer",
      "adapter"
    ],
    "get_maybe_topk_scalings": [
      "self",
      "scalings"
    ]
  },
  "XLoraLinearLayer": {
    "__init__": [
      "self",
      "model",
      "target",
      "target_forward",
      "layer_number",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "XLoraEmbeddingLayer": {
    "__init__": [
      "self",
      "model",
      "target",
      "target_forward",
      "layer_number",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "XLoraConv2dLayer": {
    "__init__": [
      "self",
      "model",
      "target",
      "target_forward",
      "layer_number",
      "config"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "LNTuningConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "LNTuningModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "peft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "self",
      "peft_config",
      "target",
      "adapter_name"
    ],
    "_unloading_checks": [
      "self",
      "adapter_names"
    ],
    "_unload_and_optionally_merge": [
      "self",
      "merge",
      "progressbar",
      "safe_merge",
      "adapter_names"
    ],
    "_cast_adapter_dtype": [
      "self",
      "adapter_name",
      "autocast_adapter_dtype"
    ]
  },
  "LNTuningLayer": {
    "adapter_layer_names": [],
    "__init__": [
      "self",
      "base_layer",
      "adapter_name"
    ],
    "update_layer": [
      "self",
      "layer",
      "adapter_name",
      "inference_mode"
    ],
    "enable_adapters": [
      "self",
      "enabled"
    ],
    "merge": [
      "self",
      "adapter_names",
      "safe_merge"
    ],
    "unmerge": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "FourierFTConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "FourierFTModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "fourierft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "fourierft_config",
      "adapter_name",
      "target"
    ]
  },
  "FourierFTLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "n_frequency",
      "scaling",
      "init_weights",
      "random_loc_seed",
      "inference_mode"
    ],
    "reset_fourier_parameters": [
      "self",
      "adapter_name"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ]
  },
  "FourierFTLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "n_frequency",
      "scaling",
      "fan_in_fan_out",
      "init_weights",
      "random_loc_seed"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "RoadVariant": [],
  "RoadConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_adapter_names_pre_forward_hook": [
    "target",
    "args",
    "kwargs",
    "adapter_names"
  ],
  "RoadModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "road_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "road_config",
      "adapter_name",
      "target"
    ],
    "_enable_peft_forward_hooks": [
      "self"
    ]
  },
  "RoadLayer": {
    "__init__": [
      "self",
      "base_layer",
      "ephemeral_gpu_offload"
    ],
    "_available_adapters": [
      "self"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "variant",
      "group_size",
      "init_weights",
      "inference_mode"
    ],
    "reset_parameters": [
      "self",
      "adapter_name",
      "init_weights"
    ]
  },
  "_get_delta_weight": [
    "variant",
    "group_size",
    "road_theta",
    "road_alpha"
  ],
  "_prepare_cols": [
    "variant",
    "group_size",
    "road_theta",
    "road_alpha"
  ],
  "_apply_road": [
    "variant",
    "group_size",
    "road_theta",
    "road_alpha",
    "x"
  ],
  "dispatch_default": [
    "target",
    "adapter_name",
    "road_config"
  ],
  "get_circulant_fast": [
    "w"
  ],
  "BlockCircularConvolution": {
    "forward": [
      "ctx",
      "x",
      "w"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "C3AConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "C3AModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "c3a_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "c3a_config",
      "adapter_name",
      "target"
    ]
  },
  "C3ALayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "block_size",
      "init_weights",
      "inference_mode"
    ],
    "reset_c3a_parameters": [
      "self",
      "adapter_name",
      "init_weights"
    ]
  },
  "C3ALinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "block_size",
      "init_weights"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "COMPATIBLE_TUNER_TYPES": [],
  "PREFIXES": [],
  "Configs": [],
  "Layers": [],
  "MixedModel": {
    "__init__": [
      "self",
      "model",
      "config",
      "adapter_name"
    ],
    "_check_new_adapter_config": [
      "self",
      "config"
    ],
    "_create_and_replace": [
      "self",
      "config"
    ],
    "_replace_module": [
      "self",
      "parent",
      "child_name",
      "new_module",
      "child"
    ],
    "_mark_only_adapters_as_trainable": [
      "self",
      "model"
    ],
    "_create_new_module": [
      "config",
      "adapter_name",
      "target"
    ],
    "set_adapter": [
      "self",
      "adapter_name",
      "inference_mode"
    ],
    "_prepare_adapter_config": [
      "peft_config",
      "model_config"
    ],
    "_unload_and_optionally_merge": [
      "self",
      "merge",
      "progressbar",
      "safe_merge",
      "adapter_names"
    ],
    "add_weighted_adapter": [
      "self"
    ],
    "delete_adapter": [
      "self",
      "adapter_name"
    ],
    "generate": [
      "self"
    ]
  },
  "AwqLoraLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "lora_bias"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "dispatch_awq": [
    "target",
    "adapter_name"
  ],
  "AqlmLoraLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "lora_bias"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "dispatch_aqlm": [
    "target",
    "adapter_name"
  ],
  "CordaEigens": {},
  "target_modules": [
    "model",
    "config"
  ],
  "get_model_device": [
    "model"
  ],
  "preprocess_corda": [
    "model",
    "lora_config",
    "run_model",
    "hooked_model"
  ],
  "calib_cov_distribution": [
    "model",
    "config",
    "run_model",
    "hooked_model",
    "covariance_file"
  ],
  "collect_eigens": [
    "model",
    "config",
    "verbose"
  ],
  "collect_eigens_for_layer": [
    "linear",
    "config"
  ],
  "crop_corda_eigens": [
    "model",
    "config"
  ],
  "LoraParallelLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "backend",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out",
      "is_target_conv_1d_layer",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "lora_bias"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "init_method",
      "input_is_parallel",
      "gather_output",
      "inference_mode"
    ],
    "forward": [
      "self",
      "x"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "__repr__": [
      "self"
    ]
  },
  "dispatch_megatron": [
    "target",
    "adapter_name",
    "lora_config"
  ],
  "TorchaoLoraLinear": {
    "__init__": [
      "self"
    ],
    "_check_dtype_supported": [
      "self"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "dispatch_torchao": [
    "target",
    "adapter_name",
    "lora_config"
  ],
  "DoraLinearLayer": {
    "__init__": [
      "self",
      "fan_in_fan_out"
    ],
    "get_weight_norm": [
      "self",
      "weight",
      "lora_weight",
      "scaling"
    ],
    "update_layer": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "DoraEmbeddingLayer": {
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_DoraConvNdLayer": {
    "get_weight_norm": [
      "self",
      "weight",
      "lora_weight",
      "scaling"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "DoraConv1dLayer": {
    "__init__": [
      "self",
      "fan_in_fan_out"
    ]
  },
  "DoraConv2dLayer": {
    "__init__": [
      "self",
      "fan_in_fan_out"
    ]
  },
  "DoraConv3dLayer": {
    "__init__": [
      "self",
      "fan_in_fan_out"
    ]
  },
  "dispatch_inc": [
    "target",
    "adapter_name"
  ],
  "VARIANT_KWARG_KEYS": [],
  "dispatch_eetq": [
    "target",
    "adapter_name"
  ],
  "UNSUPPORTED_LORA_MODULES": [],
  "_Hook": {
    "__init__": [
      "self",
      "name",
      "prepare_layer_inputs_fn",
      "gather_distributed_inputs"
    ],
    "_prepare_layer_inputs_fn_default": [
      "layer_input",
      "model_input",
      "layer_name"
    ],
    "prepare_layer_inputs": [
      "self",
      "layer_input"
    ],
    "gather_layer_inputs": [
      "self",
      "layer_input"
    ]
  },
  "SVDHook": {
    "__init__": [
      "self",
      "n_components",
      "sim_thresh"
    ],
    "__call__": [
      "self",
      "model",
      "input",
      "output"
    ]
  },
  "HashHook": {
    "__init__": [
      "self"
    ],
    "hash_fn": [
      "tensor"
    ],
    "__call__": [
      "self",
      "model",
      "input",
      "output"
    ]
  },
  "find_equal_values": [
    "dictionary"
  ],
  "get_device_with_meta_params": [
    "model"
  ],
  "move_inputs_to_device": [
    "inputs",
    "device"
  ],
  "prepare_model_inputs_fn_language_modeling": [
    "model_input",
    "peft_config"
  ],
  "prepare_layer_inputs_fn_language_modeling": [
    "layer_input",
    "model_input",
    "layer_name"
  ],
  "forward_fn_dict": [
    "model",
    "inputs"
  ],
  "_get_eva_state_dict": [
    "model",
    "dataloader",
    "peft_config",
    "target_module_check_fn",
    "forward_fn",
    "prepare_model_inputs_fn",
    "prepare_layer_inputs_fn",
    "gather_distributed_inputs",
    "show_progress_bar"
  ],
  "_load_eva_state_dict": [
    "model",
    "eva_state_dict",
    "adapter_name"
  ],
  "get_eva_state_dict": [
    "model",
    "dataloader",
    "peft_config",
    "forward_fn",
    "prepare_model_inputs_fn",
    "prepare_layer_inputs_fn",
    "adapter_name",
    "gather_distributed_inputs",
    "show_progress_bar"
  ],
  "initialize_lora_eva_weights": [
    "model",
    "dataloader",
    "eva_state_dict",
    "forward_fn",
    "prepare_model_inputs_fn",
    "prepare_layer_inputs_fn",
    "adapter_name",
    "gather_distributed_inputs",
    "show_progress_bar"
  ],
  "dispatch_hqq": [
    "target",
    "adapter_name"
  ],
  "LoraRuntimeConfig": {},
  "LoftQConfig": {},
  "ArrowConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "EvaConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "CordaConfig": {},
  "LoraConfig": {
    "to_dict": [
      "self"
    ],
    "__post_init__": [
      "self"
    ],
    "_register_custom_module": [
      "self",
      "mapping"
    ]
  },
  "_alora_offsets_pre_forward_hook": [
    "target",
    "args",
    "kwargs",
    "alora_offsets"
  ],
  "LoraModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_prepare_model": [
      "self",
      "peft_config",
      "model"
    ],
    "_create_and_replace": [
      "self",
      "lora_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_replace_module": [
      "self",
      "parent",
      "child_name",
      "new_module",
      "child"
    ],
    "_create_new_module": [
      "lora_config",
      "adapter_name",
      "target"
    ],
    "_enable_peft_forward_hooks": [
      "self"
    ],
    "_check_merge_allowed": [
      "self"
    ],
    "_prepare_adapter_config": [
      "self",
      "peft_config",
      "model_config"
    ],
    "_check_add_weighted_adapter": [
      "self",
      "adapters",
      "combination_type",
      "svd_rank"
    ],
    "add_weighted_adapter": [
      "self",
      "adapters",
      "weights",
      "adapter_name",
      "combination_type",
      "svd_rank",
      "svd_clamp",
      "svd_full_matrices",
      "svd_driver",
      "density",
      "majority_sign_method"
    ],
    "_svd_generalized_task_arithmetic_weighted_adapter": [
      "self",
      "combination_type",
      "adapters",
      "weights",
      "new_rank",
      "target",
      "target_lora_A",
      "target_lora_B",
      "density",
      "majority_sign_method",
      "clamp",
      "full_matrices",
      "driver"
    ],
    "_generalized_task_arithmetic_weighted_adapter": [
      "self",
      "combination_type",
      "adapters",
      "weights",
      "target",
      "density",
      "majority_sign_method"
    ],
    "subtract_mutated_init": [
      "self",
      "output_state_dict",
      "adapter_name",
      "kwargs"
    ],
    "_add_modules_to_tie": [
      "self",
      "peft_config",
      "tied_weight_keys"
    ]
  },
  "LoraVariant": {
    "init": [
      "module",
      "adapter_name"
    ],
    "merge_safe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "merge_unsafe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "unmerge": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "forward": [
      "module",
      "active_adapter",
      "x",
      "result"
    ]
  },
  "LoraLayer": {
    "__init__": [
      "self",
      "base_layer",
      "ephemeral_gpu_offload"
    ],
    "resolve_lora_variant": [
      "self"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "use_alora",
      "use_qalora",
      "lora_bias",
      "arrow_config",
      "qalora_group_size",
      "inference_mode"
    ],
    "reset_lora_parameters": [
      "self",
      "adapter_name",
      "init_lora_weights"
    ],
    "olora_init": [
      "self",
      "adapter_name"
    ],
    "pissa_init": [
      "self",
      "adapter_name",
      "init_lora_weights"
    ],
    "corda_init": [
      "self",
      "adapter_name",
      "init_lora_weights"
    ],
    "loftq_init": [
      "self",
      "adapter_name"
    ],
    "orthogonal_init": [
      "self",
      "adapter_name"
    ],
    "_cache_store": [
      "self",
      "key",
      "value"
    ],
    "_cache_pop": [
      "self",
      "key"
    ],
    "set_scale": [
      "self",
      "adapter",
      "scale"
    ],
    "scale_layer": [
      "self",
      "scale"
    ],
    "unscale_layer": [
      "self",
      "scale"
    ],
    "_check_forward_args": [
      "self",
      "x"
    ],
    "_mixed_batch_forward": [
      "self",
      "x"
    ]
  },
  "Embedding": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "arrow_config",
      "lora_bias"
    ],
    "resolve_lora_variant": [
      "self"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "lora_bias",
      "arrow_config",
      "inference_mode"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "_mixed_batch_forward": [
      "self",
      "x"
    ],
    "_embed": [
      "self",
      "input",
      "weight"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_ConvNd": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "arrow_config",
      "lora_bias"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "lora_bias",
      "arrow_config",
      "inference_mode"
    ],
    "_get_dora_factor_view": [
      "self"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "Conv3d": {
    "__init__": [
      "self"
    ],
    "resolve_lora_variant": [
      "self"
    ]
  },
  "MultiheadAttention": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora"
    ],
    "embed_dim": [
      "self"
    ],
    "kdim": [
      "self"
    ],
    "vdim": [
      "self"
    ],
    "_qkv_same_embed_dim": [
      "self"
    ],
    "num_heads": [
      "self"
    ],
    "dropout": [
      "self"
    ],
    "batch_first": [
      "self"
    ],
    "head_dim": [
      "self"
    ],
    "in_proj_weight": [
      "self"
    ],
    "in_proj_bias": [
      "self"
    ],
    "out_proj": [
      "self"
    ],
    "bias_k": [
      "self"
    ],
    "bias_v": [
      "self"
    ],
    "merge_masks": [
      "self"
    ],
    "add_zero_attn": [
      "self"
    ],
    "update_layer": [
      "self"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "unload_and_optionally_merge_module": [
      "self",
      "merge",
      "safe_merge",
      "adapter_names"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "_check_forward_args": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "query"
    ],
    "_restore_weights": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "named_modules": [
      "self"
    ],
    "__repr__": [
      "self"
    ]
  },
  "_LoraParameterProxy": {
    "__init__": [
      "self",
      "delta_weight"
    ],
    "forward": [
      "self",
      "W"
    ]
  },
  "_register_parameter_or_buffer": [
    "module",
    "name",
    "X"
  ],
  "ParamWrapper": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "parameter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out",
      "is_target_conv_1d_layer",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "lora_bias"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "use_qalora",
      "lora_bias",
      "qalora_group_size",
      "inference_mode"
    ],
    "_move_adapter_to_device_of_base_layer": [
      "self",
      "adapter_name",
      "device"
    ],
    "get_param": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter_name"
    ],
    "_activate_lora": [
      "self",
      "active_adapters"
    ],
    "_remove_parametrizations": [
      "self"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "_check_forward_args": [
      "self",
      "x"
    ],
    "unload_and_optionally_merge_module": [
      "self",
      "merge",
      "safe_merge",
      "adapter_names"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "TASK_ADAPTER_PREFIX": [],
  "GKS_ADAPTER_PREFIX": [],
  "ArrowLoraLinearLayer": {
    "__init__": [
      "self",
      "in_features",
      "arrow_config"
    ],
    "on_adapter_change": [
      "self",
      "lora_A",
      "lora_B"
    ],
    "top_right_singular_vec_from_BA": [
      "self",
      "A",
      "B",
      "iters",
      "eps"
    ],
    "build_prototypes": [
      "self",
      "lora_A",
      "lora_B"
    ],
    "gen_know_sub": [
      "self",
      "lora_A",
      "lora_B"
    ],
    "_cast_input_dtype": [
      "self",
      "x",
      "dtype"
    ],
    "forward": [
      "self",
      "x",
      "lora_A",
      "lora_B",
      "dropout",
      "scaling"
    ]
  },
  "check_loaded_lora_compatibility_arrow": [
    "model",
    "adapter_names"
  ],
  "ensure_adapters_target_linear_layers_only": [
    "model",
    "adapter_names"
  ],
  "_resolve_adapter_source": [
    "path"
  ],
  "create_arrow_model": [
    "base_model",
    "task_specific_adapter_paths",
    "arrow_config",
    "general_adapter_paths"
  ],
  "ArrowLinearVariant": {
    "init": [
      "module",
      "adapter_name"
    ],
    "forward": [
      "module"
    ],
    "merge_safe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "merge_unsafe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "unmerge": [
      "module",
      "active_adapter",
      "orig_weight"
    ]
  },
  "DoraLinearVariant": {
    "init": [
      "module",
      "adapter_name"
    ],
    "merge_safe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "merge_unsafe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "unmerge": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "forward": [
      "module",
      "active_adapter",
      "x",
      "result"
    ]
  },
  "DoraEmbeddingVariant": {
    "init": [
      "module",
      "adapter_name"
    ],
    "merge_safe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "merge_unsafe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "unmerge": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "forward": [
      "module",
      "active_adapter",
      "x",
      "result"
    ]
  },
  "_DoraConvNdVariant": {
    "init_convd_variant": [
      "module",
      "adapter_name",
      "dora_layer"
    ],
    "merge_safe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "merge_unsafe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "unmerge": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "forward": [
      "module",
      "active_adapter",
      "x",
      "result"
    ]
  },
  "DoraConv1dVariant": {
    "init": [
      "module",
      "adapter_name"
    ]
  },
  "DoraConv2dVariant": {
    "init": [
      "module",
      "adapter_name"
    ]
  },
  "DoraConv3dVariant": {
    "init": [
      "module",
      "adapter_name"
    ]
  },
  "QALoraLinearVariant": {
    "init": [
      "module",
      "adapter_name"
    ],
    "get_delta_weight": [
      "module",
      "active_adapter"
    ],
    "merge_safe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "merge_unsafe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "unmerge": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "forward": [
      "module",
      "active_adapter",
      "x",
      "result"
    ]
  },
  "ALoraLinearVariant": {
    "init": [
      "module",
      "adapter_name"
    ],
    "merge_safe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "merge_unsafe": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "unmerge": [
      "module",
      "active_adapter",
      "orig_weight"
    ],
    "forward": [
      "module",
      "active_adapter",
      "x",
      "result"
    ]
  },
  "calculate_alora_offsets": [
    "peft_config",
    "active_adapter",
    "input_ids",
    "adapter_names"
  ],
  "is_alora_relevant_in_batch": [
    "model",
    "adapter_names"
  ],
  "get_alora_offsets_for_forward": [
    "model",
    "input_ids",
    "inputs_embeds"
  ],
  "get_alora_offsets_for_generate": [
    "model"
  ],
  "GPTQLoraLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "use_rslora",
      "use_dora",
      "use_qalora",
      "lora_bias",
      "qalora_group_size"
    ],
    "resolve_lora_variant": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "dispatch_gptq": [
    "target",
    "adapter_name"
  ],
  "AwqOFTLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "oft_block_size",
      "module_dropout",
      "coft",
      "eps",
      "block_share",
      "fan_in_fan_out",
      "init_weights",
      "use_cayley_neumann",
      "num_cayley_neumann_terms"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "AqlmOFTLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "oft_block_size",
      "module_dropout",
      "init_weights",
      "coft",
      "eps",
      "block_share",
      "fan_in_fan_out",
      "use_cayley_neumann",
      "num_cayley_neumann_terms"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "OFTConfig": {
    "__post_init__": [
      "self"
    ],
    "check_kwargs": [
      "cls"
    ]
  },
  "OFTModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "oft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "oft_config",
      "adapter_name",
      "target"
    ],
    "_check_merge_allowed": [
      "self"
    ]
  },
  "MultiplicativeDropoutLayer": {
    "__init__": [
      "self",
      "p"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "OFTRotationModule": {
    "__init__": [
      "self",
      "r",
      "n_elements",
      "block_size",
      "in_features",
      "coft",
      "eps",
      "block_share",
      "kernel_size",
      "use_cayley_neumann",
      "num_cayley_neumann_terms"
    ],
    "_pytorch_skew_symmetric": [
      "self",
      "vec",
      "block_size"
    ],
    "_pytorch_skew_symmetric_inv": [
      "self",
      "matrix",
      "block_size"
    ],
    "_cayley_batch": [
      "self",
      "Q",
      "block_size",
      "use_cayley_neumann",
      "num_neumann_terms"
    ],
    "_project_batch": [
      "self",
      "Q",
      "eps"
    ],
    "_block_diagonal": [
      "self",
      "oft_R",
      "rank"
    ],
    "_unfold": [
      "self",
      "x"
    ],
    "_fold": [
      "self",
      "x_unfolded",
      "orig_shape"
    ],
    "forward": [
      "self",
      "x"
    ],
    "get_weight": [
      "self"
    ]
  },
  "OFTLayer": {
    "__init__": [
      "self",
      "base_layer"
    ],
    "_available_adapters": [
      "self"
    ],
    "set_scale": [
      "self",
      "adapter",
      "scale"
    ],
    "scale_layer": [
      "self",
      "scale"
    ],
    "unscale_layer": [
      "self",
      "scale"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "oft_block_size",
      "module_dropout",
      "coft",
      "eps",
      "block_share",
      "init_weights",
      "use_cayley_neumann",
      "num_cayley_neumann_terms",
      "inference_mode"
    ],
    "reset_oft_parameters": [
      "self",
      "adapter_name",
      "init_weights"
    ],
    "adjust_oft_parameters": [
      "self",
      "in_features",
      "params"
    ]
  },
  "GPTQOFTLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "oft_block_size",
      "module_dropout",
      "coft",
      "eps",
      "block_share",
      "use_cayley_neumann",
      "num_cayley_neumann_terms",
      "fan_in_fan_out",
      "init_weights"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "PromptTuningInit": {
    "TEXT": [],
    "SAMPLE_VOCAB": [],
    "RANDOM": []
  },
  "PromptTuningConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "PromptEmbedding": {
    "__init__": [
      "self",
      "config",
      "word_embeddings"
    ],
    "forward": [
      "self",
      "indices"
    ]
  },
  "IA3Config": {
    "__post_init__": [
      "self"
    ]
  },
  "IA3Model": {
    "tuner_layer_cls": [],
    "_create_new_module": [
      "ia3_config",
      "adapter_name",
      "target"
    ],
    "_create_and_replace": [
      "self",
      "ia3_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_check_target_module_feedforward": [
      "ia3_config",
      "key"
    ],
    "_prepare_adapter_config": [
      "peft_config",
      "model_config"
    ],
    "_unload_and_optionally_merge": [
      "self"
    ],
    "_check_add_weighted_adapter": [
      "self",
      "adapters"
    ],
    "add_weighted_adapter": [
      "self",
      "adapters",
      "weights",
      "adapter_name"
    ]
  },
  "IA3Layer": {
    "adapter_layer_names": [],
    "__init__": [
      "self",
      "base_layer",
      "is_feedforward"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "init_ia3_weights",
      "inference_mode"
    ],
    "reset_ia3_parameters": [
      "self",
      "adapter_name"
    ]
  },
  "RandLoraConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "_kaiming_init": [
    "tensor_or_shape",
    "generator"
  ],
  "RandLoraModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_find_dim": [
      "self",
      "config"
    ],
    "_init_randlora_A_randlora_B_sparse": [
      "self",
      "config",
      "adapter_name",
      "sparsity"
    ],
    "_init_randlora_A_randlora_B": [
      "self",
      "config",
      "adapter_name"
    ],
    "_pre_injection_hook": [
      "self",
      "model",
      "config",
      "adapter_name"
    ],
    "_check_new_adapter_config": [
      "self",
      "config"
    ],
    "_create_and_replace": [
      "self",
      "randlora_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "randlora_config",
      "randlora_A",
      "randlora_B",
      "adapter_name",
      "target"
    ]
  },
  "UniqueBaseGrad": {
    "forward": [
      "ctx",
      "randlora_A",
      "randlora_lambda",
      "randlora_gamma"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "RandLoraLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "merged": [
      "self"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "randlora_A",
      "randlora_B",
      "r",
      "randlora_alpha",
      "randlora_dropout",
      "init_weights",
      "inference_mode"
    ],
    "reset_randlora_parameters": [
      "self",
      "adapter_name"
    ]
  },
  "PromptEncoderReparameterizationType": {
    "MLP": [],
    "LSTM": []
  },
  "PromptEncoderConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "PromptEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "forward": [
      "self",
      "indices"
    ]
  },
  "_wait_if_async": [
    "tensor"
  ],
  "decompose_weight_matrix": [
    "weight",
    "top_k"
  ],
  "reconstruct_weight_matrix": [
    "svd_dict"
  ],
  "project_gradient_to_orthogonal_space": [
    "svd_dict"
  ],
  "OSFConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "OSFModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "__init__": [
      "self",
      "model",
      "config",
      "adapter_name",
      "low_cpu_mem_usage",
      "state_dict"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "_prepare_adapter_config": [
      "self",
      "peft_config",
      "model_config"
    ],
    "_create_and_replace": [
      "self",
      "osf_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_mark_only_adapters_as_trainable": [
      "self",
      "model"
    ],
    "_cast_adapter_dtype": [
      "self",
      "adapter_name",
      "autocast_adapter_dtype"
    ],
    "unmerge_adapter": [
      "self"
    ]
  },
  "OSFLayer": {
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "effective_rank"
    ],
    "_attach_hooks": [
      "self",
      "adapter_name"
    ],
    "_detach_hooks": [
      "self"
    ],
    "_reconstruct_weight": [
      "self",
      "adapter_name"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "__del__": [
      "self"
    ]
  },
  "MissConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "MissModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "miss_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "miss_config",
      "adapter_name",
      "target"
    ]
  },
  "MissLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "mini_r",
      "miss_dropout",
      "init_weights",
      "inference_mode"
    ],
    "reset_miss_parameters": [
      "self",
      "adapter_name",
      "r"
    ],
    "reset_bat_parameters": [
      "self",
      "adapter_name",
      "r"
    ],
    "reset_mini_parameters": [
      "self",
      "adapter_name",
      "r",
      "mini_r"
    ],
    "reset_miss_parameters_random": [
      "self",
      "adapter_name"
    ],
    "scale_layer": [
      "self",
      "scale"
    ],
    "unscale_layer": [
      "self",
      "scale"
    ]
  },
  "MissLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "mini_r",
      "miss_dropout",
      "init_weights"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter",
      "orig_weight",
      "re"
    ],
    "get_delta_weight_miss": [
      "self",
      "adapter",
      "orig_weight",
      "re"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "VeraConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "VeraModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_find_dim": [
      "self",
      "config"
    ],
    "_init_vera_A_vera_B": [
      "self",
      "config",
      "adapter_name"
    ],
    "_pre_injection_hook": [
      "self",
      "model",
      "config",
      "adapter_name"
    ],
    "_check_new_adapter_config": [
      "self",
      "config"
    ],
    "_create_and_replace": [
      "self",
      "vera_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "vera_config",
      "vera_A",
      "vera_B",
      "adapter_name",
      "target"
    ]
  },
  "VeraLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "merged": [
      "self"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "vera_A",
      "vera_B",
      "r",
      "vera_dropout",
      "init_weights",
      "d_initial",
      "inference_mode"
    ],
    "reset_vera_parameters": [
      "self",
      "adapter_name",
      "d_initial"
    ]
  },
  "PolyConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "PolyModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "poly_config",
      "adapter_name",
      "target",
      "target_name",
      "parent"
    ],
    "_create_new_module": [
      "poly_config",
      "adapter_name",
      "target"
    ],
    "_register_pre_hooks": [
      "self",
      "task_ids"
    ],
    "_manage_pre_hooks": [
      "self",
      "task_ids"
    ],
    "forward": [
      "self"
    ],
    "generate": [
      "self"
    ]
  },
  "PolyLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "poly_config",
      "inference_mode"
    ],
    "reset_poly_parameters": [
      "self",
      "adapter_name",
      "init_weights"
    ]
  },
  "EPS": [],
  "get_router": [
    "poly_config"
  ],
  "Router": {
    "reset": [
      "self"
    ],
    "forward": [
      "self",
      "task_ids",
      "input_ids"
    ]
  },
  "PolyRouter": {
    "__init__": [
      "self",
      "poly_config"
    ],
    "reset": [
      "self"
    ],
    "forward": [
      "self",
      "task_ids",
      "input_ids"
    ]
  },
  "HRAConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "HRAModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "hra_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "hra_config",
      "adapter_name",
      "target"
    ]
  },
  "HRALayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "apply_GS",
      "init_weights",
      "inference_mode"
    ],
    "reset_hra_parameters": [
      "self",
      "adapter_name"
    ],
    "reset_hra_parameters_random": [
      "self",
      "adapter_name"
    ],
    "scale_layer": [
      "self",
      "scale"
    ],
    "unscale_layer": [
      "self",
      "scale"
    ]
  },
  "HRALinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "apply_GS",
      "init_weights"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter_name",
      "reverse"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "HRAConv2d": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "apply_GS",
      "init_weights"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter_name",
      "reverse"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "DeloraConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "DeloraModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_check_new_adapter_config": [
      "self",
      "config"
    ],
    "_create_and_replace": [
      "self",
      "delora_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "delora_config",
      "adapter_name",
      "target"
    ]
  },
  "DeloraLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "_compute_delta": [
      "A",
      "B",
      "delora_lambda",
      "r",
      "w_norm"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "delora_lambda",
      "module_dropout",
      "init_weights",
      "inference_mode"
    ],
    "reset_delora_parameters": [
      "self",
      "adapter_name",
      "init_weights",
      "delora_lambda"
    ]
  },
  "DeloraLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "delora_lambda",
      "module_dropout",
      "init_weights"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "TrainableTokensConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "TrainableTokensModel": {
    "tuner_layer_cls": [],
    "_prepare_adapter_config": [
      "self",
      "peft_config",
      "model_config"
    ],
    "inject_adapter": [
      "self",
      "model",
      "adapter_name",
      "autocast_adapter_dtype",
      "low_cpu_mem_usage"
    ],
    "_get_tied_target_modules": [
      "self"
    ],
    "_create_and_replace_dict": [
      "self",
      "peft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_and_replace": [
      "self",
      "peft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "peft_config",
      "adapter_name",
      "target"
    ]
  },
  "TrainableTokensLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "token_indices",
      "tied_adapter"
    ],
    "tied_adapter": [
      "self"
    ],
    "_collect_token_weights": [
      "self",
      "weight",
      "rows",
      "embed_dim"
    ],
    "update_layer": [
      "self",
      "adapter_name"
    ],
    "_check_overlapping_tokens": [
      "self",
      "adapter_names"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_merged_weights": [
      "self",
      "active_adapters"
    ],
    "forward_adapters": [
      "self",
      "x",
      "active_adapters"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BoneConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "BoneModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "bone_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "bone_config",
      "adapter_name",
      "target"
    ]
  },
  "BoneLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "init_weights",
      "inference_mode"
    ],
    "reset_bone_parameters": [
      "self",
      "adapter_name",
      "r"
    ],
    "reset_bat_parameters": [
      "self",
      "adapter_name",
      "r"
    ],
    "reset_bone_parameters_random": [
      "self",
      "adapter_name"
    ],
    "scale_layer": [
      "self",
      "scale"
    ],
    "unscale_layer": [
      "self",
      "scale"
    ]
  },
  "BoneLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "init_weights"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter",
      "orig_weight",
      "re"
    ],
    "get_delta_weight_bone": [
      "self",
      "adapter",
      "orig_weight",
      "re"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "AdaLoraConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "AdaLoraModel": {
    "target_module_mapping": [],
    "__init__": [
      "self",
      "model",
      "config",
      "adapter_name"
    ],
    "_check_new_adapter_config": [
      "self",
      "config"
    ],
    "_create_and_replace": [
      "self",
      "lora_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "lora_config",
      "adapter_name",
      "target",
      "device_map"
    ],
    "forward": [
      "self"
    ],
    "resize_modules_by_rank_pattern": [
      "self",
      "rank_pattern",
      "adapter_name"
    ],
    "resize_state_dict_by_rank_pattern": [
      "self",
      "rank_pattern",
      "state_dict",
      "adapter_name"
    ],
    "update_and_allocate": [
      "self",
      "global_step"
    ],
    "add_weighted_adapter": [
      "self"
    ]
  },
  "AdaLoraLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights",
      "inference_mode"
    ],
    "reset_lora_parameters": [
      "self",
      "adapter_name"
    ]
  },
  "SVDLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "fan_in_fan_out",
      "init_lora_weights"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "RankAllocator": {
    "__init__": [
      "self",
      "model",
      "peft_config",
      "adapter_name"
    ],
    "set_total_step": [
      "self",
      "total_step"
    ],
    "reset_ipt": [
      "self"
    ],
    "_set_budget_scheduler": [
      "self",
      "model"
    ],
    "budget_schedule": [
      "self",
      "step"
    ],
    "update_ipt": [
      "self",
      "model"
    ],
    "_element_score": [
      "self",
      "n"
    ],
    "_combine_ipt": [
      "self",
      "ipt_E",
      "ipt_AB"
    ],
    "mask_to_budget": [
      "self",
      "model",
      "budget"
    ],
    "update_and_allocate": [
      "self",
      "model",
      "global_step",
      "force_mask"
    ],
    "mask_using_rank_pattern": [
      "self",
      "model",
      "rank_pattern"
    ]
  },
  "SVDQuantLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "r",
      "lora_alpha",
      "lora_dropout",
      "init_lora_weights"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "WaveletDetailTuple2d": {},
  "Wavelet": {
    "__len__": [
      "self"
    ]
  },
  "WaveletTensorTuple": {
    "from_wavelet": [
      "cls",
      "wavelet",
      "dtype"
    ]
  },
  "_as_wavelet": [
    "wavelet"
  ],
  "_is_dtype_supported": [
    "dtype"
  ],
  "_outer": [
    "a",
    "b"
  ],
  "_check_if_tensor": [
    "array"
  ],
  "_check_axes_argument": [
    "axes"
  ],
  "_check_same_device": [
    "tensor",
    "torch_device"
  ],
  "_check_same_dtype": [
    "tensor",
    "torch_dtype"
  ],
  "_coeff_tree_map": [
    "coeffs",
    "function"
  ],
  "_check_same_device_dtype": [
    "coeffs"
  ],
  "_get_transpose_order": [
    "axes",
    "data_shape"
  ],
  "_swap_axes": [
    "data",
    "axes"
  ],
  "_undo_swap_axes": [
    "data",
    "axes"
  ],
  "_fold_axes": [
    "data",
    "keep_no"
  ],
  "_unfold_axes": [
    "data",
    "ds",
    "keep_no"
  ],
  "_preprocess_coeffs": [
    "coeffs",
    "ndim",
    "axes",
    "add_channel_dim"
  ],
  "_postprocess_coeffs": [
    "coeffs",
    "ndim",
    "ds",
    "axes"
  ],
  "_postprocess_tensor": [
    "data",
    "ndim",
    "ds",
    "axes"
  ],
  "_get_filter_tensors": [
    "wavelet",
    "flip",
    "device",
    "dtype"
  ],
  "_adjust_padding_at_reconstruction": [
    "tensor_len",
    "coeff_len",
    "padr",
    "padl"
  ],
  "_construct_2d_filt": [
    "lo",
    "hi"
  ],
  "waverec2d": [
    "coeffs",
    "wavelet",
    "axes"
  ],
  "WaveFTConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "WaveFTModel": {
    "target_module_mapping": [],
    "_calculate_proportional_parameters": [
      "self",
      "model",
      "waveft_config"
    ],
    "_create_and_replace": [
      "self",
      "waveft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "waveft_config",
      "adapter_name",
      "target"
    ],
    "delete_adapter": [
      "self",
      "adapter_name"
    ]
  },
  "WaveFTLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "n_frequency",
      "scaling",
      "init_weights",
      "random_loc_seed",
      "wavelet_family",
      "use_idwt"
    ],
    "reset_wave_parameters": [
      "self",
      "adapter_name"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ]
  },
  "WaveFTLinear": {
    "__init__": [
      "self",
      "base_layer",
      "adapter_name",
      "n_frequency",
      "scaling",
      "fan_in_fan_out",
      "init_weights",
      "random_loc_seed",
      "wavelet_family",
      "use_idwt"
    ],
    "merge": [
      "self",
      "safe_merge",
      "adapter_names"
    ],
    "unmerge": [
      "self"
    ],
    "get_delta_weight": [
      "self",
      "adapter"
    ],
    "forward": [
      "self",
      "x"
    ],
    "__repr__": [
      "self"
    ]
  },
  "WAVELET_REDUCTIONS": [],
  "_WAVELET_COEFFS": [],
  "wavelist": [],
  "llama_rotate_half": [
    "x"
  ],
  "llama_apply_rotary_pos_emb": [
    "q",
    "cos",
    "sin",
    "position_ids"
  ],
  "llama_compute_query_states": [
    "model"
  ],
  "gpt2_compute_query_states": [
    "model",
    "hidden_states",
    "encoder_hidden_states"
  ],
  "is_adaption_prompt_trainable": [
    "params"
  ],
  "AdaptionPromptConfig": {
    "__post_init__": [
      "self"
    ],
    "is_adaption_prompt": [
      "self"
    ]
  },
  "ModelTypeConfig": [],
  "TRANSFORMERS_MODEL_CONFIG": [],
  "prepare_config": [
    "peft_config",
    "model"
  ],
  "AdaptionPromptModel": {
    "__init__": [
      "self",
      "model",
      "configs",
      "adapter_name"
    ],
    "add_adapter": [
      "self",
      "adapter_name",
      "config"
    ],
    "set_adapter": [
      "self",
      "adapter_name"
    ],
    "enable_adapter_layers": [
      "self"
    ],
    "disable_adapter_layers": [
      "self"
    ],
    "_create_adapted_attentions": [
      "self",
      "config",
      "parents"
    ],
    "_set_adapted_attentions": [
      "self",
      "adapter_name"
    ],
    "_remove_adapted_attentions": [
      "self",
      "adapter_name"
    ],
    "_mark_only_adaption_prompts_as_trainable": [
      "self",
      "model"
    ],
    "__getattr__": [
      "self",
      "name"
    ]
  },
  "_BaseAdaptedAttention": {
    "__init__": [
      "self",
      "model_type",
      "adapter_len",
      "model",
      "target_dtype"
    ]
  },
  "AdaptedAttentionGPT": {
    "__init__": [
      "self",
      "model_type",
      "adapter_len",
      "model"
    ],
    "forward": [
      "self",
      "hidden_states",
      "layer_past",
      "attention_mask",
      "head_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_cache",
      "output_attentions"
    ]
  },
  "AdaptedAttention": {
    "__init__": [
      "self",
      "model_type",
      "adapter_len",
      "model"
    ],
    "forward": [
      "self"
    ]
  },
  "BOFTConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "BOFTModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "boft_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "boft_config",
      "adapter_name",
      "target"
    ]
  },
  "_FBD_CUDA": [],
  "patch_environment": [],
  "get_fbd_cuda": [],
  "FastBlockDiag": {
    "forward": [
      "ctx",
      "input"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "BOFTLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "set_scale": [
      "self",
      "adapter",
      "scale"
    ],
    "scale_layer": [
      "self",
      "scale"
    ],
    "unscale_layer": [
      "self",
      "scale"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "boft_block_size",
      "boft_block_num",
      "boft_n_butterfly_factor",
      "boft_dropout",
      "init_weights",
      "inference_mode"
    ],
    "reset_boft_parameters": [
      "self",
      "adapter_name",
      "init_weights"
    ],
    "perm2mat": [
      "self",
      "indices"
    ],
    "block_butterfly_perm": [
      "self",
      "n",
      "b",
      "r",
      "n_butterfly_factor"
    ],
    "cayley_batch": [
      "self",
      "data"
    ]
  },
  "LoKrConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "LoKrModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ]
  },
  "LoKrLayer": {
    "adapter_layer_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "_available_adapters": [
      "self"
    ],
    "create_adapter_parameters": [
      "self",
      "adapter_name",
      "r",
      "shape",
      "use_w1",
      "use_w2",
      "use_effective_conv2d"
    ],
    "reset_adapter_parameters": [
      "self",
      "adapter_name"
    ],
    "reset_adapter_parameters_random": [
      "self",
      "adapter_name"
    ],
    "reset_adapter_parameters_lycoris_way": [
      "self",
      "adapter_name"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "r",
      "alpha",
      "rank_dropout",
      "module_dropout",
      "init_weights",
      "use_effective_conv2d",
      "decompose_both",
      "decompose_factor",
      "inference_mode"
    ],
    "get_delta_weight": [
      "self",
      "adapter_name"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "factorization": [
    "dimension",
    "factor"
  ],
  "make_kron": [
    "w1",
    "w2",
    "scale"
  ],
  "random_mask": [
    "base_layer",
    "r",
    "random_seed"
  ],
  "ShiraConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "ShiraModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_create_and_replace": [
      "self",
      "shira_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "shira_config",
      "adapter_name",
      "target"
    ]
  },
  "ShiraLayer": {
    "adapter_layer_names": [],
    "other_param_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "mask",
      "r",
      "init_weights",
      "inference_mode"
    ],
    "reset_shira_parameters": [
      "self",
      "adapter_name"
    ],
    "set_scale": [
      "self",
      "adapter",
      "scale"
    ]
  },
  "VBLoRAConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "VBLoRAModel": {
    "tuner_layer_cls": [],
    "target_module_mapping": [],
    "_init_vblora_vector_bank": [
      "self",
      "config",
      "adapter_name"
    ],
    "_pre_injection_hook": [
      "self",
      "model",
      "config",
      "adapter_name"
    ],
    "_create_and_replace": [
      "self",
      "vblora_config",
      "adapter_name",
      "target",
      "target_name",
      "parent",
      "current_key"
    ],
    "_create_new_module": [
      "vblora_config",
      "vblora_vector_bank",
      "adapter_name",
      "target"
    ],
    "get_nb_savable_parameters": [
      "self",
      "adapter"
    ],
    "print_savable_parameters": [
      "self"
    ]
  },
  "VBLoRALayer": {
    "adapter_layer_names": [],
    "__init__": [
      "self",
      "base_layer"
    ],
    "merged": [
      "self"
    ],
    "update_layer": [
      "self",
      "adapter_name",
      "vblora_vector_bank",
      "r",
      "topk",
      "num_vectors",
      "vector_length",
      "vblora_dropout",
      "init_logits_std",
      "inference_mode"
    ],
    "reset_vblora_logits": [
      "self",
      "adapter_name",
      "init_logits_std"
    ]
  }
}