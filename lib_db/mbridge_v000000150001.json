{
  "_AllGatherToContextParallelRegion": {
    "forward": [
      "ctx",
      "input_",
      "seq_dim",
      "bwd_op"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "all_gather_to_context_parallel_region": [
    "local_tensor",
    "gather_dim",
    "bwd_op"
  ],
  "LinearForLastLayer": {
    "__init__": [
      "self",
      "input_size",
      "output_size"
    ],
    "forward": [
      "self",
      "input_",
      "weight",
      "runtime_gather_output"
    ]
  },
  "translate_first_k_dense_replace_to_moe_layer_freq": [
    "first_k_dense_replace",
    "total_layers"
  ],
  "make_value_model": [
    "model",
    "pre_process",
    "post_process",
    "config",
    "hf_config"
  ],
  "freeze_moe_router": [
    "model",
    "pre_process",
    "post_process",
    "config",
    "hf_config"
  ],
  "Qwen2Bridge": {
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_adjust_mapping_for_shared_weights": [
      "self"
    ],
    "_get_hf_shared_weight_keys": [
      "self"
    ],
    "_build_config": [
      "self"
    ]
  },
  "Qwen2MoEBridge": {
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_build_config": [
      "self"
    ],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ]
  },
  "Qwen3MoEBridge": {
    "_build_config": [
      "self"
    ]
  },
  "GLM4MoEBridge": {
    "_MLP_MAPPING": [],
    "_MTP_MAPPING": [],
    "_weight_name_mapping_mtp": [
      "self",
      "name",
      "num_layers"
    ],
    "_weight_name_mapping_mcore_to_hf": [
      "self",
      "mcore_weights_name"
    ],
    "_build_config": [
      "self"
    ],
    "_get_gptmodel_args": [
      "self"
    ]
  },
  "MimoBridge": {
    "_build_config": [
      "self"
    ],
    "_get_gptmodel_args": [
      "self"
    ],
    "_weight_name_mapping_mcore_to_hf": [
      "self",
      "mcore_weights_name"
    ],
    "_convert_mtp_param": [
      "self",
      "name"
    ]
  },
  "MixtralBridge": {
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ],
    "_build_config": [
      "self"
    ]
  },
  "LLaMABridge": {
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_build_config": [
      "self"
    ],
    "_get_gptmodel_args": [
      "self"
    ]
  },
  "Qwen3Bridge": {
    "_build_config": [
      "self"
    ]
  },
  "DeepseekV3Bridge": {
    "_DIRECT_MAPPING": [],
    "_MLP_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_SHARED_STATE_DICT_MAPPING": [],
    "TransformerConfigClass": [],
    "_build_config": [
      "self"
    ],
    "_get_gptmodel_args": [
      "self"
    ],
    "_model_provider": [
      "self",
      "post_model_creation_callbacks"
    ],
    "_weight_name_mapping_mcore_to_hf": [
      "self",
      "mcore_weights_name"
    ],
    "_weight_to_hf_format": [
      "self",
      "mcore_weights_name",
      "mcore_weights"
    ],
    "_get_safetensor_io": [
      "self",
      "weights_path"
    ],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ],
    "_convert_mtp_param": [
      "self",
      "name"
    ]
  },
  "fast_gelu": [
    "x"
  ],
  "Gemma3TransformerConfig": {},
  "get_vision_model_config": [
    "config"
  ],
  "get_vision_projection_config": [
    "config"
  ],
  "Gemma3TransformerLayerSubmodules": {},
  "Gemma3TransformerLayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "hidden_dropout",
      "model_comm_pgs",
      "vp_stage"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "inference_context",
      "inference_params",
      "packed_seq_params",
      "sequence_len_offset"
    ]
  },
  "get_norm_mlp_module_spec_te": [],
  "get_gemma3_layer_spec_te": [
    "is_vit"
  ],
  "get_layer_spec_te": [
    "is_vit"
  ],
  "Gemma3Bridge": {
    "TransformerConfigClass": [],
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_OTHER_MAPPING": [],
    "_adjust_mapping_for_shared_weights": [
      "self"
    ],
    "_get_hf_shared_weight_keys": [
      "self"
    ],
    "_get_mcore_config_by_name": [
      "self",
      "mcore_weights_name"
    ],
    "_weight_name_mapping_attention": [
      "self",
      "name"
    ],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ],
    "_weight_name_mapping_other": [
      "self",
      "name"
    ],
    "_weight_to_hf_format": [
      "self",
      "mcore_weights_name",
      "mcore_weights"
    ],
    "_weight_to_mcore_format": [
      "self",
      "mcore_weights_name",
      "hf_weights"
    ],
    "_weight_name_mapping_mcore_local_to_global": [
      "self",
      "model",
      "consider_ep"
    ],
    "_model_provider": [
      "self",
      "post_model_creation_callbacks"
    ],
    "_build_config": [
      "self"
    ]
  },
  "Gemma3Model": {
    "__init__": [
      "self",
      "language_transformer_config",
      "language_transformer_layer_spec",
      "language_vocab_size",
      "language_max_sequence_length",
      "vision_transformer_config",
      "vision_transformer_layer_spec",
      "vision_projection_config",
      "vision_projection_layer_spec",
      "vision_projection_type",
      "parallel_output",
      "share_embeddings_and_output_weights",
      "language_position_embedding_type",
      "language_rotary_percent",
      "pre_process",
      "post_process",
      "add_encoder",
      "add_decoder",
      "img_h",
      "img_w",
      "patch_dim",
      "language_rotary_base",
      "language_rope_scaling",
      "language_rope_scaling_factor"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "freeze": [
      "self",
      "freeze_language_model",
      "freeze_vision_model",
      "freeze_vision_projection"
    ],
    "_gather_image_embedings": [
      "self",
      "image_embeddings"
    ],
    "_preprocess_data": [
      "self",
      "image_embeddings",
      "language_embeddings",
      "input_ids",
      "use_inference_kv_cache",
      "image_token_index"
    ],
    "_process_embedding_token_parallel": [
      "self",
      "combined_embeddings"
    ],
    "forward": [
      "self",
      "images",
      "input_ids",
      "position_ids",
      "attention_mask",
      "labels",
      "inference_context",
      "image_token_index",
      "runtime_gather_output",
      "packed_seq_params"
    ]
  },
  "LanguageModelEmbeddingScale": {
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "tokentype_ids"
    ]
  },
  "Gemma3GPTModel": {
    "__init__": [
      "self",
      "config",
      "transformer_layer_spec",
      "vocab_size",
      "max_sequence_length",
      "pre_process",
      "post_process",
      "fp16_lm_cross_entropy",
      "parallel_output",
      "share_embeddings_and_output_weights",
      "position_embedding_type",
      "rotary_percent",
      "rotary_base",
      "rope_scaling",
      "rope_scaling_factor",
      "scatter_embedding_sequence_parallel",
      "seq_len_interpolation_factor"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "decoder_input",
      "labels",
      "inference_context",
      "packed_seq_params",
      "extra_block_kwargs",
      "runtime_gather_output"
    ]
  },
  "Gemma3MultiModalProjectorSubmodules": {},
  "get_projector_module_spec": [],
  "get_projector_module_spec_te": [],
  "Gemma3MultiModalProjector": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "input_size"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "llama3_prepare_cu_kv_slice": [
    "cu_seqlens",
    "causal",
    "rank",
    "world_size"
  ],
  "llama3_prepare_cu_seqlens": [
    "cu_seqlens",
    "causal",
    "rank",
    "world_size"
  ],
  "llama3_prepare_cu_seqlens_zigzag": [
    "cu_seqlens",
    "causal",
    "rank",
    "world_size"
  ],
  "llama3_prepare_memory_efficient_mask": [
    "cu_seqlens",
    "causal",
    "rank",
    "world_size",
    "device"
  ],
  "forward_cat_func_zigzag": [
    "gathered_logits",
    "cp_size",
    "seq_dim"
  ],
  "return_self": [
    "input"
  ],
  "gather_cp_forward_zigzag": [
    "input",
    "seq_dim",
    "async_op",
    "process_group"
  ],
  "backward_process": [
    "local_grad",
    "shape"
  ],
  "gather_cp_backward_zigzag": [
    "grad_output",
    "seq_dim",
    "async_op",
    "reduce_op",
    "process_group"
  ],
  "forward_cat_func": [
    "tensor_list",
    "seq_dim"
  ],
  "gather_cp_forward": [
    "input",
    "seq_dim",
    "async_op",
    "process_group"
  ],
  "gather_cp_backward": [
    "grad_output",
    "seq_dim",
    "async_op",
    "reduce_op",
    "process_group"
  ],
  "GpatchPackedSeqParams": {},
  "get_bwd_op": [
    "fwd_op"
  ],
  "Llama3MemoryEfficientAttnFunc": {
    "forward": [
      "ctx",
      "query",
      "key",
      "value",
      "attn_bias",
      "p",
      "scale",
      "op",
      "output_dtype",
      "process_group",
      "heads_k_stride",
      "use_zigzag",
      "kv_slice"
    ],
    "backward": [
      "ctx",
      "dout"
    ]
  },
  "llama3_memory_efficient_attention_func": [
    "query",
    "key",
    "value",
    "attn_bias",
    "p",
    "scale"
  ],
  "MemoryEfficientAttention": {
    "__init__": [
      "self",
      "config",
      "layer_number",
      "attn_mask_type",
      "attention_type",
      "attention_dropout",
      "softmax_scale",
      "k_channels",
      "v_channels",
      "cp_comm_type",
      "model_comm_pgs"
    ],
    "repeat_kv": [
      "self",
      "hidden_states",
      "n_rep"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "attn_mask_type",
      "attention_bias",
      "packed_seq_params"
    ]
  },
  "act_quant_kernel": [
    "x_ptr",
    "y_ptr",
    "s_ptr",
    "BLOCK_SIZE"
  ],
  "act_quant": [
    "x",
    "block_size"
  ],
  "weight_dequant_kernel": [
    "x_ptr",
    "s_ptr",
    "y_ptr",
    "M",
    "N",
    "BLOCK_SIZE"
  ],
  "weight_dequant": [
    "x",
    "s",
    "block_size"
  ],
  "fp8_gemm_configs": [],
  "fp8_gemm_kernel": [
    "a_ptr",
    "b_ptr",
    "c_ptr",
    "a_s_ptr",
    "b_s_ptr",
    "M",
    "N",
    "K",
    "BLOCK_SIZE_M",
    "BLOCK_SIZE_N",
    "BLOCK_SIZE_K"
  ],
  "fp8_gemm": [
    "a",
    "a_s",
    "b",
    "b_s"
  ],
  "DequantFP8SafeTensorIO": {
    "__init__": [
      "self",
      "hf_dir"
    ],
    "load_some_hf_weight": [
      "self",
      "hf_weight_names"
    ]
  },
  "GLM4VLTransformerConfig": {},
  "Glm4TransformerLayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "hidden_dropout"
    ],
    "_forward_attention": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "inference_context",
      "packed_seq_params",
      "sequence_len_offset"
    ],
    "_forward_mlp": [
      "self",
      "pre_mlp_layernorm_output",
      "residual"
    ]
  },
  "Glm4VLBridgeDense": {
    "_VISUAL_MAPPING": [],
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_get_transformer_layer_spec": [
      "self",
      "vp_stage"
    ],
    "_build_config": [
      "self"
    ]
  },
  "attr_getter": [
    "self",
    "attr"
  ],
  "Glm4VLBridgeMoe": {
    "_VISUAL_MAPPING": [],
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_build_config": [
      "self"
    ]
  },
  "Glm4VLBridgeBase": {
    "TransformerConfigClass": [],
    "_get_gptmodel_args": [
      "self"
    ],
    "_weight_name_mapping_mcore_to_hf": [
      "self",
      "mcore_weights_name"
    ],
    "_weight_name_mapping_attention": [
      "self",
      "name"
    ],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ],
    "_weight_name_mapping_visual": [
      "self",
      "name"
    ],
    "_weight_to_hf_format": [
      "self",
      "mcore_weights_name",
      "mcore_weights"
    ],
    "_weight_to_mcore_format": [
      "self",
      "mcore_weights_name",
      "hf_weights"
    ],
    "_model_provider": [
      "self",
      "post_model_creation_callbacks"
    ]
  },
  "Glm4VLModel": {
    "__init__": [
      "self",
      "config",
      "language_transformer_layer_spec",
      "vocab_size",
      "max_sequence_length",
      "position_embedding_type",
      "hf_config",
      "hf_vision_cls",
      "parallel_output",
      "rotary_percent",
      "pre_process",
      "post_process",
      "rotary_base",
      "fp16_lm_cross_entropy",
      "share_embeddings_and_output_weights"
    ],
    "share_embeddings_and_output_weights": [
      "self"
    ],
    "decoder": [
      "self"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "freeze": [
      "self",
      "freeze_language_model",
      "freeze_vision_model",
      "freeze_vision_projection"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "labels",
      "inference_context",
      "packed_seq_params",
      "extra_block_kwargs",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "runtime_gather_output"
    ]
  },
  "VLMixin": {
    "get_rope_index": [
      "self",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw",
      "attention_mask"
    ],
    "get_video_features": [
      "self",
      "pixel_values_videos",
      "video_grid_thw"
    ],
    "get_image_features": [
      "self",
      "pixel_values",
      "image_grid_thw"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features"
    ],
    "merge_image_or_video": [
      "self",
      "inputs_embeds",
      "pixel_values",
      "pixel_values_videos",
      "input_ids",
      "image_grid_thw",
      "video_grid_thw"
    ]
  },
  "Glm4vGenerationController": {
    "prep_inference_input": [
      "self",
      "prompts_tokens",
      "active_requests"
    ]
  },
  "get_inference_engine": [
    "model",
    "tokenizer",
    "vocab_size"
  ],
  "Glm4vInferenceWrapper": {
    "prep_inference_input": [
      "self",
      "prompts_tokens",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw"
    ],
    "_build_attention_mask_and_position_ids": [
      "self",
      "prompts_tokens",
      "image_grid_thw",
      "video_grid_thw"
    ],
    "get_batch_for_context_window": [
      "self",
      "inference_input",
      "context_start_position",
      "context_end_position"
    ],
    "_forward": [
      "self",
      "inference_input"
    ]
  },
  "Glm4vInferenceRequest": {},
  "InternvlTransformerConfig": {},
  "DropPath": {
    "__init__": [
      "self",
      "config",
      "drop_prob"
    ],
    "forward": [
      "self",
      "hidden_state"
    ]
  },
  "Internvl2bVitTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "submodules",
      "layer_number",
      "hidden_dropout",
      "drop_path_rate"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "inference_context",
      "inference_params",
      "packed_seq_params",
      "sequence_len_offset"
    ]
  },
  "get_mlp_module_spec": [],
  "get_internvl2b_vit_layer_specs": [],
  "Internvl2bVitTransformerBlock": {
    "__init__": [
      "self",
      "config",
      "spec",
      "pre_process",
      "post_process"
    ],
    "_build_layers": [
      "self"
    ]
  },
  "InternvlVitModel": {
    "__init__": [
      "self",
      "config",
      "layer_spec",
      "patch_dim",
      "img_h",
      "img_w",
      "model_subtype"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "_get_pos_embed": [
      "self",
      "pos_embed",
      "H",
      "W"
    ],
    "pixel_shuffle": [
      "self",
      "x",
      "scale_factor"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "InternVL3Bridge": {
    "TransformerConfigClass": [],
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_OTHER_MAPPING": [],
    "_adjust_mapping_for_shared_weights": [
      "self"
    ],
    "_get_hf_shared_weight_keys": [
      "self"
    ],
    "_get_mcore_config_by_name": [
      "self",
      "mcore_weights_name"
    ],
    "_weight_name_mapping_attention": [
      "self",
      "name"
    ],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ],
    "_weight_name_mapping_other": [
      "self",
      "name"
    ],
    "_weight_to_hf_format": [
      "self",
      "mcore_weights_name",
      "mcore_weights"
    ],
    "_weight_to_mcore_format": [
      "self",
      "mcore_weights_name",
      "hf_weights"
    ],
    "_weight_name_mapping_mcore_local_to_global": [
      "self",
      "model",
      "consider_ep"
    ],
    "_model_provider": [
      "self",
      "post_model_creation_callbacks"
    ],
    "_build_config": [
      "self"
    ]
  },
  "InternVLModel": {
    "__init__": [
      "self",
      "language_transformer_config",
      "language_transformer_layer_spec",
      "language_vocab_size",
      "language_max_sequence_length",
      "vision_transformer_config",
      "vision_transformer_layer_spec",
      "vision_projection_config",
      "vision_projection_layer_spec",
      "vision_projection_type",
      "parallel_output",
      "language_position_embedding_type",
      "language_rotary_percent",
      "pre_process",
      "post_process",
      "add_encoder",
      "add_decoder",
      "language_rotary_base",
      "share_embeddings_and_output_weights",
      "fp16_lm_cross_entropy",
      "language_rope_scaling",
      "language_rope_scaling_factor",
      "language_seq_len_interpolation_factor"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "freeze": [
      "self",
      "freeze_language_model",
      "freeze_vision_model",
      "freeze_vision_projection"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "images",
      "attention_mask",
      "labels",
      "inference_params",
      "packed_seq_params",
      "image_token_index"
    ]
  },
  "Qwen2VLTransformerConfig": {},
  "Qwen2_5VisionTransformerBlock": {
    "_checkpointed_forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "attention_bias",
      "packed_seq_params",
      "packed_seq_params_full",
      "fullatt_block_indexes"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "context",
      "context_mask",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "inference_context",
      "packed_seq_params",
      "sequence_len_offset",
      "packed_seq_params_full",
      "fullatt_block_indexes"
    ]
  },
  "Qwen2_5VLBridge": {
    "_DIRECT_MAPPING": [],
    "_ATTENTION_MAPPING": [],
    "_MLP_MAPPING": [],
    "_adjust_mapping_for_shared_weights": [
      "self"
    ],
    "_get_hf_shared_weight_keys": [
      "self"
    ],
    "_weight_name_mapping_attention": [
      "self",
      "name"
    ],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ],
    "_weight_to_hf_format": [
      "self",
      "mcore_weights_name",
      "mcore_weights"
    ],
    "_weight_to_mcore_format": [
      "self",
      "mcore_weights_name",
      "hf_weights"
    ],
    "_get_layer_number": [
      "self",
      "vpp_rank",
      "local_layer_number",
      "models"
    ],
    "_weight_name_mapping_mcore_local_to_global": [
      "self",
      "model",
      "consider_ep"
    ],
    "_model_provider": [
      "self",
      "post_model_creation_callbacks"
    ],
    "_build_config": [
      "self"
    ]
  },
  "Qwen2_5VLSelfAttention": {
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "key_value_states",
      "inference_context",
      "rotary_pos_emb",
      "rotary_pos_cos",
      "rotary_pos_sin",
      "attention_bias",
      "packed_seq_params",
      "sequence_len_offset"
    ]
  },
  "PatchEmbed": {
    "__init__": [
      "self",
      "patch_size",
      "temporal_patch_size",
      "in_channels",
      "embed_dim"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "VisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "forward": [
      "self",
      "seqlen"
    ]
  },
  "Qwen2_5VisionModel": {
    "__init__": [
      "self",
      "transformer_config",
      "transformer_layer_spec",
      "projection_config",
      "projection_layer_spec",
      "projection_type",
      "pre_process",
      "post_process"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "rot_pos_emb": [
      "self",
      "grid_thw"
    ],
    "get_window_index": [
      "self",
      "grid_thw"
    ],
    "forward": [
      "self",
      "vision_data",
      "grid_thw",
      "inference_params",
      "extra_block_kwargs"
    ],
    "build_packed_seq_params": [
      "self",
      "grid_thw",
      "cu_seqlens"
    ]
  },
  "Qwen2_5VLModel": {
    "__init__": [
      "self",
      "language_transformer_config",
      "language_transformer_layer_spec",
      "language_vocab_size",
      "language_max_sequence_length",
      "vision_transformer_config",
      "vision_transformer_layer_spec",
      "vision_projection_config",
      "vision_projection_layer_spec",
      "vision_projection_type",
      "parallel_output",
      "language_rotary_percent",
      "pre_process",
      "post_process",
      "add_encoder",
      "add_decoder",
      "language_rotary_base",
      "fp16_lm_cross_entropy",
      "language_share_embeddings_and_output_weights",
      "image_token_id",
      "video_token_id"
    ],
    "shared_embedding_or_output_weight": [
      "self"
    ],
    "set_input_tensor": [
      "self",
      "input_tensor"
    ],
    "freeze": [
      "self",
      "freeze_language_model",
      "freeze_vision_model",
      "freeze_vision_projection"
    ],
    "forward": [
      "self",
      "input_ids",
      "position_ids",
      "attention_mask",
      "labels",
      "inference_params",
      "packed_seq_params",
      "extra_block_kwargs",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw"
    ]
  },
  "logger": [],
  "get_rope_index": [
    "input_ids",
    "image_grid_thw",
    "video_grid_thw",
    "second_per_grid_ts",
    "attention_mask"
  ],
  "apply_rotary_pos_emb_thd_absolute": [
    "t",
    "cu_seqlens",
    "freqs",
    "rotary_interleaved"
  ],
  "apply_rotary_pos_emb_absolute": [
    "t",
    "freqs",
    "config",
    "cu_seqlens"
  ],
  "LLMBridge": {
    "TransformerConfigClass": [],
    "_CONFIG_MAPPING": [],
    "_build_base_config": [
      "self",
      "text_config_key"
    ],
    "_get_gptmodel_args": [
      "self"
    ],
    "_get_transformer_layer_spec": [
      "self",
      "vp_stage"
    ],
    "_model_provider": [
      "self",
      "post_model_creation_callbacks"
    ]
  },
  "ParallelStates": {
    "get_default_parallel_states": [
      "cls"
    ],
    "get_parallel_state": [
      "cls"
    ]
  },
  "get_model": [
    "model_provider_func",
    "model_type",
    "wrap_with_ddp",
    "fp16",
    "bf16",
    "virtual_pipeline_model_parallel_size",
    "encoder_pipeline_model_parallel_size",
    "use_torch_fsdp2",
    "use_custom_fsdp",
    "use_precision_aware_optimizer",
    "use_cpu_initialization",
    "init_model_with_meta_device",
    "overlap_param_gather_with_optimizer_step",
    "data_parallel_random_init",
    "ddp_config",
    "optimizer_config"
  ],
  "unwrap_model": [
    "model",
    "module_instances"
  ],
  "broadcast_from_megatron_pp": [
    "tensor"
  ],
  "broadcast_str_from_megatron_pp": [
    "obj"
  ],
  "split_data_cp_rank": [
    "val",
    "cp_size",
    "seq_dim",
    "cp_rank"
  ],
  "Bridge": {
    "__init__": [
      "self",
      "hf_config",
      "dtype",
      "parallel_states",
      "make_vocab_size_divisible_by"
    ],
    "get_model": [
      "self",
      "weight_path",
      "model_type",
      "wrap_with_ddp",
      "fp16",
      "bf16",
      "encoder_pipeline_model_parallel_size",
      "use_torch_fsdp2",
      "use_custom_fsdp",
      "use_precision_aware_optimizer",
      "use_cpu_initialization",
      "init_model_with_meta_device",
      "overlap_param_gather_with_optimizer_step",
      "data_parallel_random_init",
      "ddp_config",
      "optimizer_config",
      "post_model_creation_callbacks",
      "extra_provider_args"
    ],
    "_get_safetensor_io": [
      "self",
      "weights_path"
    ],
    "_get_mcore_config_by_name": [
      "self",
      "mcore_weights_name"
    ],
    "load_weights": [
      "self",
      "models",
      "weights_path",
      "memory_efficient"
    ],
    "save_weights": [
      "self",
      "models",
      "weights_path",
      "memory_efficient"
    ],
    "set_extra_args": [
      "self"
    ],
    "export_weights": [
      "self",
      "models"
    ],
    "_build_config": [
      "self"
    ],
    "_model_provider": [
      "self",
      "post_model_creation_callbacks"
    ],
    "_weight_name_mapping_mcore_local_to_global": [
      "self",
      "model",
      "consider_ep"
    ],
    "_ATTENTION_MAPPING": [],
    "_weight_name_mapping_attention": [
      "self",
      "name"
    ],
    "_MLP_MAPPING": [],
    "_DIRECT_MAPPING": [],
    "_OTHER_MAPPING": [],
    "_adjust_mapping_for_shared_weights": [
      "self"
    ],
    "_get_hf_shared_weight_keys": [
      "self"
    ],
    "_weight_name_mapping_mlp": [
      "self",
      "name"
    ],
    "_weight_name_mapping_other": [
      "self",
      "name"
    ],
    "_weight_name_mapping_mcore_to_hf": [
      "self",
      "mcore_weights_name"
    ],
    "_weight_to_hf_format": [
      "self",
      "mcore_weights_name",
      "mcore_weights"
    ],
    "_weight_to_mcore_format": [
      "self",
      "mcore_weights_name",
      "hf_weights"
    ],
    "_weight_merge_across_tp": [
      "self",
      "mcore_weights_name",
      "mcore_weights",
      "param"
    ],
    "_weight_split_across_tp": [
      "self",
      "mcore_weights_name",
      "mcore_weights",
      "param",
      "tp_split_size"
    ],
    "_get_actual_hf_path": [
      "self",
      "weight_path"
    ]
  },
  "_MODEL_REGISTRY": [],
  "register_model": [
    "model_types"
  ],
  "VLMBridge": {},
  "AutoBridge": {
    "from_pretrained": [
      "cls",
      "hf_model_path",
      "trust_remote_code"
    ],
    "from_config": [
      "cls",
      "hf_config"
    ],
    "list_supported_models": [
      "cls"
    ],
    "get_model": [
      "self",
      "weight_path"
    ],
    "load_weights": [
      "self",
      "models",
      "weights_path",
      "memory_efficient"
    ],
    "save_weights": [
      "self",
      "models",
      "weights_path",
      "memory_efficient"
    ],
    "set_extra_args": [
      "self"
    ],
    "export_weights": [
      "self",
      "models"
    ]
  },
  "SafeTensorIO": {
    "__init__": [
      "self",
      "hf_dir"
    ],
    "load_some_hf_weight": [
      "self",
      "hf_weight_names"
    ],
    "load_one_hf_weight": [
      "self",
      "hf_weight_name"
    ],
    "load_hf_weight_names": [
      "self"
    ],
    "save_hf_weight": [
      "self",
      "per_tensor_generator",
      "new_hf_dir",
      "hf_shared_weight_keys"
    ],
    "save_hf_weight_memory_efficient": [
      "self",
      "per_tensor_generator",
      "new_hf_dir"
    ],
    "save_index": [
      "self",
      "new_hf_dir"
    ]
  }
}