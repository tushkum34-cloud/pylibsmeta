{
  "test_getattr_without_cache": [
    "monkeypatch"
  ],
  "test_getattr_with_cache": [
    "monkeypatch"
  ],
  "test_boolean_env_vars": [
    "monkeypatch"
  ],
  "test_boolean_env_vars_string_values": [
    "monkeypatch"
  ],
  "test_boolean_env_vars_invalid_values": [
    "monkeypatch"
  ],
  "test_boolean_env_vars_empty_string": [
    "monkeypatch"
  ],
  "test_integer_env_vars": [
    "monkeypatch"
  ],
  "test_model_impl_type_choices": [
    "monkeypatch"
  ],
  "test_string_env_vars_defaults": [
    "monkeypatch"
  ],
  "test_none_default_env_vars": [
    "monkeypatch"
  ],
  "test_ray_env_vars": [
    "monkeypatch"
  ],
  "test_invalid_attribute_raises_error": [],
  "test_dir_returns_all_env_vars": [],
  "test_tpu_multihost_env_vars": [
    "monkeypatch"
  ],
  "test_disaggregated_serving_env_vars": [
    "monkeypatch"
  ],
  "test_model_impl_type_default": [
    "monkeypatch"
  ],
  "test_cache_preserves_values_across_env_changes": [
    "monkeypatch"
  ],
  "test_enable_and_get_megacore": [],
  "test_hbm_usage_bytes_ray_backend": [],
  "test_hbm_usage_bytes_pathways_disabled": [],
  "test_hbm_usage_bytes_pathways_enabled": [
    "mock_devices",
    "mock_live_arrays"
  ],
  "test_hbm_usage_gb_pathways_disabled": [],
  "test_hbm_usage_bytes_pathways_no_arrays": [
    "mock_devices",
    "mock_live_arrays"
  ],
  "test_get_padded_head_dim": [
    "head_dim",
    "expected_padded_head_dim"
  ],
  "test_get_jax_dtype_from_str_dtype": [],
  "test_get_tpu_metadata_success": [
    "mock_get"
  ],
  "test_get_tpu_metadata_request_error": [
    "mock_get"
  ],
  "test_get_tpu_type_from_env": [
    "mock_get_tpu_metadata"
  ],
  "test_get_tpu_type_from_metadata": [
    "mock_get_tpu_metadata"
  ],
  "test_get_node_name_from_env": [
    "mock_get_tpu_metadata"
  ],
  "test_get_node_name_from_metadata": [
    "mock_get_tpu_metadata"
  ],
  "test_get_node_worker_id_from_env": [
    "mock_get_tpu_metadata"
  ],
  "test_get_node_worker_id_from_metadata": [
    "mock_get_tpu_metadata"
  ],
  "test_get_num_cores_per_chip": [
    "mock_get_tpu_type",
    "tpu_type",
    "expected"
  ],
  "test_get_num_chips_from_accel": [
    "mock_glob"
  ],
  "test_get_num_chips_from_vfio": [
    "mock_listdir",
    "mock_glob"
  ],
  "test_get_num_chips_not_found": [
    "mock_listdir",
    "mock_glob",
    "caplog"
  ],
  "vllm_logger": [],
  "original_level": [],
  "setup_vllm_config": [
    "subconfig_types",
    "overrides"
  ],
  "SimpleVllmConfig": {},
  "SimpleConfig": {
    "is_equal": [
      "self",
      "other"
    ]
  },
  "ConfigOverrideTests": {
    "test_additional_config_overrides": [
      "self"
    ],
    "test_hf_overrides": [
      "self"
    ],
    "test_additional_and_hf_overrides": [
      "self"
    ],
    "test_additional_and_generate_overrides": [
      "self"
    ],
    "test_hf_and_generate_overrides": [
      "self"
    ],
    "test_additional_and_hf_and_generate_overrides": [
      "self"
    ]
  },
  "RaggedPagedAttentionKernelTest": {
    "_test_ragged_paged_attention": [
      "self",
      "seq_lens",
      "num_heads",
      "head_dim",
      "page_size",
      "q_dtype",
      "kv_dtype",
      "num_pages"
    ],
    "test_ragged_paged_attention_basic": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_quantized_kv_cache": [
      "self",
      "q_dtype",
      "kv_dtype",
      "kv_scales"
    ],
    "test_ragged_paged_attention_quantized_attention": [
      "self",
      "q_dtype",
      "kv_dtype",
      "q_scale",
      "kv_scales"
    ],
    "test_ragged_paged_attention_decode_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_prefill_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_mixed": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_complex": [
      "self",
      "num_seqs",
      "num_heads",
      "head_dim",
      "dtype"
    ],
    "test_ragged_paged_attention_sliding_window": [
      "self",
      "sliding_window"
    ],
    "test_ragged_paged_attention_logit_soft_capping": [
      "self",
      "soft_cap"
    ],
    "test_ragged_paged_attention_sliding_window_should_be_positive": [
      "self"
    ],
    "test_ragged_paged_attention_soft_cap_cannot_be_zero": [
      "self"
    ]
  },
  "xla_quantized_matmul": [],
  "quantized_matmul_kernel": [],
  "quantize_tensor": [],
  "get_tuned_block_sizes": [],
  "QuantizedMatmulKernelTest": {
    "setUp": [
      "self"
    ],
    "_test_quantized_matmul": [
      "self",
      "dtype",
      "q_dtype",
      "bs",
      "n_input_features",
      "n_output_features",
      "quantize_activation",
      "tuned_value",
      "atol",
      "rtol"
    ],
    "test_quantized_matmul_various_input_shapes": [
      "self",
      "dtype",
      "q_dtype",
      "bs",
      "n_input_features",
      "n_output_features",
      "quantize_activation"
    ],
    "test_quantized_matmul_unaligned_input_shapes": [
      "self",
      "dtype",
      "q_dtype",
      "bs",
      "n_input_features",
      "n_output_features",
      "quantize_activation"
    ],
    "test_quantized_matmul_use_tuned_block_sizes": [
      "self",
      "dtype",
      "q_dtype",
      "bs",
      "n_input_features",
      "n_output_features",
      "quantize_activation"
    ]
  },
  "reference_gmm": [
    "lhs",
    "rhs",
    "group_sizes",
    "rhs_scale",
    "rhs_bias",
    "group_offset"
  ],
  "GmmTest": {
    "test_gmm": [
      "self",
      "batch_size",
      "in_size",
      "out_size",
      "num_groups",
      "has_bias"
    ],
    "test_gmm_weight_quantized": [
      "self",
      "batch_size",
      "in_size",
      "out_size",
      "num_groups",
      "has_bias",
      "weight_dtype",
      "block_size"
    ]
  },
  "kv_cache_update_ref": [
    "new_kv",
    "slot_mapping",
    "kv_cache"
  ],
  "KVCacheUpdateTest": {
    "_generate_data": [
      "self",
      "page_size",
      "combined_kv_head_num",
      "head_dim"
    ],
    "test_basic": [
      "self",
      "page_size",
      "combined_kv_head_num",
      "head_dim",
      "num_slices_per_block",
      "dynamic_validate_inputs"
    ],
    "test_torchax_shard_map": [
      "self",
      "page_size",
      "combined_kv_head_num",
      "head_dim",
      "num_slices_per_block"
    ],
    "test_invalid_inputs": [
      "self"
    ]
  },
  "ceil_div": [
    "x",
    "a"
  ],
  "PagedAttentionKernelTest": {
    "_test_ragged_paged_attention": [
      "self",
      "seq_lens",
      "num_heads",
      "head_dim",
      "page_size",
      "dtype",
      "num_pages"
    ],
    "test_ragged_paged_attention_basic": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_decode_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_prefill_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_mixed": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_complex": [
      "self",
      "num_seqs",
      "num_heads",
      "dtype",
      "num_kv_pages_per_block",
      "num_queries_per_block"
    ],
    "test_ragged_paged_attention_sliding_window": [
      "self",
      "num_kv_pages_per_block",
      "num_queries_per_block",
      "sliding_window"
    ],
    "test_ragged_paged_attention_logit_soft_capping": [
      "self",
      "num_kv_pages_per_block",
      "num_queries_per_block",
      "soft_cap"
    ],
    "test_ragged_paged_attention_sliding_window_should_be_positive": [
      "self"
    ],
    "test_ragged_paged_attention_soft_cap_cannot_be_zero": [
      "self"
    ]
  },
  "RaggedPagedAttentionHeadDim64KernelTest": {
    "_test_ragged_paged_attention_hd64": [
      "self",
      "seq_lens",
      "num_heads",
      "head_dim",
      "page_size",
      "q_dtype",
      "kv_dtype",
      "num_pages"
    ],
    "test_ragged_paged_attention_basic": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_quantized_kv_cache": [
      "self",
      "q_dtype",
      "kv_dtype",
      "kv_scales"
    ],
    "test_ragged_paged_attention_quantized_attention": [
      "self",
      "q_dtype",
      "kv_dtype",
      "q_scale",
      "kv_scales"
    ],
    "test_ragged_paged_attention_decode_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_prefill_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_mixed": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_complex": [
      "self",
      "num_seqs",
      "num_heads",
      "head_dim",
      "dtype"
    ],
    "test_ragged_paged_attention_sliding_window": [
      "self",
      "sliding_window"
    ],
    "test_ragged_paged_attention_sliding_window_with_attention_sink_hd64": [
      "self",
      "sliding_window",
      "num_heads"
    ],
    "test_ragged_paged_attention_logit_soft_capping": [
      "self",
      "soft_cap"
    ],
    "test_ragged_paged_attention_sliding_window_should_be_positive": [
      "self"
    ],
    "test_ragged_paged_attention_soft_cap_cannot_be_zero": [
      "self"
    ]
  },
  "MlaRaggedPagedAttentionKernelTest": {
    "_test_mla_ragged_paged_attention": [
      "self",
      "seq_lens",
      "num_heads",
      "lkv_dim",
      "r_dim",
      "page_size",
      "q_dtype",
      "kv_dtype",
      "num_pages"
    ],
    "test_update_kv_cache": [
      "self"
    ],
    "test_get_kv_cache_shape": [
      "self"
    ],
    "test_ragged_paged_attention_basic": [
      "self"
    ],
    "test_ragged_paged_attention_decode_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_prefill_only": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_mixed": [
      "self",
      "dtype"
    ],
    "test_ragged_paged_attention_sliding_window": [
      "self",
      "sliding_window"
    ],
    "test_ragged_paged_attention_logit_soft_capping": [
      "self",
      "soft_cap"
    ],
    "test_ragged_paged_attention_sliding_window_should_be_positive": [
      "self"
    ],
    "test_ragged_paged_attention_soft_cap_cannot_be_zero": [
      "self"
    ]
  },
  "cdiv": [
    "a",
    "b"
  ],
  "align_to": [
    "x",
    "a"
  ],
  "gen_moe_inputs": [
    "dtype",
    "top_k",
    "num_experts",
    "hidden_size",
    "intermediate_size",
    "num_tokens"
  ],
  "sub_channel_quantize": [
    "x",
    "quant_dtype",
    "wsz"
  ],
  "MoEKernelTest": {
    "setUp": [
      "self"
    ],
    "_test_moe": [
      "self",
      "dtype",
      "top_k",
      "num_experts",
      "hidden_size",
      "intermediate_size",
      "num_tokens",
      "seed",
      "renormalize_topk_logits",
      "bt",
      "bf",
      "bd1",
      "bd2",
      "btc",
      "bfc",
      "bd1c",
      "bd2c",
      "act_fn",
      "w_dtype",
      "subc_quant_wsz",
      "has_bias",
      "atol",
      "rtol"
    ],
    "test_basic": [
      "self",
      "renormalize_topk_logits"
    ],
    "test_activation": [
      "self",
      "act_fn"
    ],
    "test_benchmark_qwen_235": [
      "self"
    ],
    "test_benchmark_qwen_30b_a3b": [
      "self"
    ],
    "test_sub_channel_quantization": [
      "self",
      "w_dtype"
    ],
    "test_bias": [
      "self"
    ]
  },
  "P": [],
  "AllGatherMatmulTest": {
    "test_all_gather_matmul": [
      "self",
      "grid_k",
      "grid_n",
      "rhs_transpose"
    ]
  },
  "TestKVCacheManager": {
    "_setup_runner": [
      "self",
      "use_mla"
    ],
    "setup_method": [
      "self"
    ],
    "test_insert_request_with_kv_cache": [
      "self"
    ],
    "test_get_kv_cache_spec_with_compilation_cfg": [
      "self",
      "num_kv_heads",
      "head_size"
    ],
    "test_get_kv_cache_spec_with_compilation_cfg_mla": [
      "self"
    ],
    "test_get_kv_cache_spec_without_compilation_cfg": [
      "self"
    ],
    "test_get_kv_cache_spec_without_compilation_cfg_mla": [
      "self"
    ],
    "test_initialize_kv_cache": [
      "self"
    ],
    "test_get_kv_cache_spec_with_eagle3": [
      "self"
    ],
    "test_get_kv_cache_spec_with_eagle3_mla": [
      "self"
    ]
  },
  "MAX_NUM_REQS": [],
  "MAX_MODEL_LEN": [],
  "MAX_NUM_BATCHED_TOKENS": [],
  "VOCAB_SIZE": [],
  "BLOCK_SIZES": [],
  "input_batch": [],
  "create_dummy_request": [
    "req_id",
    "prompt_len",
    "output_len",
    "sampling_params",
    "block_ids"
  ],
  "test_initialization": [
    "input_batch"
  ],
  "test_add_request": [
    "input_batch"
  ],
  "test_add_multiple_requests": [
    "input_batch"
  ],
  "test_remove_request": [
    "input_batch"
  ],
  "test_condense": [
    "input_batch"
  ],
  "test_swap_states": [
    "input_batch"
  ],
  "test_all_greedy_property": [
    "input_batch"
  ],
  "TestPersistentBatchManager": {
    "test_update_states_pp_non_last_rank": [
      "self"
    ]
  },
  "TestStructuredDecodingManager": {
    "setup_method": [
      "self"
    ],
    "test_structured_decoding": [
      "self"
    ]
  },
  "test_get_padded_num_reqs_with_upper_limit": [],
  "test_get_paddings": [],
  "test_get_padded_token_len": [],
  "test_get_req_paddings": [],
  "test_latency_tracker": [
    "caplog"
  ],
  "clear_jax_cache": [],
  "jitted_function": [],
  "jnp_array_input": [],
  "jnp_array_input_same_shape": [],
  "jnp_array_input_new": [],
  "test_forbid_compile_raises_error_on_first_call": [
    "jitted_function",
    "jnp_array_input"
  ],
  "test_forbid_compile_succeeds_on_cached_call": [
    "jitted_function",
    "jnp_array_input"
  ],
  "test_forbid_compile_restores_original_function": [],
  "test_forbid_compile_with_exception": [],
  "test_forbid_compile_raises_on_new_shape": [
    "jitted_function",
    "jnp_array_input",
    "jnp_array_input_same_shape",
    "jnp_array_input_new"
  ],
  "MockInputBatch": {
    "__init__": [
      "self",
      "req_ids",
      "num_computed_tokens_cpu"
    ]
  },
  "MockSchedulerOutput": {
    "__init__": [
      "self",
      "num_scheduled_tokens"
    ]
  },
  "test_get_batch_composition_stats": [
    "scenario",
    "num_reqs",
    "req_ids",
    "computed",
    "scheduled",
    "expected_prefill",
    "expected_decode"
  ],
  "test_determine_phase_from_batch_composition_stats": [
    "prefill_tokens",
    "total_tokens",
    "expected_phase"
  ],
  "profiler_fixture": [
    "tmp_path"
  ],
  "test_phased_profiler_full_cycle": [
    "profiler_fixture"
  ],
  "test_phased_profiler_ignores_initial_request": [
    "profiler_fixture"
  ],
  "test_phased_profiler_handles_all_phases": [
    "profiler_fixture"
  ],
  "mesh": [],
  "test_create_kv_caches": [
    "mesh"
  ],
  "test_create_kv_caches_mla": [
    "mesh"
  ],
  "test_get_kv_cache_shape_with_mesh_mla": [
    "mesh"
  ],
  "test_get_attention_page_size_bytes": [
    "mesh"
  ],
  "test_get_attention_page_size_bytes_mla": [
    "mesh"
  ],
  "BlockTable": {
    "__init__": [
      "self",
      "max_num_reqs",
      "max_num_blocks_per_req",
      "max_num_batched_tokens",
      "pin_memory"
    ],
    "append_row": [
      "self",
      "block_ids",
      "row_idx"
    ],
    "add_row": [
      "self",
      "block_ids",
      "row_idx"
    ],
    "move_row": [
      "self",
      "src",
      "tgt"
    ],
    "swap_row": [
      "self",
      "src",
      "tgt"
    ],
    "commit": [
      "self",
      "num_reqs"
    ],
    "clear": [
      "self"
    ],
    "get_device_tensor": [
      "self"
    ],
    "get_cpu_tensor": [
      "self"
    ]
  },
  "MultiGroupBlockTable": {
    "__init__": [
      "self",
      "max_num_reqs",
      "max_model_len",
      "max_num_batched_tokens",
      "pin_memory",
      "block_sizes"
    ],
    "append_row": [
      "self",
      "block_ids",
      "row_idx"
    ],
    "add_row": [
      "self",
      "block_ids",
      "row_idx"
    ],
    "move_row": [
      "self",
      "src",
      "tgt"
    ],
    "swap_row": [
      "self",
      "src",
      "tgt"
    ],
    "commit": [
      "self",
      "num_reqs"
    ],
    "clear": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "block_table_params": [],
  "block_table": [
    "block_table_params"
  ],
  "TestBlockTable": {
    "test_init": [
      "self",
      "block_table",
      "block_table_params"
    ],
    "test_add_and_append_row": [
      "self",
      "block_table"
    ],
    "test_move_row": [
      "self",
      "block_table"
    ],
    "test_swap_row": [
      "self",
      "block_table"
    ],
    "test_commit": [
      "self",
      "block_table"
    ],
    "test_clear": [
      "self",
      "block_table"
    ]
  },
  "TestMultiGroupBlockTable": {
    "multi_table_params": [
      "self"
    ],
    "multi_table": [
      "self",
      "multi_table_params"
    ],
    "test_init": [
      "self",
      "multi_table",
      "multi_table_params"
    ],
    "test_add_row": [
      "self",
      "multi_table"
    ],
    "test_swap_row": [
      "self",
      "multi_table"
    ],
    "test_commit_and_clear": [
      "self",
      "multi_table"
    ]
  },
  "TestTPUJaxRunner": {
    "setup_method": [
      "self"
    ],
    "test_get_supported_tasks_runner": [
      "self"
    ],
    "test_get_input_ids_embeds": [
      "self"
    ],
    "test_prepare_inputs_hybrid_kvcache": [
      "self",
      "mock_sampling_metadata"
    ],
    "_create_mock_hybrid_kv_cache_config": [
      "self"
    ]
  },
  "TestTPUJaxRunnerMultimodalModelLoadedForTextOnly": {
    "setup_method": [
      "self"
    ],
    "_model_get_model": [
      "self"
    ],
    "test_is_multimodal_model": [
      "self"
    ]
  },
  "TestSpeculativeDecodingManager": {
    "setup_method": [
      "self"
    ],
    "test_propose_draft_token_ids_dispatches_to_eagle": [
      "self"
    ],
    "test_propose_draft_token_ids_wrong_drafter_type": [
      "self"
    ],
    "test_take_draft_token_ids": [
      "self"
    ],
    "_setup_spec_decode_metadata_test": [
      "self"
    ],
    "test_get_spec_decode_metadata_parametrized": [
      "self",
      "num_draft_tokens",
      "cu_num_scheduled_tokens",
      "padded_num_reqs",
      "expected_logits_indices",
      "expected_bonus_logits_indices",
      "expected_target_logits_indices",
      "expected_draft_token_ids"
    ],
    "test_propose_eagle3_draft_token_ids": [
      "self",
      "spec_decode_metadata_is_none"
    ]
  },
  "TestTPUJaxRunnerDPInputsLightweight": {
    "setup_method": [
      "self"
    ],
    "_create_mock_scheduler_output": [
      "self",
      "num_scheduled_tokens_dict",
      "assigned_dp_ranks",
      "scheduled_spec_decode_tokens"
    ],
    "_create_mock_hybrid_kv_cache_config": [
      "self"
    ],
    "test_prepare_inputs_dp_basic_functionality": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_inputs_dp_error_conditions": [
      "self"
    ],
    "test_prepare_inputs_dp_hybrid_kvcache": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_dp_input_metadata": [
      "self"
    ],
    "test_prepare_dp_input_metadata_empty_rank": [
      "self"
    ],
    "test_prepare_dp_input_metadata_logits_indices_selector_ordering": [
      "self"
    ],
    "test_prepare_inputs_dp_verify_content_balanced": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_inputs_dp_verify_content_empty_rank": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_inputs_dp_with_decode_requests": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_inputs_dp_all_decode_requests": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_async_token_substitution_indices_dp": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_async_token_substitution_indices_dp_no_placeholders": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_apply_async_token_substitution_empty_indices": [
      "self"
    ],
    "test_apply_async_token_substitution_with_padding": [
      "self",
      "mock_device_array"
    ],
    "test_prepare_inputs_routing_to_dp": [
      "self"
    ],
    "test_prepare_inputs_routing_to_non_dp": [
      "self"
    ],
    "test_prepare_inputs_dp_with_async_scheduling": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ],
    "test_prepare_inputs_dp_async_token_substitution_application": [
      "self",
      "mock_sampling_metadata",
      "mock_device_array",
      "mock_runner_utils",
      "mock_named_sharding"
    ]
  },
  "TestMultiModalManager": {
    "setup_method": [
      "self"
    ],
    "test_execute_mm_encoder_single_image": [
      "self"
    ],
    "test_execute_mm_encoder_multiple_images": [
      "self"
    ],
    "test_gather_mm_embeddings_chunked_prefill": [
      "self"
    ],
    "test_calc_mrope_positions": [
      "self"
    ]
  },
  "TestTPUModelRunnerMeshInit": {
    "mock_vllm_config": [
      "self"
    ],
    "mock_devices": [
      "self"
    ],
    "runner_instance": [
      "self",
      "mock_vllm_config",
      "mock_devices"
    ],
    "test_init_mesh_2d_model_without_device_order": [
      "self",
      "runner_instance",
      "mock_vllm_config"
    ],
    "test_init_mesh_2d_model_with_device_order": [
      "self",
      "runner_instance",
      "mock_vllm_config"
    ],
    "test_init_mesh_new_model_single_slice": [
      "self",
      "runner_instance",
      "mock_vllm_config"
    ],
    "test_init_mesh_new_model_multi_slice": [
      "self",
      "runner_instance",
      "mock_vllm_config"
    ],
    "test_multi_slice_mesh_dp_inner_calculation": [
      "self",
      "runner_instance",
      "mock_vllm_config",
      "num_slices",
      "expected_dp_inner"
    ]
  },
  "MockParam": {
    "__init__": [
      "self",
      "shape"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ]
  },
  "MockVllmConfig": {
    "__init__": [
      "self",
      "model_name",
      "random_weights",
      "tensor_parallelism"
    ]
  },
  "rng": [],
  "mock_vllm_config_8b": [],
  "mock_vllm_config_70b": [],
  "mock_vllm_config_unknown": [],
  "TestLlamaForCausalLM": {
    "test_init_8b_variant": [
      "self",
      "mock_vllm_config_8b",
      "rng",
      "mesh"
    ],
    "test_init_70b_variant": [
      "self",
      "mock_vllm_config_70b",
      "rng",
      "mesh"
    ],
    "test_init_unknown_variant_raises_error": [
      "self",
      "mock_vllm_config_unknown",
      "rng",
      "mesh"
    ],
    "test_create_model_with_random_weights": [
      "self",
      "mock_vllm_config_8b",
      "rng",
      "mesh"
    ],
    "test_load_weights_called_correctly": [
      "self",
      "mock_loader_cls",
      "rng",
      "mesh"
    ]
  },
  "TestLlama3WeightLoader": {
    "weight_loader": [
      "self"
    ],
    "test_load_weights_transformation": [
      "self",
      "weight_loader",
      "rng",
      "mesh"
    ]
  },
  "dist_init": [],
  "DummyLoRAManager": {
    "__init__": [
      "self",
      "device"
    ],
    "set_module_lora": [
      "self",
      "module_name",
      "lora"
    ],
    "get_module_lora": [
      "self",
      "module_name"
    ],
    "init_random_lora": [
      "self",
      "module_name",
      "weight",
      "rank"
    ],
    "init_lora": [
      "self",
      "module_name",
      "input_dim",
      "output_dim",
      "rank",
      "noop",
      "embeddings_tensor"
    ],
    "reset_lora": [
      "self"
    ],
    "init_packed_lora": [
      "self",
      "module_name",
      "input_dim",
      "output_dims",
      "noop_lora_index",
      "rank"
    ]
  },
  "test_bgmv_torch": [],
  "_ref_bgmv_torch": [
    "inputs",
    "loras",
    "idxs"
  ],
  "TP": [],
  "test_lora_performance": [
    "tp"
  ],
  "TOLERANCES": [],
  "pytestmark": [],
  "STAGES": [],
  "check_punica_wrapper": [
    "punica_wrapper"
  ],
  "get_random_index_to_id": [
    "num_loras",
    "num_slots",
    "log"
  ],
  "populate_loras": [
    "index_to_id",
    "lora_layer",
    "baselayer_weights",
    "repeats"
  ],
  "create_random_inputs": [
    "active_lora_ids",
    "num_inputs",
    "input_size",
    "input_range",
    "input_type",
    "device"
  ],
  "test_column_parallel_packed": [
    "dist_init",
    "num_loras",
    "repeats",
    "stage"
  ],
  "test_linear_parallel": [
    "dist_init",
    "num_loras",
    "layer_type",
    "stage"
  ],
  "_create_random_linear_parallel_layer": [
    "layer_type",
    "vllm_config",
    "mesh"
  ],
  "_get_devices": [],
  "_create_mesh": [],
  "_verify_lora_linear_layer": [
    "linear",
    "lora_linear"
  ],
  "_shard_and_move_inputs_to_tpu": [
    "inputs",
    "mesh"
  ],
  "_update_punica_wrapper_metadata": [
    "punica_wrapper",
    "index_mapping",
    "prompt_mapping",
    "stage",
    "index_to_id",
    "lora_config"
  ],
  "_create_column_parallel_packed_layer": [
    "repeats",
    "vllm_config",
    "mesh"
  ],
  "_create_lora_wrapper": [
    "linear",
    "base_linear",
    "lora_cls",
    "vllm_config",
    "mesh",
    "repeats"
  ],
  "setup_vllm": [
    "num_loras",
    "tp"
  ],
  "test_single_lora": [
    "tp"
  ],
  "test_lora_hotswapping": [
    "tp"
  ],
  "test_multi_lora": [
    "tp"
  ],
  "TpuDevice": [],
  "test_get_device_topology_order_id": [],
  "test_get_device_topology_order_id_empty_local": [],
  "test_get_device_topology_order_id_not_in_global": [],
  "TestTPUConnector": {
    "setUp": [
      "self"
    ],
    "test_init_scheduler_role": [
      "self",
      "mock_scheduler_cls",
      "mock_worker_cls"
    ],
    "test_init_worker_role": [
      "self",
      "mock_scheduler_cls",
      "mock_worker_cls"
    ],
    "test_scheduler_methods_are_called": [
      "self",
      "mock_scheduler_cls",
      "mock_worker_cls"
    ],
    "test_worker_methods_are_called": [
      "self",
      "mock_scheduler_cls",
      "mock_worker_cls"
    ]
  },
  "TestTPUConnectorScheduler": {
    "setUp": [
      "self"
    ],
    "test_get_num_new_matched_tokens_producer": [
      "self"
    ],
    "test_get_num_new_matched_tokens_consumer_needs_loading": [
      "self"
    ],
    "test_get_num_new_matched_tokens_consumer_no_loading": [
      "self"
    ],
    "test_update_state_after_alloc_producer": [
      "self"
    ],
    "test_update_state_after_alloc_consumer_with_external_tokens": [
      "self"
    ],
    "test_update_state_after_alloc_consumer_no_external_tokens": [
      "self"
    ],
    "test_build_connector_meta": [
      "self"
    ],
    "test_request_finished_consumer": [
      "self"
    ],
    "test_request_finished_producer_finished_by_length": [
      "self",
      "mock_get_uuid"
    ],
    "test_request_finished_producer_not_finished": [
      "self"
    ],
    "test_request_finished_producer_prompt_too_short": [
      "self"
    ]
  },
  "TestTPUConnectorWorker": {
    "setUp": [
      "self"
    ],
    "test_init_producer": [
      "self"
    ],
    "test_init_consumer": [
      "self"
    ],
    "test_register_runner": [
      "self"
    ],
    "test_process_send_load_for_producer": [
      "self"
    ],
    "test_process_send_load_for_consumer_loading": [
      "self"
    ],
    "test_process_send_load_for_consumer_notifying": [
      "self"
    ],
    "test_get_finished_recving": [
      "self"
    ],
    "test_get_finished_sending_expired": [
      "self"
    ]
  },
  "QuantizationTest": {
    "test_mxfp4_quantization": [
      "self",
      "axis"
    ],
    "test_quantization": [
      "self",
      "dtype",
      "axis"
    ],
    "test_block_quantization": [
      "self",
      "dtype",
      "axis",
      "block_size"
    ],
    "test_multi_block_quantization": [
      "self",
      "dtype",
      "axis",
      "block_size"
    ],
    "test_unaligned_block_quantization_raises_error": [
      "self"
    ],
    "test_block_quantization_padding": [
      "self"
    ],
    "test_quantize_kv": [
      "self",
      "kv_quant_dtype"
    ]
  },
  "TOTAL_TOKENS": [],
  "NUM_SEQS": [],
  "MAX_NUM_SEQS": [],
  "NUM_HEADS": [],
  "NUM_KV_HEADS": [],
  "NUM_BLOCKS": [],
  "BLOCK_SIZE": [],
  "MAX_BLOCKS_PER_SEQ": [],
  "_test_attention": [
    "monkeypatch",
    "mesh",
    "head_dim",
    "use_sinks"
  ],
  "test_attention": [
    "monkeypatch",
    "mesh"
  ],
  "test_attention_hd64": [
    "monkeypatch",
    "mesh"
  ],
  "test_attention_sink": [
    "monkeypatch",
    "mesh"
  ],
  "test_attention_sink_no_64_raises_error": [
    "monkeypatch",
    "mesh"
  ],
  "TestSharding": {
    "setUp": [
      "self"
    ],
    "tearDown": [
      "self"
    ],
    "test_sharding_strategy_init": [
      "self"
    ],
    "test_sharding_config_init": [
      "self"
    ],
    "test_apply_overrides": [
      "self"
    ],
    "test_default_sharding_config": [
      "self"
    ],
    "test_sharding_init_with_overrides": [
      "self"
    ],
    "test_get_overrides_from_vllm_config": [
      "self"
    ]
  },
  "TestTransformerBlock": {
    "test_transformer_block_dense_logic": [
      "self"
    ],
    "test_shared_experts_transformer_block_logic": [
      "self"
    ]
  },
  "TestLayers": {
    "setUp": [
      "self"
    ],
    "test_rmsnorm_forward_pass": [
      "self"
    ],
    "test_denseffw_forward_pass": [
      "self"
    ],
    "test_embedder_forward_pass": [
      "self"
    ],
    "test_embedder_normalization": [
      "self"
    ]
  },
  "RotaryEmbeddingTest": {
    "test_apply_rope": [
      "self"
    ]
  },
  "DeepseekScalingRotaryEmbeddingTest": {
    "test_apply_rope": [
      "self"
    ]
  },
  "mock_nnx": [],
  "mock_jax": [],
  "module_mocks": [],
  "TestParseQwixConfigToRules": {
    "test_empty_config": [
      "self"
    ],
    "test_single_rule": [
      "self"
    ],
    "test_multiple_rules": [
      "self"
    ],
    "test_invalid_rule_key_raises_error": [
      "self"
    ]
  },
  "SimpleModel": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self"
    ]
  },
  "TestQwixQuantizeNnxModel": {
    "setUp": [
      "self"
    ],
    "test_quantization_call_with_correct_args": [
      "self",
      "mock_quantize_model"
    ]
  },
  "TestApplyQwixQuantization": {
    "setUp": [
      "self"
    ],
    "test_no_quantization_config": [
      "self"
    ],
    "test_quantization_applied_from_dict": [
      "self",
      "mock_jit"
    ]
  },
  "TestQuantizationConfigFileToDict": {
    "test_file_not_found_raises_value_error": [
      "self",
      "mock_join",
      "mock_listdir"
    ],
    "test_file_found_and_loaded_successfully": [
      "self",
      "mock_file",
      "mock_join",
      "mock_listdir"
    ]
  },
  "TestApplyQwixQuantizationLogic": {
    "setUp": [
      "self"
    ],
    "test_quantization_config_without_qwix_rules": [
      "self"
    ],
    "test_apply_to_abstract_model": [
      "self",
      "mock_utils",
      "mock_quantize_func"
    ],
    "test_apply_to_abstract_model_with_initialize_cache": [
      "self",
      "mock_utils",
      "mock_quantize_func"
    ]
  },
  "TestDetermineWhetherToApplyQwixOnAbstractModel": {
    "setUp": [
      "self"
    ],
    "test_returns_false_when_additional_config_is_missing": [
      "self"
    ],
    "test_returns_true_when_additional_config_is_present": [
      "self"
    ],
    "test_returns_false_when_use_abstract_model_is_false": [
      "self"
    ]
  },
  "TestLoadRandomWeightsIntoQwixAbstractModel": {
    "setUp": [
      "self"
    ],
    "test_successful_initialization": [
      "self",
      "mock_get_random_array",
      "mock_iter_graph"
    ],
    "test_invalid_config_raises_assertion_error": [
      "self"
    ],
    "test_param_shape_setting_no_scale_map": [
      "self",
      "mock_iter_graph"
    ],
    "test_param_shape_setting_with_scale_map": [
      "self",
      "mock_iter_graph"
    ],
    "test_get_random_sharded_array_dtype_dispatch": [
      "self",
      "mock_make_array",
      "mock_normal",
      "mock_randint"
    ],
    "test_get_random_sharded_array_sharding_fallback": [
      "self",
      "mock_make_array",
      "mock_logger_warning"
    ]
  },
  "TestManualQwixQuantization": {
    "setUp": [
      "self"
    ],
    "test_manually_quantize_qwix_weight": [
      "self",
      "mock_create_param"
    ],
    "test_manually_quantize_qwix_activation": [
      "self",
      "mock_get_rule",
      "mock_quantize_act"
    ],
    "test_manually_quantize_qwix_activation_static_scale_raises_error": [
      "self",
      "mock_get_rule"
    ]
  },
  "TestGetQuantDtypeFromQwixConfig": {
    "setUp": [
      "self"
    ],
    "test_get_quant_dtype_success": [
      "self"
    ],
    "test_get_quant_dtype_default_scale": [
      "self"
    ],
    "test_no_quantization_config_returns_defaults": [
      "self"
    ],
    "test_get_quant_dtype_no_wildcard_rule_returns_none": [
      "self"
    ],
    "test_get_quant_dtype_wildcard_rule_missing_qtype_raises_error": [
      "self"
    ],
    "test_get_quant_dtype_no_rules_key_returns_none": [
      "self"
    ]
  },
  "TestGetDefaultQwixQuantizationConfig": {
    "setUp": [
      "self"
    ],
    "tearDown": [
      "self"
    ],
    "test_skip_quantization_returns_none": [
      "self"
    ],
    "test_unsupported_model_returns_none": [
      "self"
    ],
    "test_deepseek_v3_success": [
      "self"
    ],
    "test_deepseek_v3_invalid_block_size": [
      "self"
    ],
    "test_deepseek_v3_invalid_block_size_2d_subchannel": [
      "self"
    ],
    "test_deepseek_v3_no_weight_block_size": [
      "self"
    ],
    "test_deepseek_v3_tile_size_assertion": [
      "self"
    ],
    "test_llama4_success": [
      "self"
    ],
    "test_gpt_oss_success": [
      "self"
    ],
    "test_missing_attributes_handled": [
      "self"
    ]
  },
  "TestSampling": {
    "test_compute_logprobs": [
      "self"
    ],
    "test_gather_logprobs": [
      "self"
    ],
    "test_gather_logprobs_with_ties": [
      "self"
    ]
  },
  "test_from_input_batch_all_greedy": [
    "mesh"
  ],
  "test_from_input_batch_with_sampling_and_padding": [
    "mesh"
  ],
  "test_from_input_batch_no_padding_needed": [
    "mesh"
  ],
  "test_jax_tree_util_registration": [],
  "test_from_input_batch_with_logprobs": [
    "mesh"
  ],
  "test_from_input_batch_sampling_with_logprobs": [
    "mesh"
  ],
  "PAD_TOKEN_ID": [],
  "DEFAULT_PADDING_FACTOR": [],
  "RejectionSamplerTestCase": {},
  "TestDataFactory": {
    "create_test_case": [
      "name",
      "draft_tokens",
      "target_tokens",
      "num_draft_per_seq",
      "bonus_tokens",
      "expected",
      "description",
      "use_padding"
    ],
    "create_with_padding_variant": [
      "cls",
      "name",
      "draft_tokens",
      "target_tokens",
      "num_draft_per_seq",
      "bonus_tokens",
      "expected",
      "description"
    ],
    "get_basic_test_cases": [
      "cls"
    ],
    "get_variable_length_test_cases": [
      "cls"
    ],
    "get_edge_case_test_cases": [
      "cls"
    ],
    "get_all_test_cases": [
      "cls"
    ]
  },
  "RejectionSamplerTestHelper": {
    "create_target_logits_from_tokens": [
      "target_token_ids",
      "vocab_size"
    ],
    "create_sampling_metadata": [
      "all_greedy",
      "batch_size",
      "top_k",
      "top_p",
      "temperature"
    ],
    "create_padded_draft_tokens": [
      "draft_tokens",
      "padding_factor"
    ],
    "prepare_test_inputs": [
      "test_case",
      "vocab_size"
    ],
    "run_rejection_sampler_test": [
      "rejection_sampler",
      "test_case",
      "vocab_size"
    ]
  },
  "rejection_sampler": [],
  "test_helper": [],
  "test_factory": [],
  "TestRejectionSampler": {
    "test_rejection_sampler_scenarios": [
      "self",
      "rejection_sampler",
      "test_case"
    ],
    "test_multiple_mismatches": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_parse_output_basic": [
      "self",
      "rejection_sampler"
    ],
    "test_parse_output_with_placeholders": [
      "self",
      "rejection_sampler"
    ],
    "test_parse_output_invalid_tokens": [
      "self",
      "rejection_sampler"
    ],
    "test_parse_output_empty_sequences": [
      "self",
      "rejection_sampler"
    ],
    "test_padding_ignored_correctly": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_extreme_padding": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_realistic_flattened_with_padding": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_all_sequences_immediate_mismatch": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_all_sequences_perfect_match": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_extreme_length_imbalance": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_mixed_accept_reject_patterns": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_mismatches_at_same_position": [
      "self",
      "rejection_sampler",
      "test_factory"
    ],
    "test_single_long_sequence": [
      "self",
      "rejection_sampler",
      "test_helper"
    ]
  },
  "TestNonGreedyRejectionSampler": {
    "test_non_greedy_basic_functionality": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_non_greedy_deterministic_with_seed": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_non_greedy_with_draft_probs_none": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_non_greedy_multiple_sequences": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_non_greedy_with_all_accepted_tokens": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_non_greedy_empty_sequence": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_non_greedy_requires_key": [
      "self",
      "rejection_sampler",
      "test_helper"
    ],
    "test_non_greedy_vs_greedy_same_perfect_case": [
      "self",
      "rejection_sampler",
      "test_helper"
    ]
  },
  "TestStatisticalDistributionValidation": {
    "test_rejection_sampling_approximates_target_distribution": [
      "self"
    ],
    "_estimate_rejection_sampling_pdf": [
      "self",
      "draft_probs",
      "target_logits",
      "k",
      "vocab_size",
      "num_samples"
    ],
    "_get_ratio_first_to_last": [
      "self",
      "elements"
    ]
  },
  "TestTopKTopPSampling": {
    "_test_masked_logits": [
      "self",
      "rejection_sampler",
      "batch_size",
      "num_draft_tokens",
      "vocab_size",
      "target_logits",
      "allowed_tokens_per_pos",
      "sampling_metadata"
    ],
    "test_top_k": [
      "self",
      "rejection_sampler",
      "test_helper",
      "top_k"
    ],
    "test_top_p": [
      "self",
      "rejection_sampler",
      "test_helper",
      "top_p"
    ],
    "test_top_k_and_top_p_combined": [
      "self",
      "rejection_sampler",
      "test_helper"
    ]
  },
  "TestDeepSeekV3Router": {
    "setUp": [
      "self"
    ],
    "test_get_topk_indices_single_group": [
      "self"
    ],
    "test_get_topk_indices_2_groups": [
      "self"
    ],
    "test_router_e2e": [
      "self"
    ]
  },
  "TestSparseMoE": {
    "setUp": [
      "self"
    ],
    "test_token_replicated_expert_parallel_fwd": [
      "self"
    ]
  },
  "TestMLA": {
    "setUp": [
      "self"
    ],
    "test_mla_forward_pass": [
      "self",
      "kv_cache_str"
    ],
    "test_mla_kernel_forward_pass": [
      "self",
      "kv_cache_str"
    ]
  },
  "tearDownModule": [],
  "KVCache": [],
  "TestAttention": {
    "setUp": [
      "self"
    ],
    "test_attention_forward_pass": [
      "self",
      "kv_cache_str"
    ]
  },
  "SimpleVLLMConfig": {},
  "Llama4AttentionTest": {
    "setUp": [
      "self"
    ],
    "test_l2norm_forward_pass": [
      "self"
    ],
    "test_l2norm_with_zeros": [
      "self"
    ],
    "test_l2norm_eps_effect": [
      "self"
    ],
    "test_apply_temperature_tuning": [
      "self"
    ]
  },
  "MODELS": [],
  "ref_quantize_int8": [
    "x"
  ],
  "ref_w8a8_int8": [
    "x",
    "w_q",
    "w_s",
    "b"
  ],
  "setup_environment": [],
  "test_quant_override": [
    "model",
    "mesh"
  ],
  "test_loading_model": [
    "model",
    "mesh"
  ],
  "test_row_parallel_linear": [
    "model",
    "bias",
    "num_devices",
    "enable_sp",
    "enable_attn_dp"
  ],
  "test_column_parallel_linear": [
    "model",
    "bias",
    "num_devices",
    "enable_sp",
    "enable_attn_dp"
  ],
  "test_qkv_parallel_linear": [
    "model",
    "bias",
    "num_devices",
    "enable_sp",
    "fuse_matmuls",
    "enable_attn_dp"
  ],
  "test_merged_column_parallel_linear": [
    "model",
    "bias",
    "num_devices",
    "fuse_matmuls",
    "enable_sp",
    "enable_attn_dp"
  ],
  "MODEL": [],
  "_ref_math_in_bf16": [
    "w1",
    "w2",
    "w3",
    "x",
    "router_logits",
    "top_k"
  ],
  "test_fused_moe_method": [
    "mesh",
    "num_tokens",
    "intermediate_size",
    "hidden_size",
    "num_experts",
    "topk",
    "use_ep"
  ],
  "HEAD_DIM": [],
  "create_inputs": [
    "mesh",
    "q_dtype",
    "kv_dtype",
    "total_tokens",
    "num_seqs",
    "max_num_seqs",
    "num_heads",
    "num_kv_heads",
    "head_dim",
    "num_blocks",
    "block_size",
    "max_blocks_per_seq"
  ],
  "TestPallasAttentionBackend": {
    "test_get_name": [
      "self"
    ],
    "test_get_impl_cls": [
      "self"
    ]
  },
  "TestPallasAttentionBackendImpl": {
    "test_init_valid_params": [
      "self"
    ],
    "test_init_with_alibi_slopes_raises_error": [
      "self"
    ],
    "test_init_with_encoder_attention_raises_error": [
      "self"
    ],
    "test_forward": [
      "self",
      "mesh"
    ],
    "test_forward_with_fp8_kv_cache": [
      "self",
      "mesh"
    ],
    "test_forward_with_w8a8": [
      "self",
      "mesh"
    ],
    "test_forward_with_vllm_kv_cache_raises_error": [
      "self",
      "mesh"
    ],
    "test_forward_with_output_scale_raises_error": [
      "self",
      "mesh"
    ],
    "test_forward_with_attention_sink": [
      "self",
      "mesh"
    ],
    "test_forward_with_attention_sink_head_dim_128_raises_error": [
      "self",
      "mesh"
    ]
  },
  "get_spmd_mesh": [
    "num_devices",
    "enable_attn_dp"
  ],
  "find_all_layer_type": [
    "module",
    "layer_type"
  ],
  "ref_moe": [
    "x",
    "router_logits",
    "w1",
    "w2",
    "w1_bias",
    "w2_bias",
    "top_k",
    "renormalize",
    "activation"
  ],
  "ref_quantize_uint4": [
    "x",
    "group_size"
  ],
  "ref_w4a16": [
    "x",
    "w_q",
    "w_z",
    "w_s",
    "b"
  ],
  "pack_awq_weight_into_int32": [
    "weight"
  ],
  "return_ref_and_layer_output": [
    "layer",
    "qweight",
    "qzeros",
    "scales",
    "batch_size"
  ],
  "initialize_and_return_layer_weights": [
    "layer"
  ],
  "ref_quantize_fp8": [
    "x",
    "dtype",
    "per_tensor"
  ],
  "ref_w8a8_fp8_dynamic": [
    "x",
    "w_q",
    "w_s",
    "b"
  ],
  "ref_w8a8_fp8_static": [
    "x",
    "x_s",
    "w_q",
    "w_s",
    "b"
  ],
  "initialize_layer_weights": [
    "layer"
  ],
  "MXFP4_BLOCK_SIZE": [],
  "quantize_to_mxfp4": [
    "weight"
  ],
  "test_mxfp4_fused_moe": [
    "num_devices",
    "num_tokens",
    "intermediate_size",
    "hidden_size",
    "num_experts",
    "topk",
    "use_ep",
    "enable_attn_dp"
  ],
  "test_mxfp4_fused_moe_use_kernel": [
    "num_devices",
    "num_tokens",
    "intermediate_size",
    "hidden_size",
    "num_experts",
    "topk",
    "enable_attn_dp"
  ],
  "test_fused_moe": [
    "use_ep",
    "num_devices",
    "num_tokens",
    "intermediate_size",
    "hidden_size",
    "num_experts",
    "topk",
    "has_bias",
    "activation",
    "enable_attn_dp"
  ],
  "test_fused_moe_use_kernel": [
    "num_devices",
    "num_tokens",
    "intermediate_size",
    "hidden_size",
    "num_experts",
    "topk",
    "has_bias",
    "enable_attn_dp"
  ],
  "mock_vllm_config": [],
  "TestTPUWorker": {
    "test_init_success": [
      "self",
      "mock_vllm_config"
    ],
    "test_init_with_profiler_on_rank_zero": [
      "self",
      "mock_envs",
      "mock_vllm_config"
    ],
    "test_init_with_profiler_on_other_ranks": [
      "self",
      "mock_envs",
      "mock_vllm_config"
    ],
    "test_initialize_cache": [
      "self",
      "mock_vllm_config"
    ],
    "test_init_device_with_provided_devices": [
      "self",
      "mock_ensure_kv_transfer_initialized",
      "mock_jax",
      "mock_utils",
      "mock_runner_cls",
      "mock_vllm_config"
    ],
    "test_init_device_autodetects_devices": [
      "self",
      "mock_ensure_kv_transfer_initialized",
      "mock_jax",
      "mock_utils",
      "mock_runner_cls",
      "mock_vllm_config"
    ],
    "test_determine_available_memory": [
      "self",
      "mock_utils",
      "mock_vllm_config"
    ],
    "test_execute_model": [
      "self",
      "mock_runner_cls",
      "mock_vllm_config"
    ],
    "test_execute_model_non_driver_returns_none": [
      "self",
      "mock_runner_cls",
      "mock_vllm_config"
    ],
    "test_take_draft_token_ids": [
      "self",
      "mock_vllm_config"
    ],
    "test_add_lora_not_implemented": [
      "self",
      "mock_vllm_config"
    ],
    "test_add_lora_not_implemented_lora_request": [
      "self",
      "mock_vllm_config"
    ],
    "test_profile_start": [
      "self",
      "mock_jax",
      "mock_vllm_config"
    ],
    "test_profile_stop": [
      "self",
      "mock_jax",
      "mock_vllm_config"
    ],
    "test_check_health": [
      "self",
      "mock_vllm_config"
    ],
    "test_runner_passthrough_methods": [
      "self",
      "worker_method_name",
      "runner_method_name",
      "method_args",
      "mock_vllm_config"
    ],
    "test_initialize_from_config": [
      "self",
      "mock_vllm_config"
    ],
    "test_initialize_from_config_kv_cache_config": [
      "self",
      "mock_vllm_config"
    ],
    "test_compile_or_warm_up_model": [
      "self",
      "mock_vllm_config"
    ],
    "test_get_supported_tasks": [
      "self",
      "mock_vllm_config"
    ]
  },
  "model_dir": [],
  "eagle3_dir": [],
  "_create_proposer": [
    "method",
    "num_speculative_tokens"
  ],
  "test_prepare_inputs": [],
  "test_propose": [
    "method",
    "num_speculative_tokens"
  ],
  "TestTpuPlatform": {
    "vllm_config": [
      "self"
    ],
    "test_fp8_dtype": [
      "self",
      "chip_name",
      "expected_dtype"
    ]
  },
  "test_prompts": [],
  "sampling_params": [],
  "test_disaggregated_serving": [
    "test_prompts",
    "sampling_params"
  ],
  "_run_inference": [
    "model_name",
    "test_prompts",
    "sampling_params",
    "tensor_parallel_size",
    "is_disagg",
    "prefill_slices",
    "decode_slices"
  ],
  "test_disaggregated_serving_correctness": [
    "test_prompts",
    "sampling_params"
  ],
  "model_name": [],
  "_run_inference_with_config": [
    "model_name",
    "test_prompts",
    "sampling_params",
    "tensor_parallel_size",
    "kv_cache_dtype",
    "enable_prefix_caching",
    "disable_hybrid_kv_cache_manager"
  ],
  "test_hybrid_kv_cache": [
    "model_name",
    "test_prompts",
    "sampling_params"
  ],
  "test_hybrid_kv_cache_correctness": [
    "model_name",
    "test_prompts",
    "sampling_params"
  ],
  "_is_v7x": [],
  "_get_tensor_parallel_size": [],
  "get_ngram_test_prompts": [],
  "get_eagle3_test_prompts": [],
  "get_test_prompts": [
    "speculative_config"
  ],
  "sampling_config": [],
  "_test_correctness_helper": [
    "monkeypatch",
    "sampling_config",
    "model_name",
    "speculative_config"
  ],
  "test_ngram_correctness_greedy": [
    "monkeypatch",
    "sampling_config",
    "model_name"
  ],
  "test_ngram_correctness_random": [
    "monkeypatch",
    "sampling_config",
    "model_name"
  ],
  "_test_performance_helper": [
    "monkeypatch",
    "sampling_config",
    "speculative_config",
    "min_speedup"
  ],
  "test_ngram_performance_greedy": [
    "monkeypatch",
    "sampling_config"
  ],
  "test_ngram_performance_random": [
    "monkeypatch",
    "sampling_config"
  ],
  "test_eagle3_correctness": [
    "monkeypatch",
    "sampling_config"
  ],
  "test_eagle3_performance": [
    "monkeypatch",
    "sampling_config"
  ],
  "test_structured_decoding": [],
  "cleanup_registries": [],
  "DummyGoodModel": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ]
  },
  "test_register_model_success": [
    "cleanup_registries"
  ],
  "test_registered_model_passes_vllm_interface_check": [
    "cleanup_registries"
  ],
  "_run_server_and_bench": [
    "model_name",
    "model_impl_type",
    "port"
  ],
  "test_flax_nnx_vs_vllm_performance": [],
  "test_pipeline_parallelism_jax_model": [
    "model_name",
    "test_prompts",
    "sampling_params"
  ],
  "test_pipeline_parallelism_vllm_model": [
    "model_name",
    "test_prompts",
    "sampling_params"
  ],
  "test_pipeline_parallelism_jax_model_correctness": [
    "model_name",
    "test_prompts",
    "sampling_params"
  ],
  "setup_new_model_design": [],
  "_check_performance": [
    "test_name",
    "baseline_time",
    "dp_time",
    "num_prompts",
    "tol"
  ],
  "_check_correctness": [
    "test_name",
    "baseline_outputs",
    "dp_outputs"
  ],
  "test_attention_data_parallelism": [
    "test_prompts",
    "sampling_params"
  ],
  "test_data_parallelism": [
    "sampling_params",
    "test_prompts"
  ],
  "llm": [],
  "TestTemperature": {
    "test_temperature_zero_is_deterministic": [
      "self",
      "llm"
    ],
    "test_high_temperature_produces_variation": [
      "self",
      "llm"
    ]
  },
  "TestTopP": {
    "test_top_p_restricts_sampling": [
      "self",
      "llm"
    ],
    "test_top_p_with_temperature_zero": [
      "self",
      "llm"
    ]
  },
  "TestTopK": {
    "test_top_k_restricts_sampling": [
      "self",
      "llm"
    ],
    "test_top_k_with_temperature_zero": [
      "self",
      "llm"
    ]
  },
  "TestLogprobs": {
    "test_logprobs_returns_probabilities": [
      "self",
      "llm"
    ],
    "test_logprobs_none_returns_no_probabilities": [
      "self",
      "llm"
    ],
    "test_logprobs_values_are_valid": [
      "self",
      "llm"
    ]
  },
  "TestCombinedParameters": {
    "test_top_p_and_top_k_combined": [
      "self",
      "llm"
    ],
    "test_all_params_with_logprobs": [
      "self",
      "llm"
    ]
  },
  "gcs_model_name": [],
  "hf_model_name": [],
  "prompt": [],
  "test_correctness": [
    "sampling_config",
    "gcs_model_name",
    "hf_model_name",
    "prompt",
    "monkeypatch"
  ],
  "test_performance": [
    "monkeypatch",
    "sampling_config",
    "model_name"
  ],
  "test_async_correctness": [
    "monkeypatch",
    "sampling_config",
    "model_name"
  ],
  "EXPECTED_TEXT": [],
  "test_multi_modal_inference": [
    "monkeypatch",
    "enable_dynamic_image_sizes"
  ],
  "MockModelA": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ]
  },
  "MockModelB": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ]
  },
  "vllm_config": [],
  "test_get_model_architecture_supported": [
    "vllm_config"
  ],
  "test_get_model_architecture_unsupported": [],
  "clear_model_registry_after_test": [],
  "test_register_model_validation": [],
  "test_register_model_new_arch": [],
  "test_register_model_update_arch": [],
  "test_register_model_vllm_wrapper_methods": [],
  "test_get_flax_model": [
    "vllm_config",
    "mesh"
  ],
  "test_get_vllm_model": [
    "mesh"
  ],
  "test_get_vllm_model_random_weights": [
    "mesh"
  ],
  "TestGetModel": {
    "test_get_model_flax_happy_path": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "test_get_model_flax_happy_path_withPP": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "test_get_model_vllm_happy_path": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "test_get_model_flax_fallback_on_unsupported_arch": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "test_get_model_flax_reraises_other_errors": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "test_get_model_not_implemented": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "test_get_model_auto_resolves_to_flax_nnx": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "test_get_model_auto_resolves_to_vllm_for_gpt_oss": [
      "self",
      "mock_get_flax",
      "mock_get_vllm",
      "vllm_config",
      "rng",
      "mesh"
    ]
  },
  "mock_model_inputs": [],
  "mock_get_pp_group": [],
  "TestQwen2ForCausalLM": {
    "test_qwen25_1_5b": [
      "self",
      "mock_vllm_config",
      "rng",
      "mesh",
      "mock_model_inputs"
    ]
  },
  "TestQwen3ForCausalLM": {
    "test_qwen3_600M": [
      "self",
      "mock_vllm_config",
      "rng",
      "mesh",
      "mock_model_inputs"
    ]
  },
  "MockParamLlamaGuard4": {
    "__init__": [
      "self",
      "shape"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ]
  },
  "mock_vllm_config_llama_guard_4": [],
  "TestLlamaGuard4ForCausalLM": {
    "test_init_llama_guard_4": [
      "self",
      "mock_vllm_config_llama_guard_4",
      "rng",
      "mesh"
    ],
    "test_create_model_with_random_weights": [
      "self",
      "mock_vllm_config_llama_guard_4",
      "rng",
      "mesh"
    ],
    "test_load_weights_called_correctly": [
      "self",
      "mock_loader_cls",
      "rng",
      "mesh"
    ]
  },
  "TestLlamaGuard4WeightLoader": {
    "weight_loader": [
      "self"
    ],
    "test_map_loaded_to_standardized_name": [
      "self",
      "weight_loader",
      "hf_key",
      "expected"
    ],
    "test_load_weights_transformation": [
      "self",
      "weight_loader",
      "rng",
      "mesh"
    ]
  },
  "MockVariable": {
    "__init__": [
      "self",
      "shape",
      "dtype",
      "sharding"
    ]
  },
  "mock_config": [],
  "TestDeepSeekV3": {
    "test_init": [
      "self",
      "mock_config",
      "rng",
      "mesh"
    ],
    "test_random_weights": [
      "self",
      "mock_config",
      "rng",
      "mesh"
    ],
    "test_load_weights_called": [
      "self",
      "mock_loader_cls",
      "mock_config",
      "rng",
      "mesh"
    ]
  },
  "TestDeepSeekV3WeightLoader": {
    "loader": [
      "self",
      "mock_config"
    ],
    "test_key_mapping": [
      "self",
      "loader",
      "loaded_key",
      "expected_mapped"
    ],
    "test_transpose_params": [
      "self",
      "loader"
    ],
    "test_moe_stacking_logic": [
      "self",
      "loader"
    ],
    "test_mla_kernel_weight_splitting": [
      "self",
      "loader",
      "mesh"
    ],
    "test_load_individual_weight_with_mxfp4": [
      "self",
      "loader",
      "mesh"
    ],
    "test_load_weights_full_flow": [
      "self",
      "loader",
      "mesh"
    ],
    "test_load_individual_weight_unpacked": [
      "self",
      "loader",
      "mesh"
    ],
    "test_load_individual_weight_with_scale": [
      "self",
      "loader",
      "mesh"
    ]
  },
  "MockParamLlama4": {
    "__init__": [
      "self",
      "shape"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ]
  },
  "mock_vllm_config_llama4": [],
  "TestLlama4ForCausalLM": {
    "test_init_llama4": [
      "self",
      "mock_vllm_config_llama4",
      "rng",
      "mesh"
    ],
    "test_create_model_with_random_weights": [
      "self",
      "mock_vllm_config_llama4",
      "rng",
      "mesh"
    ],
    "test_load_weights_called_correctly": [
      "self",
      "mock_loader_cls",
      "rng",
      "mesh"
    ]
  },
  "TestLlama4WeightLoader": {
    "weight_loader": [
      "self"
    ],
    "test_get_layer_num": [
      "self",
      "weight_loader",
      "hf_key",
      "expected_num"
    ],
    "test_get_expert_num": [
      "self",
      "weight_loader",
      "hf_key",
      "expected_num"
    ],
    "test_map_loaded_to_standardized_name": [
      "self",
      "weight_loader",
      "hf_key",
      "expected"
    ],
    "test_load_weights_transformation": [
      "self",
      "weight_loader",
      "rng",
      "mesh"
    ],
    "test_map_llama4_gate_up_proj": [
      "self",
      "weight_loader",
      "rng",
      "mesh"
    ]
  },
  "MockSpeculativeConfig": {
    "__init__": [
      "self"
    ]
  },
  "TestEagleLlama3ForCausalLM": {
    "test_eagle3_decoder_layer_init": [
      "self",
      "mock_vllm_config",
      "rng",
      "mesh"
    ],
    "test_forward_pass": [
      "self",
      "mock_vllm_config",
      "rng",
      "mesh",
      "mock_model_inputs"
    ],
    "test_load_weights": [
      "self",
      "mock_load_hf_weights",
      "mock_vllm_config",
      "rng",
      "mesh"
    ]
  },
  "SourceLayer": {
    "__init__": [
      "self",
      "rngs"
    ]
  },
  "SourceModel": {
    "__init__": [
      "self",
      "rngs"
    ]
  },
  "TargetLinear": {
    "__init__": [
      "self",
      "rngs"
    ]
  },
  "TargetBlock": {
    "__init__": [
      "self",
      "rngs"
    ]
  },
  "TargetModel": {
    "__init__": [
      "self",
      "rngs"
    ]
  },
  "WeightTransfer": {
    "test_transfer_state": [
      "self"
    ]
  },
  "DtypeTestModel": {
    "__init__": [
      "self",
      "dtype",
      "rngs"
    ]
  },
  "MockModelConfig": {
    "get_vocab_size": [
      "self"
    ],
    "get_hidden_size": [
      "self"
    ],
    "get_head_size": [
      "self"
    ]
  },
  "MockLoadConfig": {},
  "WeightLoadingDtypeTest": {
    "setUp": [
      "self"
    ],
    "test_keep_original_dtype": [
      "self"
    ]
  },
  "rngs": [
    "rng"
  ],
  "TestUtils": {
    "test_apply_rotary_pos_emb_vision": [
      "self",
      "rng"
    ],
    "test_generate_window_segment_ids": [
      "self"
    ]
  },
  "TestQwen2_5_VisionMLP": {
    "test_forward": [
      "self",
      "mock_vllm_config",
      "rngs"
    ]
  },
  "TestQwen2_5_VisionAttention": {
    "test_forward_fullattn": [
      "self",
      "mock_flash_attention",
      "mock_vllm_config",
      "rngs",
      "mesh",
      "rng"
    ],
    "test_forward_windowed": [
      "self",
      "mock_flash_attention",
      "mock_vllm_config",
      "rngs",
      "mesh",
      "rng"
    ],
    "test_batch_fail": [
      "self",
      "mock_vllm_config",
      "rngs",
      "mesh",
      "rng"
    ]
  },
  "TestQwen2_5_VisionBlock": {
    "test_forward": [
      "self",
      "MockAttention",
      "MockMLP",
      "mock_vllm_config",
      "rngs",
      "mesh",
      "rng"
    ]
  },
  "TestQwen2_5_VisionPatchEmbed": {
    "test_forward": [
      "self",
      "mock_vllm_config",
      "rngs",
      "rng"
    ]
  },
  "TestQwen2_5_VisionPatchMerger": {
    "test_forward": [
      "self",
      "mock_vllm_config",
      "rngs",
      "rng"
    ]
  },
  "TestQwen2_5_VisionRotaryEmbedding": {
    "test_forward": [
      "self"
    ]
  },
  "TestQwen2_5_VisionTransformer": {
    "vision_transformer": [
      "self",
      "mock_vllm_config",
      "rngs",
      "mesh"
    ],
    "test_rotary_pos_emb_thw": [
      "self",
      "vision_transformer"
    ],
    "test_get_window_index_thw": [
      "self",
      "vision_transformer"
    ],
    "test_get_rope_by_thw": [
      "self",
      "vision_transformer"
    ],
    "test_call": [
      "self",
      "mock_vllm_config",
      "rngs",
      "mesh",
      "rng",
      "enable_dynamic_image_sizes"
    ]
  },
  "TestQwen2_5_VLForConditionalGeneration": {
    "model": [
      "self",
      "mock_vllm_config",
      "rng",
      "mesh"
    ],
    "test_validate_and_reshape_mm_tensor": [
      "self",
      "model"
    ],
    "test_parse_and_validate_image_input": [
      "self",
      "model"
    ],
    "test_parse_and_validate_multimodal_inputs": [
      "self",
      "model"
    ],
    "test_process_image_input_pixels": [
      "self",
      "model"
    ],
    "test_embed_multimodal": [
      "self",
      "model"
    ],
    "test_embed_input_ids": [
      "self",
      "mock_merge_embeddings",
      "model",
      "rng"
    ],
    "test_call": [
      "self",
      "model",
      "rng"
    ],
    "test_compute_logits": [
      "self",
      "model",
      "rng"
    ],
    "test_load_weights": [
      "self",
      "mock_load_weights",
      "model",
      "mock_vllm_config",
      "rng",
      "mesh"
    ],
    "test_load_weights_tied": [
      "self",
      "mock_load_weights",
      "rng",
      "mesh"
    ]
  },
  "test_sanity_check_valid_list": [],
  "test_sanity_check_valid_tuple": [],
  "test_sanity_check_valid_3d_jax_array": [],
  "test_sanity_check_invalid_type": [],
  "test_sanity_check_wrong_num_items": [],
  "test_sanity_check_wrong_dimensions_in_list": [],
  "test_flatten_single_array": [],
  "test_flatten_single_3d_array": [],
  "test_flatten_list_of_arrays": [],
  "test_flatten_nested_list": [],
  "EMBED_DIM": [],
  "base_embeds": [],
  "test_merge_single_placeholder": [
    "base_embeds"
  ],
  "test_merge_no_placeholders": [
    "base_embeds"
  ],
  "test_merge_mm_embeds_count_too_few": [
    "placeholder_id",
    "base_embeds"
  ],
  "test_merge_mm_embeds_count_too_many_no_raise": [
    "placeholder_id",
    "base_embeds"
  ],
  "DisaggExecutorTest": {
    "setUp": [
      "self"
    ],
    "test_init_with_devices": [
      "self"
    ],
    "test_check_health": [
      "self"
    ]
  },
  "TestPathwaysInit": {
    "test_VLLM_TPU_USING_PATHWAYS_enabled": [
      "self"
    ],
    "test_VLLM_TPU_USING_PATHWAYS_not_enabled": [
      "self"
    ],
    "test_VLLM_TPU_USING_PATHWAYS_case_insensitive": [
      "self"
    ]
  },
  "TestDisaggEngineCore": {
    "setUp": [
      "self"
    ],
    "test_initialization": [
      "self"
    ],
    "test_add_request": [
      "self"
    ],
    "test_shutdown": [
      "self"
    ]
  },
  "TestDisaggEngineCoreProc": {
    "setUp": [
      "self"
    ],
    "test_initialization": [
      "self"
    ],
    "test_add_request": [
      "self"
    ],
    "test_shutdown": [
      "self"
    ],
    "test_handle_client_request_add": [
      "self"
    ],
    "test_handle_client_request_abort": [
      "self"
    ],
    "test_handle_client_request_utility": [
      "self"
    ]
  },
  "TestDisaggOrchestrator": {
    "setUp": [
      "self"
    ],
    "test_initialization": [
      "self"
    ],
    "test_add_request": [
      "self"
    ],
    "test_prefill_logic": [
      "self"
    ],
    "test_transfer_logic": [
      "self"
    ],
    "test_decode_logic": [
      "self"
    ],
    "test_shutdown": [
      "self"
    ]
  },
  "DisaggUtilsTest": {
    "test_parse_slices_valid_cases": [
      "self"
    ],
    "test_parse_slices_with_whitespace": [
      "self"
    ],
    "test_parse_slices_invalid_cases": [
      "self"
    ]
  },
  "TestDPScheduler": {
    "mock_vllm_config": [
      "self"
    ],
    "mock_kv_cache_config": [
      "self"
    ],
    "mock_structured_output_manager": [
      "self"
    ],
    "test_init_creates_worker_processes": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_get_rank_token_counts": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_find_best_rank_with_cache_hit": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_find_best_rank_without_cache_hit": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_add_request_assigns_to_best_rank": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_schedule_sends_commands_and_combines_output": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_combine_cached_request_data": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_finish_requests_routes_to_workers": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_get_num_unfinished_requests": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_has_finished_requests": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_get_request_counts": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_reset_prefix_cache": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_make_stats_aggregates_from_workers": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_make_stats_returns_none_when_disabled": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_update_draft_token_ids": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ],
    "test_shutdown": [
      "self",
      "mock_vllm_config",
      "mock_kv_cache_config",
      "mock_structured_output_manager"
    ]
  },
  "TestUpdateVllmConfigForDPScheduler": {
    "test_update_config_with_dp_size_greater_than_one": [
      "self"
    ],
    "test_update_config_with_dp_size_one": [
      "self"
    ]
  },
  "TestTpuRayDistributedExecutor": {
    "setUp": [
      "self"
    ],
    "test_init_executor_basic_flow": [
      "self",
      "mock_wait_until_pg_ready",
      "mock_avail_resources",
      "mock_get_port",
      "mock_get_ip",
      "mock_platform",
      "mock_ray",
      "mock_envs"
    ],
    "test_initialize_ray_cluster_no_tpu_on_driver_raises_error": [
      "self",
      "mock_wait_until_pg_ready",
      "mock_avail_resources",
      "mock_get_port",
      "mock_get_ip",
      "mock_platform",
      "mock_ray",
      "mock_envs"
    ],
    "test_init_workers_ray_sorts_correctly": [
      "self",
      "mock_wait_until_pg_ready",
      "mock_avail_resources",
      "mock_get_port",
      "mock_get_ip",
      "mock_platform",
      "mock_ray",
      "mock_envs"
    ]
  },
  "logger": [],
  "GCE_TPU_ACCELERATOR_ENDPOINT": [],
  "GCE_TPU_HEADERS": [],
  "get_tpu_metadata": [
    "key"
  ],
  "get_tpu_type": [],
  "get_node_name": [],
  "get_node_worker_id": [],
  "get_num_cores_per_chip": [],
  "get_num_chips": [],
  "init_logger": [
    "name"
  ],
  "env_with_choices": [
    "env_name",
    "default",
    "choices",
    "case_sensitive"
  ],
  "env_bool": [
    "env_name",
    "default"
  ],
  "__getattr__": [
    "name"
  ],
  "enable_envs_cache": [],
  "__dir__": [],
  "GBYTES": [],
  "TPU_HEAD_SIZE_ALIGNMENT": [],
  "TPU_SECOND_LAST_MINOR": [],
  "_VLLM_DTYPE_STR_TO_JAX_DTYPE": [],
  "to_jax_dtype": [
    "dtype"
  ],
  "to_torch_dtype": [
    "dtype"
  ],
  "_megacore": [],
  "enable_megacore": [],
  "get_megacore": [],
  "get_num_kv_heads_by_tp": [
    "num_kv_heads",
    "tp_size"
  ],
  "hbm_usage_bytes": [
    "devices"
  ],
  "get_device_name": [
    "num_devices"
  ],
  "get_device_hbm_limit": [],
  "pathways_hbm_usage_gb": [
    "devices"
  ],
  "hbm_usage_gb": [
    "devices"
  ],
  "get_padded_head_dim": [
    "head_dim"
  ],
  "get_padded_num_heads": [
    "num_heads",
    "sharding_size"
  ],
  "get_dtype_packing": [
    "dtype"
  ],
  "make_optimized_mesh": [
    "axis_shapes",
    "axis_names"
  ],
  "device_array": [
    "mesh"
  ],
  "get_hash_fn_by_name": [
    "hash_fn_name"
  ],
  "get_jax_dtype_from_str_dtype": [
    "str_dtype"
  ],
  "get_mesh_shape_product": [
    "mesh",
    "axes"
  ],
  "time_function": [
    "func"
  ],
  "DEFAULT_MASK_VALUE": [],
  "DEFAULT_VMEM_LIMIT_BYTES": [],
  "get_kv_cache_shape": [
    "total_num_pages",
    "page_size",
    "kv_dim",
    "kv_dtype"
  ],
  "update_kv_cache": [
    "new_kv_c",
    "new_k_pe",
    "cache_kv",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution"
  ],
  "ref_mla_ragged_paged_attention": [
    "ql_nope",
    "q_pe",
    "new_kv_c",
    "new_k_pe",
    "cache_kv",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution"
  ],
  "dynamic_validate_inputs": [
    "ql_nope",
    "q_pe",
    "new_kv_c",
    "new_k_pe",
    "cache_kv",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution"
  ],
  "static_validate_inputs": [
    "ql_nope",
    "q_pe",
    "new_kv_c",
    "new_k_pe",
    "cache_kv",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution"
  ],
  "_mla_ragged_paged_attention_kernel": [
    "kv_lens_ref",
    "page_indices_ref",
    "cu_q_lens_ref",
    "distribution_ref",
    "sem_ids_ref",
    "bo_ids_ref",
    "bkv_update_ids_ref",
    "ql_nope_hbm_ref",
    "q_pe_hbm_ref",
    "new_kv_c_hbm_ref",
    "new_k_pe_hbm_ref",
    "cache_kv_hbm_ref",
    "o_hbm_ref",
    "updated_cache_kv_hbm_ref",
    "bkvc_x2_ref",
    "bkpe_x2_ref",
    "bq_nope_x2_ref",
    "bq_rope_x2_ref",
    "bo_x2_ref",
    "sems",
    "l_ref",
    "m_ref",
    "acc_ref"
  ],
  "prepare_q_inputs": [
    "q"
  ],
  "prepare_kv_inputs": [
    "kv"
  ],
  "prepare_outputs": [
    "out",
    "actual_num_q_heads",
    "actual_head_dim"
  ],
  "mla_ragged_paged_attention": [
    "ql_nope",
    "q_pe",
    "new_kv_c",
    "new_k_pe",
    "cache_kv",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution"
  ],
  "broadcast_minor": [
    "src",
    "shape"
  ],
  "swigluoai": [
    "gate",
    "up"
  ],
  "activation_fn": [
    "acc1",
    "acc3",
    "act_fn"
  ],
  "_fused_ep_moe_kernel": [
    "tokens_hbm",
    "w1_hbm",
    "w2_hbm",
    "w1_scale_hbm",
    "w2_scale_hbm",
    "b1_hbm",
    "b2_hbm",
    "gating_hbm",
    "a2a_g_hbm",
    "output_hbm",
    "t2e_routing_x2_smem",
    "d2e_count_x2_smem",
    "expert_offsets_x2_smem",
    "expert_starts_x2_smem",
    "expert_sizes_x2_smem",
    "a2a_s_sends_x2_smem",
    "a2a_s_x2_vmem",
    "a2a_s_acc_x2_vmem",
    "a2a_g_acc_vmem",
    "b_gating_x2_vmem",
    "b_output_x2_vmem",
    "b_w1_x2_vmem",
    "b_w3_x2_vmem",
    "b_w2_x2_vmem",
    "b_w1_scale_x2_vmem",
    "b_w3_scale_x2_vmem",
    "b_w2_scale_x2_vmem",
    "b_b1_x2_vmem",
    "b_b3_x2_vmem",
    "b_b2_x2_vmem",
    "b_acc_vmem",
    "local_sems",
    "send_sems",
    "recv_sems",
    "a2a_gather_sem",
    "a2a_acc_sem"
  ],
  "fused_ep_moe": [
    "mesh",
    "tokens",
    "w1",
    "w2",
    "gating_output",
    "top_k"
  ],
  "MAX_PAGES_PER_SEQ": [],
  "TUNED_BLOCK_SIZES": [],
  "next_power_of_2": [
    "x"
  ],
  "simplify_key": [
    "key"
  ],
  "get_tpu_version": [],
  "get_min_page_size": [
    "max_model_len",
    "min_page_size"
  ],
  "_ceil_div": [
    "a",
    "b"
  ],
  "_kv_cache_update_kernel": [
    "slices_ref",
    "num_slices_ref",
    "new_kv_hbm_ref",
    "kv_cache_hbm_ref",
    "_",
    "scratch",
    "sem"
  ],
  "_dynamic_validate_inputs": [
    "slices",
    "new_token_num",
    "kv_cache_token_num",
    "page_size",
    "num_slices"
  ],
  "_kv_cache_update": [
    "new_kv",
    "slices",
    "kv_cache",
    "num_slices",
    "page_size",
    "num_slices_per_block",
    "dynamic_validate_inputs",
    "vmem_limit_bytes"
  ],
  "_prev_power_of_2": [
    "n"
  ],
  "_get_page_size_bytes": [
    "block_size",
    "num_combined_kv_heads",
    "head_size",
    "kv_cache_dtype"
  ],
  "_get_num_slices_per_kv_cache_update_block": [
    "page_size_bytes",
    "vmem_limit_bytes"
  ],
  "kv_cache_update": [
    "new_kv",
    "slices",
    "kv_cache",
    "num_slices"
  ],
  "MultiPageAsyncCopyDescriptor": {
    "__init__": [
      "self",
      "pages_hbm_ref",
      "vmem_buf",
      "sem",
      "page_indices_ref",
      "metadata"
    ],
    "start": [
      "self"
    ],
    "wait": [
      "self"
    ]
  },
  "ref_ragged_paged_attention": [
    "queries",
    "kv_pages",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "num_seqs"
  ],
  "ragged_paged_attention_kernel": [
    "kv_lens_ref",
    "page_indices_ref",
    "cu_q_lens_ref",
    "seq_buf_idx_ref",
    "num_seqs_ref",
    "q_ref",
    "kv_pages_hbm_ref",
    "o_ref",
    "kv_bufs",
    "sems",
    "l_ref",
    "m_ref",
    "acc_ref"
  ],
  "get_min_heads_per_blk": [
    "num_q_heads",
    "num_combined_kv_heads",
    "q_dtype",
    "kv_dtype"
  ],
  "ragged_paged_attention": [
    "q",
    "kv_pages",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "num_seqs"
  ],
  "get_lookup_keys": [
    "page_size",
    "q_dtype",
    "kv_dtype",
    "num_q_heads",
    "num_kv_heads",
    "head_dim",
    "max_model_len",
    "sliding_window"
  ],
  "get_simplified_raw_key": [
    "page_size",
    "q_dtype",
    "kv_dtype",
    "actual_num_q_heads",
    "actual_num_kv_heads",
    "head_dim",
    "max_model_len",
    "sliding_window"
  ],
  "get_smem_estimate_bytes": [
    "max_num_seqs",
    "pages_per_seq"
  ],
  "get_vmem_estimate_bytes": [
    "actual_num_kv_heads",
    "actual_num_q_heads_per_kv_head",
    "actual_head_dim",
    "bq_sz",
    "bkv_sz",
    "q_dtype",
    "kv_dtype"
  ],
  "_ragged_paged_attention_kernel": [
    "kv_lens_ref",
    "page_indices_ref",
    "cu_q_lens_ref",
    "distribution_ref",
    "sem_ids_ref",
    "bo_ids_ref",
    "bkv_update_ids_ref",
    "q_hbm_ref",
    "kv_hbm_ref",
    "kv_cache_hbm_ref",
    "o_hbm_ref",
    "updated_kv_cache_hbm_ref",
    "bkv_x2_ref",
    "bq_x2_ref",
    "bo_x2_ref",
    "sems",
    "l_ref",
    "m_ref",
    "acc_ref"
  ],
  "merge_kv": [
    "k",
    "v"
  ],
  "prepare_inputs": [
    "q",
    "k",
    "v"
  ],
  "get_kernel_scope_name": [
    "bq_size",
    "bkv_p",
    "page_size"
  ],
  "ref_ragged_paged_attention_hd64": [
    "queries",
    "keys",
    "values",
    "kv_cache",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution",
    "attention_sink"
  ],
  "ragged_paged_attention_hd64": [
    "queries",
    "keys",
    "values",
    "kv_cache",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution",
    "attention_sink"
  ],
  "get_dtype_bitwidth": [
    "dtype"
  ],
  "NUM_LANES": [],
  "NUM_SUBLANES": [],
  "SegmentIds": {},
  "BlockSizes": {
    "__post_init__": [
      "self"
    ],
    "get_default": [
      "cls",
      "batch_size",
      "num_heads",
      "q_seq_len",
      "kv_len",
      "d_model"
    ]
  },
  "flash_attention": [
    "q",
    "k",
    "v",
    "ab",
    "segment_ids"
  ],
  "_flash_attention": [
    "q",
    "k",
    "v",
    "ab",
    "segment_ids",
    "save_residuals",
    "causal",
    "sm_scale",
    "block_sizes",
    "vmem_limit_bytes",
    "debug"
  ],
  "MIN_BLOCK_SIZE": [],
  "TRANS_B_DIM_NUMBERS": [],
  "below_or_on_diag": [
    "r",
    "r_blk_size",
    "c",
    "c_blk_size"
  ],
  "_flash_attention_kernel": [
    "q_tile_ref"
  ],
  "_flash_attention_kernel_single_batch": [
    "batch_idx",
    "q_tile_ref",
    "k_tile_ref",
    "v_tile_ref",
    "ab_tile_ref",
    "q_segment_ids_tile_ref",
    "kv_segment_ids_tile_ref",
    "o_tile_ref",
    "l_ref",
    "m_ref",
    "m_scratch_ref",
    "l_scratch_ref",
    "acc_scratch_ref"
  ],
  "_flash_attention_kernel_single_batch_single_step": [
    "batch_idx",
    "q_tile_ref",
    "k_tile_ref",
    "v_tile_ref",
    "ab_tile_ref",
    "q_segment_ids_tile_ref",
    "kv_segment_ids_tile_ref",
    "o_tile_ref",
    "l_ref",
    "m_ref"
  ],
  "_bytes": [
    "x"
  ],
  "_fwd_cost_estimate": [
    "q",
    "k",
    "v",
    "ab",
    "segment_ids"
  ],
  "_flash_attention_impl": [
    "q",
    "k",
    "v",
    "ab",
    "segment_ids",
    "save_residuals",
    "causal",
    "sm_scale",
    "block_b",
    "block_q",
    "block_k_major",
    "block_k",
    "vmem_limit_bytes",
    "debug"
  ],
  "mha_reference_no_custom_vjp": [
    "q",
    "k",
    "v",
    "ab",
    "segment_ids"
  ],
  "mha_reference": [
    "q",
    "k",
    "v",
    "ab",
    "segment_ids",
    "causal",
    "mask_value",
    "sm_scale"
  ],
  "_mha_reference": [
    "q",
    "k",
    "v",
    "ab",
    "segment_ids",
    "causal",
    "mask_value",
    "sm_scale",
    "save_residuals"
  ],
  "_verify_block": [
    "block_name",
    "dim_name",
    "block",
    "dim",
    "should_divide"
  ],
  "TunedKey": {},
  "TunedValue": {},
  "TUNED_BLOCK_SIZES_RAW": [],
  "DEVICE_VMEM_LIMIT": [],
  "get_device_vmem_limit": [],
  "get_key": [
    "n_batch",
    "n_out",
    "n_in",
    "x_q_dtype",
    "w_q_dtype"
  ],
  "quantize_array": [
    "x",
    "x_abs_max",
    "quant_dtype"
  ],
  "get_vmem_limit": [
    "n_batch",
    "n_out",
    "n_in",
    "batch_block_size",
    "out_block_size",
    "in_block_size",
    "x_dtype",
    "x_q_dtype",
    "w_q_dtype",
    "scale_dtype",
    "out_dtype",
    "acc_dtype",
    "save_acc",
    "save_x_q",
    "upper_limit_bytes"
  ],
  "validate_inputs": [
    "x",
    "w_q",
    "w_scale",
    "x_abs_max",
    "x_q_dtype",
    "batch_block_size",
    "out_block_size",
    "in_block_size"
  ],
  "matmul_kernel": [
    "x_ref",
    "w_q_ref",
    "w_scale_ref",
    "x_abs_max_ref",
    "out_ref",
    "acc_scratch",
    "x_q_scratch",
    "x_scale_scratch"
  ],
  "unfold_args": [
    "conditions",
    "fn_conditions",
    "fn"
  ],
  "next_multiple": [
    "x",
    "multiple"
  ],
  "get_kernel_name": [
    "tuned_value"
  ],
  "local_barrier": [
    "left_neighbor",
    "right_neighbor",
    "double_barrier"
  ],
  "_cdiv": [
    "x",
    "y"
  ],
  "_all_gather_kernel": [
    "x_hbm_ref",
    "y_hbm_ref",
    "o_hbm_ref",
    "x_hbm_scratch_ref",
    "x_local_copy_sem",
    "y_local_copy_sem",
    "o_local_copy_sem",
    "send_sems",
    "recv_sems",
    "x_vmem_scratch_ref",
    "y_vmem_scratch_ref",
    "o_vmem_scratch_ref",
    "acc_vmem_scratch_ref",
    "axis_name",
    "bn",
    "bk",
    "debug_mode",
    "rhs_transpose"
  ],
  "all_gather_matmul": [
    "x",
    "y",
    "mesh",
    "axis_name",
    "collective_id",
    "bn",
    "bk",
    "rhs_transpose"
  ],
  "partial": [],
  "_validate_args": [],
  "_calculate_num_tiles": [
    "x",
    "tx"
  ],
  "_calculate_irregular_num_tiles": [
    "x",
    "tx"
  ],
  "GroupMetadata": [],
  "make_group_metadata": [],
  "_get_store_mask": [],
  "_zero_uninitialized_memory": [
    "out"
  ],
  "LutFn": [],
  "gmm": [
    "lhs",
    "rhs",
    "group_sizes",
    "preferred_element_type",
    "rhs_scale",
    "rhs_bias",
    "tiling",
    "group_offset",
    "existing_out",
    "transpose_rhs",
    "interpret"
  ],
  "is_tpu": [],
  "tpu_kind": [],
  "_TPU_KIND_PATTERN": [],
  "tpu_generation": [],
  "assert_is_supported_dtype": [
    "dtype"
  ],
  "INVALID_TOKEN_ID": [],
  "MIN_NUM_SEQS": [],
  "DUMMY_METADATA": [],
  "AsyncTPUModelRunnerOutput": {
    "__init__": [
      "self",
      "model_runner_output",
      "next_tokens",
      "num_reqs",
      "discard_sampled_tokens_req_indices",
      "logits_indices_selector"
    ],
    "get_output": [
      "self"
    ]
  },
  "AsyncPreResults": {},
  "ExecuteModelState": {},
  "_substitute_placeholder_token": [
    "input_ids",
    "token_in_tpu_cur_input_indices",
    "token_in_tpu_pre_next_tokens_indices",
    "next_tokens",
    "placeholder_num"
  ],
  "_jax_logprobs_to_lists": [
    "logprobs_tensors",
    "logits_indices_selector",
    "cu_num_generated_tokens"
  ],
  "TPUModelRunner": {
    "__init__": [
      "self",
      "vllm_config",
      "devices",
      "rank",
      "is_first_rank",
      "is_last_rank"
    ],
    "_init_random": [
      "self"
    ],
    "_init_mesh": [
      "self"
    ],
    "_create_new_model_mesh": [
      "self"
    ],
    "_create_single_slice_mesh": [
      "self"
    ],
    "_create_multi_slice_mesh": [
      "self",
      "num_slices"
    ],
    "_create_2d_mesh": [
      "self"
    ],
    "_init_phased_profiling": [
      "self"
    ],
    "_init_mm": [
      "self"
    ],
    "_init_speculative_decoding": [
      "self"
    ],
    "_init_inputs": [
      "self"
    ],
    "load_model": [
      "self"
    ],
    "get_supported_tasks": [
      "self"
    ],
    "get_kv_cache_spec": [
      "self"
    ],
    "initialize_kv_cache": [
      "self",
      "kv_cache_config",
      "topology_order_id"
    ],
    "capture_model": [
      "self"
    ],
    "execute_model": [
      "self",
      "scheduler_output",
      "intermediate_tensors"
    ],
    "sample_tokens": [
      "self",
      "grammar_output"
    ],
    "_modify_prev_results": [
      "self"
    ],
    "_update_placeholder": [
      "self",
      "discard_sampled_tokens_req_indices",
      "request_seq_lens",
      "logits_indices_selector"
    ],
    "_execute_model": [
      "self",
      "scheduler_output",
      "intermediate_tensors"
    ],
    "_sample_from_logits": [
      "self",
      "scheduler_output",
      "attn_metadata",
      "input_ids",
      "hidden_states",
      "logits",
      "aux_hidden_states",
      "spec_decode_metadata",
      "kv_connector_output",
      "logits_indices_selector",
      "padded_num_reqs"
    ],
    "_select_from_array_fn": [
      "self",
      "array",
      "indices_to_select"
    ],
    "_compute_and_gather_logprobs": [
      "logits",
      "next_tokens",
      "max_logprobs"
    ],
    "_prepare_dp_input_metadata": [
      "self",
      "scheduler_output"
    ],
    "_prepare_async_token_substitution_indices_dp": [
      "self",
      "req_ids_dp",
      "scheduled_tokens_per_dp_rank",
      "padded_num_scheduled_tokens_per_dp_rank",
      "dp_size"
    ],
    "_prepare_async_token_substitution_indices_non_dp": [
      "self",
      "num_reqs",
      "num_scheduled_tokens_per_req"
    ],
    "_apply_async_token_substitution": [
      "self",
      "input_ids",
      "token_in_tpu_cur_input_indices",
      "token_in_tpu_pre_next_tokens_indices"
    ],
    "_prepare_inputs": [
      "self",
      "scheduler_output"
    ],
    "_prepare_inputs_dp": [
      "self",
      "scheduler_output"
    ],
    "_prepare_inputs_non_dp": [
      "self",
      "scheduler_output"
    ],
    "_get_input_ids_embeds": [
      "self",
      "input_ids",
      "mm_embeds"
    ],
    "take_draft_token_ids": [
      "self"
    ],
    "get_kv_cache_for_block_ids": [
      "self",
      "block_ids"
    ],
    "transfer_kv_cache": [
      "self",
      "kv_cache_slices"
    ],
    "insert_request_with_kv_cache": [
      "self",
      "request",
      "kv_cache_slices",
      "block_ids"
    ],
    "_sync_weights": [
      "self",
      "updated_weights",
      "mappings",
      "transpose_keys",
      "reshard_fn"
    ],
    "get_intermediate_tensor_spec": [
      "self",
      "num_tokens"
    ],
    "get_uuid_for_jax_transfer": [
      "self",
      "scheduler_output",
      "rank",
      "step"
    ]
  },
  "KVCacheManager": {
    "__init__": [
      "self",
      "runner"
    ],
    "get_kv_cache_spec": [
      "self"
    ],
    "maybe_reinitialize_input_batch": [
      "self",
      "kv_cache_config"
    ],
    "initialize_kv_cache": [
      "self",
      "kv_cache_config"
    ],
    "_jitted_gather_kv_cache": [
      "kv_caches",
      "block_ids"
    ],
    "_jitted_gather_continuous_kv_cache": [
      "kv_caches",
      "start_block",
      "len_block"
    ],
    "_jitted_insert_kv_cache": [
      "block_size",
      "kv_caches",
      "kv_cache_slices",
      "block_numbers"
    ],
    "_jitted_insert_continuous_kv_cache": [
      "block_size",
      "kv_caches",
      "kv_cache_slices",
      "start_block"
    ],
    "get_kv_cache_for_block_ids": [
      "self",
      "block_ids"
    ],
    "transfer_kv_cache": [
      "self",
      "kv_cache_slices"
    ],
    "insert_request_with_kv_cache": [
      "self",
      "request",
      "kv_cache_slices",
      "block_ids"
    ]
  },
  "DEFAULT_KV_CACHE_DTYPE": [],
  "get_kv_cache_shape_with_mesh": [
    "mesh",
    "total_num_pages",
    "page_size",
    "actual_num_kv_heads",
    "actual_head_dim",
    "kv_dtype",
    "use_mla"
  ],
  "create_kv_caches": [
    "num_blocks",
    "block_size",
    "num_kv_heads",
    "head_size",
    "mesh",
    "layer_names",
    "cache_dtype",
    "use_mla"
  ],
  "get_attention_page_size_bytes": [
    "mesh",
    "kv_cache_specs"
  ],
  "BLOCK_BUCKETS": [],
  "CompilationManager": {
    "__init__": [
      "self",
      "runner"
    ],
    "_create_dummy_tensor": [
      "self",
      "shape",
      "dtype",
      "sharding"
    ],
    "_should_skip_padding_combination": [
      "self",
      "outer_val",
      "inner_val",
      "only_equal"
    ],
    "_run_compilation": [
      "self",
      "name",
      "fn"
    ],
    "capture_model": [
      "self"
    ],
    "_precompile_input_embeddings_merger": [
      "self"
    ],
    "_precompile_backbone_helper": [
      "self",
      "name"
    ],
    "_precompile_substitute_placeholder_token": [
      "self"
    ],
    "_precompile_backbone_text_only": [
      "self"
    ],
    "_precompile_backbone_with_inputs_embeds": [
      "self"
    ],
    "_precompile_select_from_array_helper": [
      "self",
      "name",
      "source_paddings",
      "indices_paddings",
      "hidden_dim",
      "input_sharding",
      "indices_sharding",
      "only_equal_paddings",
      "check_should_skip_padding"
    ],
    "_precompile_select_from_array": [
      "self"
    ],
    "_precompile_compute_logits": [
      "self"
    ],
    "_precompile_sampling": [
      "self"
    ],
    "_precompile_disagg_utils": [
      "self"
    ],
    "_precompile_gather_logprobs": [
      "self"
    ],
    "_precompile_speculative_decoding": [
      "self"
    ],
    "_precompile_rejection_sampler": [
      "self"
    ],
    "_precompile_eagle3_helpers": [
      "self"
    ],
    "_precompile_structured_decoding": [
      "self"
    ]
  },
  "_SAMPLING_EPS": [],
  "CachedRequestState": {
    "__post_init__": [
      "self"
    ],
    "num_tokens": [
      "self"
    ],
    "get_token_id": [
      "self",
      "idx"
    ]
  },
  "InputBatch": {
    "__init__": [
      "self",
      "max_num_reqs",
      "max_model_len",
      "max_num_batched_tokens",
      "pin_memory",
      "vocab_size",
      "block_sizes",
      "is_spec_decode"
    ],
    "req_ids": [
      "self"
    ],
    "add_request": [
      "self",
      "request",
      "req_index"
    ],
    "remove_request": [
      "self",
      "req_id"
    ],
    "swap_states": [
      "self",
      "i1",
      "i2"
    ],
    "condense": [
      "self",
      "empty_req_indices"
    ],
    "num_reqs": [
      "self"
    ],
    "all_greedy": [
      "self"
    ],
    "max_num_logprobs": [
      "self"
    ],
    "make_lora_inputs": [
      "self",
      "num_scheduled_tokens"
    ]
  },
  "PREFILL_HEAVY_RATIO_THRESHOLD": [],
  "DECODE_HEAVY_RATIO_THRESHOLD": [],
  "BALANCED_RATIO_THRESHOLD": [],
  "PHASED_PROFILER_NUM_STEPS_TO_PROFILE_FOR": [],
  "InferencePhase": {
    "PREFILL_HEAVY": [],
    "DECODE_HEAVY": [],
    "BALANCED": [],
    "AMBIGUOUS": []
  },
  "get_padded_num_reqs_with_upper_limit": [
    "x",
    "upper_limit"
  ],
  "get_req_paddings": [
    "min_req_size",
    "max_req_size"
  ],
  "get_token_paddings": [
    "min_token_size",
    "max_token_size",
    "padding_gap"
  ],
  "get_padded_token_len": [
    "paddings",
    "x"
  ],
  "LatencyTracker": {
    "__init__": [
      "self",
      "name"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_val",
      "exc_tb"
    ]
  },
  "ForbidCompile": {
    "__init__": [
      "self",
      "message"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_value",
      "traceback"
    ]
  },
  "get_batch_composition_stats": [
    "input_batch",
    "total_num_scheduled_tokens",
    "num_reqs",
    "padded_total_num_scheduled_tokens",
    "scheduler_output"
  ],
  "determine_phase_from_batch_composition_stats": [
    "batch_composition_stats"
  ],
  "PhasedBasedProfiler": {
    "__init__": [
      "self",
      "profile_dir"
    ],
    "_write_batch_composition_stats_to_file_helper": [
      "self",
      "batch_composition_stats"
    ],
    "_start_profiling": [
      "self",
      "batch_composition_stats"
    ],
    "_step_or_stop_profiling": [
      "self",
      "batch_composition_stats"
    ],
    "step": [
      "self",
      "batch_composition_stats"
    ]
  },
  "LoraUtils": {
    "__init__": [
      "self",
      "runner"
    ],
    "set_active_loras": [
      "self",
      "num_scheduled_tokens_per_req",
      "total_num_scheduled_tokens",
      "padded_total_num_scheduled_tokens"
    ],
    "extract_lora_metadata": [
      "self"
    ]
  },
  "replace_lora_metadata": [
    "model",
    "metadata",
    "lora_config"
  ],
  "StructuredDecodingManager": {
    "__init__": [
      "self",
      "runner"
    ],
    "structured_decode_fn": [
      "self",
      "require_struct_decoding",
      "grammar_bitmask",
      "logits",
      "arange"
    ],
    "_apply_grammar_bitmask_kernel": [
      "self",
      "logits",
      "grammar_bitmask",
      "require_struct_decoding",
      "arange"
    ],
    "prepare_structured_decoding_input": [
      "self",
      "logits",
      "grammar_output"
    ]
  },
  "MultiModalManager": {
    "__init__": [
      "self",
      "runner"
    ],
    "calc_mrope_positions": [
      "self",
      "scheduler_output"
    ],
    "execute_mm_encoder": [
      "self",
      "scheduler_output"
    ],
    "gather_mm_embeddings": [
      "self",
      "scheduler_output",
      "target_pad_len"
    ]
  },
  "SpecDecodeMetadata": {},
  "SpeculativeDecodingManager": {
    "__init__": [
      "self",
      "runner"
    ],
    "take_draft_token_ids": [
      "self"
    ],
    "propose_draft_token_ids": [
      "self",
      "sampled_token_ids",
      "aux_hidden_states",
      "attn_metadata",
      "spec_decode_metadata",
      "scheduler_output",
      "input_ids"
    ],
    "propose_eagle3_draft_token_ids": [
      "self",
      "sampled_token_ids",
      "aux_hidden_states",
      "attn_metadata",
      "spec_decode_metadata",
      "scheduler_output",
      "input_ids"
    ],
    "get_spec_decode_metadata": [
      "self",
      "num_draft_tokens",
      "cu_num_scheduled_tokens",
      "padded_num_reqs"
    ]
  },
  "PersistentBatchManager": {
    "__init__": [
      "self",
      "requests",
      "input_batch",
      "encoder_cache",
      "uses_mrope",
      "model_config",
      "is_last_rank"
    ],
    "_reorder_batch": [
      "self",
      "scheduler_output"
    ],
    "update_states": [
      "self",
      "scheduler_output",
      "get_mrope_input_positions_fn"
    ]
  },
  "LlamaForCausalLM": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh",
      "force_random_weights"
    ],
    "load_weights": [
      "self",
      "rng",
      "cache_dir"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ]
  },
  "Llama3WeightLoader": {
    "__init__": [
      "self",
      "vllm_config",
      "hidden_size",
      "attn_heads",
      "num_key_value_heads",
      "attn_head_dim"
    ],
    "load_weights": [
      "self",
      "model_for_loading"
    ]
  },
  "PunicaWrapperTPU": {
    "__init__": [
      "self",
      "max_num_batched_tokens",
      "max_batches",
      "device"
    ],
    "_get_token_lora_indices": [
      "self",
      "x"
    ],
    "embeddings_indices": [
      "self"
    ],
    "sampler_indices_padded": [
      "self"
    ],
    "add_shrink": [
      "self",
      "y",
      "x",
      "lora_a_stacked",
      "scale"
    ],
    "add_expand": [
      "self",
      "y",
      "x",
      "lora_b_stacked",
      "output_slices",
      "offset_start",
      "add_inputs"
    ],
    "add_lora_embedding": [
      "self",
      "y",
      "x",
      "lora_b_stacked",
      "add_inputs"
    ],
    "add_lora_linear": [
      "self",
      "y",
      "x",
      "lora_a_stacked",
      "lora_b_stacked",
      "scale",
      "output_slices"
    ],
    "add_lora_logits": [
      "self",
      "y",
      "x",
      "lora_a_stacked",
      "lora_b_stacked",
      "scale"
    ],
    "token_lora_indices": [
      "self"
    ],
    "_update_base_metadata": [
      "self",
      "mapping",
      "lora_index_to_id",
      "max_loras",
      "vocab_size"
    ],
    "_update_prefill_metadata": [
      "self",
      "token_lora_tensor"
    ],
    "_pad_prompt_mapping": [
      "self",
      "prompt_mapping"
    ],
    "_pad_to_shape": [
      "self",
      "src",
      "target_shape",
      "dims"
    ]
  },
  "bgmv_jax": [
    "inputs",
    "loras",
    "idxs"
  ],
  "bgmv_torch": [
    "inputs",
    "loras",
    "idxs"
  ],
  "bgmv_shrink": [
    "inputs",
    "lora_b_weights",
    "lora_indices_tensor",
    "scaling"
  ],
  "bgmv_expand_slice": [
    "inputs",
    "lora_b_weights",
    "output_tensor",
    "lora_indices_tensor",
    "slice_offset",
    "slice_size",
    "add_inputs"
  ],
  "ReqId": [],
  "P2P_WAIT_PULL_TIMEOUT": [],
  "SendMeta": {},
  "LoadMeta": {},
  "_kv_transfer_params": {},
  "TPUConnectorMetadata": {},
  "TPUConnector": {
    "__init__": [
      "self",
      "vllm_config",
      "role"
    ],
    "get_num_new_matched_tokens": [
      "self",
      "request",
      "num_computed_tokens"
    ],
    "update_state_after_alloc": [
      "self",
      "request",
      "blocks",
      "num_external_tokens"
    ],
    "build_connector_meta": [
      "self",
      "scheduler_output"
    ],
    "request_finished": [
      "self",
      "request",
      "block_ids"
    ],
    "get_finished_count": [
      "self"
    ],
    "register_kv_caches": [
      "self",
      "kv_caches"
    ],
    "register_runner": [
      "self",
      "runner"
    ],
    "start_load_kv": [
      "self",
      "_"
    ],
    "wait_for_layer_load": [
      "self",
      "layer_name"
    ],
    "save_kv_layer": [
      "self"
    ],
    "wait_for_save": [
      "self"
    ],
    "get_finished": [
      "self",
      "finished_req_ids"
    ]
  },
  "TPUConnectorScheduler": {
    "__init__": [
      "self",
      "vllm_config"
    ],
    "get_num_new_matched_tokens": [
      "self",
      "request",
      "num_computed_tokens"
    ],
    "update_state_after_alloc": [
      "self",
      "request",
      "blocks",
      "num_external_tokens"
    ],
    "build_connector_meta": [
      "self"
    ],
    "get_finished_count": [
      "self"
    ],
    "request_finished": [
      "self",
      "request",
      "block_ids"
    ]
  },
  "TPUConnectorWorker": {
    "__init__": [
      "self",
      "vllm_config"
    ],
    "__del__": [
      "self"
    ],
    "register_runner": [
      "self",
      "runner"
    ],
    "_maybe_start_p2p_server": [
      "self"
    ],
    "_pull_notify_listener": [
      "self",
      "ready_event"
    ],
    "process_send_load": [
      "self",
      "metadata"
    ],
    "_prepare_kv_and_wait": [
      "self",
      "req_id",
      "req_meta"
    ],
    "_maybe_build_kv_connection": [
      "self",
      "req_meta"
    ],
    "_pull_kv": [
      "self",
      "conn",
      "req_meta"
    ],
    "_get_kv_spec": [
      "self",
      "num_blocks"
    ],
    "_maybe_build_notif_socket": [
      "self",
      "req_meta"
    ],
    "_notify_pull_done": [
      "self",
      "sock",
      "req_id"
    ],
    "get_finished": [
      "self"
    ]
  },
  "get_uuid": [],
  "select_from_kv_caches": [
    "kv_caches",
    "indices"
  ],
  "scatter_kv_slices": [
    "kv_caches",
    "kv_slices",
    "indices"
  ],
  "_NODES_KV_IP_PORT": [],
  "set_node_kv_ip_port": [
    "ip_port"
  ],
  "get_kv_ips": [],
  "get_kv_ports": [],
  "get_host_ip": [],
  "get_kv_transfer_port": [],
  "get_side_channel_port": [],
  "get_device_topology_order_id": [
    "local_devices",
    "global_devices"
  ],
  "BASE_JAX_PORT": [],
  "GroupCoordinator": {
    "__init__": [
      "self",
      "rank_in_group",
      "world_size"
    ],
    "send_tensor_dict": [
      "self",
      "uuid",
      "tensor_dict"
    ],
    "recv_tensor_dict": [
      "self",
      "uuid",
      "tensor_spec"
    ],
    "is_first_rank": [
      "self"
    ],
    "is_last_rank": [
      "self"
    ]
  },
  "init_pp_distributed_environment": [
    "ip",
    "rank",
    "world_size",
    "device",
    "need_pp"
  ],
  "connect": [
    "prev_ip",
    "prev_rank"
  ],
  "get_pp_group": [],
  "MAX_ALLOWED_PAGE_INDICES_N": [],
  "get_kv_cache_shape_hd64": [],
  "sharded_flash_attention": [
    "mesh",
    "causal",
    "sm_scale",
    "vmem_limit_bytes"
  ],
  "sharded_paged_attention": [
    "mesh",
    "attn_logits_soft_cap"
  ],
  "paged_attention_with_guarded_smem": [
    "paged_attention_kernel",
    "q",
    "k_pages",
    "v_pages",
    "lengths",
    "page_indices"
  ],
  "update_cache": [
    "is_prefill",
    "cache",
    "indices",
    "operand",
    "prefill_seq_len",
    "sliding_window"
  ],
  "apply_splash": [
    "q",
    "k",
    "v",
    "window_size",
    "attn_logits_soft_cap",
    "is_mqa"
  ],
  "sharded_splash_attention": [
    "mesh",
    "window_size",
    "attn_logits_soft_cap",
    "is_mqa"
  ],
  "sharded_ragged_paged_attention": [
    "mesh",
    "q",
    "k",
    "v",
    "kv_cache",
    "kv_lens",
    "page_indices",
    "cu_q_lens",
    "distribution",
    "attention_sink",
    "sm_scale",
    "attention_chunk_size",
    "q_scale",
    "k_scale",
    "v_scale"
  ],
  "attention": [
    "kv_cache",
    "q",
    "k",
    "v",
    "attention_metadata",
    "mesh",
    "head_dim_original",
    "attention_chunk_size",
    "q_scale",
    "k_scale",
    "v_scale",
    "sinks"
  ],
  "AttentionMetadata": {},
  "quantize_tensor_to_mxfp4_packed": [
    "tensor",
    "axis"
  ],
  "u8_unpack_e2m1": [
    "u8_packed_e2m1"
  ],
  "e8m0_to_fp32": [
    "u8"
  ],
  "dequantize_tensor": [
    "tensor_q",
    "scale",
    "axis",
    "out_dtype"
  ],
  "dequantize_tensor_from_mxfp4_packed": [
    "tensor_q",
    "scale",
    "axis",
    "out_dtype"
  ],
  "static_per_tensor_quantize_tensor": [
    "dtype",
    "tensor",
    "scale"
  ],
  "quantize_kv": [
    "dtype",
    "key",
    "value",
    "k_scale",
    "v_scale"
  ],
  "MESH_AXIS_NAMES": [],
  "MESH_AXIS_NAMES_2D": [],
  "ShardingAxisNameBase": {
    "SEQUENCE": [],
    "ATTN_DATA": [],
    "MLP_DATA": [],
    "ATTN_HEAD": [],
    "ATTN_TENSOR": [],
    "MLP_TENSOR": [],
    "MOE_TENSOR": [],
    "EXPERT": [],
    "VOCAB": []
  },
  "ShardingAxisName2D": {
    "SEQUENCE": [],
    "ATTN_DATA": [],
    "MLP_DATA": [],
    "ATTN_HEAD": [],
    "ATTN_TENSOR": [],
    "MLP_TENSOR": [],
    "MOE_TENSOR": [],
    "EXPERT": [],
    "VOCAB": []
  },
  "ShardingStrategy": {},
  "ShardingConfigManager": {
    "__init__": [
      "self",
      "sharding_strategy",
      "device_indexes"
    ],
    "from_vllm_config": [
      "cls",
      "vllm_config"
    ],
    "validate": [
      "cls",
      "vllm_config",
      "sharding_strategy"
    ],
    "total_dp_size": [
      "self"
    ],
    "model_dp_size": [
      "self"
    ],
    "attn_dp_size": [
      "self"
    ],
    "tp_size": [
      "self"
    ],
    "expert_size": [
      "self"
    ],
    "sequence_size": [
      "self"
    ],
    "total_devices": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "ShardingRulesConfig": {},
  "ShardingConfig": {
    "__init__": [
      "self",
      "prefill_rules",
      "generate_rules",
      "default_rules_cls"
    ]
  },
  "build_mesh": [
    "devices",
    "strategy"
  ],
  "Sharding": {
    "__init__": [
      "self",
      "prefill_rules",
      "generate_rules",
      "default_rules_cls",
      "vllm_config"
    ],
    "_get_overrides": [
      "self",
      "sharding_phase"
    ],
    "__str__": [
      "self"
    ],
    "validate_sharding_strategy": [
      "self"
    ],
    "get_sharding_cfg": [
      "self"
    ],
    "_apply_overrides": [
      "self",
      "config_obj",
      "overrides"
    ],
    "_make_default_sharding_config": [
      "self",
      "prefill_rules",
      "generate_rules"
    ],
    "make_sharding_config": [
      "self",
      "default_rules_cls",
      "prefill_overrides",
      "generate_overrides"
    ]
  },
  "ShardingInfo": {},
  "int32_bsearch": [
    "batch_shape",
    "predicate"
  ],
  "_monotonic_int32_to_float32_bit_pattern": [
    "x"
  ],
  "_monotonic_int32_to_float32": [
    "x"
  ],
  "float32_bsearch": [
    "batch_shape",
    "predicate"
  ],
  "topk_mask": [
    "x",
    "k",
    "replace_val"
  ],
  "topp_mask": [
    "logits",
    "p",
    "replace_val"
  ],
  "UNQUANTIZED": [],
  "MXFP4": [],
  "AWQ": [],
  "COMPRESSED_TENSORS": [],
  "FP8": [],
  "get_tpu_quant_method": [
    "quant_method"
  ],
  "Initializer": [],
  "_scale_initializer": [],
  "_sharded_initializer": [],
  "_init_fn": [],
  "Config": {
    "from_cfg": [
      "cls",
      "cfg"
    ],
    "maybe_apply_overrides": [
      "self"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "create_param": [
    "rngs",
    "shape",
    "sharding",
    "dtype",
    "random_init"
  ],
  "PPMissingLayer": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self"
    ]
  },
  "LayerFn": {
    "__call__": [
      "self"
    ]
  },
  "make_layers": [
    "num_hidden_layers",
    "layer_fn"
  ],
  "FlaxUtils": {
    "ACT2FN": []
  },
  "modeling_flax_utils": [],
  "RuntimeParams": {},
  "RMSNorm": {
    "__call__": [
      "self",
      "x_TD",
      "op_mode"
    ],
    "__post_init__": [
      "self",
      "rngs"
    ]
  },
  "DenseFFW": {
    "__call__": [
      "self",
      "x_TD"
    ],
    "__post_init__": [
      "self",
      "rngs"
    ]
  },
  "Embedder": {
    "__post_init__": [
      "self",
      "rngs"
    ],
    "__call__": [
      "self",
      "x",
      "decode"
    ],
    "decode": [
      "self",
      "x_TD"
    ],
    "encode": [
      "self",
      "x_T"
    ]
  },
  "LMhead": {
    "__post_init__": [
      "self",
      "rngs"
    ],
    "__call__": [
      "self",
      "x"
    ],
    "decode": [
      "self",
      "x_TD"
    ]
  },
  "shard_put": [
    "x",
    "sharding_names",
    "mesh"
  ],
  "RouterType": {
    "TOP_K": []
  },
  "OPERATION_MODE": {
    "PREFILL": [],
    "DECODE": []
  },
  "HuggingFaceArgNames": {},
  "apply_rope": [
    "inputs",
    "positions",
    "head_dim",
    "rope_theta",
    "rope_scaling",
    "rope_input_ordering"
  ],
  "apply_longrope": [
    "inputs",
    "positions",
    "head_dim",
    "rope_scaling",
    "original_max_position_embeddings",
    "max_position_embeddings",
    "rope_theta"
  ],
  "apply_rope_scaling": [
    "freqs",
    "rope_scaling"
  ],
  "RotaryEmbedding": {
    "initialize_cache": [
      "self"
    ],
    "_compute_inv_freq": [
      "self"
    ],
    "_compute_sin_cos": [
      "self"
    ],
    "apply_rope": [
      "self",
      "positions",
      "x_TNH"
    ]
  },
  "DeepseekScalingRotaryEmbedding": {
    "initialize_cache": [
      "self",
      "mesh"
    ],
    "_compute_inv_freq": [
      "self"
    ],
    "_compute_sin_cos": [
      "self"
    ],
    "apply_rope": [
      "self",
      "positions",
      "x_TNH"
    ]
  },
  "_yarn_get_mscale": [
    "scale",
    "mscale"
  ],
  "_yarn_find_correction_dim": [
    "num_rotations",
    "dim",
    "base",
    "max_position_embeddings"
  ],
  "_yarn_find_correction_range": [
    "low_rot",
    "high_rot",
    "dim",
    "base",
    "max_position_embeddings"
  ],
  "_yarn_linear_ramp_mask": [
    "min",
    "max",
    "dim"
  ],
  "GptOssRotaryEmbedding": {
    "_compute_concentration_and_inv_freq": [
      "self"
    ],
    "_compute_cos_sin": [
      "self",
      "positions"
    ],
    "__call__": [
      "self",
      "query_TNH",
      "key_TNH",
      "positions"
    ]
  },
  "TransformerBlock": {
    "__call__": [
      "self",
      "x_TD",
      "is_prefill",
      "kv_cache",
      "attention_metadata"
    ]
  },
  "SharedExpertsTransformerBlock": {
    "__call__": [
      "self",
      "x_TD",
      "is_prefill",
      "kv_cache",
      "attention_metadata"
    ]
  },
  "PLACEHOLDER_TOKEN_ID": [],
  "GREEDY_TEMPERATURE": [],
  "RejectionSampler": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "draft_token_ids",
      "num_draft_tokens",
      "draft_probs",
      "target_logits",
      "bonus_token_ids",
      "sampling_metadata",
      "key"
    ],
    "forward": [
      "self",
      "draft_token_ids",
      "num_draft_tokens",
      "draft_probs",
      "target_logits",
      "bonus_token_ids",
      "sampling_metadata",
      "key"
    ],
    "parse_output": [
      "output_token_ids",
      "vocab_size",
      "num_draft_tokens_cpu",
      "batch_size",
      "padded_tokens_length"
    ]
  },
  "_compute_probs": [
    "logits",
    "num_draft_tokens",
    "sampling_metadata"
  ],
  "_get_segment_info": [
    "num_draft_tokens",
    "total_tokens"
  ],
  "_sample_recovered_tokens": [
    "draft_token_ids",
    "draft_probs",
    "target_probs",
    "key"
  ],
  "rejection_sample": [
    "draft_token_ids",
    "num_draft_tokens",
    "draft_probs",
    "target_probs",
    "bonus_token_ids",
    "sampling_metadata",
    "key"
  ],
  "_random_rejection_sample_with_segment": [
    "draft_token_ids",
    "draft_probs",
    "target_probs",
    "num_draft_tokens",
    "bonus_token_ids",
    "key"
  ],
  "_greedy_rejection_sample_with_segment": [
    "draft_token_ids",
    "target_probs",
    "num_draft_tokens",
    "bonus_token_ids"
  ],
  "DEFAULT_SAMPLING_PARAMS": [],
  "TPUSupportedSamplingMetadata": {
    "from_input_batch": [
      "cls",
      "mesh",
      "input_batch",
      "padded_num_reqs",
      "sharding"
    ]
  },
  "sample": [
    "rng",
    "mesh",
    "logits",
    "tpu_sampling_metadata"
  ],
  "compute_logprobs": [
    "logits"
  ],
  "gather_logprobs": [
    "logprobs",
    "token_ids",
    "num_logprobs"
  ],
  "DeepSeekV3Router": {
    "get_topk_indices": [
      "self",
      "scores_TE"
    ],
    "__call__": [
      "self",
      "x_TD"
    ],
    "__post_init__": [
      "self",
      "rngs"
    ]
  },
  "SparseMoE": {
    "__post_init__": [
      "self",
      "rngs"
    ],
    "_sort_activations": [
      "self",
      "inputs",
      "sort_indices"
    ],
    "get_all_to_all_params": [
      "all_shards_group_sizes",
      "shard_id",
      "num_expert_parallelism",
      "is_batch_sharded"
    ],
    "_local_permute": [
      "self",
      "inputs",
      "global_group_sizes",
      "local_expert_size",
      "shard_index",
      "is_offset",
      "global_sorted_experts"
    ],
    "_permute": [
      "self",
      "inputs_TD",
      "selected_experts_TX"
    ],
    "_unpermute": [
      "self",
      "processed_tokens",
      "sort_indices",
      "router_weights_TX"
    ],
    "_gmm": [
      "self",
      "inputs",
      "kernel",
      "group_sizes"
    ],
    "_distributed_sparse_moe_fwd": [
      "self",
      "x_TD",
      "router_weights_TX",
      "selected_experts_TX",
      "kernel_gating",
      "kernel_up_proj",
      "kernel_down_proj"
    ],
    "__call__": [
      "self",
      "x_TD"
    ]
  },
  "CombineExperts": {
    "__call__": [
      "self",
      "expert_outputs_TED",
      "weights_TE"
    ]
  },
  "Router": {
    "__call__": [
      "self",
      "x_TD"
    ],
    "__post_init__": [
      "self",
      "rngs"
    ]
  },
  "MoE": {
    "__call__": [
      "self",
      "x_TD"
    ],
    "__post_init__": [
      "self",
      "rngs"
    ],
    "_moe_fwd_preapply_router_weights": [
      "self",
      "x_TD",
      "weights_TE"
    ],
    "_moe_fwd": [
      "self",
      "x_TD",
      "weights"
    ]
  },
  "GptOssRouter": {
    "__post_init__": [
      "self",
      "rngs"
    ],
    "__call__": [
      "self",
      "x_TD"
    ]
  },
  "_swiglu": [
    "x",
    "alpha",
    "limit"
  ],
  "GptOssMoE": {
    "__call__": [
      "self",
      "x_TD"
    ],
    "__post_init__": [
      "self",
      "rngs"
    ]
  },
  "L2Norm": {
    "__init__": [
      "self",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Llama4Attention": {
    "__call__": [
      "self",
      "x",
      "is_prefill",
      "kv_cache",
      "attention_metadata",
      "use_attention_rope"
    ],
    "apply_temperature_tuning": [
      "self",
      "md",
      "input_arr_TNH"
    ]
  },
  "Attention": {
    "kv_cache_quantized_dtype": [],
    "__post_init__": [
      "self",
      "rngs"
    ],
    "__call__": [
      "self",
      "x",
      "is_prefill",
      "kv_cache",
      "attention_metadata",
      "use_attention_rope"
    ],
    "attention": [
      "self",
      "is_prefill",
      "kv_cache",
      "q_TNH",
      "k_SKH",
      "v_SKH",
      "attention_metadata",
      "mesh",
      "q_scale",
      "k_scale",
      "v_scale"
    ]
  },
  "MLA": {
    "__post_init__": [
      "self",
      "rngs"
    ],
    "__call__": [
      "self",
      "x",
      "is_prefill",
      "kv_cache",
      "attention_metadata",
      "use_attention_rope"
    ],
    "attention": [
      "self",
      "is_prefill",
      "kv_cache",
      "q_TNH",
      "k_SKH",
      "v_SKH",
      "attention_metadata",
      "mesh",
      "q_scale",
      "k_scale",
      "v_scale"
    ],
    "mla_attention": [
      "self",
      "kv_cache",
      "q_TNA",
      "q_rope_TNH",
      "k_SA",
      "k_rope_SH",
      "attention_metadata",
      "mesh"
    ]
  },
  "GptOssAttention": {
    "kv_cache_quantized_dtype": [],
    "__post_init__": [
      "self",
      "rngs"
    ],
    "attention": [
      "self",
      "kv_cache",
      "q_TNH",
      "k_SKH",
      "v_SKH",
      "sinks",
      "attention_metadata",
      "mesh",
      "q_scale",
      "k_scale",
      "v_scale"
    ],
    "__call__": [
      "self",
      "x_TD",
      "is_prefill",
      "kv_cache",
      "attention_metadata",
      "use_attention_rope"
    ]
  },
  "TORCH_TO_JAX_DTYPE_MAP": [],
  "shard_model_to_tpu": [
    "model",
    "mesh"
  ],
  "update_lora": [
    "model",
    "initial_params_buffers"
  ],
  "_extract_all_params_buffers": [
    "model"
  ],
  "_tensor_is_in_cpu": [
    "tensor"
  ],
  "_convert_to_torchax_and_shard": [
    "tensor",
    "sharding"
  ],
  "_shard_tensor_to_tpu_replicated": [
    "tensor",
    "mesh"
  ],
  "_shard_vocab_parallel_embedding": [
    "layer",
    "mesh"
  ],
  "_shard_lm_head": [
    "layer",
    "mesh"
  ],
  "_shard_base_linear_lora_replicated": [
    "layer",
    "mesh"
  ],
  "_shard_column_linear_lora": [
    "layer",
    "mesh"
  ],
  "_shard_qkv_linear_lora": [
    "layer",
    "mesh"
  ],
  "_shard_merged_column_parallel_linear_lora": [
    "layer",
    "mesh"
  ],
  "_shard_merged_qkv_parallel_linear_lora": [
    "layer",
    "mesh"
  ],
  "_shard_row_parallel_linear_lora": [
    "layer",
    "mesh"
  ],
  "MODULE_TYPE_TO_SHARDING_FUNC": [],
  "_shard_module_to_tpu": [
    "model",
    "mesh"
  ],
  "_sharded_device_put": [
    "tensor",
    "sharding"
  ],
  "PallasAttentionBackend": {
    "get_name": [],
    "get_impl_cls": []
  },
  "PallasAttentionBackendImpl": {
    "__init__": [
      "self",
      "num_heads",
      "head_size",
      "scale",
      "num_kv_heads",
      "alibi_slopes",
      "sliding_window",
      "kv_cache_dtype",
      "logits_soft_cap",
      "attn_type",
      "kv_sharing_target_layer_name",
      "sinks"
    ],
    "process_weights_after_loading": [
      "self",
      "act_dtype"
    ],
    "forward": [
      "self",
      "layer",
      "query",
      "key",
      "value",
      "kv_cache",
      "attn_metadata",
      "output",
      "output_scale"
    ]
  },
  "_jax_attn_func": [
    "kv_cache",
    "q",
    "k",
    "v",
    "sinks",
    "attention_metadata",
    "mesh",
    "scale",
    "head_size",
    "num_heads",
    "num_kv_heads",
    "q_scale",
    "k_scale",
    "v_scale",
    "sliding_window"
  ],
  "sharded_quantized_matmul": [
    "x",
    "w_q",
    "w_s",
    "mesh",
    "weight_sharding"
  ],
  "reorder_concatenated_tensor_for_sharding": [
    "concatenated_tensor",
    "split_sizes",
    "n_shards",
    "dim"
  ],
  "slice_sharded_tensor_for_concatenation": [
    "sharded_tensor",
    "split_sizes",
    "n_shards"
  ],
  "torch_to_jax_param": [
    "tensor",
    "sharding",
    "output_sizes",
    "n_shards",
    "fused",
    "dim",
    "jax_dtype"
  ],
  "MODEL_MATMUL_FUSION_TRUTH_TABLE": [],
  "get_model_matmul_fusion_assignment": [
    "model_name",
    "batch_size",
    "tp_size",
    "layer_name"
  ],
  "_swigluoai": [
    "x1",
    "x2",
    "alpha",
    "limit"
  ],
  "_round_up_to_multiple_of_128_within_limit": [
    "x",
    "limit"
  ],
  "_get_tiling_size_for_gmm_kernel": [
    "m",
    "k",
    "n",
    "g"
  ],
  "tensor_sharded_gmm_merged_column_parallel": [
    "lhs",
    "rhs",
    "rhs_scale",
    "rhs_bias",
    "group_sizes",
    "mesh"
  ],
  "tensor_sharded_gmm_row_parallel": [
    "lhs",
    "rhs",
    "rhs_scale",
    "rhs_bias",
    "group_sizes",
    "mesh"
  ],
  "expert_sharded_gmm": [
    "lhs",
    "rhs",
    "rhs_scale",
    "rhs_bias",
    "group_sizes",
    "is_last_expert",
    "mesh"
  ],
  "fused_moe_func": [
    "hidden_states",
    "w1",
    "w2",
    "w1_scale",
    "w2_scale",
    "w1_bias",
    "w2_bias",
    "gating_output",
    "topk",
    "renormalize",
    "mesh",
    "use_ep",
    "activation"
  ],
  "JaxCommonLinearConfig": {
    "__init__": [
      "self",
      "vllm_config",
      "mesh",
      "layer"
    ],
    "get_input_sharding": [
      "self",
      "x"
    ],
    "get_output_sharding": [
      "self",
      "x"
    ]
  },
  "JaxCommonConfig": {
    "set_configs": [
      "cls",
      "vllm_config",
      "mesh"
    ],
    "get_linear_config": [
      "self",
      "layer"
    ],
    "get_moe_config": [
      "self",
      "layer"
    ]
  },
  "VllmFp8Config": {
    "get_name": [
      "cls"
    ],
    "get_supported_act_dtypes": [
      "self"
    ],
    "get_quant_method": [
      "self",
      "layer",
      "prefix"
    ]
  },
  "VllmFp8LinearMethod": {
    "__init__": [
      "self",
      "quant_config",
      "jax_config"
    ],
    "_configure_sharding": [
      "self"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_fused": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_split": [
      "self",
      "layer",
      "x",
      "bias"
    ]
  },
  "VllmAWQConfig": {
    "get_name": [
      "cls"
    ],
    "get_supported_act_dtypes": [
      "self"
    ],
    "get_quant_method": [
      "self",
      "layer",
      "prefix"
    ]
  },
  "VllmAWQLinearMethod": {
    "__init__": [
      "self",
      "quant_config",
      "jax_config"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_fused": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_split": [
      "self",
      "layer",
      "x",
      "bias"
    ]
  },
  "unpack_awq_weight": [
    "weight",
    "packed_dim"
  ],
  "REQUANTIZED_BLOCK_SIZE": [],
  "VllmMxfp4Config": {
    "get_name": [
      "cls"
    ],
    "get_quant_method": [
      "self",
      "layer",
      "prefix"
    ]
  },
  "VllmMxfp4MoEMethod": {
    "__init__": [
      "self",
      "moe",
      "mesh",
      "ep_axis_name"
    ],
    "get_fused_moe_quant_config": [
      "self",
      "layer"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "router_logits"
    ]
  },
  "get_tpu_quantization_config": [
    "vllm_config",
    "mesh"
  ],
  "VllmUnquantizedConfig": {
    "get_name": [
      "cls"
    ],
    "get_supported_act_dtypes": [
      "cls"
    ],
    "get_min_capability": [
      "cls"
    ],
    "get_config_filenames": [
      "cls"
    ],
    "from_config": [
      "cls",
      "_"
    ],
    "get_quant_method": [
      "self",
      "layer",
      "prefix"
    ]
  },
  "VllmUnquantizedLinearMethod": {
    "__init__": [
      "self",
      "jax_config"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_fused": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_split": [
      "self",
      "layer",
      "x",
      "bias"
    ]
  },
  "VllmUnquantizedFusedMoEMethod": {
    "__init__": [
      "self",
      "moe",
      "mesh",
      "ep_axis_name"
    ],
    "select_gemm_impl": [
      "self",
      "prepare_finalize",
      "moe",
      "layer"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "router_logits"
    ]
  },
  "VllmCompressedTensorsConfig": {
    "get_name": [
      "cls"
    ],
    "get_scheme": [
      "self",
      "layer",
      "layer_name"
    ],
    "get_quant_method": [
      "self",
      "layer",
      "prefix"
    ]
  },
  "VllmCompressedTensorsMoEMethod": {
    "get_moe_method": [
      "quant_config",
      "layer",
      "layer_name"
    ]
  },
  "VllmCompressedTensorsW8A8Fp8MoEMethod": {
    "__init__": [
      "self",
      "weight_quant",
      "input_quant",
      "moe",
      "mesh"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply": [
      "self",
      "layer",
      "x",
      "router_logits"
    ]
  },
  "requantize_with_max_scale": [
    "weight",
    "weight_scale",
    "logical_widths"
  ],
  "VllmCompressedTensorsW8A8Fp8": {
    "__init__": [
      "self",
      "weight_quant",
      "is_static_input_scheme",
      "jax_config"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply_weights": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_fused": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_split": [
      "self",
      "layer",
      "x",
      "bias"
    ]
  },
  "VllmCompressedTensorsW8A8Int8": {
    "__init__": [
      "self",
      "strategy",
      "is_static_input_scheme",
      "input_symmetric",
      "jax_config"
    ],
    "process_weights_after_loading": [
      "self",
      "layer"
    ],
    "apply_weights": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_fused": [
      "self",
      "layer",
      "x",
      "bias"
    ],
    "_apply_split": [
      "self",
      "layer",
      "x",
      "bias"
    ]
  },
  "PPConfig": {
    "__post_init__": [
      "self"
    ]
  },
  "TPUWorker": {
    "__init__": [
      "self",
      "vllm_config",
      "local_rank",
      "rank",
      "distributed_init_method",
      "is_driver_worker",
      "devices",
      "ip",
      "prev_worker_ip"
    ],
    "initialize_cache": [
      "self",
      "num_gpu_blocks",
      "num_cpu_blocks"
    ],
    "init_device": [
      "self",
      "tpu_process_bounds",
      "tpu_chips_per_process_bounds",
      "tpu_visible_chips"
    ],
    "initialize_pp_transfer_connect": [
      "self"
    ],
    "determine_available_memory": [
      "self"
    ],
    "execute_model": [
      "self",
      "scheduler_output"
    ],
    "sample_tokens": [
      "self",
      "grammar_output"
    ],
    "take_draft_token_ids": [
      "self"
    ],
    "add_lora": [
      "self",
      "lora_request"
    ],
    "profile": [
      "self",
      "is_start"
    ],
    "load_model": [
      "self"
    ],
    "compile_or_warm_up_model": [
      "self"
    ],
    "reset_mm_cache": [
      "self"
    ],
    "get_model": [
      "self"
    ],
    "get_supported_tasks": [
      "self"
    ],
    "get_kv_cache_spec": [
      "self"
    ],
    "initialize_from_config": [
      "self",
      "kv_cache_config"
    ],
    "get_node_kv_ip_port": [
      "self"
    ],
    "check_health": [
      "self"
    ],
    "sync_weights": [
      "self",
      "updated_weights",
      "mappings",
      "transpose_keys",
      "reshard_fn"
    ],
    "shutdown": [
      "self"
    ],
    "get_kv_connector_handshake_metadata": [
      "self"
    ]
  },
  "Eagle3Proposer": {
    "__init__": [
      "self",
      "vllm_config",
      "runner"
    ],
    "load_model": [
      "self",
      "target_model"
    ],
    "_prepare_input_ids": [
      "self",
      "query_start_loc",
      "target_token_ids",
      "next_token_ids",
      "num_reqs"
    ],
    "_update_inputs_for_loop_speculation": [
      "self",
      "positions",
      "seq_lens",
      "block_tables"
    ],
    "_stack_draft_token_ids": [
      "self",
      "draft_token_ids_list"
    ],
    "_prepare_hidden_states_and_input_ids": [
      "self",
      "state",
      "aux_hidden_states",
      "query_start_loc",
      "target_token_ids",
      "next_token_ids",
      "num_reqs"
    ],
    "prepare_inputs": [
      "self",
      "attn_metadata",
      "input_ids",
      "aux_hidden_states",
      "next_token_ids",
      "num_rejected_tokens"
    ],
    "_filter_token_and_prepare_initial_inputs": [
      "self",
      "state",
      "token_indices",
      "query_start_loc",
      "seq_lens",
      "input_ids",
      "aux_hidden_states",
      "attn_metadata",
      "next_token_ids",
      "num_reqs"
    ],
    "_select_draft_token_ids": [
      "self",
      "state",
      "hidden_states",
      "last_token_indices"
    ],
    "_get_draft_token_ids": [
      "self",
      "state",
      "hidden_states"
    ],
    "_select_inputs_for_loop_speculation": [
      "self",
      "state",
      "positions",
      "residual",
      "hidden_states",
      "last_token_indices"
    ],
    "propose": [
      "self",
      "kv_caches",
      "input_ids",
      "attn_metadata",
      "last_token_indices",
      "target_hidden_states"
    ]
  },
  "TpuPlatform": {
    "_enum": [],
    "get_attn_backend_cls": [
      "cls",
      "selected_backend",
      "attn_selector_config"
    ],
    "get_device_name": [
      "cls",
      "device_id"
    ],
    "fp8_dtype": [
      "cls"
    ],
    "get_device_total_memory": [
      "cls",
      "device_id"
    ],
    "is_async_output_supported": [
      "cls",
      "enforce_eager"
    ],
    "get_punica_wrapper": [
      "cls"
    ],
    "get_infinity_values": [
      "cls",
      "dtype"
    ],
    "can_update_inplace": [
      "cls"
    ],
    "get_lora_vocab_padding_size": [
      "cls"
    ],
    "inference_mode": [
      "cls"
    ],
    "_initialize_sharding_config": [
      "cls",
      "vllm_config"
    ],
    "check_and_update_config": [
      "cls",
      "vllm_config"
    ],
    "is_pin_memory_available": [
      "cls"
    ],
    "get_device_communicator_cls": [
      "cls"
    ],
    "use_all_gather": [
      "cls"
    ],
    "supports_v1": [
      "cls",
      "model_config"
    ],
    "validate_request": [
      "cls",
      "prompt",
      "params",
      "processed_inputs"
    ],
    "is_kv_cache_dtype_supported": [
      "cls",
      "kv_cache_dtype",
      "model_config"
    ],
    "use_sync_weight_loader": [
      "cls"
    ],
    "support_hybrid_kv_cache": [
      "cls"
    ]
  },
  "_MODEL_REGISTRY": [],
  "UnsupportedArchitectureError": {},
  "_get_model_architecture": [
    "config"
  ],
  "_get_nnx_model": [
    "model_class",
    "vllm_config",
    "rng",
    "mesh"
  ],
  "get_flax_model": [
    "vllm_config",
    "rng",
    "mesh",
    "is_draft_model"
  ],
  "get_vllm_model": [
    "vllm_config",
    "rng",
    "mesh"
  ],
  "get_model": [
    "vllm_config",
    "rng",
    "mesh",
    "is_draft_model"
  ],
  "_validate_model_interface": [
    "model"
  ],
  "register_model": [
    "arch",
    "model"
  ],
  "init_fn": [],
  "Qwen2MLP": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen2Attention": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng",
      "mesh",
      "kv_cache_dtype"
    ],
    "__call__": [
      "self",
      "kv_cache",
      "x",
      "attention_metadata"
    ]
  },
  "Qwen2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng",
      "mesh",
      "kv_cache_dtype"
    ],
    "__call__": [
      "self",
      "kv_cache",
      "x",
      "attention_metadata"
    ]
  },
  "Qwen2Model": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata",
      "inputs_embeds"
    ]
  },
  "Qwen2ForCausalLM": {
    "__init__": [
      "self",
      "vllm_config",
      "rng_key",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata",
      "inputs_embeds"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ],
    "load_weights": [
      "self",
      "rng_key"
    ]
  },
  "DEFAULT_BLOCK_K_MAJOR": [],
  "Qwen2_5_VLImagePixelInputs": {},
  "Qwen2_5_VLImageEmbeddingInputs": {},
  "Qwen2_5_VLImageInputs": [],
  "Qwen2_5_VisionMLP": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rngs"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "apply_rotary_pos_emb_vision": [
    "x",
    "rotary_pos_emb"
  ],
  "generate_window_segment_ids": [
    "cu_seqlens",
    "seq_len",
    "padded_seq_len"
  ],
  "Qwen2_5_VisionAttention": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rngs",
      "mesh"
    ],
    "__call__": [
      "self",
      "x",
      "rotary_pos_emb",
      "cu_window_seqlens",
      "use_fullattn"
    ]
  },
  "Qwen2_5_VisionBlock": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rngs",
      "mesh"
    ],
    "__call__": [
      "self",
      "x",
      "rotary_pos_emb",
      "cu_window_seqlens",
      "use_fullattn"
    ]
  },
  "Qwen2_5_VisionPatchEmbed": {
    "__init__": [
      "self",
      "rngs",
      "patch_size",
      "temporal_patch_size",
      "in_channels",
      "hidden_size",
      "dtype"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen2_5_VisionPatchMerger": {
    "__init__": [
      "self",
      "d_model",
      "context_dim",
      "norm_layer",
      "spatial_merge_size",
      "dtype",
      "rngs"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen2_5_VisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "__call__": [
      "self",
      "seqlen"
    ]
  },
  "Qwen2_5_VisionTransformer": {
    "__init__": [
      "self",
      "vllm_config",
      "rngs",
      "mesh",
      "norm_eps"
    ],
    "rotary_pos_emb_thw": [
      "self",
      "t",
      "h",
      "w"
    ],
    "get_window_index_thw": [
      "self",
      "grid_t",
      "grid_h",
      "grid_w"
    ],
    "get_rope_by_thw": [
      "self",
      "t",
      "h",
      "w"
    ],
    "compute_attn_mask_seqlen": [
      "self",
      "cu_seqlens"
    ],
    "compute_aux_arrays": [
      "self",
      "grid_thw"
    ],
    "pad_inputs": [
      "self",
      "x",
      "window_index",
      "rotary_pos_emb",
      "cu_seqlens",
      "cu_window_seqlens"
    ],
    "compute_hidden_states": [
      "self",
      "x",
      "window_index",
      "rotary_pos_emb",
      "cu_seqlens",
      "cu_window_seqlens"
    ],
    "encode_padded_jit": [
      "self",
      "x_padded",
      "window_index",
      "rotary_pos_emb",
      "cu_seqlens",
      "cu_window_seqlens"
    ],
    "encode_jit": [
      "self",
      "x",
      "grid_thw"
    ],
    "__call__": [
      "self",
      "x",
      "grid_thw"
    ]
  },
  "Qwen2_5_VLForConditionalGeneration": {
    "__init__": [
      "self",
      "vllm_config",
      "rng_key",
      "mesh"
    ],
    "get_mrope_input_positions": [
      "self",
      "input_tokens",
      "hf_config",
      "image_grid_thw",
      "video_grid_thw",
      "second_per_grid_ts",
      "context_len",
      "seq_len",
      "audio_feature_lengths",
      "use_audio_in_video"
    ],
    "_validate_and_reshape_mm_tensor": [
      "self",
      "mm_input",
      "name"
    ],
    "_parse_and_validate_image_input": [
      "self",
      "image_grid_thw"
    ],
    "_parse_and_validate_multimodal_inputs": [
      "self",
      "image_grid_thw"
    ],
    "get_single_image_embedding": [
      "self",
      "image_pixel_values",
      "image_grid_thw"
    ],
    "_process_image_input": [
      "self",
      "image_input"
    ],
    "embed_multimodal": [
      "self",
      "image_grid_thw"
    ],
    "embed_input_ids": [
      "self",
      "input_ids",
      "multimodal_embeddings"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata",
      "inputs_embeds"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ],
    "load_weights": [
      "self",
      "rng_key"
    ],
    "precompile_vision_encoder": [
      "self",
      "run_compilation_fn"
    ]
  },
  "LlamaGuard4ForCausalLM": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh",
      "force_random_weights"
    ],
    "_print_model_architecture": [
      "self"
    ],
    "load_weights": [
      "self",
      "rng",
      "cache_dir"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata",
      "inputs_embeds",
      "layer_metadata_tuple",
      "lora_metadata"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ],
    "embed_input_ids": [
      "self",
      "input_ids",
      "multimodal_embeddings"
    ]
  },
  "LlamaGuard4WeightLoader": {
    "__init__": [
      "self",
      "vllm_config",
      "hidden_size",
      "attn_heads",
      "num_key_value_heads",
      "attn_head_dim"
    ],
    "map_loaded_to_standardized_name": [
      "self",
      "loaded_key"
    ],
    "load_weights": [
      "self",
      "model_for_loading"
    ]
  },
  "Llama4ForCausalLM": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh",
      "force_random_weights"
    ],
    "_print_model_architecture": [
      "self"
    ],
    "load_weights": [
      "self",
      "rng",
      "cache_dir"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4WeightLoader": {
    "__init__": [
      "self",
      "vllm_config",
      "hidden_size",
      "attn_heads",
      "num_key_value_heads",
      "attn_head_dim"
    ],
    "map_loaded_to_standardized_name": [
      "self",
      "loaded_key"
    ],
    "_map_llama4_gate_up_proj": [
      "self",
      "model_for_loading",
      "model_params",
      "loaded_name",
      "loaded_weight"
    ],
    "_get_layer_num": [
      "self",
      "loaded_key"
    ],
    "_get_expert_num": [
      "self",
      "loaded_key"
    ],
    "load_weights": [
      "self",
      "model_for_loading"
    ]
  },
  "JaxIntermediateTensors": {
    "tree_flatten": [
      "self"
    ],
    "tree_unflatten": [
      "cls",
      "aux_data",
      "children"
    ],
    "from_torch": [
      "cls",
      "torch_obj"
    ],
    "to_torch": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ],
    "keys": [
      "self"
    ],
    "items": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "block_until_ready": [
      "self"
    ]
  },
  "LlamaMLP": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LlamaAttention": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng",
      "mesh",
      "kv_cache_dtype"
    ],
    "__call__": [
      "self",
      "kv_cache",
      "x",
      "attention_metadata"
    ]
  },
  "LlamaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng",
      "mesh",
      "kv_cache_dtype"
    ],
    "__call__": [
      "self",
      "kv_cache",
      "x",
      "attention_metadata"
    ]
  },
  "LlamaModel": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "get_eagle3_aux_hidden_state_layers": [
      "self"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata",
      "intermediate_tensors"
    ]
  },
  "Qwen3Attention": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng",
      "mesh",
      "kv_cache_dtype"
    ],
    "__call__": [
      "self",
      "kv_cache",
      "x",
      "attention_metadata"
    ]
  },
  "Qwen3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng",
      "mesh",
      "kv_cache_dtype"
    ]
  },
  "Qwen3Model": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ]
  },
  "Qwen3ForCausalLM": {
    "__init__": [
      "self",
      "vllm_config",
      "rng_key",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ],
    "load_weights": [
      "self",
      "rng_key"
    ]
  },
  "Eagle3LlamaDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "dtype",
      "rng",
      "mesh",
      "kv_cache_dtype"
    ],
    "_norm_before_residual": [
      "self",
      "hidden_states"
    ],
    "_norm_after_residual": [
      "self",
      "hidden_states"
    ],
    "__call__": [
      "self",
      "kv_cache",
      "embeds",
      "hidden_states",
      "attention_metadata"
    ]
  },
  "Eagle3LlamaModel": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "hidden_states",
      "attention_metadata"
    ]
  },
  "update_reshape_map_for_eagle3": [
    "vllm_config",
    "metadata_map"
  ],
  "EagleLlama3ForCausalLM": {
    "__init__": [
      "self",
      "vllm_config",
      "rng_key",
      "mesh"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "hidden_states",
      "attention_metadata"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ],
    "combine_hidden_states": [
      "self",
      "hidden_states"
    ],
    "load_weights": [
      "self",
      "rng_key"
    ]
  },
  "DTYPE_VIEW_MAP": [],
  "GptOss": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh",
      "force_random_weights"
    ],
    "apply": [
      "self",
      "variables"
    ],
    "load_weights": [
      "self",
      "rng",
      "cache_dir"
    ],
    "_build_mxfp4_pool": [
      "self",
      "names_and_weights_generator",
      "mappings"
    ],
    "_load_mxfp4": [
      "self",
      "model_weight",
      "codes_fp32_t",
      "scales_fp32_t",
      "transform_fn"
    ],
    "_load_regular_param": [
      "self",
      "model_weight",
      "loaded_weight",
      "cast_type",
      "transform_fn",
      "target_shape",
      "jax_path_template"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ]
  },
  "DeepSeekV3": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh",
      "force_random_weights"
    ],
    "_print_model_architecture": [
      "self"
    ],
    "apply": [
      "self",
      "variables"
    ],
    "load_weights": [
      "self",
      "rng",
      "cache_dir"
    ],
    "initialize_cache": [
      "self"
    ],
    "__call__": [
      "self",
      "kv_caches",
      "input_ids",
      "attention_metadata"
    ],
    "compute_logits": [
      "self",
      "hidden_states"
    ]
  },
  "DeepSeekV3WeightLoader": {
    "__init__": [
      "self",
      "vllm_config",
      "num_layers",
      "hidden_size",
      "q_lora_rank",
      "kv_lora_rank",
      "attn_heads",
      "qk_nope_head_dim",
      "qk_rope_head_dim",
      "v_head_dim",
      "num_local_experts",
      "model_dtype",
      "use_mla_kernel"
    ],
    "map_loaded_to_standardized_name": [
      "self",
      "loaded_key"
    ],
    "_transpose_params": [
      "self",
      "param_key",
      "param_tensor"
    ],
    "_process_moe_weights": [
      "self",
      "loaded_name",
      "loaded_weight",
      "weights_dict"
    ],
    "_load_individual_weight": [
      "self",
      "name",
      "weight",
      "model_params",
      "model_mesh",
      "scale"
    ],
    "load_weights": [
      "self",
      "model_for_loading"
    ]
  },
  "weights_dequant_cpu": [
    "x",
    "s",
    "output_dtype",
    "block_size"
  ],
  "hfs": [],
  "LOCK_DIR": [],
  "run_cmd": [
    "cmd"
  ],
  "delete_file": [
    "path"
  ],
  "list_files": [
    "dir",
    "pattern"
  ],
  "get_lock": [
    "model_name_or_path"
  ],
  "get_free_disk_size": [
    "path"
  ],
  "is_hf_repo": [
    "repo_id"
  ],
  "list_hf_repo": [
    "repo_id",
    "pattern"
  ],
  "get_hf_model_weights_size": [
    "repo_id",
    "weights_format"
  ],
  "DisabledTqdm": {
    "__init__": [
      "self"
    ]
  },
  "download_model_weights_from_hf": [
    "model_path",
    "cache_dir",
    "weights_format"
  ],
  "HF_WEIGHTS_FORMAT": [],
  "MetadataMap": {},
  "print_param_info": [
    "param",
    "name"
  ],
  "transpose_params": [
    "param_key",
    "param_tensor",
    "transpose_map"
  ],
  "reshape_params": [
    "param_key",
    "param_tensor",
    "shape_map"
  ],
  "model_file_generator": [
    "model_name_or_path",
    "download_dir"
  ],
  "model_weights_generator": [
    "model_name_or_path",
    "framework",
    "filter_regex",
    "download_dir"
  ],
  "convert_torch_to_jax_with_view": [
    "loaded_weight",
    "cast_type"
  ],
  "get_model_weights_files": [
    "model_name_or_path",
    "download_dir"
  ],
  "model_weights_single_file_generator": [
    "weights_file",
    "framework",
    "filter_regex"
  ],
  "get_param": [
    "params",
    "path"
  ],
  "get_param_and_sharding": [
    "params",
    "shardings",
    "path"
  ],
  "get_default_maps": [
    "model_config",
    "mesh",
    "name_map"
  ],
  "_load_and_shard_weight": [
    "vllm_config",
    "params",
    "shardings",
    "metadata_map",
    "mesh",
    "hf_key",
    "hf_weight",
    "keep_original_dtype_keys_regex",
    "pp_missing_layers"
  ],
  "_is_pp_missing_layer": [
    "hf_key",
    "pp_missing_layers"
  ],
  "_load_hf_weights_on_thread": [
    "vllm_config",
    "params",
    "metadata_map",
    "mesh",
    "weights_file",
    "filter_regex",
    "keep_original_dtype_keys_regex",
    "pp_missing_layers"
  ],
  "load_hf_weights": [
    "vllm_config",
    "model",
    "metadata_map",
    "mesh",
    "filter_regex",
    "is_draft_model",
    "keep_original_dtype_keys_regex",
    "pp_missing_layers"
  ],
  "check_all_loaded": [
    "params"
  ],
  "build_flat_dict": [
    "flat_state",
    "mappings"
  ],
  "transfer_state_with_mappings": [
    "src_state",
    "tgt_state",
    "mappings",
    "transpose_keys",
    "shard"
  ],
  "MultiModalEmbeddings": [],
  "sanity_check_mm_encoder_outputs": [
    "mm_embeddings",
    "expected_num_items"
  ],
  "flatten_embeddings": [
    "embeddings"
  ],
  "_embedding_count_expression": [
    "embeddings"
  ],
  "_merge_multimodal_embeddings": [
    "inputs_embeds",
    "is_multimodal",
    "multimodal_embeddings"
  ],
  "merge_multimodal_embeddings": [
    "input_ids",
    "inputs_embeds",
    "multimodal_embeddings",
    "placeholder_token_id"
  ],
  "QUANTIZATION_CONFIG_PATH": [],
  "DEFAULT_NUM_BLOCKS_FOR_JIT_KV_CACHE": [],
  "DEFAULT_NUM_TOKENS_FOR_MODEL_INPUTS": [],
  "DEFAULT_MAX_NUM_SEQS_FOR_MODEL_INPUTS": [],
  "DEFAULT_MAX_NUM_BLOCKS_PER_REQ": [],
  "DEFAULT_DEEPSEEK_FP4_MLP_MOE_FP8_ATTN_CONFIG": [],
  "DEFAULT_LLAMA4_FP8_CONFIG": [],
  "DEFAULT_GPT_OSS_FP4_CONFIG": [],
  "parse_qwix_config_to_rules": [
    "qwix_config"
  ],
  "qwix_quantize_nnx_model": [
    "model",
    "qwix_config",
    "rng",
    "mesh",
    "num_hidden_layers",
    "kv_cache_block_size",
    "kv_cache_num_kv_heads",
    "kv_cache_head_size",
    "kv_cache_dtype"
  ],
  "quantization_config_file_path_to_dict": [
    "quantization_config_file_path"
  ],
  "apply_qwix_quantization": [
    "vllm_config",
    "model_or_model_fn",
    "rng",
    "mesh",
    "apply_to_abstract_model"
  ],
  "apply_qwix_on_abstract_model": [
    "vllm_config"
  ],
  "get_default_qwix_quantization_config": [
    "hf_config",
    "skip_quantization"
  ],
  "update_vllm_config_for_qwix_quantization": [
    "vllm_config"
  ],
  "get_random_sharded_array": [
    "key",
    "mesh",
    "param",
    "param_shape",
    "dtype",
    "param_name"
  ],
  "load_random_weights_into_qwix_abstract_model": [
    "rng",
    "model",
    "mesh",
    "quantization_config"
  ],
  "manually_quantize_qwix_weight": [
    "weight",
    "qtype",
    "channelwise_axes",
    "tiled_axes",
    "calibration_method"
  ],
  "manually_quantize_qwix_activation": [
    "inputs",
    "rule_name",
    "qtype",
    "channelwise_axes",
    "tiled_axes",
    "calibration_method"
  ],
  "get_quant_dtype_from_qwix_config": [
    "vllm_config"
  ],
  "_VllmRunner": {
    "__init__": [
      "self",
      "vllm_model"
    ],
    "forward": [
      "self"
    ],
    "compute_hidden_state": [
      "self",
      "input_ids",
      "positions",
      "intermediate_tensors",
      "inputs_embeds"
    ],
    "compute_logits": [
      "self",
      "hidden_state"
    ]
  },
  "VllmModelWrapper": {
    "__init__": [
      "self",
      "vllm_config",
      "rng",
      "mesh"
    ],
    "load_weights": [
      "self"
    ],
    "jit_step_func": [
      "self"
    ],
    "jit_compute_logits_func": [
      "self"
    ]
  },
  "load_lora_model": [
    "model",
    "vllm_config",
    "device"
  ],
  "replace_set_lora": [
    "model"
  ],
  "VllmModelWrapperContext": {},
  "get_vllm_model_wrapper_context": [],
  "set_vllm_model_wrapper_context": [],
  "POLLING_TIMEOUT_S": [],
  "HANDSHAKE_TIMEOUT_MINS": [],
  "_R": [],
  "JetThread": {
    "run": [
      "self"
    ]
  },
  "_DisaggOrchestrator": {
    "__init__": [
      "self",
      "config",
      "output_queue",
      "prefill_engines",
      "decode_engines",
      "prefill_slice_sizes",
      "decode_slice_sizes"
    ],
    "add_request": [
      "self",
      "request"
    ],
    "_prefill": [
      "self",
      "idx"
    ],
    "_transfer": [
      "self",
      "idx"
    ],
    "_decode": [
      "self",
      "idx"
    ],
    "shutdown": [
      "self"
    ]
  },
  "_create_engine_cores": [
    "slice_sizes",
    "vllm_config",
    "log_stats",
    "executor_fail_callback"
  ],
  "_get_slice_sizes": [
    "devices"
  ],
  "DisaggEngineCore": {
    "is_supported": [],
    "__init__": [
      "self",
      "vllm_config",
      "executor_class",
      "log_stats",
      "executor_fail_callback"
    ],
    "get_supported_tasks": [
      "self"
    ],
    "add_request": [
      "self",
      "request",
      "request_wave"
    ],
    "step": [
      "self"
    ],
    "shutdown": [
      "self"
    ],
    "reset_mm_cache": [
      "self"
    ],
    "reset_prefix_cache": [
      "self"
    ]
  },
  "DisaggEngineCoreProc": {
    "is_supported": [],
    "__init__": [
      "self",
      "vllm_config",
      "local_client",
      "handshake_address",
      "executor_class",
      "log_stats",
      "client_handshake_address",
      "engine_index"
    ],
    "add_request": [
      "self",
      "request",
      "request_wave"
    ],
    "_handle_client_request": [
      "self",
      "request_type",
      "request"
    ],
    "run_busy_loop": [
      "self"
    ],
    "shutdown": [
      "self"
    ]
  },
  "is_disagg_enabled": [],
  "_parse_slices": [
    "slices_str"
  ],
  "get_prefill_slices": [],
  "get_decode_slices": [],
  "DisaggExecutor": {
    "_init_executor": [
      "self"
    ],
    "collective_rpc": [
      "self",
      "method",
      "timeout",
      "args",
      "kwargs",
      "non_block"
    ],
    "check_health": [
      "self"
    ]
  },
  "SchedulerCommand": {
    "ADD_REQUEST": [],
    "SCHEDULE": [],
    "FINISH_REQUESTS": [],
    "UPDATE_DRAFT_TOKEN_IDS": [],
    "UPDATE_FROM_OUTPUT": [],
    "GET_GRAMMAR_BITMASK": [],
    "MAKE_STATS": [],
    "RESET_PREFIX_CACHE": [],
    "GET_NUM_UNFINISHED_REQUESTS": [],
    "HAS_FINISHED_REQUESTS": [],
    "GET_REQUEST_COUNTS": [],
    "GET_TOKEN_COUNT": [],
    "GET_COMPUTED_BLOCKS": [],
    "SHUTDOWN": []
  },
  "SchedulerWorkerError": {
    "__init__": [
      "self",
      "rank",
      "message"
    ]
  },
  "_original_dumps": [],
  "_original_loads": [],
  "_cloudpickle_dumps": [
    "obj",
    "protocol"
  ],
  "_cloudpickle_loads": [
    "data"
  ],
  "_enable_cloudpickle": [],
  "_disable_cloudpickle": [],
  "_scheduler_worker_process": [
    "rank",
    "input_queue",
    "output_queues",
    "vllm_config",
    "kv_cache_config",
    "structured_output_manager",
    "block_size",
    "mm_registry",
    "include_finished_set",
    "log_stats",
    "original_scheduler_cls"
  ],
  "DPSchedulerOutput": {
    "__init__": [
      "self"
    ]
  },
  "DPScheduler": {
    "__init__": [
      "self",
      "vllm_config",
      "kv_cache_config",
      "structured_output_manager",
      "block_size",
      "mm_registry",
      "include_finished_set",
      "log_stats"
    ],
    "_create_per_rank_configs": [
      "self",
      "kv_cache_config"
    ],
    "_get_result_from_queue": [
      "self",
      "rank",
      "command"
    ],
    "_get_rank_token_counts": [
      "self"
    ],
    "_find_best_rank_for_request": [
      "self",
      "request"
    ],
    "add_request": [
      "self",
      "request"
    ],
    "schedule": [
      "self"
    ],
    "_combine_scheduler_outputs": [
      "self",
      "rank_outputs"
    ],
    "_combine_cached_request_data": [
      "self",
      "rank_outputs"
    ],
    "get_grammar_bitmask": [
      "self",
      "scheduler_output"
    ],
    "update_from_output": [
      "self",
      "scheduler_output",
      "model_runner_output"
    ],
    "_split_model_output_by_rank": [
      "self",
      "global_model_output"
    ],
    "_cleanup_finished_requests": [
      "self",
      "finished_req_ids"
    ],
    "finish_requests": [
      "self",
      "request_ids",
      "finished_status"
    ],
    "get_num_unfinished_requests": [
      "self"
    ],
    "has_finished_requests": [
      "self"
    ],
    "get_request_counts": [
      "self"
    ],
    "reset_prefix_cache": [
      "self"
    ],
    "make_stats": [
      "self",
      "spec_decoding_stats",
      "kv_connector_stats"
    ],
    "update_draft_token_ids": [
      "self",
      "draft_token_ids"
    ],
    "shutdown": [
      "self"
    ]
  },
  "update_vllm_config_for_dp_scheduler": [
    "vllm_config"
  ],
  "_encode_hook": [
    "obj"
  ],
  "RayDistributedExecutor": {
    "_init_executor": [
      "self"
    ],
    "_initialize_ray_cluster": [
      "self"
    ],
    "_init_workers_ray": [
      "self",
      "placement_group"
    ],
    "get_kv_connector_handshake_metadata": [
      "self"
    ]
  }
}