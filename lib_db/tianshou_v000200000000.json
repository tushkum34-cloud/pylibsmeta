{
  "log": [],
  "TrainerParams": {
    "__setstate__": [
      "self",
      "state"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "OnlineTrainerParams": {
    "__setstate__": [
      "self",
      "state"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "OnPolicyTrainerParams": {},
  "OffPolicyTrainerParams": {},
  "OfflineTrainerParams": {},
  "TTrainerParams": [],
  "TOnlineTrainerParams": [],
  "TAlgorithm": [],
  "Trainer": {
    "__init__": [
      "self",
      "algorithm",
      "params"
    ],
    "_compute_score_fn_default": [
      "stat"
    ],
    "_pbar": [
      "self"
    ],
    "_reset_collectors": [
      "self",
      "reset_buffer"
    ],
    "reset": [
      "self",
      "reset_collectors",
      "reset_collector_buffers"
    ],
    "_log_params": [
      "self",
      "module"
    ],
    "_create_epoch_pbar_data_dict": [
      "self",
      "training_step_result"
    ],
    "_create_info_stats": [
      "self"
    ],
    "execute_epoch": [
      "self"
    ],
    "_should_stop_training_early": [
      "self"
    ],
    "_collect_test_episodes": [
      "self"
    ],
    "_test_step": [
      "self",
      "force_update_best",
      "log_msg_prefix"
    ],
    "_training_step": [
      "self"
    ],
    "_update_moving_avg_stats_and_log_update_data": [
      "self",
      "update_stat"
    ],
    "_update_moving_avg_stats_and_get_averaged_data": [
      "self",
      "data"
    ],
    "run": [
      "self",
      "reset_collectors",
      "reset_collector_buffers"
    ]
  },
  "OfflineTrainer": {
    "__init__": [
      "self",
      "algorithm",
      "params"
    ],
    "_training_step": [
      "self"
    ],
    "_create_epoch_pbar_data_dict": [
      "self",
      "training_step_result"
    ]
  },
  "OnlineTrainer": {
    "__init__": [
      "self",
      "algorithm",
      "params"
    ],
    "_reset_collectors": [
      "self",
      "reset_buffer"
    ],
    "reset": [
      "self",
      "reset_collectors",
      "reset_collector_buffers"
    ],
    "_training_step": [
      "self"
    ],
    "_collect_training_data": [
      "self"
    ],
    "_test_in_train": [
      "self",
      "train_collect_stats"
    ],
    "_update_step": [
      "self",
      "collect_stats"
    ],
    "_create_epoch_pbar_data_dict": [
      "self",
      "training_step_result"
    ]
  },
  "OffPolicyTrainer": {
    "_update_step": [
      "self",
      "collect_stats"
    ],
    "_sample_and_update": [
      "self",
      "buffer"
    ]
  },
  "OnPolicyTrainer": {
    "_update_step": [
      "self",
      "collect_stats"
    ]
  },
  "__version__": [],
  "_register_log_config_callback": [],
  "__all__": [],
  "ENABLE_VALIDATION": [],
  "GYM_RESERVED_KEYS": [],
  "BaseVectorEnv": {
    "__init__": [
      "self",
      "env_fns",
      "worker_fn",
      "wait_num",
      "timeout"
    ],
    "_assert_is_not_closed": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__getattribute__": [
      "self",
      "key"
    ],
    "get_env_attr": [
      "self",
      "key",
      "id"
    ],
    "set_env_attr": [
      "self",
      "key",
      "value",
      "id"
    ],
    "_wrap_id": [
      "self",
      "id"
    ],
    "_assert_id": [
      "self",
      "id"
    ],
    "reset": [
      "self",
      "env_id"
    ],
    "step": [
      "self",
      "action",
      "id"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "render": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "DummyVectorEnv": {
    "__init__": [
      "self",
      "env_fns",
      "wait_num",
      "timeout"
    ]
  },
  "SubprocVectorEnv": {
    "__init__": [
      "self",
      "env_fns",
      "wait_num",
      "timeout",
      "share_memory",
      "context"
    ]
  },
  "ShmemVectorEnv": {
    "__init__": [
      "self",
      "env_fns",
      "wait_num",
      "timeout"
    ]
  },
  "RayVectorEnv": {
    "__init__": [
      "self",
      "env_fns",
      "wait_num",
      "timeout"
    ]
  },
  "ContinuousToDiscrete": {
    "__init__": [
      "self",
      "env",
      "action_per_dim"
    ],
    "action": [
      "self",
      "act"
    ]
  },
  "MultiDiscreteToDiscrete": {
    "__init__": [
      "self",
      "env"
    ],
    "action": [
      "self",
      "act"
    ]
  },
  "TruncatedAsTerminated": {
    "__init__": [
      "self",
      "env"
    ],
    "step": [
      "self",
      "act"
    ]
  },
  "VectorEnvWrapper": {
    "__init__": [
      "self",
      "venv"
    ],
    "__len__": [
      "self"
    ],
    "__getattribute__": [
      "self",
      "key"
    ],
    "get_env_attr": [
      "self",
      "key",
      "id"
    ],
    "set_env_attr": [
      "self",
      "key",
      "value",
      "id"
    ],
    "reset": [
      "self",
      "env_id"
    ],
    "step": [
      "self",
      "action",
      "id"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "render": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "VectorEnvNormObs": {
    "__init__": [
      "self",
      "venv",
      "update_obs_rms"
    ],
    "reset": [
      "self",
      "env_id"
    ],
    "step": [
      "self",
      "action",
      "id"
    ],
    "_norm_obs": [
      "self",
      "obs"
    ],
    "set_obs_rms": [
      "self",
      "obs_rms"
    ],
    "get_obs_rms": [
      "self"
    ]
  },
  "ENV_TYPE": [],
  "gym_new_venv_step_type": [],
  "CloudpickleWrapper": {
    "__init__": [
      "self",
      "data"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "data"
    ]
  },
  "PettingZooEnv": {
    "__init__": [
      "self",
      "env"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "close": [
      "self"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "render": [
      "self"
    ]
  },
  "envpool_is_available": [],
  "_parse_reset_result": [
    "reset_result"
  ],
  "get_space_dtype": [
    "obs_space"
  ],
  "NoopResetEnv": {
    "__init__": [
      "self",
      "env",
      "noop_max"
    ],
    "reset": [
      "self"
    ]
  },
  "MaxAndSkipEnv": {
    "__init__": [
      "self",
      "env",
      "skip"
    ],
    "step": [
      "self",
      "action"
    ]
  },
  "EpisodicLifeEnv": {
    "__init__": [
      "self",
      "env"
    ],
    "step": [
      "self",
      "action"
    ],
    "reset": [
      "self"
    ]
  },
  "FireResetEnv": {
    "__init__": [
      "self",
      "env"
    ],
    "reset": [
      "self"
    ]
  },
  "WarpFrame": {
    "__init__": [
      "self",
      "env"
    ],
    "observation": [
      "self",
      "frame"
    ]
  },
  "ScaledFloatFrame": {
    "__init__": [
      "self",
      "env"
    ],
    "observation": [
      "self",
      "observation"
    ]
  },
  "ClipRewardEnv": {
    "__init__": [
      "self",
      "env"
    ],
    "reward": [
      "self",
      "reward"
    ]
  },
  "FrameStack": {
    "__init__": [
      "self",
      "env",
      "n_frames"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "_get_ob": [
      "self"
    ]
  },
  "wrap_deepmind": [
    "env",
    "episode_life",
    "clip_rewards",
    "frame_stack",
    "scale",
    "warp_frame"
  ],
  "make_atari_env": [
    "task",
    "seed",
    "num_training_envs",
    "num_test_envs",
    "scale",
    "frame_stack"
  ],
  "AtariEnvFactory": {
    "__init__": [
      "self",
      "task",
      "frame_stack",
      "scale",
      "use_envpool_if_available",
      "venv_type"
    ],
    "_create_env": [
      "self",
      "mode"
    ]
  },
  "AtariEpochStopCallback": {
    "__init__": [
      "self",
      "task"
    ],
    "should_stop": [
      "self",
      "mean_rewards",
      "context"
    ]
  },
  "layer_init": [
    "layer",
    "std",
    "bias_const"
  ],
  "T": [],
  "ScaledObsInputActionReprNet": {
    "__init__": [
      "self",
      "module",
      "denom"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "DQNet": {
    "__init__": [
      "self",
      "c",
      "h",
      "w",
      "action_shape",
      "features_only",
      "output_dim_added_layer",
      "layer_init"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "C51Net": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "RainbowNet": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "QRDQNet": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "ActorFactoryAtariDQN": {
    "USE_SOFTMAX_OUTPUT": [],
    "__init__": [
      "self",
      "scale_obs",
      "features_only",
      "output_dim_added_layer"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ]
  },
  "IntermediateModuleFactoryAtariDQN": {
    "__init__": [
      "self",
      "features_only",
      "net_only"
    ],
    "create_intermediate_module": [
      "self",
      "envs",
      "device"
    ]
  },
  "IntermediateModuleFactoryAtariDQNFeatures": {
    "__init__": [
      "self"
    ]
  },
  "_SetAttrWrapper": {
    "set_env_attr": [
      "self",
      "key",
      "value"
    ],
    "get_env_attr": [
      "self",
      "key"
    ]
  },
  "RayEnvWorker": {
    "__init__": [
      "self",
      "env_fn"
    ],
    "get_env_attr": [
      "self",
      "key"
    ],
    "set_env_attr": [
      "self",
      "key",
      "value"
    ],
    "reset": [
      "self"
    ],
    "wait": [
      "workers",
      "wait_num",
      "timeout"
    ],
    "send": [
      "self",
      "action"
    ],
    "recv": [
      "self"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "render": [
      "self"
    ],
    "close_env": [
      "self"
    ]
  },
  "EnvWorker": {
    "__init__": [
      "self",
      "env_fn"
    ],
    "get_env_attr": [
      "self",
      "key"
    ],
    "set_env_attr": [
      "self",
      "key",
      "value"
    ],
    "send": [
      "self",
      "action"
    ],
    "recv": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "step": [
      "self",
      "action"
    ],
    "wait": [
      "workers",
      "wait_num",
      "timeout"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "render": [
      "self"
    ],
    "close_env": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "DummyEnvWorker": {
    "__init__": [
      "self",
      "env_fn"
    ],
    "get_env_attr": [
      "self",
      "key"
    ],
    "set_env_attr": [
      "self",
      "key",
      "value"
    ],
    "reset": [
      "self"
    ],
    "wait": [
      "workers",
      "wait_num",
      "timeout"
    ],
    "send": [
      "self",
      "action"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "render": [
      "self"
    ],
    "close_env": [
      "self"
    ]
  },
  "_NP_TO_CT": [],
  "ShArray": {
    "__init__": [
      "self",
      "dtype",
      "shape",
      "ctx"
    ],
    "save": [
      "self",
      "ndarray"
    ],
    "get": [
      "self"
    ]
  },
  "_setup_buf": [
    "space",
    "ctx"
  ],
  "_worker": [
    "parent",
    "p",
    "env_fn_wrapper",
    "obs_bufs"
  ],
  "SubprocEnvWorker": {
    "__init__": [
      "self",
      "env_fn",
      "share_memory",
      "context"
    ],
    "get_env_attr": [
      "self",
      "key"
    ],
    "set_env_attr": [
      "self",
      "key",
      "value"
    ],
    "_decode_obs": [
      "self"
    ],
    "wait": [
      "workers",
      "wait_num",
      "timeout"
    ],
    "send": [
      "self",
      "action"
    ],
    "recv": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "render": [
      "self"
    ],
    "close_env": [
      "self"
    ]
  },
  "CHECKPOINT_DICT_KEY_MODEL": [],
  "CHECKPOINT_DICT_KEY_OBS_RMS": [],
  "TParams": [],
  "TActorCriticParams": [],
  "TActorDualCriticsParams": [],
  "TDiscreteCriticOnlyParams": [],
  "TPolicy": [],
  "TTrainingConfig": [],
  "AlgorithmFactory": {
    "__init__": [
      "self",
      "training_config",
      "optim_factory"
    ],
    "set_collector_factory": [
      "self",
      "collector_factory"
    ],
    "create_train_test_collectors": [
      "self",
      "algorithm",
      "envs",
      "reset_collectors"
    ],
    "set_policy_wrapper_factory": [
      "self",
      "policy_wrapper_factory"
    ],
    "set_trainer_callbacks": [
      "self",
      "callbacks"
    ],
    "_create_policy_from_args": [
      "constructor",
      "params_dict",
      "policy_params"
    ],
    "_create_algorithm": [
      "self",
      "envs",
      "device"
    ],
    "create_algorithm": [
      "self",
      "envs",
      "device"
    ],
    "create_trainer": [
      "self",
      "world",
      "policy_persistence"
    ]
  },
  "OnPolicyAlgorithmFactory": {
    "create_trainer": [
      "self",
      "world",
      "policy_persistence"
    ]
  },
  "OffPolicyAlgorithmFactory": {
    "create_trainer": [
      "self",
      "world",
      "policy_persistence"
    ]
  },
  "ReinforceAlgorithmFactory": {
    "__init__": [
      "self",
      "params",
      "training_config",
      "actor_factory",
      "optim_factory"
    ],
    "_create_algorithm": [
      "self",
      "envs",
      "device"
    ]
  },
  "ActorCriticOnPolicyAlgorithmFactory": {
    "__init__": [
      "self",
      "params",
      "training_config",
      "actor_factory",
      "critic_factory",
      "optimizer_factory"
    ],
    "_get_algorithm_class": [
      "self"
    ],
    "_create_kwargs": [
      "self",
      "envs",
      "device"
    ],
    "_create_algorithm": [
      "self",
      "envs",
      "device"
    ]
  },
  "A2CAlgorithmFactory": {
    "_get_algorithm_class": [
      "self"
    ]
  },
  "PPOAlgorithmFactory": {
    "_get_algorithm_class": [
      "self"
    ]
  },
  "NPGAlgorithmFactory": {
    "_get_algorithm_class": [
      "self"
    ]
  },
  "TRPOAlgorithmFactory": {
    "_get_algorithm_class": [
      "self"
    ]
  },
  "DiscreteCriticOnlyOffPolicyAlgorithmFactory": {
    "__init__": [
      "self",
      "params",
      "training_config",
      "model_factory",
      "optim_factory"
    ],
    "_get_algorithm_class": [
      "self"
    ],
    "_create_policy": [
      "self",
      "model",
      "params",
      "action_space",
      "observation_space"
    ],
    "_create_algorithm": [
      "self",
      "envs",
      "device"
    ]
  },
  "DQNAlgorithmFactory": {
    "_create_policy": [
      "self",
      "model",
      "params",
      "action_space",
      "observation_space"
    ],
    "_get_algorithm_class": [
      "self"
    ]
  },
  "IQNAlgorithmFactory": {
    "_create_policy": [
      "self",
      "model",
      "params",
      "action_space",
      "observation_space"
    ],
    "_get_algorithm_class": [
      "self"
    ]
  },
  "DDPGAlgorithmFactory": {
    "__init__": [
      "self",
      "params",
      "training_config",
      "actor_factory",
      "critic_factory",
      "optim_factory"
    ],
    "_create_algorithm": [
      "self",
      "envs",
      "device"
    ]
  },
  "REDQAlgorithmFactory": {
    "__init__": [
      "self",
      "params",
      "training_config",
      "actor_factory",
      "critic_ensemble_factory",
      "optim_factory"
    ],
    "_create_algorithm": [
      "self",
      "envs",
      "device"
    ]
  },
  "ActorDualCriticsOffPolicyAlgorithmFactory": {
    "__init__": [
      "self",
      "params",
      "training_config",
      "actor_factory",
      "critic1_factory",
      "critic2_factory",
      "optim_factory"
    ],
    "_get_algorithm_class": [
      "self"
    ],
    "_get_discrete_last_size_use_action_shape": [
      "self"
    ],
    "_get_critic_use_action": [
      "envs"
    ],
    "_create_policy": [
      "self",
      "actor",
      "envs",
      "params"
    ],
    "_create_algorithm": [
      "self",
      "envs",
      "device"
    ]
  },
  "SACAlgorithmFactory": {
    "_create_policy": [
      "self",
      "actor",
      "envs",
      "params"
    ],
    "_get_algorithm_class": [
      "self"
    ]
  },
  "DiscreteSACAlgorithmFactory": {
    "_create_policy": [
      "self",
      "actor",
      "envs",
      "params"
    ],
    "_get_algorithm_class": [
      "self"
    ]
  },
  "TD3AlgorithmFactory": {
    "_create_policy": [
      "self",
      "actor",
      "envs",
      "params"
    ],
    "_get_algorithm_class": [
      "self"
    ]
  },
  "LoggerFactory": {
    "create_logger": [
      "self",
      "log_dir",
      "experiment_name",
      "run_id",
      "config_dict"
    ],
    "get_logger_class": [
      "self"
    ]
  },
  "LoggerFactoryDefault": {
    "__init__": [
      "self",
      "logger_type",
      "wand_entity",
      "wandb_project",
      "group",
      "job_type",
      "save_interval"
    ],
    "create_logger": [
      "self",
      "log_dir",
      "experiment_name",
      "run_id",
      "config_dict"
    ],
    "_create_writer": [
      "self",
      "log_dir"
    ],
    "get_logger_class": [
      "self"
    ]
  },
  "TrainingContext": {
    "__init__": [
      "self",
      "algorithm",
      "envs",
      "logger"
    ]
  },
  "EpochTrainCallback": {
    "callback": [
      "self",
      "epoch",
      "env_step",
      "context"
    ],
    "get_trainer_fn": [
      "self",
      "context"
    ]
  },
  "EpochTestCallback": {
    "callback": [
      "self",
      "epoch",
      "env_step",
      "context"
    ],
    "get_trainer_fn": [
      "self",
      "context"
    ]
  },
  "EpochStopCallback": {
    "should_stop": [
      "self",
      "mean_rewards",
      "context"
    ],
    "get_trainer_fn": [
      "self",
      "context"
    ]
  },
  "TrainerCallbacks": {},
  "EpochTrainCallbackDQNSetEps": {
    "__init__": [
      "self",
      "eps"
    ],
    "callback": [
      "self",
      "epoch",
      "env_step",
      "context"
    ]
  },
  "EpochTrainCallbackDQNEpsLinearDecay": {
    "__init__": [
      "self",
      "eps_train",
      "eps_train_final",
      "decay_steps"
    ],
    "callback": [
      "self",
      "epoch",
      "env_step",
      "context"
    ]
  },
  "EpochTestCallbackDQNSetEps": {
    "__init__": [
      "self",
      "eps"
    ],
    "callback": [
      "self",
      "epoch",
      "env_step",
      "context"
    ]
  },
  "EpochStopCallbackRewardThreshold": {
    "__init__": [
      "self",
      "threshold"
    ],
    "should_stop": [
      "self",
      "mean_rewards",
      "context"
    ]
  },
  "TrainingConfig": {
    "__setstate__": [
      "self",
      "state"
    ],
    "__post_init__": [
      "self"
    ]
  },
  "OnlineTrainingConfig": {
    "__setstate__": [
      "self",
      "state"
    ]
  },
  "OnPolicyTrainingConfig": {},
  "OffPolicyTrainingConfig": {},
  "World": {
    "persist_path": [
      "self",
      "filename"
    ],
    "restore_path": [
      "self",
      "filename"
    ]
  },
  "PersistEvent": {
    "PERSIST_POLICY": []
  },
  "RestoreEvent": {
    "RESTORE_POLICY": []
  },
  "Persistence": {
    "persist": [
      "self",
      "event",
      "world"
    ],
    "restore": [
      "self",
      "event",
      "world"
    ]
  },
  "PersistenceGroup": {
    "__init__": [
      "self"
    ],
    "persist": [
      "self",
      "event",
      "world"
    ],
    "restore": [
      "self",
      "event",
      "world"
    ]
  },
  "PolicyPersistence": {
    "__init__": [
      "self",
      "additional_persistence",
      "enabled",
      "mode"
    ],
    "persist": [
      "self",
      "policy",
      "world"
    ],
    "restore": [
      "self",
      "policy",
      "world",
      "device"
    ],
    "get_save_best_fn": [
      "self",
      "world"
    ],
    "get_save_checkpoint_fn": [
      "self",
      "world"
    ]
  },
  "EnvType": {
    "CONTINUOUS": [],
    "DISCRETE": [],
    "is_discrete": [
      "self"
    ],
    "is_continuous": [
      "self"
    ],
    "assert_continuous": [
      "self",
      "requiring_entity"
    ],
    "assert_discrete": [
      "self",
      "requiring_entity"
    ],
    "from_env": [
      "env"
    ]
  },
  "EnvMode": {
    "TRAINING": [],
    "TEST": [],
    "WATCH": []
  },
  "VectorEnvType": {
    "DUMMY": [],
    "SUBPROC": [],
    "SUBPROC_SHARED_MEM_DEFAULT_CONTEXT": [],
    "SUBPROC_SHARED_MEM_FORK_CONTEXT": [],
    "RAY": [],
    "SUBPROC_SHARED_MEM_AUTO": [],
    "create_venv": [
      "self",
      "factories"
    ]
  },
  "Environments": {
    "__init__": [
      "self",
      "env",
      "training_envs",
      "test_envs",
      "watch_env"
    ],
    "from_factory_and_type": [
      "factory_fn",
      "env_type",
      "venv_type",
      "num_training_envs",
      "num_test_envs",
      "create_watch_env"
    ],
    "_tostring_includes": [
      "self"
    ],
    "_tostring_additional_entries": [
      "self"
    ],
    "info": [
      "self"
    ],
    "set_persistence": [
      "self"
    ],
    "get_action_shape": [
      "self"
    ],
    "get_observation_shape": [
      "self"
    ],
    "get_action_space": [
      "self"
    ],
    "get_observation_space": [
      "self"
    ],
    "get_type": [
      "self"
    ]
  },
  "ContinuousEnvironments": {
    "__init__": [
      "self",
      "env",
      "training_envs",
      "test_envs",
      "watch_env"
    ],
    "from_factory": [
      "factory_fn",
      "venv_type",
      "num_training_envs",
      "num_test_envs",
      "create_watch_env"
    ],
    "info": [
      "self"
    ],
    "_get_continuous_env_info": [
      "env"
    ],
    "get_action_shape": [
      "self"
    ],
    "get_observation_shape": [
      "self"
    ],
    "get_type": [
      "self"
    ]
  },
  "DiscreteEnvironments": {
    "__init__": [
      "self",
      "env",
      "training_envs",
      "test_envs",
      "watch_env"
    ],
    "from_factory": [
      "factory_fn",
      "venv_type",
      "num_training_envs",
      "num_test_envs",
      "create_watch_env"
    ],
    "get_action_shape": [
      "self"
    ],
    "get_observation_shape": [
      "self"
    ],
    "get_type": [
      "self"
    ]
  },
  "EnvPoolFactory": {
    "_transform_task": [
      "self",
      "task"
    ],
    "_transform_kwargs": [
      "self",
      "kwargs",
      "mode"
    ],
    "create_venv": [
      "self",
      "task",
      "num_envs",
      "mode",
      "seed",
      "kwargs"
    ]
  },
  "EnvFactory": {
    "__init__": [
      "self",
      "venv_type"
    ],
    "_create_rng": [
      "seed"
    ],
    "_next_seed": [
      "rng"
    ],
    "_create_env": [
      "self",
      "mode"
    ],
    "create_env": [
      "self",
      "mode",
      "seed"
    ],
    "create_venv": [
      "self",
      "num_envs",
      "mode",
      "seed"
    ],
    "create_envs": [
      "self",
      "num_training_envs",
      "num_test_envs",
      "create_watch_env",
      "seed"
    ]
  },
  "EnvFactoryRegistered": {
    "__init__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "_create_kwargs": [
      "self",
      "mode"
    ],
    "_create_env": [
      "self",
      "mode"
    ],
    "create_venv": [
      "self",
      "num_envs",
      "mode",
      "seed"
    ]
  },
  "ExperimentConfig": {},
  "ExperimentResult": {},
  "Experiment": {
    "LOG_FILENAME": [],
    "EXPERIMENT_PICKLE_FILENAME": [],
    "__init__": [
      "self",
      "config",
      "env_factory",
      "algorithm_factory",
      "training_config",
      "name",
      "logger_factory"
    ],
    "from_directory": [
      "cls",
      "directory",
      "restore_policy"
    ],
    "seeding_info_str_static": [
      "seed"
    ],
    "get_seeding_info_as_str": [
      "self"
    ],
    "_set_seed": [
      "self"
    ],
    "_build_config_dict": [
      "self"
    ],
    "save": [
      "self",
      "directory"
    ],
    "persistence_dir_static": [
      "persistence_base_dir",
      "experiment_name",
      "seed"
    ],
    "create_experiment_world": [
      "self",
      "override_experiment_name",
      "logger_run_id",
      "raise_error_on_dirname_collision",
      "reset_collectors"
    ],
    "run": [
      "self",
      "run_name",
      "logger_run_id",
      "raise_error_on_dirname_collision"
    ],
    "_watch_agent": [
      "num_episodes",
      "policy",
      "env",
      "render"
    ]
  },
  "ExperimentCollection": {
    "__init__": [
      "self",
      "experiments"
    ],
    "run": [
      "self",
      "launcher"
    ]
  },
  "ExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "_create_training_config": [
      "self"
    ],
    "copy": [
      "self"
    ],
    "experiment_config": [
      "self",
      "experiment_config"
    ],
    "training_config": [
      "self",
      "config"
    ],
    "with_logger_factory": [
      "self",
      "logger_factory"
    ],
    "with_algorithm_wrapper_factory": [
      "self",
      "algorithm_wrapper_factory"
    ],
    "with_optim_default": [
      "self",
      "optim_factory"
    ],
    "with_epoch_train_callback": [
      "self",
      "callback"
    ],
    "with_epoch_test_callback": [
      "self",
      "callback"
    ],
    "with_epoch_stop_callback": [
      "self",
      "callback"
    ],
    "with_name": [
      "self",
      "name"
    ],
    "with_collector_factory": [
      "self",
      "collector_factory"
    ],
    "_create_algorithm_factory": [
      "self"
    ],
    "_get_optim_factory": [
      "self"
    ],
    "build": [
      "self"
    ],
    "build_seeded_collection": [
      "self",
      "num_experiments"
    ],
    "build_and_run": [
      "self",
      "num_experiments",
      "launcher",
      "perform_rliable_analysis"
    ]
  },
  "OnPolicyExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "_create_training_config": [
      "self"
    ]
  },
  "OffPolicyExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "_create_training_config": [
      "self"
    ]
  },
  "_BuilderMixinActorFactory": {
    "__init__": [
      "self",
      "continuous_actor_type"
    ],
    "with_actor_factory": [
      "self",
      "actor_factory"
    ],
    "_with_actor_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation",
      "continuous_unbounded",
      "continuous_conditioned_sigma"
    ],
    "get_actor_future": [
      "self"
    ],
    "_get_actor_factory": [
      "self"
    ]
  },
  "_BuilderMixinActorFactory_ContinuousGaussian": {
    "__init__": [
      "self"
    ],
    "with_actor_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation",
      "continuous_unbounded",
      "continuous_conditioned_sigma"
    ]
  },
  "_BuilderMixinActorFactory_ContinuousDeterministic": {
    "__init__": [
      "self"
    ],
    "with_actor_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ]
  },
  "_BuilderMixinActorFactory_DiscreteOnly": {
    "__init__": [
      "self"
    ],
    "with_actor_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ]
  },
  "_BuilderMixinCriticsFactory": {
    "__init__": [
      "self",
      "num_critics",
      "actor_future_provider"
    ],
    "_with_critic_factory": [
      "self",
      "idx",
      "critic_factory"
    ],
    "_with_critic_factory_default": [
      "self",
      "idx",
      "hidden_sizes",
      "hidden_activation"
    ],
    "_with_critic_factory_use_actor": [
      "self",
      "idx"
    ],
    "_get_critic_factory": [
      "self",
      "idx"
    ]
  },
  "_BuilderMixinSingleCriticFactory": {
    "__init__": [
      "self",
      "actor_future_provider"
    ],
    "with_critic_factory": [
      "self",
      "critic_factory"
    ],
    "with_critic_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ]
  },
  "_BuilderMixinSingleCriticCanUseActorFactory": {
    "__init__": [
      "self",
      "actor_future_provider"
    ],
    "with_critic_factory_use_actor": [
      "self"
    ]
  },
  "_BuilderMixinDualCriticFactory": {
    "__init__": [
      "self",
      "actor_future_provider"
    ],
    "with_common_critic_factory": [
      "self",
      "critic_factory"
    ],
    "with_common_critic_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ],
    "with_common_critic_factory_use_actor": [
      "self"
    ],
    "with_critic1_factory": [
      "self",
      "critic_factory"
    ],
    "with_critic1_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ],
    "with_critic1_factory_use_actor": [
      "self"
    ],
    "with_critic2_factory": [
      "self",
      "critic_factory"
    ],
    "with_critic2_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ],
    "with_critic2_factory_use_actor": [
      "self"
    ]
  },
  "_BuilderMixinCriticEnsembleFactory": {
    "__init__": [
      "self"
    ],
    "with_critic_ensemble_factory": [
      "self",
      "factory"
    ],
    "with_critic_ensemble_factory_default": [
      "self",
      "hidden_sizes"
    ],
    "_get_critic_ensemble_factory": [
      "self"
    ]
  },
  "ReinforceExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_reinforce_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "A2CExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_a2c_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "PPOExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_ppo_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "NPGExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_npg_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "TRPOExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_trpo_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "DQNExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_dqn_params": [
      "self",
      "params"
    ],
    "with_model_factory": [
      "self",
      "module_factory"
    ],
    "with_model_factory_default": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "IQNExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_iqn_params": [
      "self",
      "params"
    ],
    "with_preprocess_network_factory": [
      "self",
      "module_factory"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "DDPGExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_ddpg_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "REDQExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_redq_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "SACExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_sac_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "DiscreteSACExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_sac_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "TD3ExperimentBuilder": {
    "__init__": [
      "self",
      "env_factory",
      "experiment_config",
      "training_config"
    ],
    "with_td3_params": [
      "self",
      "params"
    ],
    "_create_algorithm_factory": [
      "self"
    ]
  },
  "CollectorFactory": {
    "create_collector": [
      "self",
      "algorithm",
      "vector_env",
      "buffer",
      "exploration_noise"
    ]
  },
  "CollectorFactoryDefault": {
    "create_collector": [
      "self",
      "algorithm",
      "vector_env",
      "buffer",
      "exploration_noise"
    ]
  },
  "ParamTransformerData": {},
  "ParamTransformer": {
    "transform": [
      "self",
      "params",
      "data"
    ],
    "get": [
      "d",
      "key",
      "drop",
      "default_factory"
    ]
  },
  "ParamTransformerDrop": {
    "__init__": [
      "self"
    ],
    "transform": [
      "self",
      "kwargs",
      "data"
    ]
  },
  "ParamTransformerRename": {
    "__init__": [
      "self",
      "renamed_params"
    ],
    "transform": [
      "self",
      "kwargs",
      "data"
    ]
  },
  "ParamTransformerChangeValue": {
    "__init__": [
      "self",
      "key"
    ],
    "transform": [
      "self",
      "params",
      "data"
    ],
    "change_value": [
      "self",
      "value",
      "data"
    ]
  },
  "ParamTransformerOptimFactory": {
    "__init__": [
      "self",
      "key_optim_factory_factory",
      "key_lr",
      "key_lr_scheduler_factory_factory",
      "key_optim_output"
    ],
    "transform": [
      "self",
      "params",
      "data"
    ]
  },
  "ParamTransformerAutoAlpha": {
    "__init__": [
      "self",
      "key"
    ],
    "transform": [
      "self",
      "kwargs",
      "data"
    ]
  },
  "ParamTransformerNoiseFactory": {
    "change_value": [
      "self",
      "value",
      "data"
    ]
  },
  "ParamTransformerFloatEnvParamFactory": {
    "change_value": [
      "self",
      "value",
      "data"
    ]
  },
  "ParamTransformerActionScaling": {
    "change_value": [
      "self",
      "value",
      "data"
    ]
  },
  "GetParamTransformersProtocol": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "Params": {
    "create_kwargs": [
      "self",
      "data"
    ],
    "_get_param_transformers": [
      "self"
    ]
  },
  "ParamsMixinSingleModel": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "ParamsMixinActorAndCritic": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "ParamsMixinActionScaling": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "ParamsMixinActionScalingAndBounding": {},
  "ParamsMixinExplorationNoise": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "ParamsMixinNStepReturnHorizon": {},
  "ParamsMixinGamma": {},
  "ParamsMixinTau": {},
  "ParamsMixinDeterministicEval": {},
  "OnPolicyAlgorithmParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "ReinforceParams": {},
  "ParamsMixinGeneralAdvantageEstimation": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "ActorCriticOnPolicyParams": {},
  "A2CParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "PPOParams": {},
  "NPGParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "TRPOParams": {},
  "ParamsMixinActorAndDualCritics": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "ParamsMixinAlpha": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "_SACParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "SACParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "DiscreteSACParams": {},
  "QLearningOffPolicyParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "DQNParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "IQNParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "DDPGParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "REDQParams": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "TD3Params": {
    "_get_param_transformers": [
      "self"
    ]
  },
  "TValue": [],
  "TEnvs": [],
  "EnvValueFactory": {
    "create_value": [
      "self",
      "envs"
    ]
  },
  "FloatEnvValueFactory": {},
  "FloatEnvValueFactoryMaxActionScaled": {
    "__init__": [
      "self",
      "value"
    ],
    "create_value": [
      "self",
      "envs"
    ]
  },
  "MaxActionScaled": {},
  "AutoAlphaFactory": {
    "create_auto_alpha": [
      "self",
      "envs",
      "device"
    ]
  },
  "AutoAlphaFactoryDefault": {
    "__init__": [
      "self",
      "lr",
      "target_entropy_coefficient",
      "log_alpha",
      "optim"
    ],
    "create_auto_alpha": [
      "self",
      "envs",
      "device"
    ]
  },
  "LRSchedulerFactoryFactory": {
    "create_lr_scheduler_factory": [
      "self"
    ]
  },
  "LRSchedulerFactoryFactoryLinear": {
    "__init__": [
      "self",
      "training_config"
    ],
    "create_lr_scheduler_factory": [
      "self"
    ]
  },
  "DistributionFunctionFactory": {
    "create_dist_fn": [
      "self",
      "envs"
    ]
  },
  "DistributionFunctionFactoryCategorical": {
    "__init__": [
      "self",
      "is_probs_input"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ],
    "_dist_fn": [
      "logits"
    ],
    "_dist_fn_probs": [
      "probs"
    ]
  },
  "DistributionFunctionFactoryIndependentGaussians": {
    "create_dist_fn": [
      "self",
      "envs"
    ],
    "_dist_fn": [
      "loc_scale"
    ]
  },
  "OptimizerWithLearningRateProtocol": {
    "__call__": [
      "self",
      "parameters",
      "lr"
    ]
  },
  "OptimizerFactoryFactory": {
    "default": [],
    "create_optimizer_factory": [
      "self",
      "lr"
    ]
  },
  "OptimizerFactoryFactoryTorch": {
    "__init__": [
      "self",
      "optim_class"
    ],
    "create_optimizer_factory": [
      "self",
      "lr"
    ]
  },
  "OptimizerFactoryFactoryAdam": {
    "__init__": [
      "self",
      "betas",
      "eps",
      "weight_decay"
    ],
    "create_optimizer_factory": [
      "self",
      "lr"
    ]
  },
  "OptimizerFactoryFactoryRMSprop": {
    "__init__": [
      "self",
      "alpha",
      "eps",
      "weight_decay",
      "momentum",
      "centered"
    ],
    "create_optimizer_factory": [
      "self",
      "lr"
    ]
  },
  "TAlgorithmOut": [],
  "AlgorithmWrapperFactory": {
    "create_wrapped_algorithm": [
      "self",
      "policy",
      "envs",
      "optim_factory",
      "device"
    ]
  },
  "AlgorithmWrapperFactoryIntrinsicCuriosity": {
    "__init__": [
      "self"
    ],
    "create_wrapped_algorithm": [
      "self",
      "algorithm",
      "envs",
      "optim_factory_default",
      "device"
    ]
  },
  "NoiseFactory": {
    "create_noise": [
      "self",
      "envs"
    ]
  },
  "NoiseFactoryMaxActionScaledGaussian": {
    "__init__": [
      "self",
      "std_fraction"
    ],
    "create_noise": [
      "self",
      "envs"
    ]
  },
  "MaxActionScaledGaussian": {},
  "init_linear_orthogonal": [
    "module"
  ],
  "ModuleFactory": {
    "create_module": [
      "self",
      "envs",
      "device"
    ]
  },
  "ContinuousActorType": {
    "GAUSSIAN": [],
    "DETERMINISTIC": [],
    "UNSUPPORTED": []
  },
  "ActorFuture": {},
  "ActorFutureProviderProtocol": {
    "get_actor_future": [
      "self"
    ]
  },
  "ActorFactory": {
    "create_module": [
      "self",
      "envs",
      "device"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ],
    "_init_linear": [
      "actor"
    ]
  },
  "ActorFactoryDefault": {
    "DEFAULT_HIDDEN_SIZES": [],
    "__init__": [
      "self",
      "continuous_actor_type",
      "hidden_sizes",
      "hidden_activation",
      "continuous_unbounded",
      "continuous_conditioned_sigma",
      "discrete_softmax"
    ],
    "_create_factory": [
      "self",
      "envs"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ]
  },
  "ActorFactoryContinuous": {},
  "ActorFactoryContinuousDeterministicNet": {
    "__init__": [
      "self",
      "hidden_sizes",
      "activation"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ]
  },
  "ActorFactoryContinuousGaussianNet": {
    "__init__": [
      "self",
      "hidden_sizes",
      "unbounded",
      "conditioned_sigma",
      "activation"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ]
  },
  "ActorFactoryDiscreteNet": {
    "__init__": [
      "self",
      "hidden_sizes",
      "softmax_output",
      "activation"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ]
  },
  "ActorFactoryTransientStorageDecorator": {
    "__init__": [
      "self",
      "actor_factory",
      "actor_future"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "_tostring_excludes": [
      "self"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ],
    "create_dist_fn": [
      "self",
      "envs"
    ]
  },
  "IntermediateModuleFactoryFromActorFactory": {
    "__init__": [
      "self",
      "actor_factory"
    ],
    "create_intermediate_module": [
      "self",
      "envs",
      "device"
    ]
  },
  "IntermediateModule": {
    "get_module_with_vector_output": [
      "self"
    ]
  },
  "IntermediateModuleFactory": {
    "create_intermediate_module": [
      "self",
      "envs",
      "device"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ]
  },
  "ImplicitQuantileNetworkFactory": {
    "__init__": [
      "self",
      "preprocess_net_factory",
      "hidden_sizes",
      "num_cosines"
    ],
    "create_module": [
      "self",
      "envs",
      "device"
    ]
  },
  "CriticFactory": {
    "create_module": [
      "self",
      "envs",
      "device",
      "use_action",
      "discrete_last_size_use_action_shape"
    ]
  },
  "CriticFactoryDefault": {
    "DEFAULT_HIDDEN_SIZES": [],
    "__init__": [
      "self",
      "hidden_sizes",
      "hidden_activation"
    ],
    "create_module": [
      "self",
      "envs",
      "device",
      "use_action",
      "discrete_last_size_use_action_shape"
    ]
  },
  "CriticFactoryContinuousNet": {
    "__init__": [
      "self",
      "hidden_sizes",
      "activation"
    ],
    "create_module": [
      "self",
      "envs",
      "device",
      "use_action",
      "discrete_last_size_use_action_shape"
    ]
  },
  "CriticFactoryDiscreteNet": {
    "__init__": [
      "self",
      "hidden_sizes",
      "activation"
    ],
    "create_module": [
      "self",
      "envs",
      "device",
      "use_action",
      "discrete_last_size_use_action_shape"
    ]
  },
  "CriticFactoryReuseActor": {
    "__init__": [
      "self",
      "actor_future"
    ],
    "_tostring_excludes": [
      "self"
    ],
    "create_module": [
      "self",
      "envs",
      "device",
      "use_action",
      "discrete_last_size_use_action_shape"
    ]
  },
  "CriticEnsembleFactory": {
    "create_module": [
      "self",
      "envs",
      "device",
      "ensemble_size",
      "use_action"
    ]
  },
  "CriticEnsembleFactoryDefault": {
    "DEFAULT_HIDDEN_SIZES": [],
    "__init__": [
      "self",
      "hidden_sizes"
    ],
    "create_module": [
      "self",
      "envs",
      "device",
      "ensemble_size",
      "use_action"
    ]
  },
  "CriticEnsembleFactoryContinuousNet": {
    "__init__": [
      "self",
      "hidden_sizes"
    ],
    "create_module": [
      "self",
      "envs",
      "device",
      "ensemble_size",
      "use_action"
    ]
  },
  "torch_train_mode": [
    "module",
    "enabled"
  ],
  "policy_within_training_step": [
    "policy",
    "enabled"
  ],
  "create_uniform_action_dist": [
    "action_space",
    "batch_size"
  ],
  "torch_device": [
    "module"
  ],
  "DataclassPPrintMixin": {
    "pprint_asdict": [
      "self",
      "exclude_fields",
      "indent"
    ],
    "pprints_asdict": [
      "self",
      "exclude_fields",
      "indent"
    ]
  },
  "tqdm_config": [],
  "DummyTqdm": {
    "__init__": [
      "self",
      "total"
    ],
    "set_postfix": [
      "self"
    ],
    "update": [
      "self",
      "n"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self"
    ]
  },
  "format_log_message": [
    "logger",
    "level",
    "msg",
    "formatter",
    "stacklevel"
  ],
  "TraceLogger": {
    "is_enabled": [],
    "verbose": [],
    "MESSAGE_TAG": [],
    "LOG_LEVEL": [],
    "log": [
      "cls",
      "logger",
      "message_generator"
    ]
  },
  "TraceLog": {
    "save_log": [
      "self",
      "path"
    ],
    "print_log": [
      "self"
    ],
    "get_full_log": [
      "self"
    ],
    "reduce_log_to_messages": [
      "self"
    ],
    "filter_messages": [
      "self",
      "required_messages",
      "optional_messages",
      "ignored_messages"
    ]
  },
  "TraceLoggerContext": {
    "__init__": [
      "self",
      "enable_log_buffer",
      "log_format"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_val",
      "exc_tb"
    ],
    "get_log": [
      "self"
    ]
  },
  "torch_param_hash": [
    "module"
  ],
  "TraceDeterminismTest": {
    "__init__": [
      "self",
      "base_path",
      "core_messages",
      "ignored_messages",
      "log_filename"
    ],
    "check": [
      "self",
      "current_log",
      "name",
      "create_reference_result",
      "pass_if_core_messages_unchanged"
    ]
  },
  "MovAvg": {
    "__init__": [
      "self",
      "size"
    ],
    "add": [
      "self",
      "data_array"
    ],
    "get": [
      "self"
    ],
    "mean": [
      "self"
    ],
    "std": [
      "self"
    ]
  },
  "RunningMeanStd": {
    "__init__": [
      "self",
      "mean",
      "std",
      "clip_max",
      "epsilon"
    ],
    "norm": [
      "self",
      "data_array"
    ],
    "update": [
      "self",
      "data_array"
    ]
  },
  "to_optional_float": [
    "x"
  ],
  "ActionSpaceInfo": {
    "action_dim": [
      "self"
    ],
    "from_space": [
      "cls",
      "space"
    ],
    "_tostring_additional_entries": [
      "self"
    ]
  },
  "ObservationSpaceInfo": {
    "obs_dim": [
      "self"
    ],
    "from_space": [
      "cls",
      "space"
    ],
    "_tostring_additional_entries": [
      "self"
    ]
  },
  "SpaceInfo": {
    "from_env": [
      "cls",
      "env"
    ],
    "from_spaces": [
      "cls",
      "action_space",
      "observation_space"
    ]
  },
  "set_numerical_fields_to_precision": [
    "data",
    "precision"
  ],
  "polyak_parameter_update": [
    "tgt",
    "src",
    "tau"
  ],
  "EvalModeModuleWrapper": {
    "__init__": [
      "self",
      "m"
    ],
    "forward": [
      "self"
    ],
    "train": [
      "self",
      "mode"
    ]
  },
  "LaggedNetworkPair": {},
  "LaggedNetworkCollection": {
    "__init__": [
      "self"
    ],
    "add_lagged_network": [
      "self",
      "source"
    ],
    "polyak_parameter_update": [
      "self",
      "tau"
    ],
    "full_parameter_update": [
      "self"
    ]
  },
  "deprecation": [
    "msg"
  ],
  "ModuleType": [],
  "ArgsType": [],
  "miniblock": [
    "input_size",
    "output_size",
    "norm_layer",
    "norm_args",
    "activation",
    "act_args",
    "linear_layer"
  ],
  "ModuleWithVectorOutput": {
    "__init__": [
      "self",
      "output_dim"
    ],
    "from_module": [
      "module",
      "output_dim"
    ],
    "get_output_dim": [
      "self"
    ]
  },
  "ModuleWithVectorOutputAdapter": {
    "__init__": [
      "self",
      "module",
      "output_dim"
    ],
    "forward": [
      "self"
    ]
  },
  "MLP": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs"
    ]
  },
  "TRecurrentState": [],
  "ActionReprNet": {
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "ActionReprNetWithVectorOutput": {
    "__init__": [
      "self",
      "output_dim"
    ]
  },
  "Actor": {
    "get_preprocess_net": [
      "self"
    ]
  },
  "Net": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "Recurrent": {
    "__init__": [
      "self"
    ],
    "get_preprocess_net": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "ActorCritic": {
    "__init__": [
      "self",
      "actor",
      "critic"
    ]
  },
  "DataParallelNet": {
    "__init__": [
      "self",
      "net"
    ],
    "forward": [
      "self",
      "obs"
    ]
  },
  "ActionReprNetDataParallelWrapper": {
    "__init__": [
      "self",
      "net"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "EnsembleLinear": {
    "__init__": [
      "self",
      "ensemble_size",
      "in_feature",
      "out_feature",
      "bias"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BranchingNet": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "get_dict_state_decorator": [
    "state_shape",
    "keys"
  ],
  "AbstractContinuousActorProbabilistic": {},
  "AbstractDiscreteActor": {},
  "RandomActor": {
    "__init__": [
      "self",
      "action_space"
    ],
    "action_space": [
      "self"
    ],
    "space_info": [
      "self"
    ],
    "get_preprocess_net": [
      "self"
    ],
    "get_output_dim": [
      "self"
    ],
    "is_discrete": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ],
    "compute_action_batch": [
      "self",
      "obs"
    ]
  },
  "SIGMA_MIN": [],
  "SIGMA_MAX": [],
  "AbstractContinuousActorDeterministic": {},
  "ContinuousActorDeterministic": {
    "__init__": [
      "self"
    ],
    "get_preprocess_net": [
      "self"
    ],
    "get_output_dim": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "AbstractContinuousCritic": {
    "forward": [
      "self",
      "obs",
      "act",
      "info"
    ]
  },
  "ContinuousCritic": {
    "__init__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "forward": [
      "self",
      "obs",
      "act",
      "info"
    ]
  },
  "ContinuousActorProbabilistic": {
    "__init__": [
      "self"
    ],
    "get_preprocess_net": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "RecurrentActorProb": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "RecurrentCritic": {
    "__init__": [
      "self",
      "layer_num",
      "state_shape",
      "action_shape",
      "hidden_layer_size"
    ],
    "forward": [
      "self",
      "obs",
      "act",
      "info"
    ]
  },
  "Perturbation": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "state",
      "action"
    ]
  },
  "VAE": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "state",
      "action"
    ],
    "decode": [
      "self",
      "state",
      "latent_z"
    ]
  },
  "dist_fn_categorical_from_logits": [
    "logits"
  ],
  "DiscreteActor": {
    "__init__": [
      "self"
    ],
    "get_preprocess_net": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "DiscreteCritic": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "CosineEmbeddingNetwork": {
    "__init__": [
      "self",
      "num_cosines",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "taus"
    ]
  },
  "ImplicitQuantileNetwork": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "obs",
      "sample_size"
    ]
  },
  "FractionProposalNetwork": {
    "__init__": [
      "self",
      "num_fractions",
      "embedding_dim"
    ],
    "forward": [
      "self",
      "obs_embeddings"
    ]
  },
  "FullQuantileFunction": {
    "__init__": [
      "self"
    ],
    "_compute_quantiles": [
      "self",
      "obs",
      "taus"
    ],
    "forward": [
      "self",
      "obs",
      "propose_model",
      "fractions"
    ]
  },
  "NoisyLinear": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "noisy_std"
    ],
    "reset": [
      "self"
    ],
    "f": [
      "self",
      "x"
    ],
    "sample": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "IntrinsicCuriosityModule": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "s1",
      "act",
      "s2"
    ]
  },
  "WandbLogger": {
    "__init__": [
      "self",
      "training_interval",
      "test_interval",
      "update_interval",
      "info_interval",
      "save_interval",
      "write_flush",
      "project",
      "name",
      "entity",
      "run_id",
      "group",
      "job_type",
      "config",
      "monitor_gym",
      "disable_stats",
      "log_dir"
    ],
    "prepare_dict_for_logging": [
      "self",
      "log_data"
    ],
    "load": [
      "self",
      "writer"
    ],
    "write": [
      "self",
      "step_type",
      "step",
      "data"
    ],
    "finalize": [
      "self"
    ],
    "save_data": [
      "self",
      "epoch",
      "env_step",
      "update_step",
      "save_checkpoint_fn"
    ],
    "restore_data": [
      "self"
    ],
    "restore_logged_data": [
      "log_path"
    ]
  },
  "TensorboardLogger": {
    "__init__": [
      "self",
      "writer",
      "training_interval",
      "test_interval",
      "update_interval",
      "info_interval",
      "save_interval",
      "write_flush"
    ],
    "prepare_dict_for_logging": [
      "self",
      "input_dict",
      "parent_key",
      "delimiter",
      "exclude_arrays"
    ],
    "write": [
      "self",
      "step_type",
      "step",
      "data"
    ],
    "finalize": [
      "self"
    ],
    "save_data": [
      "self",
      "epoch",
      "env_step",
      "update_step",
      "save_checkpoint_fn"
    ],
    "restore_data": [
      "self"
    ],
    "restore_logged_data": [
      "log_path"
    ]
  },
  "VALID_LOG_VALS_TYPE": [],
  "VALID_LOG_VALS": [],
  "TRestoredData": [],
  "DataScope": {
    "TRAINING": [],
    "TEST": [],
    "UPDATE": [],
    "INFO": []
  },
  "BaseLogger": {
    "__init__": [
      "self",
      "training_interval",
      "test_interval",
      "update_interval",
      "info_interval",
      "save_interval",
      "exclude_arrays"
    ],
    "write": [
      "self",
      "step_type",
      "step",
      "data"
    ],
    "prepare_dict_for_logging": [
      "self",
      "log_data"
    ],
    "finalize": [
      "self"
    ],
    "log_training_data": [
      "self",
      "log_data",
      "step"
    ],
    "log_test_data": [
      "self",
      "log_data",
      "step"
    ],
    "log_update_data": [
      "self",
      "log_data",
      "step"
    ],
    "log_info_data": [
      "self",
      "log_data",
      "step"
    ],
    "save_data": [
      "self",
      "epoch",
      "env_step",
      "update_step",
      "save_checkpoint_fn"
    ],
    "restore_data": [
      "self"
    ],
    "restore_logged_data": [
      "log_path"
    ]
  },
  "LazyLogger": {
    "__init__": [
      "self"
    ],
    "prepare_dict_for_logging": [
      "self",
      "data"
    ],
    "write": [
      "self",
      "step_type",
      "step",
      "data"
    ],
    "finalize": [
      "self"
    ],
    "save_data": [
      "self",
      "epoch",
      "env_step",
      "update_step",
      "save_checkpoint_fn"
    ],
    "restore_data": [
      "self"
    ],
    "restore_logged_data": [
      "log_path"
    ]
  },
  "JoblibConfig": {},
  "default_experiment_execution": [
    "exp"
  ],
  "ExpLauncher": {
    "__init__": [
      "self",
      "experiment_runner"
    ],
    "get_name": [
      "self"
    ],
    "_launch": [
      "self",
      "experiments"
    ],
    "_safe_execute": [
      "self",
      "exp"
    ],
    "_return_from_successful_and_failed_exps": [
      "successful_exp_stats",
      "failed_exps"
    ],
    "launch": [
      "self",
      "experiments"
    ]
  },
  "SequentialExpLauncher": {
    "_launch": [
      "self",
      "experiments"
    ]
  },
  "JoblibExpLauncher": {
    "__init__": [
      "self",
      "joblib_cfg",
      "experiment_runner"
    ],
    "_launch": [
      "self",
      "experiments"
    ]
  },
  "RegisteredExpLauncher": {
    "JOBLIB": [],
    "SEQUENTIAL": [],
    "create_launcher": [
      "self"
    ]
  },
  "EvaluationSequenceEntry": {},
  "LoggedSummaryData": {},
  "LoggedCollectStats": {
    "from_data_dict": [
      "cls",
      "data"
    ]
  },
  "MultiRunExperimentResult": {
    "load_from_disk": [
      "cls",
      "exp_dir",
      "exp_name",
      "max_env_step"
    ],
    "_get_env_steps_and_returns": [
      "self",
      "scope"
    ],
    "_get_data_in_rliable_format": [
      "self",
      "algo_name",
      "score_thresholds",
      "scope"
    ],
    "_compute_iqm_scores": [
      "self",
      "scope"
    ],
    "eval_results": [
      "self",
      "algo_name",
      "score_thresholds",
      "save_as_json",
      "save_plots",
      "show_plots",
      "scope",
      "ax_iqm_sample_efficiency",
      "ax_performance_profile",
      "algo2color"
    ],
    "to_evaluation_sequence": [
      "self",
      "scope"
    ]
  },
  "load_and_eval_experiment": [
    "log_dir",
    "show_plots",
    "save_plots",
    "save_as_json",
    "scope",
    "max_env_step"
  ],
  "DEFAULT_BUFFER_MAXSIZE": [],
  "_TArrLike": [],
  "TScalarArrayShape": [],
  "CollectActionBatchProtocol": {},
  "CollectStepBatchProtocol": {},
  "EpisodeBatchProtocol": {},
  "get_stddev_from_dist": [
    "dist"
  ],
  "CollectStatsBase": {},
  "CollectStats": {
    "with_autogenerated_stats": [
      "cls",
      "returns",
      "lens",
      "n_collected_episodes",
      "n_collected_steps",
      "collect_time",
      "collect_speed"
    ],
    "update_at_step_batch": [
      "self",
      "step_batch",
      "refresh_sequence_stats"
    ],
    "update_at_episode_done": [
      "self",
      "episode_batch",
      "episode_return",
      "refresh_sequence_stats"
    ],
    "set_collect_time": [
      "self",
      "collect_time",
      "update_collect_speed"
    ],
    "refresh_return_stats": [
      "self"
    ],
    "refresh_len_stats": [
      "self"
    ],
    "refresh_std_array_stats": [
      "self"
    ],
    "refresh_all_sequence_stats": [
      "self"
    ]
  },
  "TCollectStats": [],
  "_nullable_slice": [
    "obj",
    "indices"
  ],
  "_dict_of_arr_to_arr_of_dicts": [
    "dict_of_arr"
  ],
  "_HACKY_create_info_batch": [
    "info_array"
  ],
  "BaseCollector": {
    "__init__": [
      "self",
      "policy",
      "env",
      "buffer",
      "exploration_noise",
      "collect_stats_class",
      "raise_on_nan_in_buffer"
    ],
    "_validate_buffer": [
      "self"
    ],
    "env_num": [
      "self"
    ],
    "action_space": [
      "self"
    ],
    "close": [
      "self"
    ],
    "reset": [
      "self",
      "reset_buffer",
      "reset_stats",
      "gym_reset_kwargs"
    ],
    "reset_stat": [
      "self"
    ],
    "reset_buffer": [
      "self",
      "keep_statistics"
    ],
    "reset_env": [
      "self",
      "gym_reset_kwargs"
    ],
    "_collect": [
      "self",
      "n_step",
      "n_episode",
      "random",
      "render",
      "gym_reset_kwargs"
    ],
    "collect": [
      "self",
      "n_step",
      "n_episode",
      "random",
      "render",
      "reset_before_collect",
      "gym_reset_kwargs"
    ],
    "_validate_n_step_n_episode": [
      "self",
      "n_episode",
      "n_step"
    ]
  },
  "Collector": {
    "__init__": [
      "self",
      "policy",
      "env",
      "buffer",
      "exploration_noise",
      "on_episode_done_hook",
      "on_step_hook",
      "raise_on_nan_in_buffer",
      "collect_stats_class"
    ],
    "set_on_episode_done_hook": [
      "self",
      "hook"
    ],
    "set_on_step_hook": [
      "self",
      "hook"
    ],
    "get_on_episode_done_hook": [
      "self"
    ],
    "get_on_step_hook": [
      "self"
    ],
    "close": [
      "self"
    ],
    "run_on_episode_done": [
      "self",
      "episode_batch"
    ],
    "run_on_step_hook": [
      "self",
      "action_batch",
      "rollout_batch"
    ],
    "reset_env": [
      "self",
      "gym_reset_kwargs"
    ],
    "_compute_action_policy_hidden": [
      "self",
      "random",
      "ready_env_ids_R",
      "last_obs_RO",
      "last_info_R",
      "last_hidden_state_RH"
    ],
    "_collect": [
      "self",
      "n_step",
      "n_episode",
      "random",
      "render",
      "gym_reset_kwargs"
    ],
    "_reset_hidden_state_based_on_type": [
      "env_ind_local_D",
      "last_hidden_state_RH"
    ]
  },
  "AsyncCollector": {
    "__init__": [
      "self",
      "policy",
      "env",
      "buffer",
      "exploration_noise",
      "raise_on_nan_in_buffer"
    ],
    "reset": [
      "self",
      "reset_buffer",
      "reset_stats",
      "gym_reset_kwargs"
    ],
    "reset_env": [
      "self",
      "gym_reset_kwargs"
    ],
    "_collect": [
      "self",
      "n_step",
      "n_episode",
      "random",
      "render",
      "gym_reset_kwargs"
    ]
  },
  "StepHookProtocol": {
    "__call__": [
      "self",
      "action_batch",
      "rollout_batch"
    ]
  },
  "StepHook": {
    "__call__": [
      "self",
      "action_batch",
      "rollout_batch"
    ]
  },
  "StepHookAddActionDistribution": {
    "ACTION_DIST_KEY": [],
    "__call__": [
      "self",
      "action_batch",
      "rollout_batch"
    ]
  },
  "EpisodeRolloutHookProtocol": {
    "__call__": [
      "self",
      "episode_batch"
    ]
  },
  "EpisodeRolloutHook": {
    "__call__": [
      "self",
      "episode_batch"
    ]
  },
  "EpisodeRolloutHookMCReturn": {
    "MC_RETURN_TO_GO_KEY": [],
    "FULL_EPISODE_MC_RETURN_KEY": [],
    "__init__": [
      "self",
      "gamma"
    ],
    "__call__": [
      "self",
      "episode_batch"
    ]
  },
  "EpisodeRolloutHookMerged": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "episode_batch"
    ]
  },
  "_SingleIndexType": [],
  "IndexType": [],
  "TBatch": [],
  "TDistribution": [],
  "TArr": [],
  "TObsArr": [],
  "_is_batch_set": [
    "obj"
  ],
  "_is_scalar": [
    "value"
  ],
  "_is_number": [
    "value"
  ],
  "_to_array_with_correct_type": [
    "obj"
  ],
  "create_value": [
    "inst",
    "size",
    "stack"
  ],
  "_assert_type_keys": [
    "keys"
  ],
  "_parse_value": [
    "obj"
  ],
  "alloc_by_keys_diff": [
    "meta",
    "batch",
    "size",
    "stack"
  ],
  "ProtocolCalledException": {},
  "get_sliced_dist": [
    "dist",
    "index"
  ],
  "get_len_of_dist": [
    "dist"
  ],
  "dist_to_atleast_2d": [
    "dist"
  ],
  "BatchProtocol": {
    "shape": [
      "self"
    ],
    "__setattr__": [
      "self",
      "key",
      "value"
    ],
    "__getattr__": [
      "self",
      "key"
    ],
    "__iter__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "__setitem__": [
      "self",
      "index",
      "value"
    ],
    "__iadd__": [
      "self",
      "other"
    ],
    "__add__": [
      "self",
      "other"
    ],
    "__imul__": [
      "self",
      "value"
    ],
    "__mul__": [
      "self",
      "value"
    ],
    "__itruediv__": [
      "self",
      "value"
    ],
    "__truediv__": [
      "self",
      "value"
    ],
    "__repr__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "to_numpy": [
      "self"
    ],
    "to_numpy_": [
      "self"
    ],
    "to_torch": [
      "self",
      "dtype",
      "device"
    ],
    "to_torch_": [
      "self",
      "dtype",
      "device"
    ],
    "cat_": [
      "self",
      "batches"
    ],
    "cat": [
      "batches"
    ],
    "stack_": [
      "self",
      "batches",
      "axis"
    ],
    "stack": [
      "batches",
      "axis"
    ],
    "empty_": [
      "self",
      "index"
    ],
    "empty": [
      "batch",
      "index"
    ],
    "update": [
      "self",
      "batch"
    ],
    "__len__": [
      "self"
    ],
    "split": [
      "self",
      "size",
      "shuffle",
      "merge_last"
    ],
    "to_dict": [
      "self",
      "recurse"
    ],
    "to_list_of_dicts": [
      "self"
    ],
    "get_keys": [
      "self"
    ],
    "set_array_at_key": [
      "self",
      "seq",
      "key",
      "index",
      "default_value"
    ],
    "isnull": [
      "self"
    ],
    "hasnull": [
      "self"
    ],
    "dropnull": [
      "self"
    ],
    "apply_values_transform": [
      "self",
      "values_transform",
      "inplace"
    ],
    "get": [
      "self",
      "key",
      "default"
    ],
    "pop": [
      "self",
      "key",
      "default"
    ],
    "to_at_least_2d": [
      "self"
    ]
  },
  "Batch": {
    "__doc__": [],
    "__init__": [
      "self",
      "batch_dict",
      "copy"
    ],
    "to_dict": [
      "self",
      "recursive"
    ],
    "get_keys": [
      "self"
    ],
    "get": [
      "self",
      "key",
      "default"
    ],
    "pop": [
      "self",
      "key",
      "default"
    ],
    "to_list_of_dicts": [
      "self"
    ],
    "__setattr__": [
      "self",
      "key",
      "value"
    ],
    "__getattr__": [
      "self",
      "key"
    ],
    "__contains__": [
      "self",
      "key"
    ],
    "__getstate__": [
      "self"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__iter__": [
      "self"
    ],
    "__setitem__": [
      "self",
      "index",
      "value"
    ],
    "__iadd__": [
      "self",
      "other"
    ],
    "__add__": [
      "self",
      "other"
    ],
    "__imul__": [
      "self",
      "value"
    ],
    "__mul__": [
      "self",
      "value"
    ],
    "__itruediv__": [
      "self",
      "value"
    ],
    "__truediv__": [
      "self",
      "value"
    ],
    "__repr__": [
      "self"
    ],
    "to_numpy": [
      "self"
    ],
    "to_numpy_": [
      "self"
    ],
    "to_torch": [
      "self",
      "dtype",
      "device"
    ],
    "to_torch_": [
      "self",
      "dtype",
      "device"
    ],
    "__cat": [
      "self",
      "batches",
      "lens"
    ],
    "cat_": [
      "self",
      "batches"
    ],
    "cat": [
      "batches"
    ],
    "stack_": [
      "self",
      "batches",
      "axis"
    ],
    "stack": [
      "batches",
      "axis"
    ],
    "empty_": [
      "self",
      "index"
    ],
    "empty": [
      "batch",
      "index"
    ],
    "update": [
      "self",
      "batch"
    ],
    "__len__": [
      "self"
    ],
    "shape": [
      "self"
    ],
    "split": [
      "self",
      "size",
      "shuffle",
      "merge_last"
    ],
    "apply_values_transform": [
      "self",
      "values_transform",
      "inplace"
    ],
    "set_array_at_key": [
      "self",
      "arr",
      "key",
      "index",
      "default_value"
    ],
    "isnull": [
      "self"
    ],
    "hasnull": [
      "self"
    ],
    "dropnull": [
      "self"
    ],
    "replace_empty_batches_by_none": [
      "self"
    ],
    "to_at_least_2d": [
      "self"
    ]
  },
  "_apply_batch_values_func_recursively": [
    "batch",
    "values_transform",
    "inplace"
  ],
  "SequenceSummaryStats": {
    "from_sequence": [
      "cls",
      "sequence"
    ],
    "from_single_value": [
      "cls",
      "value"
    ]
  },
  "compute_dim_to_summary_stats": [
    "arr"
  ],
  "TimingStats": {},
  "InfoStats": {},
  "EpochStats": {},
  "TObs": [],
  "TNestedDictValue": [],
  "ObsBatchProtocol": {},
  "RolloutBatchProtocol": {},
  "BatchWithReturnsProtocol": {},
  "PrioBatchProtocol": {},
  "RecurrentStateBatch": {},
  "ActBatchProtocol": {},
  "ActStateBatchProtocol": {},
  "ModelOutputBatchProtocol": {},
  "FQFBatchProtocol": {},
  "BatchWithAdvantagesProtocol": {},
  "DistBatchProtocol": {},
  "DistLogProbBatchProtocol": {},
  "LogpOldProtocol": {},
  "QuantileRegressionBatchProtocol": {},
  "ImitationBatchProtocol": {},
  "SegmentTree": {
    "__init__": [
      "self",
      "size"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "__setitem__": [
      "self",
      "index",
      "value"
    ],
    "reduce": [
      "self",
      "start",
      "end"
    ],
    "get_prefix_sum_idx": [
      "self",
      "value"
    ],
    "_compile": [
      "self"
    ]
  },
  "_setitem": [
    "tree",
    "index",
    "value"
  ],
  "_reduce": [
    "tree",
    "start",
    "end"
  ],
  "_get_prefix_sum_idx": [
    "value",
    "bound",
    "sums"
  ],
  "to_numpy": [
    "x"
  ],
  "to_torch": [
    "x",
    "dtype",
    "device"
  ],
  "to_torch_as": [
    "x",
    "y"
  ],
  "Hdf5ConvertibleValues": [],
  "Hdf5ConvertibleType": [],
  "to_hdf5": [
    "x",
    "y",
    "compression"
  ],
  "from_hdf5": [
    "x",
    "device"
  ],
  "VectorReplayBuffer": {
    "__init__": [
      "self",
      "total_size",
      "buffer_num"
    ]
  },
  "PrioritizedVectorReplayBuffer": {
    "__init__": [
      "self",
      "total_size",
      "buffer_num"
    ],
    "set_beta": [
      "self",
      "beta"
    ]
  },
  "HERVectorReplayBuffer": {
    "__init__": [
      "self",
      "total_size",
      "buffer_num"
    ]
  },
  "CachedReplayBuffer": {
    "__init__": [
      "self",
      "main_buffer",
      "cached_buffer_num",
      "max_episode_length"
    ],
    "add": [
      "self",
      "batch",
      "buffer_ids"
    ]
  },
  "HERReplayBuffer": {
    "__init__": [
      "self",
      "size",
      "compute_reward_fn",
      "horizon",
      "future_k"
    ],
    "_restore_cache": [
      "self"
    ],
    "reset": [
      "self",
      "keep_statistics"
    ],
    "save_hdf5": [
      "self",
      "path",
      "compression"
    ],
    "set_batch": [
      "self",
      "batch"
    ],
    "update": [
      "self",
      "buffer"
    ],
    "add": [
      "self",
      "batch",
      "buffer_ids"
    ],
    "sample_indices": [
      "self",
      "batch_size"
    ],
    "rewrite_transitions": [
      "self",
      "indices"
    ],
    "_compute_reward": [
      "self",
      "obs",
      "lead_dims"
    ]
  },
  "TBuffer": [],
  "MalformedBufferError": {},
  "ReplayBuffer": {
    "_reserved_keys": [],
    "_input_keys": [],
    "__init__": [
      "self",
      "size",
      "stack_num",
      "ignore_obs_next",
      "save_only_last_obs",
      "sample_avail",
      "random_seed"
    ],
    "__setstate__": [
      "self",
      "state"
    ],
    "subbuffer_edges": [
      "self"
    ],
    "_get_start_stop_tuples_for_edge_crossing_interval": [
      "self",
      "start",
      "stop"
    ],
    "get_buffer_indices": [
      "self",
      "start",
      "stop"
    ],
    "__len__": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__getattr__": [
      "self",
      "key"
    ],
    "__setattr__": [
      "self",
      "key",
      "value"
    ],
    "save_hdf5": [
      "self",
      "path",
      "compression"
    ],
    "load_hdf5": [
      "cls",
      "path",
      "device"
    ],
    "from_data": [
      "cls",
      "obs",
      "act",
      "rew",
      "terminated",
      "truncated",
      "done",
      "obs_next"
    ],
    "reset": [
      "self",
      "keep_statistics"
    ],
    "set_batch": [
      "self",
      "batch"
    ],
    "unfinished_index": [
      "self"
    ],
    "prev": [
      "self",
      "index"
    ],
    "next": [
      "self",
      "index"
    ],
    "update": [
      "self",
      "buffer"
    ],
    "_update_state_pre_add": [
      "self",
      "rew",
      "done"
    ],
    "add": [
      "self",
      "batch",
      "buffer_ids"
    ],
    "sample_indices": [
      "self",
      "batch_size"
    ],
    "sample": [
      "self",
      "batch_size"
    ],
    "get": [
      "self",
      "index",
      "key",
      "default_value",
      "stack_num"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "set_array_at_key": [
      "self",
      "seq",
      "key",
      "index",
      "default_value"
    ],
    "hasnull": [
      "self"
    ],
    "isnull": [
      "self"
    ],
    "dropnull": [
      "self"
    ]
  },
  "_backward_compatibility": [],
  "ReplayBufferManager": {
    "__init__": [
      "self",
      "buffer_list"
    ],
    "subbuffer_edges": [
      "self"
    ],
    "_compile": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "reset": [
      "self",
      "keep_statistics"
    ],
    "_set_batch_for_children": [
      "self"
    ],
    "set_batch": [
      "self",
      "batch"
    ],
    "unfinished_index": [
      "self"
    ],
    "prev": [
      "self",
      "index"
    ],
    "next": [
      "self",
      "index"
    ],
    "update": [
      "self",
      "buffer"
    ],
    "add": [
      "self",
      "batch",
      "buffer_ids"
    ],
    "sample_indices": [
      "self",
      "batch_size"
    ]
  },
  "PrioritizedReplayBufferManager": {
    "__init__": [
      "self",
      "buffer_list"
    ]
  },
  "HERReplayBufferManager": {
    "__init__": [
      "self",
      "buffer_list"
    ],
    "_restore_cache": [
      "self"
    ],
    "save_hdf5": [
      "self",
      "path",
      "compression"
    ],
    "set_batch": [
      "self",
      "batch"
    ],
    "update": [
      "self",
      "buffer"
    ],
    "add": [
      "self",
      "batch",
      "buffer_ids"
    ]
  },
  "_prev_index": [
    "index",
    "offset",
    "done",
    "last_index",
    "lengths"
  ],
  "_next_index": [
    "index",
    "offset",
    "done",
    "last_index",
    "lengths"
  ],
  "PrioritizedReplayBuffer": {
    "__init__": [
      "self",
      "size",
      "alpha",
      "beta",
      "weight_norm"
    ],
    "init_weight": [
      "self",
      "index"
    ],
    "update": [
      "self",
      "buffer"
    ],
    "add": [
      "self",
      "batch",
      "buffer_ids"
    ],
    "sample_indices": [
      "self",
      "batch_size"
    ],
    "get_weight": [
      "self",
      "index"
    ],
    "update_weight": [
      "self",
      "index",
      "new_weight"
    ],
    "__getitem__": [
      "self",
      "index"
    ],
    "sample": [
      "self",
      "batch_size"
    ],
    "set_beta": [
      "self",
      "beta"
    ]
  },
  "logger": [],
  "TArrOrActBatch": [],
  "TrainingStats": {
    "_non_loss_fields": [],
    "_get_self_dict": [
      "self"
    ],
    "get_loss_stats_dict": [
      "self"
    ]
  },
  "TrainingStatsWrapper": {
    "_setattr_frozen": [],
    "_training_stats_public_fields": [],
    "__init__": [
      "self",
      "wrapped_stats"
    ],
    "_get_self_dict": [
      "self"
    ],
    "wrapped_stats": [
      "self"
    ],
    "__getattr__": [
      "self",
      "name"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ]
  },
  "Policy": {
    "__init__": [
      "self",
      "action_space",
      "observation_space",
      "action_scaling",
      "action_bound_method"
    ],
    "action_type": [
      "self"
    ],
    "_action_to_numpy": [
      "act"
    ],
    "map_action": [
      "self",
      "act"
    ],
    "map_action_inverse": [
      "self",
      "act"
    ],
    "compute_action": [
      "self",
      "obs",
      "info",
      "state"
    ],
    "_compile": [],
    "_TArrOrActBatch": [],
    "add_exploration_noise": [
      "self",
      "act",
      "batch"
    ]
  },
  "LaggedNetworkAlgorithmMixin": {
    "__init__": [
      "self"
    ],
    "_add_lagged_network": [
      "self",
      "src"
    ],
    "_update_lagged_network_weights": [
      "self"
    ]
  },
  "LaggedNetworkFullUpdateAlgorithmMixin": {
    "_update_lagged_network_weights": [
      "self"
    ]
  },
  "LaggedNetworkPolyakUpdateAlgorithmMixin": {
    "__init__": [
      "self",
      "tau"
    ],
    "_update_lagged_network_weights": [
      "self"
    ]
  },
  "Algorithm": {
    "_STATE_DICT_KEY_OPTIMIZERS": [],
    "__init__": [
      "self"
    ],
    "_create_optimizer": [
      "self",
      "module",
      "factory",
      "max_grad_norm"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict",
      "strict",
      "assign"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_postprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update": [
      "self",
      "sample_size",
      "buffer",
      "update_with_batch_fn"
    ],
    "value_mask": [
      "buffer",
      "indices"
    ],
    "compute_episodic_return": [
      "batch",
      "buffer",
      "indices",
      "v_s_",
      "v_s",
      "gamma",
      "gae_lambda"
    ],
    "compute_nstep_return": [
      "batch",
      "buffer",
      "indices",
      "target_q_fn",
      "gamma",
      "n_step"
    ],
    "create_trainer": [
      "self",
      "params"
    ],
    "run_training": [
      "self",
      "params"
    ]
  },
  "OnPolicyAlgorithm": {
    "create_trainer": [
      "self",
      "params"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ],
    "update": [
      "self",
      "buffer",
      "batch_size",
      "repeat"
    ]
  },
  "OffPolicyAlgorithm": {
    "create_trainer": [
      "self",
      "params"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ],
    "update": [
      "self",
      "buffer",
      "sample_size"
    ]
  },
  "OfflineAlgorithm": {
    "process_buffer": [
      "self",
      "buffer"
    ],
    "run_training": [
      "self",
      "params"
    ],
    "create_trainer": [
      "self",
      "params"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ],
    "update": [
      "self",
      "buffer",
      "sample_size"
    ]
  },
  "OnPolicyWrapperAlgorithm": {
    "__init__": [
      "self",
      "wrapped_algorithm"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_postprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ],
    "_wrapper_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat",
      "original_stats"
    ]
  },
  "OffPolicyWrapperAlgorithm": {
    "__init__": [
      "self",
      "wrapped_algorithm"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_postprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ],
    "_wrapper_update_with_batch": [
      "self",
      "batch",
      "original_stats"
    ]
  },
  "RandomActionPolicy": {
    "__init__": [
      "self",
      "action_space"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "_gae": [
    "v_s",
    "v_s_",
    "rew",
    "end_flag",
    "gamma",
    "gae_lambda"
  ],
  "episode_mc_return_to_go": [
    "rewards",
    "gamma"
  ],
  "_nstep_return": [
    "rew_B",
    "end_flag_B",
    "target_q_IA",
    "stacked_indices_NI",
    "gamma",
    "n_step"
  ],
  "LRSchedulerFactory": {
    "create_scheduler": [
      "self",
      "optim"
    ]
  },
  "LRSchedulerFactoryLinear": {
    "__init__": [
      "self",
      "max_epochs",
      "epoch_num_steps",
      "collection_step_num_env_steps"
    ],
    "create_scheduler": [
      "self",
      "optim"
    ]
  },
  "OptimizerFactory": {
    "__init__": [
      "self"
    ],
    "with_lr_scheduler_factory": [
      "self",
      "lr_scheduler_factory"
    ],
    "create_instances": [
      "self",
      "module"
    ],
    "_create_optimizer_for_params": [
      "self",
      "params"
    ]
  },
  "TorchOptimizerFactory": {
    "__init__": [
      "self",
      "optim_class"
    ],
    "_create_optimizer_for_params": [
      "self",
      "params"
    ]
  },
  "AdamOptimizerFactory": {
    "__init__": [
      "self",
      "lr",
      "betas",
      "eps",
      "weight_decay"
    ],
    "_create_optimizer_for_params": [
      "self",
      "params"
    ]
  },
  "RMSpropOptimizerFactory": {
    "__init__": [
      "self",
      "lr",
      "alpha",
      "eps",
      "weight_decay",
      "momentum",
      "centered"
    ],
    "_create_optimizer_for_params": [
      "self",
      "params"
    ]
  },
  "MARLRandomTrainingStats": {},
  "MARLRandomDiscreteMaskedOffPolicyAlgorithm": {
    "__init__": [
      "self",
      "action_space"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "MapTrainingStats": {
    "__init__": [
      "self",
      "agent_id_to_stats",
      "train_time_aggregator"
    ],
    "get_loss_stats_dict": [
      "self"
    ]
  },
  "MAPRolloutBatchProtocol": {
    "__getitem__": [
      "self",
      "index"
    ]
  },
  "MultiAgentPolicy": {
    "__init__": [
      "self",
      "policies"
    ],
    "_TArrOrActBatch": [],
    "add_exploration_noise": [
      "self",
      "act",
      "batch"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "MARLDispatcher": {
    "__init__": [
      "self",
      "algorithms",
      "env"
    ],
    "create_policy": [
      "self"
    ],
    "dispatch_process_fn": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "dispatch_update_with_batch": [
      "self",
      "batch",
      "algorithm_update_with_batch_fn"
    ]
  },
  "MultiAgentOffPolicyAlgorithm": {
    "__init__": [
      "self"
    ],
    "get_algorithm": [
      "self",
      "agent_id"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "MultiAgentOnPolicyAlgorithm": {
    "__init__": [
      "self"
    ],
    "get_algorithm": [
      "self",
      "agent_id"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ]
  },
  "ICMTrainingStats": {
    "__init__": [
      "self",
      "wrapped_stats"
    ]
  },
  "_ICMMixin": {
    "__init__": [
      "self"
    ],
    "_icm_preprocess_batch": [
      "self",
      "batch"
    ],
    "_icm_postprocess_batch": [
      "batch"
    ],
    "_icm_update": [
      "self",
      "batch",
      "original_stats"
    ]
  },
  "ICMOffPolicyWrapper": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_postprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_wrapper_update_with_batch": [
      "self",
      "batch",
      "original_stats"
    ]
  },
  "ICMOnPolicyWrapper": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_postprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_wrapper_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat",
      "original_stats"
    ]
  },
  "PSRLTrainingStats": {},
  "PSRLModel": {
    "__init__": [
      "self",
      "trans_count_prior",
      "rew_mean_prior",
      "rew_std_prior",
      "gamma",
      "epsilon"
    ],
    "observe": [
      "self",
      "trans_count",
      "rew_sum",
      "rew_square_sum",
      "rew_count"
    ],
    "sample_trans_prob": [
      "self"
    ],
    "sample_reward": [
      "self"
    ],
    "solve_policy": [
      "self"
    ],
    "value_iteration": [
      "trans_prob",
      "rew",
      "gamma",
      "eps",
      "value"
    ],
    "__call__": [
      "self",
      "obs",
      "state",
      "info"
    ]
  },
  "PSRLPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "PSRL": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ]
  },
  "A2CTrainingStats": {},
  "ActorCriticOnPolicyAlgorithm": {
    "__init__": [
      "self"
    ],
    "_add_returns_and_advantages": [
      "self",
      "batch",
      "buffer",
      "indices"
    ]
  },
  "A2C": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ]
  },
  "PPO": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ]
  },
  "IQNPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state",
      "model"
    ]
  },
  "IQN": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "correct_log_prob_gaussian_tanh": [
    "log_prob",
    "tanh_squashed_action",
    "eps"
  ],
  "SACTrainingStats": {},
  "TSACTrainingStats": [],
  "SACPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "Alpha": {
    "from_float_or_instance": [
      "alpha"
    ],
    "value": [
      "self"
    ],
    "update": [
      "self",
      "entropy"
    ]
  },
  "FixedAlpha": {
    "__init__": [
      "self",
      "alpha"
    ],
    "value": [
      "self"
    ],
    "update": [
      "self",
      "entropy"
    ]
  },
  "AutoAlpha": {
    "__init__": [
      "self",
      "target_entropy",
      "log_alpha",
      "optim"
    ],
    "value": [
      "self"
    ],
    "update": [
      "self",
      "entropy"
    ]
  },
  "SAC": {
    "__init__": [
      "self"
    ],
    "_check_field_validity": [
      "self"
    ],
    "_target_q_compute_value": [
      "self",
      "obs_batch",
      "act_batch"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "TDistFnContinuous": [],
  "TDistFnDiscrete": [],
  "TDistFnDiscrOrCont": [],
  "LossSequenceTrainingStats": {},
  "SimpleLossTrainingStats": {},
  "ProbabilisticActorPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "DiscreteActorPolicy": {
    "__init__": [
      "self"
    ]
  },
  "TActorPolicy": [],
  "DiscountedReturnComputation": {
    "__init__": [
      "self",
      "gamma",
      "return_standardization"
    ],
    "add_discounted_returns": [
      "self",
      "batch",
      "buffer",
      "indices"
    ]
  },
  "Reinforce": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ]
  },
  "DDPGTrainingStats": {},
  "ContinuousPolicyWithExplorationNoise": {
    "__init__": [
      "self"
    ],
    "set_exploration_noise": [
      "self",
      "noise"
    ],
    "add_exploration_noise": [
      "self",
      "act",
      "batch"
    ]
  },
  "ContinuousDeterministicPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state",
      "model"
    ]
  },
  "TActBatchProtocol": [],
  "ActorCriticOffPolicyAlgorithm": {
    "__init__": [
      "self"
    ],
    "_minimize_critic_squared_loss": [
      "batch",
      "critic",
      "optimizer"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_target_q_compute_action": [
      "self",
      "obs_batch"
    ],
    "_target_q_compute_value": [
      "self",
      "obs_batch",
      "act_batch"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ]
  },
  "DDPG": {
    "__init__": [
      "self"
    ],
    "_target_q_compute_action": [
      "self",
      "obs_batch"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "QRDQNPolicy": {
    "compute_q_value": [
      "self",
      "logits",
      "mask"
    ]
  },
  "TQRDQNPolicy": [],
  "QRDQN": {
    "__init__": [
      "self"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "TModel": [],
  "DiscreteQLearningPolicy": {
    "__init__": [
      "self"
    ],
    "set_eps_training": [
      "self",
      "eps"
    ],
    "set_eps_inference": [
      "self",
      "eps"
    ],
    "forward": [
      "self",
      "batch",
      "state",
      "model"
    ],
    "compute_q_value": [
      "self",
      "logits",
      "mask"
    ],
    "add_exploration_noise": [
      "self",
      "act",
      "batch"
    ]
  },
  "TDQNPolicy": [],
  "QLearningOffPolicyAlgorithm": {
    "__init__": [
      "self"
    ],
    "_create_policy_optimizer": [
      "self",
      "optim"
    ],
    "use_target_network": [
      "self"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_periodically_update_lagged_network_weights": [
      "self"
    ]
  },
  "DQN": {
    "__init__": [
      "self"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "TRPOTrainingStats": {},
  "TRPO": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ]
  },
  "TD3TrainingStats": {},
  "ActorDualCriticsOffPolicyAlgorithm": {
    "__init__": [
      "self"
    ],
    "_target_q_compute_value": [
      "self",
      "obs_batch",
      "act_batch"
    ]
  },
  "TD3": {
    "__init__": [
      "self"
    ],
    "_target_q_compute_action": [
      "self",
      "obs_batch"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "FQFTrainingStats": {},
  "FQFPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state",
      "model",
      "fractions"
    ]
  },
  "FQF": {
    "__init__": [
      "self"
    ],
    "_create_policy_optimizer": [
      "self",
      "optim"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "BDQNPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state",
      "model"
    ],
    "add_exploration_noise": [
      "self",
      "act",
      "batch"
    ]
  },
  "BDQN": {
    "__init__": [
      "self"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ],
    "_compute_return": [
      "self",
      "batch",
      "buffer",
      "indice",
      "gamma"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "DiscreteSACTrainingStats": {},
  "TDiscreteSACTrainingStats": [],
  "DiscreteSACPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "DiscreteSAC": {
    "__init__": [
      "self"
    ],
    "_target_q_compute_value": [
      "self",
      "obs_batch",
      "act_batch"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "RainbowTrainingStats": {},
  "RainbowDQN": {
    "__init__": [
      "self"
    ],
    "_sample_noise": [
      "model"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "REDQTrainingStats": {},
  "TREDQTrainingStats": [],
  "REDQPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "REDQ": {
    "__init__": [
      "self"
    ],
    "_target_q_compute_value": [
      "self",
      "obs_batch",
      "act_batch"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "C51Policy": {
    "__init__": [
      "self",
      "model",
      "action_space",
      "observation_space",
      "num_atoms",
      "v_min",
      "v_max",
      "eps_training",
      "eps_inference"
    ],
    "compute_q_value": [
      "self",
      "logits",
      "mask"
    ]
  },
  "C51": {
    "__init__": [
      "self"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ],
    "_target_dist": [
      "self",
      "batch"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "NPGTrainingStats": {},
  "NPG": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ],
    "_MVP": [
      "self",
      "v",
      "flat_kl_grad"
    ],
    "_conjugate_gradients": [
      "self",
      "minibatch",
      "flat_kl_grad",
      "nsteps",
      "residual_tol"
    ],
    "_get_flat_grad": [
      "self",
      "y",
      "model"
    ],
    "_set_from_flat_params": [
      "self",
      "model",
      "flat_params"
    ]
  },
  "BCQTrainingStats": {},
  "TBCQTrainingStats": [],
  "BCQPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "BCQ": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "ImitationTrainingStats": {},
  "ImitationPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state"
    ]
  },
  "ImitationLearningAlgorithmMixin": {
    "_imitation_update": [
      "self",
      "batch",
      "policy",
      "optim"
    ]
  },
  "OffPolicyImitationLearning": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "OfflineImitationLearning": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "CQLTrainingStats": {},
  "CQL": {
    "__init__": [
      "self"
    ],
    "_policy_pred": [
      "self",
      "obs"
    ],
    "_calc_policy_loss": [
      "self",
      "obs"
    ],
    "_calc_pi_values": [
      "self",
      "obs_pi",
      "obs_to_pred"
    ],
    "_calc_random_values": [
      "self",
      "obs",
      "act"
    ],
    "process_buffer": [
      "self",
      "buffer"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "TD3BC": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "GailTrainingStats": {},
  "GAIL": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "disc": [
      "self",
      "batch"
    ],
    "_update_with_batch": [
      "self",
      "batch",
      "batch_size",
      "repeat"
    ]
  },
  "float_info": [],
  "INF": [],
  "DiscreteBCQTrainingStats": {},
  "DiscreteBCQPolicy": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "batch",
      "state",
      "model"
    ]
  },
  "DiscreteBCQ": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_target_q": [
      "self",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "DiscreteCQLTrainingStats": {},
  "DiscreteCQL": {
    "__init__": [
      "self"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "DiscreteCRRTrainingStats": {},
  "DiscreteCRR": {
    "__init__": [
      "self"
    ],
    "_preprocess_batch": [
      "self",
      "batch",
      "buffer",
      "indices"
    ],
    "_update_with_batch": [
      "self",
      "batch"
    ]
  },
  "BaseNoise": {
    "reset": [
      "self"
    ],
    "__call__": [
      "self",
      "size"
    ]
  },
  "GaussianNoise": {
    "__init__": [
      "self",
      "mu",
      "sigma"
    ],
    "__call__": [
      "self",
      "size"
    ],
    "reset": [
      "self"
    ]
  },
  "OUNoise": {
    "__init__": [
      "self",
      "mu",
      "sigma",
      "theta",
      "dt",
      "x0"
    ],
    "reset": [
      "self"
    ],
    "__call__": [
      "self",
      "size",
      "mu"
    ]
  }
}