{
  "SIMULTANEOUS_PLAYER_ID": [],
  "TimeStep": {
    "__slots__": [],
    "first": [
      "self"
    ],
    "mid": [
      "self"
    ],
    "last": [
      "self"
    ],
    "is_simultaneous_move": [
      "self"
    ],
    "current_player": [
      "self"
    ]
  },
  "StepType": {
    "FIRST": [],
    "MID": [],
    "LAST": [],
    "first": [
      "self"
    ],
    "mid": [
      "self"
    ],
    "last": [
      "self"
    ]
  },
  "registered_games": [],
  "ChanceEventSampler": {
    "__init__": [
      "self",
      "seed"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "__call__": [
      "self",
      "state"
    ]
  },
  "ObservationType": {
    "OBSERVATION": [],
    "INFORMATION_STATE": []
  },
  "Environment": {
    "__init__": [
      "self",
      "game",
      "discount",
      "chance_event_sampler",
      "observation_type",
      "include_full_state",
      "mfg_distribution",
      "mfg_population",
      "enable_legality_check"
    ],
    "seed": [
      "self",
      "seed"
    ],
    "get_time_step": [
      "self"
    ],
    "_check_legality": [
      "self",
      "actions"
    ],
    "step": [
      "self",
      "actions"
    ],
    "reset": [
      "self"
    ],
    "_sample_external_events": [
      "self"
    ],
    "observation_spec": [
      "self"
    ],
    "action_spec": [
      "self"
    ],
    "use_observation": [
      "self"
    ],
    "name": [
      "self"
    ],
    "num_players": [
      "self"
    ],
    "num_actions_per_step": [
      "self"
    ],
    "is_turn_based": [
      "self"
    ],
    "max_game_length": [
      "self"
    ],
    "is_chance_node": [
      "self"
    ],
    "game": [
      "self"
    ],
    "set_state": [
      "self",
      "new_state"
    ],
    "get_state": [
      "self"
    ],
    "mfg_distribution": [
      "self"
    ],
    "update_mfg_distribution": [
      "self",
      "mfg_distribution"
    ]
  },
  "SyncVectorEnv": {
    "__init__": [
      "self",
      "envs"
    ],
    "__len__": [
      "self"
    ],
    "observation_spec": [
      "self"
    ],
    "num_players": [
      "self"
    ],
    "step": [
      "self",
      "step_outputs",
      "reset_if_done"
    ],
    "reset": [
      "self",
      "envs_to_reset"
    ]
  },
  "JointRLAgentPolicy": {
    "__init__": [
      "self",
      "game",
      "agents",
      "use_observation"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "RLAgentPolicy": {
    "__init__": [
      "self",
      "game",
      "agent",
      "player_id",
      "use_observation"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "ValueSchedule": {
    "__init__": [
      "self"
    ],
    "step": [
      "self"
    ],
    "value": [
      "self"
    ]
  },
  "ConstantSchedule": {
    "__init__": [
      "self",
      "value"
    ],
    "step": [
      "self"
    ],
    "value": [
      "self"
    ]
  },
  "LinearSchedule": {
    "__init__": [
      "self",
      "init_val",
      "final_val",
      "num_steps"
    ],
    "step": [
      "self"
    ],
    "value": [
      "self"
    ]
  },
  "random_playout": [
    "state",
    "seed"
  ],
  "GameParameter": [],
  "Linear": {
    "__init__": [
      "self",
      "in_size",
      "out_size",
      "activate_relu",
      "name"
    ],
    "__call__": [
      "self",
      "tensor"
    ]
  },
  "Sequential": {
    "__init__": [
      "self",
      "layers",
      "name"
    ],
    "__call__": [
      "self",
      "tensor"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "input_size",
      "hidden_sizes",
      "output_size",
      "activate_final",
      "name"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MLPTorso": {
    "__init__": [
      "self",
      "input_size",
      "hidden_sizes",
      "name"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "child": [
    "state",
    "action"
  ],
  "joint_action_probabilities_aux": [
    "state",
    "policy"
  ],
  "joint_action_probabilities": [
    "state",
    "policy"
  ],
  "Policy": {
    "__init__": [
      "self",
      "game",
      "player_ids",
      "random_state"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ],
    "__call__": [
      "self",
      "state",
      "player_id"
    ],
    "sample_action": [
      "self",
      "state",
      "player_id"
    ],
    "to_tabular": [
      "self",
      "states"
    ]
  },
  "TabularPolicy": {
    "__init__": [
      "self",
      "game",
      "players",
      "to_string",
      "states"
    ],
    "_state_key": [
      "self",
      "state",
      "player"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ],
    "state_index": [
      "self",
      "state"
    ],
    "policy_for_key": [
      "self",
      "key"
    ],
    "to_dict": [
      "self"
    ],
    "__copy__": [
      "self",
      "copy_action_probability_array"
    ],
    "copy_with_noise": [
      "self",
      "alpha",
      "beta",
      "random_state"
    ]
  },
  "UniformRandomPolicy": {
    "__init__": [
      "self",
      "game"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "FirstActionPolicy": {
    "__init__": [
      "self",
      "game"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "get_tabular_policy_states": [
    "game"
  ],
  "tabular_policy_from_callable": [
    "game",
    "callable_policy",
    "players"
  ],
  "pyspiel_policy_to_python_policy": [
    "game",
    "pyspiel_tabular_policy",
    "players"
  ],
  "python_policy_to_pyspiel_policy": [
    "python_tabular_policy"
  ],
  "python_policies_to_pyspiel_policies": [
    "policies"
  ],
  "merge_tabular_policies": [
    "tabular_policies",
    "game"
  ],
  "INFO_STATE_OBS_TYPE": [],
  "_Observation": {
    "__init__": [
      "self",
      "game",
      "observer"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ],
    "compress": [
      "self"
    ],
    "decompress": [
      "self",
      "compressed_observation"
    ]
  },
  "make_observation": [
    "game",
    "imperfect_information_observation_type",
    "params"
  ],
  "IIGObserverForPublicInfoGame": {
    "__init__": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "StepOutput": [],
  "AbstractAgent": {
    "__init__": [
      "self",
      "player_id",
      "session",
      "observation_spec",
      "name"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ]
  },
  "_PLAYER_SHAPES": [],
  "_PLAYER_COLORS": [],
  "_FONTSIZE": [],
  "_WIDTH": [],
  "_HEIGHT": [],
  "_ARROWSIZE": [],
  "_MARGIN": [],
  "default_node_decorator": [
    "state"
  ],
  "default_edge_decorator": [
    "parent",
    "unused_child",
    "action"
  ],
  "GameTree": {
    "__init__": [
      "self",
      "game",
      "depth_limit",
      "node_decorator",
      "edge_decorator",
      "group_terminal",
      "group_infosets",
      "group_pubsets",
      "target_pubset",
      "infoset_attrs",
      "pubset_attrs"
    ],
    "state_to_str": [
      "self",
      "state"
    ],
    "_build_tree": [
      "self",
      "state",
      "depth",
      "depth_limit"
    ],
    "_repr_svg_": [
      "self"
    ]
  },
  "SEED": [],
  "get_alg_config": [],
  "LeastCoreLagrangianTest": {
    "setUp": [
      "self"
    ],
    "test_ice_cream_example_full_lagrangian": [
      "self"
    ]
  },
  "ShapleyValuesTest": {
    "setUp": [
      "self"
    ],
    "test_ice_cream_game": [
      "self"
    ],
    "test_ice_cream_game_approximate": [
      "self"
    ],
    "test_deon_larson20_games": [
      "self"
    ]
  },
  "WeightedVotingGame": {
    "__init__": [
      "self",
      "weights",
      "quota"
    ],
    "coalition_value": [
      "self",
      "coalition"
    ]
  },
  "LeastCoreValue": {},
  "compute_least_core_value": [
    "cvc",
    "alg_config"
  ],
  "payoff_evaluation": [
    "cv_calc",
    "payoffs",
    "epsilon",
    "batch_size",
    "max_exponent"
  ],
  "CoreSolver": {
    "__init__": [
      "self",
      "cvc"
    ],
    "logits_to_payoff": [
      "self",
      "logits"
    ],
    "loss_deficit": [
      "self",
      "current_payoff",
      "coalitions",
      "coalition_values",
      "epsilon"
    ]
  },
  "CoreOptimization": {
    "__init__": [
      "self",
      "cvc",
      "opt",
      "epsilon"
    ],
    "loss": [
      "self",
      "params",
      "data"
    ],
    "update_step": [
      "self",
      "params",
      "data",
      "opt_state"
    ],
    "solve": [
      "self",
      "n_iter",
      "batch_size",
      "save_every",
      "evaluate_every",
      "evaluation_iterations",
      "seed"
    ]
  },
  "CoreOptimizationLogits": {
    "__init__": [
      "self",
      "cvc",
      "opt",
      "epsilon"
    ],
    "loss": [
      "self",
      "params",
      "data"
    ],
    "update_step": [
      "self",
      "params",
      "data",
      "opt_state"
    ],
    "solve": [
      "self",
      "n_iter",
      "batch_size",
      "save_every",
      "evaluate_every",
      "evaluation_iterations",
      "seed"
    ]
  },
  "CoreLagrangian": {
    "__init__": [
      "self",
      "cvc",
      "opt_primal",
      "opt_dual"
    ],
    "lagrangian": [
      "self",
      "primal",
      "dual",
      "data"
    ],
    "update_step": [
      "self",
      "params",
      "data",
      "opt_state"
    ],
    "solve": [
      "self",
      "n_iter",
      "batch_size",
      "gamma",
      "mu_init",
      "save_every",
      "evaluate_every",
      "evaluation_iterations",
      "seed"
    ]
  },
  "LeastCoreLPTest": {
    "setUp": [
      "self"
    ],
    "test_ice_cream_example_full_lp": [
      "self"
    ],
    "test_ice_cream_example_uniform_sample_lp": [
      "self"
    ]
  },
  "IceCreamGame": {
    "__init__": [
      "self"
    ],
    "coalition_value": [
      "self",
      "coalition"
    ]
  },
  "TabularGame": {
    "__init__": [
      "self",
      "table"
    ],
    "coalition_value": [
      "self",
      "coalition"
    ]
  },
  "SHAPLEY_VALUES": [],
  "make_game": [
    "name"
  ],
  "WeightedVotingGamesTest": {
    "setUp": [
      "self"
    ],
    "test_basic_wvg_equal_weights": [
      "self"
    ],
    "test_basic_wvg_unequal_weights": [
      "self"
    ]
  },
  "compute_payoff_epsilon": [
    "game",
    "p"
  ],
  "ConstraintsSamplingFuncType": [],
  "add_all_constraints": [
    "game",
    "x",
    "e",
    "constraints"
  ],
  "make_uniform_sampling_constraints_function": [
    "num"
  ],
  "solve_least_core_lp": [
    "game",
    "constraint_function"
  ],
  "CoalitionalGame": {
    "__init__": [
      "self",
      "num_players"
    ],
    "coalition_value": [
      "self",
      "coalition"
    ],
    "coalition_values": [
      "self",
      "coalitions"
    ],
    "num_players": [
      "self"
    ]
  },
  "compute_shapley_values": [
    "game"
  ],
  "compute_approximate_shapley_values": [
    "game",
    "num_samples"
  ],
  "go": [],
  "GamesGoTest": {
    "test_json": [
      "self"
    ]
  },
  "chess": [],
  "FLAGS": [],
  "GamesChessTest": {
    "test_bindings_sim": [
      "self"
    ],
    "test_state_from_fen": [
      "self"
    ],
    "test_chess960_sim_specific_fens": [
      "self",
      "initial_fen"
    ],
    "test_chess_action_conversions": [
      "self"
    ],
    "test_chess960_game_sim": [
      "self"
    ]
  },
  "gin_rummy": [],
  "GamesGinRummyTest": {
    "test_bindings": [
      "self"
    ],
    "test_structs": [
      "self"
    ]
  },
  "MAX_ACTIONS_PER_GAME": [],
  "SPIEL_GAMES_LIST": [],
  "SPIEL_LOADABLE_GAMES_LIST": [],
  "SPIEL_EXCLUDE_SIMS_TEST_GAMES_LIST": [],
  "SPIEL_EXCLUDE_CHECKING_GAME_TYPE_SERIALIZATION_GAMES_LIST": [],
  "SPIEL_SIMULTANEOUS_GAMES_LIST": [],
  "SPIEL_MULTIPLAYER_GAMES_LIST": [],
  "GamesSimTest": {
    "apply_action": [
      "self",
      "state",
      "action"
    ],
    "apply_action_test_clone": [
      "self",
      "state",
      "action"
    ],
    "serialize_deserialize": [
      "self",
      "game",
      "state",
      "check_pyspiel_serialization",
      "check_pickle_serialization"
    ],
    "sim_game": [
      "self",
      "game",
      "check_pyspiel_serialization",
      "check_pickle_serialization",
      "check_game_type_serialization_when_checking_pickle_serialization",
      "make_distribution_fn"
    ],
    "test_game_sim": [
      "self",
      "game_info"
    ],
    "test_simultaneous_game_as_turn_based": [
      "self",
      "game_info"
    ],
    "test_multiplayer_game": [
      "self",
      "game_info",
      "num_players"
    ],
    "test_breakthrough": [
      "self"
    ],
    "test_pig": [
      "self"
    ],
    "test_efg_game": [
      "self"
    ],
    "test_backgammon_checker_moves": [
      "self"
    ],
    "test_backgammon_checker_moves_with_hit_info": [
      "self"
    ],
    "test_leduc_get_and_set_private_cards": [
      "self"
    ],
    "test_dots_and_boxes_with_notation": [
      "self"
    ],
    "test_spades_get_and_set_scores": [
      "self"
    ],
    "test_restricted_nash_response_test": [
      "self",
      "game_name"
    ],
    "test_has_at_least_an_action": [
      "self",
      "game_name"
    ]
  },
  "main": [
    "_"
  ],
  "GamesBridgeTest": {
    "test_contract_names": [
      "self"
    ],
    "test_possible_contracts": [
      "self"
    ],
    "test_scoring": [
      "self"
    ],
    "test_score_single_contract": [
      "self"
    ],
    "test_benchmark_score_single": [
      "self"
    ],
    "test_public_observation": [
      "self"
    ],
    "test_private_observation": [
      "self"
    ],
    "test_benchmark_observation": [
      "self"
    ],
    "test_new_duplicate_bridge_initial_state": [
      "self"
    ]
  },
  "euchre": [],
  "GamesEuchreTest": {
    "test_bindings": [
      "self"
    ]
  },
  "SPIEL_SAMPLED_STOCHASTIC_GAMES_LIST": [],
  "NUM_RUNS": [],
  "SampledStochasticGamesTest": {
    "test_stateful_game_serialization": [
      "self",
      "game_info"
    ]
  },
  "_FULLY_OPTIONAL_PYTHON_GAMES": [],
  "EXPECTED_MANDATORY_GAMES": [],
  "PyspielTest": {
    "test_registered_names_is_sorted": [
      "self"
    ],
    "test_registered_names_contains_expected_games": [
      "self"
    ],
    "test_default_loadable": [
      "self"
    ],
    "test_registered_game_attributes": [
      "self"
    ],
    "test_create_game": [
      "self"
    ],
    "test_play_kuhn_poker": [
      "self"
    ],
    "test_othello": [
      "self"
    ],
    "test_tic_tac_toe": [
      "self"
    ],
    "test_game_string": [
      "self"
    ],
    "test_game_parameters_from_string_empty": [
      "self"
    ],
    "test_game_parameters_from_string_simple": [
      "self"
    ],
    "test_game_parameters_from_string_with_options": [
      "self"
    ],
    "test_game_parameters_from_string_with_subgame": [
      "self"
    ],
    "test_game_parameters_to_string_empty": [
      "self"
    ],
    "test_game_parameters_to_string_simple": [
      "self"
    ],
    "test_game_parameters_to_string_with_options": [
      "self"
    ],
    "test_game_parameters_to_string_with_subgame": [
      "self"
    ],
    "test_game_type": [
      "self"
    ],
    "test_error_handling": [
      "self"
    ],
    "test_can_create_cpp_tabular_policy": [
      "self"
    ],
    "test_simultaneous_game_history": [
      "self"
    ],
    "test_record_batched_trajectories": [
      "self"
    ],
    "test_get_game_parameters": [
      "self"
    ],
    "test_game_serialize_deserialize": [
      "self",
      "game_name",
      "game_string"
    ],
    "test_game_structs": [
      "self"
    ]
  },
  "TensorGamesUtilsTest": {
    "test_extensive_to_tensor_game_type": [
      "self"
    ],
    "test_extensive_to_tensor_game_payoff_tensor": [
      "self"
    ]
  },
  "blackjack": [],
  "NUM_SIM_GAMES": [],
  "GamesBlackjackTest": {
    "test_blackjack_game_sim": [
      "self"
    ],
    "test_card_to_string_conversion": [
      "self"
    ],
    "test_blackjack_three_aces": [
      "self"
    ]
  },
  "NFGGameTest": {
    "test_pd": [
      "self"
    ],
    "test_native_export_import": [
      "self"
    ]
  },
  "GameTransformsTest": {
    "setUp": [
      "self"
    ],
    "test_create_repeated_game": [
      "self"
    ],
    "test_cached_tree_sim": [
      "self"
    ],
    "test_cached_tree_cfr_kuhn": [
      "self"
    ],
    "test_turn_based_simultaneous_game": [
      "self"
    ],
    "test_turn_based_simultaneous_python_game": [
      "self"
    ]
  },
  "GamesRepeatedPokerTest": {
    "test_bindings": [
      "self"
    ],
    "test_state_struct": [
      "self"
    ]
  },
  "_TIC_TAC_TOE_STATES": [],
  "DerivedPolicyTest": {
    "test_derive_from_policy": [
      "self"
    ],
    "test_cpp_policy_from_py": [
      "self"
    ]
  },
  "test_policy_on_game": [
    "self",
    "game",
    "policy_object",
    "player"
  ],
  "_LEDUC_POKER": [],
  "CommonTest": {
    "test_policy_on_leduc": [
      "self",
      "policy_object"
    ],
    "test_cpp_policies_on_leduc": [
      "self",
      "policy_object"
    ],
    "test_cpp_player_policies_on_leduc": [
      "self",
      "policy_object",
      "player"
    ]
  },
  "TabularTicTacToePolicyTest": {
    "setUpClass": [
      "cls"
    ],
    "test_policy_shape": [
      "self"
    ],
    "test_policy_attributes": [
      "self"
    ],
    "test_policy_at_state": [
      "self",
      "state",
      "legal_actions"
    ],
    "test_legal_actions_at_state": [
      "self",
      "state",
      "legal_actions"
    ],
    "test_call_for_state": [
      "self"
    ],
    "test_states_ordered_by_player": [
      "self"
    ],
    "test_state_in": [
      "self"
    ],
    "test_policy_for_state_string": [
      "self",
      "state",
      "legal_actions"
    ]
  },
  "TabularPolicyTest": {
    "test_update_elementwise": [
      "self"
    ],
    "test_update_slice": [
      "self"
    ],
    "test_state_ordering_is_deterministic": [
      "self"
    ],
    "test_partial_tabular_policy_empty_uniform": [
      "self"
    ],
    "test_partial_tabular_policy_set_full": [
      "self"
    ],
    "test_partial_tabular_policy_override_fallback": [
      "self"
    ],
    "test_states": [
      "self"
    ],
    "test_can_turn_policy_into_tabular_policy": [
      "self",
      "policy_class",
      "game_name"
    ]
  },
  "TabularRockPaperScissorsPolicyTest": {
    "setUpClass": [
      "cls"
    ],
    "test_policy_attributes": [
      "self"
    ],
    "test_tabular_policy": [
      "self"
    ],
    "test_states_lookup": [
      "self"
    ],
    "test_legal_actions_mask": [
      "self"
    ]
  },
  "UniformRandomPolicyTest": {
    "test_policy_attributes": [
      "self"
    ],
    "test_policy_at_state": [
      "self"
    ],
    "test_players_have_different_legal_actions": [
      "self"
    ]
  },
  "MergeTabularPoliciesTest": {
    "test_identity": [
      "self"
    ],
    "test_identity_redundant": [
      "self"
    ],
    "test_identity_missing": [
      "self"
    ],
    "assertIdentityPoliciesEqual": [
      "self",
      "tabular_policies",
      "merged_tabular_policy",
      "game"
    ]
  },
  "JointActionProbTest": {
    "test_joint_action_probabilities": [
      "self"
    ],
    "test_joint_action_probabilities_failure_on_seq_game": [
      "self"
    ]
  },
  "ChildTest": {
    "test_child_function_expected_behavior_for_seq_game": [
      "self"
    ],
    "test_child_function_expected_behavior_for_sim_game": [
      "self"
    ],
    "test_child_function_failure_behavior_for_sim_game": [
      "self"
    ]
  },
  "barg": [],
  "GamesBargainingTest": {
    "test_constants": [
      "self"
    ],
    "test_game_specific_constants": [
      "self"
    ],
    "test_game_mechanism": [
      "self"
    ],
    "test_offer_and_instance_map": [
      "self"
    ],
    "test_get_possible_opponent_values": [
      "self"
    ]
  },
  "connect_four": [],
  "GamesConnectFourTest": {
    "test_json": [
      "self"
    ]
  },
  "HeartsGameTest": {
    "test_structs": [
      "self"
    ]
  },
  "ObservationTest": {
    "test_leduc_observation": [
      "self"
    ],
    "test_leduc_info_state": [
      "self"
    ],
    "test_leduc_info_state_as_single_tensor": [
      "self"
    ],
    "test_leduc_all_player_privates": [
      "self"
    ],
    "test_benchmark_state_generation": [
      "self"
    ],
    "test_compression_binary": [
      "self"
    ],
    "test_compression_none": [
      "self"
    ]
  },
  "GamesUniversalPokerTest": {
    "test_load_game": [
      "self"
    ],
    "test_state_struct": [
      "self"
    ],
    "test_random_game": [
      "self"
    ],
    "test_pickle": [
      "self"
    ]
  },
  "RLEnvironmentTest": {
    "test_create_game": [
      "self"
    ],
    "test_create_game_with_args": [
      "self"
    ],
    "test_create_env_from_game_instance": [
      "self"
    ],
    "test_reset": [
      "self"
    ],
    "test_initial_info_state_is_decision_node": [
      "self"
    ],
    "test_full_game": [
      "self"
    ],
    "test_spec_fields": [
      "self"
    ],
    "test_full_game_simultaneous_move": [
      "self"
    ],
    "test_set_and_get_state": [
      "self"
    ]
  },
  "GamesCrazyEightsTest": {
    "test_crazy_eights_game_sim": [
      "self"
    ]
  },
  "SPIEL_BOTS_LIST": [],
  "BotTest": {
    "test_python_and_cpp_bot": [
      "self"
    ],
    "test_registered_bots": [
      "self"
    ],
    "test_cpp_mcts_bot": [
      "self"
    ],
    "test_can_play_game": [
      "self"
    ],
    "test_passing_params": [
      "self"
    ],
    "test_roshambo_bot": [
      "self"
    ]
  },
  "ttt": [],
  "GamesTicTacToeTest": {
    "setUp": [
      "self"
    ],
    "test_constants": [
      "self"
    ],
    "test_player_to_cellstate": [
      "self",
      "player",
      "expected_cellstate"
    ],
    "test_cellstate_to_string": [
      "self",
      "cellstate",
      "expected_string"
    ],
    "test_board_at": [
      "self"
    ],
    "test_board": [
      "self"
    ],
    "test_json": [
      "self"
    ],
    "test_state_to_from_dict": [
      "self",
      "actions",
      "expected_dict"
    ],
    "test_state_from_invalid_dict": [
      "self",
      "invalid_dict"
    ],
    "test_empty_starting_state": [
      "self"
    ],
    "test_starting_state_from_dict": [
      "self"
    ],
    "test_starting_state_from_json": [
      "self"
    ],
    "test_starting_state_unchanged_after_action": [
      "self"
    ],
    "test_starting_state_unchanged_after_clone": [
      "self"
    ],
    "test_state_serialization": [
      "self"
    ],
    "test_starting_state_serialization": [
      "self"
    ]
  },
  "NFGWriterTest": {
    "test_rps": [
      "self"
    ],
    "test_pd": [
      "self"
    ],
    "test_mp3p": [
      "self"
    ]
  },
  "MatrixGamesUtilsTest": {
    "test_num_deterministic_policies": [
      "self"
    ],
    "test_extensive_to_matrix_game": [
      "self"
    ],
    "test_extensive_to_matrix_game_type": [
      "self"
    ],
    "test_extensive_to_matrix_game_payoff_matrix": [
      "self"
    ]
  },
  "_get_next_states": [
    "state",
    "next_states",
    "to_string"
  ],
  "_next_states": [
    "states",
    "to_string"
  ],
  "type_from_states": [
    "states"
  ],
  "FiniteHorizonTest": {
    "test_is_finite_horizon": [
      "self",
      "game_name"
    ],
    "test_has_at_least_an_action": [
      "self",
      "game_name"
    ],
    "test_rl_environment": [
      "self",
      "game_name"
    ]
  },
  "_eval_dynamics_2x2_grid": [
    "dynamics",
    "num_points"
  ],
  "_rk12_step": [
    "func",
    "y0",
    "dt"
  ],
  "Dynamics2x2Axes": {
    "name": [],
    "cla": [
      "self"
    ],
    "quiver": [
      "self",
      "dynamics",
      "num_points",
      "normalize",
      "pivot"
    ],
    "streamplot": [
      "self",
      "dynamics",
      "num_points",
      "linewidth",
      "color"
    ]
  },
  "SimplexTransform": {
    "input_dims": [],
    "output_dims": [],
    "_MATRIX": [],
    "transform_affine": [
      "self",
      "values"
    ]
  },
  "SimplexStreamMask": {
    "__init__": [
      "self",
      "density"
    ],
    "index": [
      "self",
      "point"
    ],
    "point": [
      "self",
      "index"
    ],
    "__getitem__": [
      "self",
      "point"
    ],
    "__setitem__": [
      "self",
      "point",
      "val"
    ]
  },
  "Dynamics3x3Axes": {
    "name": [],
    "_VERTICES": [],
    "__init__": [
      "self",
      "fig",
      "rect"
    ],
    "cla": [
      "self"
    ],
    "_create_bgpatch": [
      "self"
    ],
    "_create_grid": [
      "self",
      "step"
    ],
    "_create_ticks": [
      "self",
      "step",
      "tick_length"
    ],
    "_create_labels": [
      "self",
      "labels",
      "padding"
    ],
    "get_labels": [
      "self"
    ],
    "set_labels": [
      "self",
      "labels",
      "padding"
    ],
    "labels": [],
    "can_zoom": [
      "self"
    ],
    "can_pan": [
      "self"
    ],
    "plot": [
      "self",
      "points"
    ],
    "scatter": [
      "self",
      "points"
    ],
    "quiver": [
      "self",
      "dynamics",
      "step",
      "boundary",
      "normalize",
      "pivot"
    ],
    "_linecollection": [
      "self",
      "points",
      "linewidth",
      "color"
    ],
    "_integrate": [
      "self",
      "x",
      "func",
      "mask",
      "dt",
      "min_dist"
    ],
    "streamplot": [
      "self",
      "dynamics",
      "initial_points",
      "dt",
      "density",
      "min_length",
      "linewidth",
      "color"
    ]
  },
  "ModuleLevelTest": {
    "test__multinomial_coefficients": [
      "self"
    ]
  },
  "PayoffTableTest": {
    "test_construction": [
      "self",
      "num_players",
      "num_strategies"
    ],
    "test_from_heuristic_payoff_table": [
      "self"
    ],
    "test_from_matrix_game": [
      "self",
      "game"
    ],
    "test_expected_payoff": [
      "self",
      "strategy"
    ],
    "test_from_elo_scores": [
      "self"
    ]
  },
  "n_choose_k": [
    "n",
    "k"
  ],
  "grid_simplex": [
    "step",
    "boundary"
  ],
  "sample_from_simplex": [
    "n",
    "dim",
    "vmin"
  ],
  "game_payoffs_array": [
    "game"
  ],
  "distribute": [
    "num_items",
    "num_slots",
    "normalize"
  ],
  "assert_is_1d_numpy_array": [
    "array"
  ],
  "assert_probabilities": [
    "array"
  ],
  "sort_rows_lexicographically": [
    "array"
  ],
  "get_valid_next_profiles": [
    "num_strats_per_population",
    "cur_profile"
  ],
  "get_num_strats_per_population": [
    "payoff_tables",
    "payoffs_are_hpt_format"
  ],
  "get_num_profiles": [
    "num_strats_per_population"
  ],
  "get_strat_profile_labels": [
    "payoff_tables",
    "payoffs_are_hpt_format"
  ],
  "get_strat_profile_from_id": [
    "num_strats_per_population",
    "profile_id"
  ],
  "get_label_from_strat_profile": [
    "num_populations",
    "strat_profile",
    "strat_labels"
  ],
  "get_id_from_strat_profile": [
    "num_strats_per_population",
    "strat_profile"
  ],
  "compute_payoff": [
    "row_profile",
    "col_profile",
    "row_payoff_table"
  ],
  "check_is_constant_sum": [
    "payoff_table",
    "payoffs_are_hpt_format"
  ],
  "cluster_strats": [
    "pi",
    "matching_decimals"
  ],
  "print_rankings_table": [
    "payoff_tables",
    "pi",
    "strat_labels",
    "num_top_strats_to_print"
  ],
  "is_symmetric_matrix_game": [
    "payoff_tables"
  ],
  "check_payoffs_are_hpt": [
    "payoff_tables"
  ],
  "_build_dynamics2x2": [],
  "_build_dynamics3x3": [],
  "_identity_dynamics": [
    "x"
  ],
  "VisualizationTest": {
    "test_meshgrid": [
      "self"
    ],
    "test_quiver2x2": [
      "self"
    ],
    "test_streamplot2x2": [
      "self"
    ],
    "test_quiver3x3": [
      "self"
    ],
    "test_streamplot3x3": [
      "self"
    ]
  },
  "_generate_prob_profiles": [
    "num_items",
    "num_slots"
  ],
  "UtilsTest": {
    "test_distribution": [
      "self",
      "num_items",
      "num_slots",
      "normalize"
    ],
    "test_distribution_equivalent_implementation": [
      "self",
      "num_items",
      "num_slots"
    ],
    "test_sort_rows_lexicographically": [
      "self"
    ],
    "test_id_profile_mapping": [
      "self"
    ],
    "test_get_valid_next_profiles": [
      "self"
    ],
    "test_constant_sum_checker": [
      "self"
    ],
    "test_game_payoffs_array_rps": [
      "self"
    ],
    "test_game_payoffs_array_pd": [
      "self"
    ],
    "test_sample_from_simplex": [
      "self",
      "n",
      "dim",
      "vmin"
    ]
  },
  "_sum_j_x_j_ln_x_j_over_x_i": [
    "x"
  ],
  "_q_learning_dynamics": [
    "composition",
    "payoff",
    "temperature"
  ],
  "_InternalTest": {
    "test__sum_j_x_j_ln_x_j_over_x_i": [
      "self"
    ]
  },
  "DynamicsTest": {
    "test_boltzmann_q": [
      "self"
    ],
    "test_rd_rps_pure_fixed_points": [
      "self"
    ],
    "test_dynamics_rps_mixed_fixed_point": [
      "self",
      "func"
    ],
    "test_multi_population_rps": [
      "self"
    ],
    "test_multi_population_three_populations": [
      "self"
    ],
    "test_multi_population_four_populations": [
      "self"
    ],
    "test_time_average": [
      "self"
    ]
  },
  "_get_payoff": [
    "payoff_table_k",
    "payoffs_are_hpt_format",
    "strat_profile",
    "k"
  ],
  "_get_singlepop_2player_fitness": [
    "payoff_table",
    "payoffs_are_hpt_format",
    "m",
    "my_popsize",
    "my_strat",
    "opponent_strat",
    "use_local_selection_model"
  ],
  "_get_rho_sr": [
    "payoff_table",
    "payoffs_are_hpt_format",
    "m",
    "r",
    "s",
    "alpha",
    "game_is_constant_sum",
    "use_local_selection_model",
    "payoff_sum"
  ],
  "_get_rho_sr_multipop": [
    "payoff_table_k",
    "payoffs_are_hpt_format",
    "k",
    "m",
    "r",
    "s",
    "alpha",
    "use_fast_compute"
  ],
  "_get_singlepop_transition_matrix": [
    "payoff_table",
    "payoffs_are_hpt_format",
    "m",
    "alpha",
    "game_is_constant_sum",
    "use_local_selection_model",
    "payoff_sum",
    "use_inf_alpha",
    "inf_alpha_eps"
  ],
  "_get_multipop_transition_matrix": [
    "payoff_tables",
    "payoffs_are_hpt_format",
    "m",
    "alpha",
    "use_inf_alpha",
    "inf_alpha_eps"
  ],
  "_get_stationary_distr": [
    "c"
  ],
  "print_results": [
    "payoff_tables",
    "payoffs_are_hpt_format",
    "rhos",
    "rho_m",
    "c",
    "pi"
  ],
  "sweep_pi_vs_epsilon": [
    "payoff_tables",
    "strat_labels",
    "warm_start_epsilon",
    "visualize",
    "return_epsilon",
    "min_iters",
    "max_iters",
    "min_epsilon",
    "num_strats_to_label",
    "legend_sort_clusters"
  ],
  "sweep_pi_vs_alpha": [
    "payoff_tables",
    "strat_labels",
    "warm_start_alpha",
    "visualize",
    "return_alpha",
    "m",
    "rtol",
    "atol",
    "num_strats_to_label",
    "legend_sort_clusters"
  ],
  "compute_and_report_alpharank": [
    "payoff_tables",
    "m",
    "alpha",
    "verbose",
    "num_top_strats_to_print"
  ],
  "compute": [
    "payoff_tables",
    "m",
    "alpha",
    "use_local_selection_model",
    "verbose",
    "use_inf_alpha",
    "inf_alpha_eps"
  ],
  "suggest_alpha": [
    "payoff_tables",
    "tol"
  ],
  "AlphaRankTest": {
    "test_stationary_distribution": [
      "self"
    ],
    "test_constant_sum_transition_matrix": [
      "self"
    ]
  },
  "AlpharankVisualizerTest": {
    "test_plot_pi_vs_alpha": [
      "self",
      "mock_plt"
    ]
  },
  "_inc_average": [
    "count",
    "average",
    "value"
  ],
  "from_match_results": [
    "df",
    "consider_agents"
  ],
  "from_matrix_game": [
    "matrix_game"
  ],
  "from_heuristic_payoff_table": [
    "hpt"
  ],
  "_compute_win_probability_from_elo": [
    "rating_1",
    "rating_2"
  ],
  "from_elo_scores": [
    "elo_ratings",
    "num_agents"
  ],
  "_PayoffTableInterface": {
    "__call__": [
      "self"
    ],
    "num_strategies": [
      "self"
    ],
    "num_players": [
      "self"
    ],
    "num_rows": [
      "self"
    ],
    "expected_payoff": [
      "self",
      "strategy"
    ],
    "_payoffs": [
      "self"
    ],
    "_distributions": [
      "self"
    ]
  },
  "NumpyPayoffTable": {
    "__init__": [
      "self",
      "payoff_table",
      "writeable"
    ],
    "__call__": [
      "self"
    ],
    "writeable": [
      "self",
      "writeable"
    ],
    "num_strategies": [
      "self"
    ],
    "num_players": [
      "self"
    ],
    "num_rows": [
      "self"
    ]
  },
  "PayoffTable": {
    "__init__": [
      "self",
      "num_players",
      "num_strategies",
      "initialize_payoff_table"
    ],
    "__call__": [
      "self"
    ],
    "_payoffs": [
      "self"
    ],
    "_distributions": [
      "self"
    ],
    "num_strategies": [
      "self"
    ],
    "num_players": [
      "self"
    ],
    "num_rows": [
      "self"
    ],
    "__setitem__": [
      "self",
      "distribution",
      "payoff"
    ],
    "__getitem__": [
      "self",
      "distribution"
    ],
    "item_is_uninitialized": [
      "self",
      "distribution"
    ],
    "get_distribution_from_profile": [
      "self",
      "strat_profile"
    ]
  },
  "_multinomial_coefficients": [
    "distributions"
  ],
  "_row_probabilities": [
    "coefficients",
    "distributions",
    "strategy"
  ],
  "_expected_payoff": [
    "row_probabilities",
    "payoffs",
    "strategy",
    "num_players"
  ],
  "replicator": [
    "state",
    "fitness"
  ],
  "boltzmannq": [
    "state",
    "fitness",
    "temperature"
  ],
  "qpg": [
    "state",
    "fitness"
  ],
  "SinglePopulationDynamics": {
    "__init__": [
      "self",
      "payoff_matrix",
      "dynamics"
    ],
    "__call__": [
      "self",
      "state",
      "time"
    ]
  },
  "MultiPopulationDynamics": {
    "__init__": [
      "self",
      "payoff_tensor",
      "dynamics"
    ],
    "__call__": [
      "self",
      "state",
      "time"
    ]
  },
  "time_average": [
    "traj"
  ],
  "NetworkPlot": {
    "__init__": [
      "self",
      "payoff_tables",
      "rhos",
      "rho_m",
      "pi",
      "state_labels",
      "num_top_profiles"
    ],
    "_reset_cycle_counter": [
      "self"
    ],
    "_draw_network": [
      "self"
    ],
    "compute_and_draw_network": [
      "self"
    ]
  },
  "_draw_pie": [
    "ax",
    "ratios",
    "colors",
    "x_center",
    "y_center",
    "size",
    "clip_on",
    "zorder"
  ],
  "generate_sorted_masses_strats": [
    "pi_list",
    "curr_alpha_idx",
    "strats_to_go"
  ],
  "plot_pi_vs_alpha": [
    "pi_list",
    "alpha_list",
    "num_populations",
    "num_strats_per_population",
    "strat_labels",
    "num_strats_to_label",
    "plot_semilogx",
    "xlabel",
    "ylabel",
    "legend_sort_clusters"
  ],
  "get_kuhn_poker_data": [
    "num_players"
  ],
  "tensor_to_matrix": [
    "tensor"
  ],
  "with_one_hot_action_features": [
    "state_features",
    "legal_actions",
    "num_distinct_actions"
  ],
  "sequence_features": [
    "state",
    "num_distinct_actions"
  ],
  "num_features": [
    "game"
  ],
  "RootStateWrapper": {
    "__init__": [
      "self",
      "state"
    ],
    "_walk_descendants": [
      "self",
      "state"
    ],
    "sequence_weights_to_policy": [
      "self",
      "sequence_weights",
      "state"
    ],
    "sequence_weights_to_policy_fn": [
      "self",
      "player_sequence_weights"
    ],
    "sequence_weights_to_tabular_profile": [
      "self",
      "player_sequence_weights"
    ],
    "counterfactual_regrets_and_reach_weights": [
      "self",
      "regret_player",
      "reach_weight_player"
    ]
  },
  "normalized_by_sum": [
    "v",
    "axis",
    "mutate"
  ],
  "relu": [
    "v"
  ],
  "_descendant_states": [
    "state",
    "depth_limit",
    "depth",
    "include_terminals",
    "include_chance_states"
  ],
  "all_states": [
    "initial_state",
    "depth_limit",
    "include_terminals",
    "include_chance_states"
  ],
  "sequence_weights_to_tabular_profile": [
    "root",
    "policy_fn"
  ],
  "feedforward_evaluate": [
    "layers",
    "x",
    "use_skip_connections",
    "hidden_are_factored",
    "hidden_activation"
  ],
  "DeepRcfrModel": {
    "__init__": [
      "self",
      "game",
      "num_hidden_units",
      "num_hidden_layers",
      "num_hidden_factors",
      "hidden_activation",
      "use_skip_connections",
      "regularizer"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "_RcfrSolver": {
    "__init__": [
      "self",
      "game",
      "models",
      "truncate_negative"
    ],
    "_sequence_weights": [
      "self",
      "player"
    ],
    "evaluate_and_update_policy": [
      "self",
      "train_fn"
    ],
    "current_policy": [
      "self"
    ],
    "average_policy": [
      "self"
    ],
    "_previous_player": [
      "self",
      "player"
    ],
    "_average_policy_update_player": [
      "self",
      "regret_player"
    ]
  },
  "RcfrSolver": {
    "__init__": [
      "self",
      "game",
      "models",
      "bootstrap",
      "truncate_negative"
    ],
    "evaluate_and_update_policy": [
      "self",
      "train_fn"
    ]
  },
  "ReservoirBuffer": {
    "__init__": [
      "self",
      "size"
    ],
    "buffer": [
      "self"
    ],
    "insert": [
      "self",
      "candidate"
    ],
    "insert_all": [
      "self",
      "candidates"
    ],
    "num_available_spaces": [
      "self"
    ]
  },
  "ReservoirRcfrSolver": {
    "__init__": [
      "self",
      "game",
      "models",
      "buffer_size",
      "truncate_negative"
    ],
    "evaluate_and_update_policy": [
      "self",
      "train_fn"
    ]
  },
  "INVALID_ACTION_PENALTY": [],
  "layer_init": [
    "layer",
    "std",
    "bias_const"
  ],
  "CategoricalMasked": {
    "__init__": [
      "self",
      "probs",
      "logits",
      "validate_args",
      "masks",
      "mask_value"
    ]
  },
  "PPOAgent": {
    "__init__": [
      "self",
      "num_actions",
      "observation_shape",
      "device"
    ],
    "get_value": [
      "self",
      "x"
    ],
    "get_action_and_value": [
      "self",
      "x",
      "legal_actions_mask",
      "action"
    ]
  },
  "PPOAtariAgent": {
    "__init__": [
      "self",
      "num_actions",
      "observation_shape",
      "device"
    ],
    "get_value": [
      "self",
      "x"
    ],
    "get_action_and_value": [
      "self",
      "x",
      "legal_actions_mask",
      "action"
    ]
  },
  "legal_actions_to_mask": [
    "legal_actions_list",
    "num_actions"
  ],
  "PPO": {
    "__init__": [
      "self",
      "input_shape",
      "num_actions",
      "num_players",
      "player_id",
      "num_envs",
      "steps_per_batch",
      "num_minibatches",
      "update_epochs",
      "learning_rate",
      "gae",
      "gamma",
      "gae_lambda",
      "normalize_advantages",
      "clip_coef",
      "clip_vloss",
      "entropy_coef",
      "value_coef",
      "max_grad_norm",
      "target_kl",
      "device",
      "writer",
      "agent_fn"
    ],
    "get_value": [
      "self",
      "x"
    ],
    "get_action_and_value": [
      "self",
      "x",
      "legal_actions_mask",
      "action"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ],
    "post_step": [
      "self",
      "reward",
      "done"
    ],
    "learn": [
      "self",
      "time_step"
    ],
    "anneal_learning_rate": [
      "self",
      "update",
      "num_total_updates"
    ]
  },
  "_GAME": [],
  "_new_model": [],
  "NeurdTest": {
    "setUp": [
      "self"
    ],
    "test_neurd": [
      "self"
    ]
  },
  "thresholded": [
    "logits",
    "regrets",
    "threshold"
  ],
  "train": [
    "model",
    "data",
    "batch_size",
    "step_size",
    "threshold",
    "autoencoder_loss"
  ],
  "DeepNeurdModel": {
    "__init__": [
      "self",
      "game",
      "num_hidden_units",
      "num_hidden_layers",
      "num_hidden_factors",
      "hidden_activation",
      "use_skip_connections",
      "autoencode"
    ],
    "forward": [
      "self",
      "x",
      "training"
    ]
  },
  "CounterfactualNeurdSolver": {
    "__init__": [
      "self",
      "game",
      "models"
    ],
    "_sequence_weights": [
      "self",
      "player"
    ],
    "current_policy": [
      "self"
    ],
    "average_policy": [
      "self"
    ],
    "_previous_player": [
      "self",
      "player"
    ],
    "_average_policy_update_player": [
      "self",
      "regret_player"
    ],
    "evaluate_and_update_policy": [
      "self",
      "train_fn"
    ]
  },
  "PolicyGradientPolicies": {
    "__init__": [
      "self",
      "env",
      "nfsp_policies"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "PolicyGradientTest": {
    "test_run_game": [
      "self",
      "loss_str",
      "game_name"
    ],
    "test_neurd_kuhn": [
      "self"
    ],
    "test_run_hanabi": [
      "self"
    ],
    "test_loss_modes": [
      "self"
    ]
  },
  "Transition": [],
  "PolicyGradient": {
    "__init__": [
      "self",
      "player_id",
      "info_state_size",
      "num_actions",
      "loss_str",
      "loss_class",
      "hidden_layers_sizes",
      "batch_size",
      "critic_learning_rate",
      "pi_learning_rate",
      "entropy_cost",
      "num_critic_before_pi",
      "additional_discount_factor",
      "max_global_gradient_norm",
      "optimizer_str"
    ],
    "_get_loss_class": [
      "self",
      "loss_str"
    ],
    "minimize_with_clipping": [
      "self",
      "model",
      "optimizer",
      "loss"
    ],
    "_act": [
      "self",
      "info_state",
      "legal_actions"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ],
    "_full_checkpoint_name": [
      "self",
      "checkpoint_dir",
      "name"
    ],
    "_latest_checkpoint_filename": [
      "self",
      "name"
    ],
    "save": [
      "self",
      "checkpoint_dir"
    ],
    "has_checkpoint": [
      "self",
      "checkpoint_dir"
    ],
    "restore": [
      "self",
      "checkpoint_dir"
    ],
    "loss": [
      "self"
    ],
    "_add_episode_data_to_dataset": [
      "self"
    ],
    "_add_transition": [
      "self",
      "time_step"
    ],
    "_critic_update": [
      "self"
    ],
    "_pi_update": [
      "self"
    ],
    "get_weights": [
      "self"
    ],
    "copy_with_noise": [
      "self",
      "sigma",
      "copy_weights"
    ]
  },
  "MEM_KEY_NAME": [],
  "ValueBufferElement": [],
  "ReplayBufferElement": [],
  "QueryableFixedSizeRingBuffer": {
    "knn": [
      "self",
      "key",
      "key_name",
      "k",
      "trajectory_len"
    ]
  },
  "EVAAgent": {
    "__init__": [
      "self",
      "game",
      "player_id",
      "state_size",
      "num_actions",
      "embedding_network_layers",
      "embedding_size",
      "dqn_hidden_layers",
      "batch_size",
      "trajectory_len",
      "num_neighbours",
      "learning_rate",
      "mixing_parameter",
      "memory_capacity",
      "discount_factor",
      "update_target_network_every",
      "epsilon_start",
      "epsilon_end",
      "epsilon_decay_duration",
      "embedding_as_parametric_input"
    ],
    "env": [
      "self"
    ],
    "loss": [
      "self"
    ],
    "_add_transition_value": [
      "self",
      "infostate_embedding",
      "value"
    ],
    "_add_transition_replay": [
      "self",
      "infostate_embedding",
      "time_step"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ],
    "_trajectory_centric_planning": [
      "self",
      "trajectories"
    ],
    "_epsilon_greedy": [
      "self",
      "q_values",
      "legal_actions",
      "epsilon"
    ],
    "_get_epsilon": [
      "self",
      "step_counter",
      "is_evaluation"
    ],
    "action_probabilities": [
      "self",
      "state"
    ]
  },
  "EVATest": {
    "test_run_games": [
      "self",
      "game"
    ]
  },
  "QueryableFixedSizeRingBufferTest": {
    "test_replay_buffer_add": [
      "self"
    ],
    "test_replay_buffer_max_capacity": [
      "self"
    ],
    "test_replay_buffer_sample": [
      "self"
    ]
  },
  "SIMPLE_EFG_DATA": [],
  "PPOTest": {
    "test_simple_game": [
      "self"
    ]
  },
  "ILLEGAL_ACTION_LOGITS_PENALTY": [],
  "SonnetLinear": {
    "__init__": [
      "self",
      "in_size",
      "out_size",
      "activate_relu"
    ],
    "forward": [
      "self",
      "tensor"
    ]
  },
  "DQN": {
    "__init__": [
      "self",
      "player_id",
      "state_representation_size",
      "num_actions",
      "hidden_layers_sizes",
      "replay_buffer_capacity",
      "batch_size",
      "replay_buffer_class",
      "learning_rate",
      "update_target_network_every",
      "learn_every",
      "discount_factor",
      "min_buffer_size_to_learn",
      "epsilon_start",
      "epsilon_end",
      "epsilon_decay_duration",
      "optimizer_str",
      "loss_str"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation",
      "add_transition_record"
    ],
    "add_transition": [
      "self",
      "prev_time_step",
      "prev_action",
      "time_step"
    ],
    "_epsilon_greedy": [
      "self",
      "info_state",
      "legal_actions",
      "epsilon"
    ],
    "_get_epsilon": [
      "self",
      "is_evaluation",
      "power"
    ],
    "learn": [
      "self"
    ],
    "q_values": [
      "self"
    ],
    "replay_buffer": [
      "self"
    ],
    "loss": [
      "self"
    ],
    "prev_timestep": [
      "self"
    ],
    "prev_action": [
      "self"
    ],
    "step_counter": [
      "self"
    ],
    "get_weights": [
      "self"
    ],
    "copy_with_noise": [
      "self",
      "sigma",
      "copy_weights"
    ],
    "save": [
      "self",
      "data_path",
      "optimizer_data_path"
    ],
    "load": [
      "self",
      "data_path",
      "optimizer_data_path"
    ]
  },
  "AdvantageMemory": [],
  "StrategyMemory": [],
  "DeepCFRSolver": {
    "__init__": [
      "self",
      "game",
      "policy_network_layers",
      "advantage_network_layers",
      "num_iterations",
      "num_traversals",
      "learning_rate",
      "batch_size_advantage",
      "batch_size_strategy",
      "memory_capacity",
      "policy_network_train_steps",
      "advantage_network_train_steps",
      "reinitialize_advantage_networks"
    ],
    "advantage_buffers": [
      "self"
    ],
    "strategy_buffer": [
      "self"
    ],
    "clear_advantage_buffers": [
      "self"
    ],
    "reinitialize_advantage_network": [
      "self",
      "player"
    ],
    "reinitialize_advantage_networks": [
      "self"
    ],
    "solve": [
      "self"
    ],
    "_traverse_game_tree": [
      "self",
      "state",
      "player"
    ],
    "_sample_action_from_advantage": [
      "self",
      "state",
      "player"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ],
    "_learn_advantage_network": [
      "self",
      "player"
    ],
    "_learn_strategy_network": [
      "self"
    ]
  },
  "_BOOLEANS": [],
  "_BATCH_SIZE": [],
  "RcfrTest": {
    "setUp": [
      "self"
    ],
    "assertListAlmostEqual": [
      "self",
      "list1",
      "list2",
      "delta"
    ],
    "test_with_one_hot_action_features_single_state_vector": [
      "self"
    ],
    "test_sequence_features": [
      "self"
    ],
    "test_num_features": [
      "self"
    ],
    "test_root_state_wrapper_num_sequences": [
      "self"
    ],
    "test_root_state_wrapper_sequence_indices": [
      "self"
    ],
    "test_root_state_wrapper_sequence_features": [
      "self"
    ],
    "test_root_state_wrapper_sequence_terminal_values": [
      "self"
    ],
    "test_normalized_by_sum": [
      "self"
    ],
    "test_counterfactual_regrets_and_reach_weights_value_error": [
      "self"
    ],
    "test_counterfactual_regrets_and_reach_weights": [
      "self"
    ],
    "test_all_states": [
      "self"
    ],
    "test_sequence_weights_to_tabular_profile": [
      "self"
    ],
    "test_cfr": [
      "self"
    ],
    "test_rcfr_functions": [
      "self"
    ],
    "test_rcfr": [
      "self",
      "bootstrap",
      "truncate_negative"
    ],
    "test_reservior_buffer_insert": [
      "self"
    ],
    "test_reservior_buffer_insert_all": [
      "self"
    ],
    "test_rcfr_with_buffer": [
      "self"
    ]
  },
  "DQNTest": {
    "test_simple_game": [
      "self"
    ],
    "test_run_tic_tac_toe": [
      "self"
    ],
    "test_run_hanabi": [
      "self"
    ]
  },
  "DeepCFRPyTorchTest": {
    "test_deep_cfr_runs": [
      "self",
      "game_name"
    ],
    "test_matching_pennies_3p": [
      "self"
    ]
  },
  "RLLossesTest": {
    "test_batch_qpg_loss_with_entropy_cost": [
      "self",
      "entropy_cost"
    ],
    "test_batch_rm_loss_with_entropy_cost": [
      "self",
      "entropy_cost"
    ],
    "test_batch_rpg_loss_with_entropy_cost": [
      "self",
      "entropy_cost"
    ],
    "test_batch_a2c_loss_with_entropy_cost": [
      "self",
      "entropy_cost"
    ]
  },
  "_assert_rank_and_shape_compatibility": [
    "tensors",
    "rank"
  ],
  "compute_baseline": [
    "policy",
    "action_values"
  ],
  "compute_regrets": [
    "policy_logits",
    "action_values"
  ],
  "compute_advantages": [
    "policy_logits",
    "action_values",
    "use_relu",
    "threshold_fn"
  ],
  "compute_a2c_loss": [
    "policy_logits",
    "actions",
    "advantages"
  ],
  "compute_entropy": [
    "policy_logits"
  ],
  "BatchQPGLoss": {
    "__init__": [
      "self",
      "entropy_cost",
      "name"
    ],
    "loss": [
      "self",
      "policy_logits",
      "action_values"
    ]
  },
  "BatchNeuRDLoss": {
    "__init__": [
      "self",
      "entropy_cost",
      "name"
    ],
    "loss": [
      "self",
      "policy_logits",
      "action_values"
    ]
  },
  "BatchRMLoss": {
    "__init__": [
      "self",
      "entropy_cost",
      "name"
    ],
    "loss": [
      "self",
      "policy_logits",
      "action_values"
    ]
  },
  "BatchRPGLoss": {
    "__init__": [
      "self",
      "entropy_cost",
      "name"
    ],
    "loss": [
      "self",
      "policy_logits",
      "action_values"
    ]
  },
  "BatchA2CLoss": {
    "__init__": [
      "self",
      "entropy_cost",
      "name"
    ],
    "loss": [
      "self",
      "policy_logits",
      "baseline",
      "actions",
      "returns"
    ]
  },
  "DataLoggerJsonLines": {
    "__init__": [
      "self",
      "path",
      "name",
      "flush"
    ],
    "__del__": [
      "self"
    ],
    "close": [
      "self"
    ],
    "flush": [
      "self"
    ],
    "write": [
      "self",
      "data"
    ]
  },
  "run_episodes": [
    "envs",
    "agents",
    "num_episodes",
    "is_evaluation"
  ],
  "find_file": [
    "filename",
    "levels"
  ],
  "FileLogger": {
    "__init__": [
      "self",
      "path",
      "name",
      "quiet",
      "also_to_stdout"
    ],
    "print": [
      "self"
    ],
    "opt_print": [
      "self"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "unused_exception_type",
      "unused_exc_value",
      "unused_traceback"
    ],
    "close": [
      "self"
    ],
    "__del__": [
      "self"
    ]
  },
  "create_default_writer": [
    "logdir",
    "just_logging"
  ],
  "FileLoggerTest": {
    "test_file_logger": [
      "self"
    ]
  },
  "LruCacheTest": {
    "test_lru_cache": [
      "self"
    ]
  },
  "SharedValue": {
    "__init__": [
      "self",
      "value"
    ],
    "__deepcopy__": [
      "self",
      "memo"
    ]
  },
  "BasicStats": {
    "__slots__": [],
    "__init__": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self",
      "val"
    ],
    "num": [
      "self"
    ],
    "min": [
      "self"
    ],
    "max": [
      "self"
    ],
    "avg": [
      "self"
    ],
    "std_dev": [
      "self"
    ],
    "merge": [
      "self",
      "other"
    ],
    "as_dict": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "SlidingWindowAccumulator": {
    "__init__": [
      "self",
      "max_window_size"
    ],
    "add": [
      "self",
      "value"
    ],
    "mean": [
      "self"
    ]
  },
  "StatCounter": {
    "__init__": [
      "self"
    ],
    "add": [
      "self",
      "value"
    ],
    "variance": [
      "self"
    ],
    "sample_variance": [
      "self"
    ],
    "stddev": [
      "self"
    ],
    "mean": [
      "self"
    ],
    "max": [
      "self"
    ],
    "min": [
      "self"
    ],
    "n": [
      "self"
    ],
    "ci95": [
      "self"
    ]
  },
  "HistogramNumbered": {
    "__init__": [
      "self",
      "num_buckets"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self",
      "bucket_id"
    ],
    "data": [
      "self"
    ]
  },
  "HistogramNamed": {
    "__init__": [
      "self",
      "bucket_names"
    ],
    "reset": [
      "self"
    ],
    "add": [
      "self",
      "bucket_id"
    ],
    "data": [
      "self"
    ]
  },
  "ReplayBuffer": {
    "__init__": [
      "self",
      "replay_buffer_capacity"
    ],
    "add": [
      "self",
      "element"
    ],
    "sample": [
      "self",
      "num_samples"
    ],
    "reset": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__iter__": [
      "self"
    ]
  },
  "Empty": [],
  "main_handler": [],
  "Process": {
    "__init__": [
      "self",
      "target",
      "args",
      "kwargs"
    ],
    "join": [
      "self"
    ],
    "exitcode": [
      "self"
    ],
    "queue": [
      "self"
    ]
  },
  "_ProcessQueue": {
    "__init__": [
      "self",
      "q_in",
      "q_out"
    ],
    "empty": [
      "self"
    ],
    "full": [
      "self"
    ],
    "get": [
      "self",
      "block",
      "timeout"
    ],
    "get_nowait": [
      "self"
    ],
    "put": [
      "self",
      "obj",
      "block",
      "timeout"
    ],
    "put_nowait": [
      "self",
      "obj"
    ]
  },
  "CacheInfo": {
    "usage": [
      "self"
    ],
    "total": [
      "self"
    ],
    "hit_rate": [
      "self"
    ]
  },
  "LRUCache": {
    "__init__": [
      "self",
      "max_size"
    ],
    "clear": [
      "self"
    ],
    "make": [
      "self",
      "key",
      "fn"
    ],
    "get": [
      "self",
      "key"
    ],
    "set": [
      "self",
      "key",
      "val"
    ],
    "info": [
      "self"
    ],
    "__len__": [
      "self"
    ]
  },
  "ReplayBufferTest": {
    "test_replay_buffer_add": [
      "self"
    ],
    "test_replay_buffer_max_capacity": [
      "self"
    ],
    "test_replay_buffer_sample": [
      "self"
    ],
    "test_replay_buffer_reset": [
      "self"
    ]
  },
  "Exists": [],
  "IsDirectory": [],
  "ListDir": [],
  "MakeDirs": [],
  "Open": [],
  "MetricsTest": {
    "test_create": [
      "self",
      "just_logging"
    ]
  },
  "SpawnTest": {
    "test_spawn_works": [
      "self"
    ]
  },
  "CommandError": {},
  "GTPBot": {
    "__init__": [
      "self",
      "game",
      "exec_path",
      "player_colors",
      "suppress_stderr"
    ],
    "__del__": [
      "self"
    ],
    "close": [
      "self"
    ],
    "gtp_cmd": [
      "self"
    ],
    "inform_action": [
      "self",
      "state",
      "player_id",
      "action"
    ],
    "step": [
      "self",
      "state"
    ],
    "restart": [
      "self"
    ],
    "restart_at": [
      "self",
      "state"
    ],
    "name": [
      "self"
    ],
    "version": [
      "self"
    ],
    "running": [
      "self"
    ],
    "pid": [
      "self"
    ]
  },
  "_shutdown_proc": [
    "p",
    "timeout"
  ],
  "GAME_STR": [],
  "_CONNECT": [],
  "_PLAYER_ACTION": [],
  "_READY_FOR_OTHER": [],
  "_READY_FOR_TEAMS": [],
  "_READY_TO_START": [],
  "_READY_FOR_DEAL": [],
  "_READY_FOR_CARDS": [],
  "_READY_FOR_BID": [],
  "_SEATED": [],
  "_TEAMS": [],
  "_START_BOARD": [],
  "_DEAL": [],
  "_CARDS": [],
  "_OTHER_PLAYER_ACTION": [],
  "_PLAYER_TO_LEAD": [],
  "_DUMMY_CARDS": [],
  "_SEATS": [],
  "_TRUMP_SUIT": [],
  "_NUMBER_TRUMP_SUITS": [],
  "_SUIT": [],
  "_NUMBER_SUITS": [],
  "_RANKS": [],
  "_LSUIT": [],
  "_LRANKS": [],
  "_ACTION_PASS": [],
  "_ACTION_DBL": [],
  "_ACTION_RDBL": [],
  "_ACTION_BID": [],
  "_bid_to_action": [
    "action_str"
  ],
  "_play_to_action": [
    "action_str"
  ],
  "_action_to_string": [
    "action"
  ],
  "_expect_regex": [
    "controller",
    "regex"
  ],
  "_expect": [
    "controller",
    "expected"
  ],
  "_hand_string": [
    "cards"
  ],
  "_connect": [
    "controller",
    "seat"
  ],
  "_new_deal": [
    "controller",
    "seat",
    "hand",
    "board"
  ],
  "BlueChipBridgeBot": {
    "__init__": [
      "self",
      "game",
      "player_id",
      "controller_factory"
    ],
    "player_id": [
      "self"
    ],
    "restart": [
      "self"
    ],
    "_update_for_state": [
      "self"
    ],
    "inform_action": [
      "self",
      "state",
      "player",
      "action"
    ],
    "inform_state": [
      "self",
      "state"
    ],
    "step": [
      "self",
      "state"
    ],
    "terminate": [
      "self"
    ]
  },
  "BluechipBridgeWrapperTest": {
    "test_complete_deal_east": [
      "self"
    ]
  },
  "_SELF_BID_OR_PASS": [],
  "_OTHER_PLAYER_PASS": [],
  "_OTHER_PLAYER_BID": [],
  "_OPPONENTS": [],
  "_PASS_ACTION": [],
  "_string_to_action": [
    "call_str"
  ],
  "_actions": [
    "state_vec"
  ],
  "ISMCTSBotTest": {
    "ismcts_play_game": [
      "self",
      "game"
    ],
    "test_basic_sim_kuhn": [
      "self"
    ],
    "test_basic_sim_leduc": [
      "self"
    ]
  },
  "PolicyBot": {
    "__init__": [
      "self",
      "player_id",
      "rng",
      "policy"
    ],
    "player_id": [
      "self"
    ],
    "restart_at": [
      "self",
      "state"
    ],
    "step_with_policy": [
      "self",
      "state"
    ],
    "step": [
      "self",
      "state"
    ]
  },
  "UniformRandomBot": {
    "__init__": [
      "self",
      "player_id",
      "rng"
    ],
    "restart_at": [
      "self",
      "state"
    ],
    "player_id": [
      "self"
    ],
    "provides_policy": [
      "self"
    ],
    "step_with_policy": [
      "self",
      "state"
    ],
    "step": [
      "self",
      "state"
    ]
  },
  "_MAX_WIDTH": [],
  "_print_columns": [
    "strings"
  ],
  "HumanBot": {
    "step_with_policy": [
      "self",
      "state"
    ],
    "step": [
      "self",
      "state"
    ],
    "restart_at": [
      "self",
      "state"
    ]
  },
  "_BID_1D": [],
  "_BID_1H": [],
  "_BID_2H": [],
  "Scenario": {},
  "CATCH_SCENARIOS": [],
  "SCENARIOS": [],
  "get_default_scenarios": [
    "game_name"
  ],
  "play_bot_in_scenarios": [
    "game",
    "bots",
    "scenarios"
  ],
  "RankedPairsRankOutcome": {
    "__init__": [
      "self",
      "rankings",
      "scores",
      "graph"
    ],
    "graph": [
      "self"
    ]
  },
  "RankedPairsVoting": {
    "__init__": [
      "self"
    ],
    "name": [
      "self"
    ],
    "_would_create_cycle": [
      "self",
      "alternatives",
      "graph",
      "from_idx",
      "to_idx"
    ],
    "_is_source": [
      "self",
      "graph",
      "idx"
    ],
    "_remove_node": [
      "self",
      "graph",
      "idx"
    ],
    "_get_score": [
      "self",
      "graph",
      "margin_matrix",
      "node_idx"
    ],
    "_get_ranked_pairs": [
      "self",
      "alternatives",
      "margin_matrix"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "_SIMPLE_VOTE": [],
  "_SIMPLE_WINNER": [],
  "_WEIGHTED_WINNER": [],
  "PluralityVotingTest": {
    "setUp": [
      "self"
    ],
    "test_plurality_with_votes_in_profile_constructor": [
      "self",
      "votes",
      "winner"
    ],
    "test_plurality_with_alternatives_specified": [
      "self",
      "votes",
      "winner"
    ],
    "test_plurality_with_no_default_votes": [
      "self",
      "votes",
      "winner"
    ],
    "test_plurality_with_weighted_votes": [
      "self",
      "votes",
      "weights",
      "correct_scores",
      "winner"
    ]
  },
  "CopelandVoting": {
    "__init__": [
      "self"
    ],
    "name": [
      "self"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "SchulzeVoting": {
    "__init__": [
      "self"
    ],
    "name": [
      "self"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "KemenyYoungTest": {
    "test_ranked_pairs_wikipedia_example": [
      "self"
    ],
    "test_meeple_pentathlon": [
      "self"
    ]
  },
  "BordaVoting": {
    "__init__": [
      "self"
    ],
    "name": [
      "self"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "DEFAULT_K": [],
  "ApprovalVoting": {
    "__init__": [
      "self",
      "k"
    ],
    "name": [
      "self"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "AlternativeId": [],
  "PreferenceList": [],
  "WeightedVote": {},
  "PreferenceProfile": {
    "__init__": [
      "self",
      "votes",
      "alternatives"
    ],
    "_register_index_based_alternatives": [
      "self",
      "num"
    ],
    "_register_alternative": [
      "self",
      "alternative"
    ],
    "_register_alternatives_from_votes": [
      "self"
    ],
    "add_vote": [
      "self",
      "vote",
      "weight"
    ],
    "add_vote_from_values": [
      "self",
      "values",
      "tie_tolerance",
      "weight"
    ],
    "votes": [
      "self"
    ],
    "alternatives": [
      "self"
    ],
    "alternatives_dict": [
      "self"
    ],
    "num_alternatives": [
      "self"
    ],
    "num_votes": [
      "self"
    ],
    "pref_matrix": [
      "self"
    ],
    "pairwise_count_matrix": [
      "self"
    ],
    "margin_matrix": [
      "self"
    ],
    "condorcet_winner": [
      "self",
      "strong",
      "margin_matrix"
    ],
    "group": [
      "self"
    ],
    "ungroup": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "total_weight": [
      "self"
    ],
    "get_weight": [
      "self",
      "vote"
    ],
    "set_weight": [
      "self",
      "index",
      "value"
    ],
    "set_all_weights": [
      "self",
      "value"
    ],
    "to_list_of_tuples": [
      "self",
      "convert_alternatives_to_strings"
    ]
  },
  "RankOutcome": {
    "__init__": [
      "self",
      "rankings",
      "scores"
    ],
    "unpack_from": [
      "self",
      "ranked_alternatives_and_scores"
    ],
    "ranking": [
      "self"
    ],
    "scores": [
      "self"
    ],
    "ranking_with_scores": [
      "self"
    ],
    "make_rank_dict": [
      "self"
    ],
    "get_rank": [
      "self",
      "alternative"
    ],
    "get_score": [
      "self",
      "alternative"
    ],
    "get_index": [
      "self",
      "alternative"
    ],
    "__str__": [
      "self"
    ],
    "pretty_table_string": [
      "self",
      "top"
    ],
    "pretty_latex_table": [
      "self",
      "header",
      "top"
    ]
  },
  "AbstractVotingMethod": {
    "__init__": [
      "self"
    ],
    "name": [
      "self"
    ],
    "run_election": [
      "self",
      "profile"
    ],
    "is_valid_profile": [
      "self",
      "profile"
    ]
  },
  "BaseTest": {
    "test_basic_preference_profile": [
      "self"
    ],
    "test_basic_preference_profile_weighted": [
      "self"
    ],
    "test_preference_profile_incremental_group": [
      "self"
    ],
    "test_pref_margin_matrices_strong_condorcet": [
      "self"
    ],
    "test_weak_condorcet": [
      "self"
    ]
  },
  "SCOTest": {
    "test_simple_case": [
      "self"
    ],
    "test_meeple_pentathlon_sigmoid": [
      "self"
    ],
    "test_meeple_pentathlon_fenchel_young": [
      "self"
    ],
    "test_cpp_meeple_pentathlon_sigmoid": [
      "self"
    ],
    "test_cpp_meeple_pentathlon_fenchel_young": [
      "self"
    ]
  },
  "BordaVotingTest": {
    "test_borda_setup": [
      "self"
    ],
    "test_borda_basic_run": [
      "self",
      "votes",
      "ranking",
      "scores"
    ]
  },
  "MutableVote": {
    "__init__": [
      "self",
      "idx",
      "weight",
      "vote"
    ]
  },
  "STVVoting": {
    "__init__": [
      "self",
      "num_winners",
      "verbose"
    ],
    "name": [
      "self"
    ],
    "_is_still_active": [
      "self",
      "alternative",
      "winners",
      "losers"
    ],
    "_next_idx_in_the_running": [
      "self",
      "mutable_vote",
      "winners",
      "losers"
    ],
    "_initial_scores_for_round": [
      "self",
      "profile",
      "winners",
      "losers"
    ],
    "_remove_winning_votes": [
      "self",
      "winning_alt",
      "num_to_remove",
      "all_votes"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "EloTest": {
    "test_meeple_pentathlon": [
      "self"
    ],
    "test_meeple_pentathlon_with_integer_alternatives": [
      "self"
    ]
  },
  "Optimizer": {
    "__init__": [
      "self",
      "profile",
      "batch_size",
      "rating_lower_bound",
      "rating_upper_bound",
      "compute_norm_freq",
      "initial_param_noise",
      "verbose"
    ],
    "ratings": [
      "self"
    ],
    "initial_noise": [
      "self"
    ],
    "total_iterations": [
      "self"
    ],
    "avg_l2_grad_norm": [
      "self"
    ],
    "avg_l1_sum_grad_norm": [
      "self"
    ],
    "_gradient": [
      "self",
      "ratings",
      "batch"
    ],
    "step": [
      "self",
      "learning_rate",
      "batch"
    ],
    "ranking": [
      "self"
    ],
    "run_solver": [
      "self",
      "iterations",
      "learning_rate"
    ],
    "approximate_posterior": [
      "self",
      "num_posterior_samples",
      "num_cov_samples"
    ]
  },
  "SoftCondorcetOptimizer": {
    "__init__": [
      "self",
      "profile",
      "batch_size",
      "rating_lower_bound",
      "rating_upper_bound",
      "compute_norm_freq",
      "initial_param_noise",
      "temperature"
    ],
    "_gradient": [
      "self",
      "ratings",
      "batch"
    ]
  },
  "FenchelYoungOptimizer": {
    "__init__": [
      "self",
      "profile",
      "batch_size",
      "rating_lower_bound",
      "rating_upper_bound",
      "compute_norm_freq",
      "initial_param_noise",
      "sigma"
    ],
    "_gradient": [
      "self",
      "ratings",
      "batch"
    ]
  },
  "KemenyYoungVoting": {
    "__init__": [
      "self"
    ],
    "name": [
      "self"
    ],
    "_score": [
      "self",
      "pref_mat",
      "perm"
    ],
    "_permutation_to_ranking": [
      "self",
      "alternatives",
      "permutation"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "compute_ratings_from_preference_profile": [
    "profile",
    "smoothing_factor",
    "max_iterations",
    "convergence_delta"
  ],
  "EloVoting": {
    "__init__": [
      "self",
      "smoothing_factor",
      "max_iterations",
      "convergence_delta"
    ],
    "name": [
      "self"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "balance_profile": [
    "profile",
    "num_votes_per_matchup"
  ],
  "SchulzeTest": {
    "test_shulze_construction": [
      "self"
    ],
    "test_shulze_wikipedia_example": [
      "self"
    ],
    "test_meeple_pentathlon": [
      "self"
    ]
  },
  "CopelandVotingTest": {
    "test_copeland_construction": [
      "self"
    ],
    "test_copeland_basic_run": [
      "self"
    ],
    "test_copeland_basic_run2": [
      "self"
    ]
  },
  "STVTest": {
    "test_stv_records_number": [
      "self",
      "num"
    ],
    "test_ranked_pairs_wikipedia_example": [
      "self"
    ],
    "test_meeple_pentathlon": [
      "self"
    ]
  },
  "parse_preflib_data": [
    "string_data"
  ],
  "parse_preflib_datafile": [
    "filename"
  ],
  "ApprovalVotingTest": {
    "test_approval_name_correct": [
      "self"
    ],
    "test_approval_basic_run": [
      "self"
    ],
    "test_approval_basic_run_with_weights": [
      "self"
    ]
  },
  "RankedPairsTest": {
    "test_ranked_pairs_wikipedia_example1": [
      "self"
    ],
    "test_ranked_pairs_wikipedia_example2": [
      "self"
    ],
    "test_meeple_pentathlon": [
      "self"
    ],
    "test_ranked_pairs_simple_cycle": [
      "self"
    ]
  },
  "PluralityVoting": {
    "__init__": [
      "self"
    ],
    "name": [
      "self"
    ],
    "run_election": [
      "self",
      "profile"
    ]
  },
  "TEST_DATA": [],
  "UtilTest": {
    "test_load_preflib": [
      "self"
    ]
  },
  "MaximalLotteriesTest": {
    "test_stv_records_number": [
      "self",
      "iterate"
    ],
    "test_maximal_lotteries_basic_run": [
      "self"
    ],
    "test_maximal_lotteries_basic_iterative": [
      "self"
    ],
    "test_maximal_lotteries_cycle": [
      "self"
    ],
    "test_maximal_lotteries_iterative_cycle": [
      "self"
    ]
  },
  "MaximalLotteriesVoting": {
    "__init__": [
      "self",
      "iterative",
      "verbose",
      "zero_tolerance"
    ],
    "name": [
      "self"
    ],
    "_create_matrix_game": [
      "self",
      "matrix"
    ],
    "_solve_game": [
      "self",
      "margin_matrix"
    ],
    "run_election": [
      "self",
      "profile"
    ],
    "_iterate": [
      "self",
      "alternatives",
      "margin_matrix",
      "p0_sol"
    ]
  },
  "_DATASET_PATH_PREFIX": [],
  "DATASET_FILE": [],
  "parse_battles_dataset": [
    "filter_ties"
  ],
  "chatbot_arena_vase": [
    "model_names",
    "dataset"
  ],
  "ranked_pairs_viz": [
    "model_names",
    "dataset"
  ],
  "RAINBOW_TABLE5": [],
  "RAINBOW_TABLE6": [],
  "AGENT57_TABLE": [],
  "MUESLI_TABLE11": [],
  "DataSet": {
    "__init__": [
      "self",
      "agent_names",
      "game_names",
      "table_data"
    ],
    "get_column": [
      "self",
      "agent_name"
    ],
    "delete_column": [
      "self",
      "agent_name"
    ],
    "delete_game": [
      "self",
      "game_name"
    ],
    "add_column": [
      "self",
      "column",
      "agent_name"
    ],
    "to_task_by_agent_matrix": [
      "self"
    ]
  },
  "parse_value": [
    "val_str"
  ],
  "parse_values": [
    "string_values_list"
  ],
  "delete_agent": [
    "dataset",
    "agent"
  ],
  "make_subset": [
    "dataset",
    "agent_subset"
  ],
  "parse_atari_table": [
    "filename"
  ],
  "GambitTest": {
    "test_gambit_export_can_be_imported": [
      "self"
    ],
    "_recursive_check": [
      "self",
      "g",
      "h"
    ],
    "_check_infoset_isomorphism": [
      "self",
      "a",
      "b"
    ]
  },
  "DEFAULT_ECOS_SOLVER_KWARGS": [],
  "DEFAULT_OSQP_SOLVER_KWARGS": [],
  "DEFAULT_CVXOPT_SOLVER_KWARGS": [],
  "INIT_POLICIES": [],
  "UPDATE_PLAYERS_STRATEGY": [],
  "BRS": [],
  "BR_SELECTIONS": [],
  "META_SOLVERS": [],
  "LOG_STRING": [],
  "DIST_TOL": [],
  "GAP_TOL": [],
  "RETURN_TOL": [],
  "_eliminate_dominated_payoff": [
    "payoff",
    "epsilon",
    "action_labels",
    "action_repeats",
    "weakly"
  ],
  "_reconstruct_dist": [
    "eliminated_dist",
    "action_labels",
    "num_actions"
  ],
  "_eliminate_dominated_decorator": [
    "func"
  ],
  "_try_two_solvers": [
    "func"
  ],
  "_indices": [
    "p",
    "a",
    "num_players"
  ],
  "_sparse_indices_generator": [
    "player",
    "action",
    "num_actions"
  ],
  "_partition_by_player": [
    "val",
    "p_vec",
    "num_players"
  ],
  "_cce_constraints": [
    "payoff",
    "epsilons",
    "remove_null",
    "zero_tolerance"
  ],
  "_ace_constraints": [
    "payoff",
    "epsilons",
    "remove_null",
    "zero_tolerance"
  ],
  "_get_repeat_factor": [
    "action_repeats"
  ],
  "_linear": [
    "payoff",
    "a_mat",
    "e_vec",
    "action_repeats",
    "solver_kwargs",
    "cost"
  ],
  "_qp_cce": [
    "payoff",
    "a_mats",
    "e_vecs",
    "assume_full_support",
    "action_repeats",
    "solver_kwargs",
    "min_epsilon"
  ],
  "_qp_ce": [
    "payoff",
    "a_mats",
    "e_vecs",
    "assume_full_support",
    "action_repeats",
    "solver_kwargs",
    "min_epsilon"
  ],
  "_expand_meta_game": [
    "meta_game",
    "per_player_repeats"
  ],
  "_unexpand_meta_dist": [
    "meta_dist",
    "per_player_repeats"
  ],
  "_uni": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_undominated_uni": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_rj": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_undominated_rj": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_rd": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_undominated_rd": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_prd": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_alpharank": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_mgce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_min_epsilon_mgce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_approx_mgce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats",
    "epsilon"
  ],
  "_rmwce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_mwce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_rvce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_mgcce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_min_epsilon_mgcce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_approx_mgcce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats",
    "epsilon"
  ],
  "_rmwcce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_mwcce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "_rvcce": [
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "FLAG_TO_FUNC": [],
  "initialize_policy": [
    "game",
    "player",
    "policy_init"
  ],
  "add_new_policies": [
    "per_player_new_policies",
    "per_player_gaps",
    "per_player_repeats",
    "per_player_policies",
    "joint_policies",
    "joint_returns",
    "game",
    "br_selection"
  ],
  "add_meta_game": [
    "meta_games",
    "per_player_policies",
    "joint_returns"
  ],
  "add_meta_dist": [
    "meta_dists",
    "meta_values",
    "meta_solver",
    "meta_game",
    "per_player_repeats",
    "ignore_repeats"
  ],
  "find_best_response": [
    "game",
    "meta_dist",
    "meta_game",
    "iteration",
    "joint_policies",
    "target_equilibrium",
    "update_players_strategy",
    "action_value_tolerance"
  ],
  "initialize": [
    "game",
    "train_meta_solver",
    "eval_meta_solver",
    "policy_init",
    "ignore_repeats",
    "br_selection"
  ],
  "initialize_callback_": [
    "iteration",
    "per_player_repeats",
    "per_player_policies",
    "joint_policies",
    "joint_returns",
    "meta_games",
    "train_meta_dists",
    "eval_meta_dists",
    "train_meta_values",
    "eval_meta_values",
    "train_meta_gaps",
    "eval_meta_gaps",
    "game"
  ],
  "callback_": [
    "iteration",
    "per_player_repeats",
    "per_player_policies",
    "joint_policies",
    "joint_returns",
    "meta_games",
    "train_meta_dists",
    "eval_meta_dists",
    "train_meta_values",
    "eval_meta_values",
    "train_meta_gaps",
    "eval_meta_gaps",
    "kwargs",
    "checkpoint"
  ],
  "run_loop": [
    "game",
    "game_name",
    "seed",
    "iterations",
    "policy_init",
    "update_players_strategy",
    "target_equilibrium",
    "br_selection",
    "train_meta_solver",
    "eval_meta_solver",
    "ignore_repeats",
    "initialize_callback",
    "action_value_tolerance",
    "callback"
  ],
  "_max_entropy_symmetric_nash": [
    "p_mat",
    "eps"
  ],
  "_max_entropy_symmetric_nash_avt": [
    "p_mat",
    "num_agents",
    "num_tasks",
    "eps"
  ],
  "nash_averaging_avt_matrix": [
    "s_mat",
    "eps"
  ],
  "nash_averaging": [
    "game",
    "eps",
    "a_v_a"
  ],
  "_KUHN_GAME": [],
  "_LEDUC_GAME": [],
  "_KUHN_UNIFORM_POLICY": [],
  "_LEDUC_UNIFORM_POLICY": [],
  "ModuleLevelFunctionTest": {
    "test__update_current_policy": [
      "self"
    ]
  },
  "CFRTest": {
    "test_policy_zero_is_uniform": [
      "self",
      "linear_averaging",
      "regret_matching_plus",
      "alternating_updates"
    ],
    "test_cfr_kuhn_poker": [
      "self"
    ],
    "test_cfr_plus_kuhn_poker": [
      "self"
    ],
    "test_cfr_plus_solver_best_response_mdp": [
      "self"
    ],
    "test_cfr_cce_ce_dist_goofspiel": [
      "self"
    ],
    "test_cfr_kuhn_poker_runs_with_multiple_players": [
      "self",
      "linear_averaging",
      "regret_matching_plus",
      "alternating_updates"
    ],
    "test_simultaneous_two_step_avg_1b_seq_in_kuhn_poker": [
      "self",
      "regret_matching_plus"
    ],
    "test_policy": [
      "self"
    ],
    "test_cpp_algorithms_identical_to_python_algorithm": [
      "self",
      "game",
      "cpp_class",
      "python_class"
    ]
  },
  "CorrDistTest": {
    "test_cce_dist_kuhn_3p_cpp": [
      "self"
    ],
    "test_cce_dist_kuhn_3p": [
      "self"
    ],
    "test_cce_dist_sheriff_cpp": [
      "self"
    ]
  },
  "EPS": [],
  "game0": [],
  "commit_strategy0": [],
  "commit_value0": [],
  "game1": [],
  "commit_strategy1": [],
  "commit_value1": [],
  "game2": [],
  "commit_strategy2": [],
  "commit_value2": [],
  "StackelbergLPTest": {
    "test_simple_games": [
      "self",
      "game",
      "commit_strategy",
      "commit_value"
    ]
  },
  "ResponseGraphUCB": {
    "__init__": [
      "self",
      "game",
      "exploration_strategy",
      "confidence_method",
      "delta",
      "ucb_eps",
      "per_payoff_confidence",
      "time_dependent_delta"
    ],
    "delta": [
      "self",
      "k",
      "s"
    ],
    "initialise_mean_and_count": [
      "self"
    ],
    "update_mean_and_count": [
      "self",
      "strat_profile",
      "game_outcome"
    ],
    "_find_focal_coord": [
      "self",
      "s1",
      "s2"
    ],
    "_initialise_queue_uniform": [
      "self"
    ],
    "_add_to_queue_uniform": [
      "self",
      "edges_removed"
    ],
    "_initialise_queue_uniform_exhaustive": [
      "self"
    ],
    "_add_to_queue_uniform_exhaustive": [
      "self",
      "edges_removed"
    ],
    "_initialise_queue_valence_weighted": [
      "self"
    ],
    "_add_to_queue_valence_weighted": [
      "self",
      "edges_removed"
    ],
    "_initialise_queue_count_weighted": [
      "self"
    ],
    "_add_to_queue_count_weighted": [
      "self",
      "edges_removed"
    ],
    "initialise_queue": [
      "self"
    ],
    "add_to_queue": [
      "self",
      "removed"
    ],
    "evaluate_strategy_profile": [
      "self",
      "yield_outcomes"
    ],
    "_ucb_standard_factor": [
      "self",
      "s",
      "k"
    ],
    "_bernoulli_upper": [
      "self",
      "p",
      "n",
      "delta"
    ],
    "_bernoulli_lower": [
      "self",
      "p",
      "n",
      "delta"
    ],
    "_ucb": [
      "self",
      "s",
      "k"
    ],
    "_lcb": [
      "self",
      "s",
      "k"
    ],
    "ucb_check": [
      "self",
      "e"
    ],
    "check_confidence": [
      "self"
    ],
    "real_edge_direction": [
      "self",
      "e"
    ],
    "construct_real_graph": [
      "self"
    ],
    "compute_graph": [
      "self"
    ],
    "forced_exploration": [
      "self"
    ],
    "run": [
      "self",
      "verbose",
      "max_total_iterations"
    ],
    "compute_total_steps": [
      "self"
    ],
    "_construct_digraph": [
      "self",
      "edges"
    ],
    "_plot_errorbars_2x2x2": [
      "self",
      "x",
      "y",
      "xerr",
      "yerr",
      "fmt"
    ],
    "visualise_2x2x2": [
      "self",
      "real_values",
      "graph"
    ],
    "plot_graph": [
      "self",
      "graph",
      "subplot",
      "axes"
    ],
    "visualise_count_history": [
      "self",
      "figsize"
    ]
  },
  "_partial_multi_dot": [
    "player_payoff_tensor",
    "strategies",
    "index_avoided"
  ],
  "_project_distribution": [
    "updated_strategy",
    "gamma"
  ],
  "_approx_simplex_projection": [
    "updated_strategy",
    "gamma"
  ],
  "_simplex_projection": [
    "updated_strategy",
    "gamma"
  ],
  "_projected_replicator_dynamics_step": [
    "payoff_tensors",
    "strategies",
    "dt",
    "gamma",
    "use_approx"
  ],
  "projected_replicator_dynamics": [
    "payoff_tensors",
    "prd_initial_strategies",
    "prd_iterations",
    "prd_dt",
    "prd_gamma",
    "average_over_last_n_strategies",
    "use_approx"
  ],
  "get_method_tuple_acronym": [
    "method_tuple"
  ],
  "get_method_tuple_linespecs": [
    "method"
  ],
  "get_method_acronym": [
    "method"
  ],
  "digraph_edge_hamming_dist": [
    "g1",
    "g2"
  ],
  "BernoulliGameSampler": {
    "__init__": [
      "self",
      "strategy_spaces",
      "means",
      "payoff_bounds"
    ],
    "rescale_payoff": [
      "self",
      "payoff"
    ],
    "observe_result": [
      "self",
      "strat_profile"
    ]
  },
  "ZeroSumBernoulliGameSampler": {
    "__init__": [
      "self",
      "strategy_spaces",
      "means",
      "payoff_bounds"
    ],
    "observe_result": [
      "self",
      "strat_profile"
    ]
  },
  "get_payoffs_bernoulli_game": [
    "size"
  ],
  "get_soccer_data": [],
  "get_game_for_sampler": [
    "game_name"
  ],
  "plot_timeseries": [
    "ax",
    "id_ax",
    "data",
    "xticks",
    "xlabel",
    "ylabel",
    "label",
    "logx",
    "logy",
    "zorder",
    "linespecs"
  ],
  "GAMES": [],
  "SWEEP_KWARGS": [],
  "TEST_COUNT_LIMIT": [],
  "interval": [],
  "get_game": [
    "game_name"
  ],
  "JPSROTest": {
    "test_jpsro_cce": [
      "self"
    ]
  },
  "lens": [
    "lists"
  ],
  "solve_subgame": [
    "subgame_payoffs"
  ],
  "DoubleOracleSolver": {
    "__init__": [
      "self",
      "game",
      "enforce_symmetry"
    ],
    "subgame_payoffs": [
      "self"
    ],
    "oracle": [
      "self",
      "subgame_solution"
    ],
    "step": [
      "self"
    ],
    "solve_yield": [
      "self",
      "initial_strategies",
      "max_steps",
      "tolerance",
      "verbose",
      "yield_subgame"
    ],
    "solve": [
      "self",
      "initial_strategies",
      "max_steps",
      "tolerance",
      "verbose"
    ]
  },
  "PolicyFunction": {
    "__init__": [
      "self",
      "pids",
      "policies",
      "game"
    ],
    "_state_key": [
      "self",
      "state",
      "player_id"
    ],
    "policy": [
      "self"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "PolicyPool": {
    "__init__": [
      "self",
      "policies"
    ],
    "__call__": [
      "self",
      "state",
      "player"
    ]
  },
  "PolicyAggregator": {
    "__init__": [
      "self",
      "game",
      "epsilon"
    ],
    "_state_key": [
      "self",
      "state",
      "player_id"
    ],
    "aggregate": [
      "self",
      "pids",
      "policies",
      "weights"
    ],
    "_sub_aggregate": [
      "self",
      "pid",
      "policies",
      "weights"
    ],
    "_rec_aggregate": [
      "self",
      "pid",
      "state",
      "my_reaches"
    ]
  },
  "BestResponseTest": {
    "test_best_response_is_a_policy": [
      "self"
    ],
    "test_cpp_and_python_implementations_are_identical": [
      "self",
      "game_name"
    ],
    "test_cpp_and_python_best_response_are_identical": [
      "self",
      "game_name",
      "num_players"
    ],
    "test_cpp_and_python_value_are_identical": [
      "self",
      "game_name",
      "num_players"
    ],
    "test_best_response_tic_tac_toe_value_is_consistent": [
      "self"
    ],
    "test_best_response_oshi_zumo_simultaneous_game": [
      "self"
    ],
    "test_best_response_prisoner_dilemma_simultaneous_game": [
      "self"
    ]
  },
  "TabularBestResponseMDPTest": {
    "test_tabular_best_response_mdp": [
      "self"
    ]
  },
  "MCTSAgentTest": {
    "test_tic_tac_toe_episode": [
      "self"
    ]
  },
  "WoLFTest": {
    "test_simple_pathfinding_run": [
      "self"
    ],
    "test_rps_run": [
      "self"
    ]
  },
  "Evaluator": {
    "evaluate": [
      "self",
      "state"
    ],
    "prior": [
      "self",
      "state"
    ]
  },
  "RandomRolloutEvaluator": {
    "__init__": [
      "self",
      "n_rollouts",
      "random_state",
      "max_length"
    ],
    "evaluate": [
      "self",
      "state"
    ],
    "prior": [
      "self",
      "state"
    ]
  },
  "SearchNode": {
    "__slots__": [],
    "__init__": [
      "self",
      "action",
      "player",
      "prior"
    ],
    "uct_value": [
      "self",
      "parent_explore_count",
      "uct_c"
    ],
    "puct_value": [
      "self",
      "parent_explore_count",
      "uct_c"
    ],
    "sort_key": [
      "self"
    ],
    "best_child": [
      "self"
    ],
    "children_str": [
      "self",
      "state"
    ],
    "to_str": [
      "self",
      "state"
    ],
    "__str__": [
      "self"
    ]
  },
  "MCTSBot": {
    "__init__": [
      "self",
      "game",
      "uct_c",
      "max_simulations",
      "evaluator",
      "solve",
      "random_state",
      "child_selection_fn",
      "dirichlet_noise",
      "verbose",
      "dont_return_chance_node"
    ],
    "restart_at": [
      "self",
      "state"
    ],
    "step_with_policy": [
      "self",
      "state"
    ],
    "step": [
      "self",
      "state"
    ],
    "_apply_tree_policy": [
      "self",
      "root",
      "state"
    ],
    "mcts_search": [
      "self",
      "state"
    ]
  },
  "PolicyAggregatorTest": {
    "test_policy_aggregation_random": [
      "self",
      "game_name"
    ],
    "test_policy_aggregation_tabular_randinit": [
      "self",
      "game_name"
    ],
    "test_policy_aggregation_variadic": [
      "self",
      "game_name"
    ]
  },
  "INITIAL_REGRET_DENOM": [],
  "_regret_matching_step": [
    "payoff_tensors",
    "strategies",
    "regrets",
    "gamma"
  ],
  "regret_matching": [
    "payoff_tensors",
    "initial_strategies",
    "iterations",
    "gamma",
    "average_over_last_n_strategies"
  ],
  "_InfoStateNode": {
    "legal_actions": [],
    "index_in_tabular_policy": [],
    "relizable_deviations": [],
    "current_history_probs": [],
    "history": [],
    "cumulative_regret": [],
    "cumulative_policy": [],
    "y_values": []
  },
  "_EFRSolverBase": {
    "__init__": [
      "self",
      "game",
      "deviation_gen"
    ],
    "return_cumulative_regret": [
      "self"
    ],
    "current_policy": [
      "self"
    ],
    "average_policy": [
      "self"
    ],
    "_initialize_info_state_nodes": [
      "self",
      "state",
      "history",
      "path_indices"
    ],
    "_update_current_policy": [
      "self",
      "state",
      "current_policy"
    ],
    "_compute_cumulative_immediate_regret_for_player": [
      "self",
      "state",
      "policies",
      "reach_probabilities",
      "player"
    ],
    "_get_infostate_policy": [
      "self",
      "info_state_str"
    ]
  },
  "_EFRSolver": {
    "evaluate_and_update_policy": [
      "self"
    ]
  },
  "EFRSolver": {
    "__init__": [
      "self",
      "game",
      "deviations_name"
    ],
    "_regret_matching": [
      "self",
      "info_set_node"
    ]
  },
  "_update_average_policy": [
    "average_policy",
    "info_state_nodes"
  ],
  "strat_dict_to_array": [
    "strategy_dictionary"
  ],
  "array_to_strat_dict": [
    "strategy_array",
    "legal_actions"
  ],
  "create_probs_from_index": [
    "indices",
    "current_policy"
  ],
  "return_blind_action": [
    "num_actions",
    "history",
    "_"
  ],
  "return_informed_action": [
    "num_actions",
    "history",
    "_"
  ],
  "return_blind_cf": [
    "num_actions",
    "history",
    "_"
  ],
  "return_informed_cf": [
    "num_actions",
    "history",
    "_"
  ],
  "return_blind_partial_sequence": [
    "num_actions",
    "history",
    "_"
  ],
  "return_cf_partial_sequence": [
    "num_actions",
    "history",
    "_"
  ],
  "return_cs_partial_sequence": [
    "num_actions",
    "history",
    "prior_legal_actions"
  ],
  "return_cs_partial_sequence_orginal": [
    "num_actions",
    "history",
    "prior_legal_actions"
  ],
  "return_twice_informed_partial_sequence": [
    "num_actions",
    "history",
    "prior_legal_actions"
  ],
  "generate_all_action_permutations": [
    "current_stem",
    "remaining_actions"
  ],
  "return_behavourial": [
    "num_actions",
    "history",
    "prior_legal_actions"
  ],
  "LocalDeviationWithTimeSelection": {
    "local_swap_transform": [],
    "prior_actions_weight": [],
    "prior_memory_actions": [],
    "use_unmodified_history": [],
    "__init__": [
      "self",
      "target",
      "source",
      "num_actions",
      "prior_actions_weight",
      "prior_memory_actions",
      "is_external",
      "use_unmodified_history"
    ],
    "deviate": [
      "self",
      "strategy"
    ],
    "return_transform_matrix": [
      "self"
    ],
    "player_deviation_reach_probability": [
      "self",
      "prior_possible_action_probabilities"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ]
  },
  "return_all_non_identity_internal_deviations": [
    "num_actions",
    "possible_prior_weights",
    "prior_memory_actions"
  ],
  "return_all_internal_modified_deviations": [
    "num_actions",
    "possible_prior_weights",
    "possible_prior_memory_actions",
    "prior_memory_actions"
  ],
  "return_all_external_deviations": [
    "num_actions",
    "possible_prior_weights",
    "prior_memory_actions"
  ],
  "return_all_external_modified_deviations": [
    "num_actions",
    "possible_prior_weights",
    "possible_prior_memory_actions",
    "prior_memory_actions"
  ],
  "return_identity_deviation": [
    "num_actions",
    "possible_prior_weights",
    "prior_memory_actions"
  ],
  "LocalSwapTransform": {
    "source_action": [],
    "target_action": [],
    "matrix_transform": [],
    "actions_num": [],
    "is_external": [],
    "__init__": [
      "self",
      "target",
      "source",
      "actions_num",
      "is_external"
    ],
    "__repr__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ],
    "__hash__": [
      "self"
    ],
    "deviate": [
      "self",
      "strategy"
    ]
  },
  "UCT_C": [],
  "_get_action": [
    "state",
    "action_str"
  ],
  "search_tic_tac_toe_state": [
    "initial_actions"
  ],
  "make_node": [
    "action",
    "player",
    "prior"
  ],
  "MctsBotTest": {
    "test_can_play_tic_tac_toe": [
      "self"
    ],
    "test_can_play_both_sides": [
      "self"
    ],
    "test_can_play_single_player": [
      "self"
    ],
    "test_throws_on_simultaneous_game": [
      "self"
    ],
    "test_can_play_three_player_stochastic_games": [
      "self"
    ],
    "assertBestChild": [
      "self",
      "choice",
      "children"
    ],
    "test_choose_most_visited_when_not_solved": [
      "self"
    ],
    "test_choose_win_over_most_visited": [
      "self"
    ],
    "test_choose_best_over_good": [
      "self"
    ],
    "test_choose_bad_over_worst": [
      "self"
    ],
    "test_choose_positive_reward_over_promising": [
      "self"
    ],
    "test_choose_most_visited_over_loss": [
      "self"
    ],
    "test_choose_most_visited_over_draw": [
      "self"
    ],
    "test_choose_uncertainty_over_most_visited_loss": [
      "self"
    ],
    "test_choose_slowest_loss": [
      "self"
    ]
  },
  "quote": [
    "x"
  ],
  "export_gambit": [
    "game"
  ],
  "evaluate_bots": [
    "state",
    "bots",
    "rng"
  ],
  "MinimaxSolver": {
    "__init__": [
      "self",
      "game_string"
    ],
    "solve": [
      "self"
    ],
    "_create_table": [
      "self",
      "state"
    ],
    "action_values_from_string": [
      "self",
      "state_string_key"
    ],
    "action_values_from_state": [
      "self",
      "state"
    ]
  },
  "BoltzmannQlearnerTest": {
    "test_simple_game": [
      "self"
    ]
  },
  "ActionValuesVsBestResponseTest": {
    "test_kuhn_poker_uniform": [
      "self"
    ],
    "test_kuhn_poker_always_pass_p0": [
      "self"
    ]
  },
  "_uniform_policy": [
    "state"
  ],
  "_callable_tabular_policy": [
    "tabular_policy"
  ],
  "JointPolicy": {
    "__init__": [
      "self",
      "game",
      "policies"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "_full_best_response_policy": [
    "br_infoset_dict"
  ],
  "_policy_dict_at_state": [
    "callable_policy",
    "state"
  ],
  "XFPSolver": {
    "__init__": [
      "self",
      "game",
      "save_oracles"
    ],
    "average_policy_tables": [
      "self"
    ],
    "average_policy": [
      "self"
    ],
    "iteration": [
      "self"
    ],
    "compute_best_responses": [
      "self"
    ],
    "update_average_policies": [
      "self"
    ],
    "_recursively_update_average_policies": [
      "self",
      "state",
      "avg_reach_probs",
      "br_reach_probs"
    ],
    "sample_episode": [
      "self",
      "state",
      "policies"
    ],
    "sample_episodes": [
      "self",
      "policies",
      "num"
    ],
    "get_empirical_metagame": [
      "self",
      "sims_per_entry",
      "seed"
    ]
  },
  "ValueIterationTest": {
    "test_solve_tic_tac_toe": [
      "self"
    ],
    "test_solve_small_goofspiel": [
      "self"
    ],
    "test_solve_small_oshi_zumo": [
      "self"
    ],
    "test_solve_small_pig": [
      "self"
    ]
  },
  "ResponseGraphUcbTest": {
    "get_example_2x2_payoffs": [
      "self"
    ],
    "test_sampler": [
      "self"
    ],
    "test_soccer_data_import": [
      "self"
    ]
  },
  "REGRET_INDEX": [],
  "AVG_POLICY_INDEX": [],
  "AveragePolicy": {
    "__init__": [
      "self",
      "game",
      "player_ids",
      "infostates"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "MCCFRSolverBase": {
    "__init__": [
      "self",
      "game"
    ],
    "_lookup_infostate_info": [
      "self",
      "info_state_key",
      "num_legal_actions"
    ],
    "_add_regret": [
      "self",
      "info_state_key",
      "action_idx",
      "amount"
    ],
    "_add_avstrat": [
      "self",
      "info_state_key",
      "action_idx",
      "amount"
    ],
    "average_policy": [
      "self"
    ],
    "_regret_matching": [
      "self",
      "regrets",
      "num_legal_actions"
    ]
  },
  "IsmctsTest": {
    "test_action_candidates_selection": [
      "self"
    ],
    "play_game": [
      "self",
      "game",
      "ismcts_bot"
    ],
    "test_play_kuhn_poker": [
      "self"
    ],
    "test_invalid_action_at_chance_node": [
      "self"
    ],
    "test_play_universal_poker": [
      "self"
    ]
  },
  "OutcomeSamplingSolver": {
    "__init__": [
      "self",
      "game"
    ],
    "iteration": [
      "self"
    ],
    "_baseline": [
      "self",
      "state",
      "info_state",
      "aidx"
    ],
    "_baseline_corrected_child_value": [
      "self",
      "state",
      "info_state",
      "sampled_aidx",
      "aidx",
      "child_value",
      "sample_prob"
    ],
    "_episode": [
      "self",
      "state",
      "update_player",
      "my_reach",
      "opp_reach",
      "sample_reach"
    ]
  },
  "neg_entropy": [
    "probs"
  ],
  "softmax": [
    "x"
  ],
  "divergence": [
    "x",
    "y",
    "psi_x",
    "psi_y",
    "grad_psi_y"
  ],
  "dilated_dgf_divergence": [
    "mmd_1",
    "mmd_2"
  ],
  "MMDDilatedEnt": {
    "__init__": [
      "self",
      "game",
      "alpha",
      "stepsize"
    ],
    "get_parent_seq": [
      "self",
      "player",
      "infostate"
    ],
    "get_infostate_seq": [
      "self",
      "player",
      "infostate"
    ],
    "dgf_eval": [
      "self"
    ],
    "dgf_grads": [
      "self"
    ],
    "update_sequences": [
      "self"
    ],
    "_update_state_sequences": [
      "self",
      "infostate",
      "g",
      "player",
      "pol"
    ],
    "get_gap": [
      "self"
    ],
    "update_avg_sequences": [
      "self"
    ],
    "current_sequences": [
      "self"
    ],
    "get_avg_sequences": [
      "self"
    ],
    "get_policies": [
      "self"
    ],
    "get_avg_policies": [
      "self"
    ]
  },
  "PolicyValueTest": {
    "test_expected_game_score_uniform_random_kuhn_poker": [
      "self"
    ],
    "test_expected_game_score_uniform_random_iterated_prisoner_dilemma": [
      "self"
    ]
  },
  "_CFRSolverBase": [],
  "_update_current_policy": [],
  "_apply_regret_matching_plus_reset": [],
  "CFRBRSolver": {
    "__init__": [
      "self",
      "game",
      "linear_averaging",
      "regret_matching_plus"
    ],
    "_compute_best_responses": [
      "self"
    ],
    "evaluate_and_update_policy": [
      "self"
    ]
  },
  "_memoize_method": [
    "key_fn"
  ],
  "compute_states_and_info_states_if_none": [
    "game",
    "all_states",
    "state_to_information_state"
  ],
  "BestResponsePolicy": {
    "__init__": [
      "self",
      "game",
      "player_id",
      "policy",
      "root_state",
      "cut_threshold"
    ],
    "info_sets": [
      "self",
      "state"
    ],
    "decision_nodes": [
      "self",
      "parent_state"
    ],
    "joint_action_probabilities_counterfactual": [
      "self",
      "state"
    ],
    "transitions": [
      "self",
      "state"
    ],
    "value": [
      "self",
      "state"
    ],
    "q_value": [
      "self",
      "state",
      "action"
    ],
    "best_response_action": [
      "self",
      "infostate"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "CPPBestResponsePolicy": {
    "__init__": [
      "self",
      "game",
      "best_responder_id",
      "policy",
      "all_states",
      "state_to_information_state",
      "best_response_processor",
      "cut_threshold"
    ],
    "decision_nodes": [
      "self",
      "parent_state"
    ],
    "transitions": [
      "self",
      "state"
    ],
    "value": [
      "self",
      "state"
    ],
    "q_value": [
      "self",
      "state",
      "action"
    ],
    "best_response_action": [
      "self",
      "infostate"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ],
    "policy": [
      "self"
    ],
    "copy_with_noise": [
      "self",
      "alpha",
      "beta"
    ]
  },
  "ProjectedReplicatorDynamicsTest": {
    "test_two_players": [
      "self"
    ],
    "test_three_players": [
      "self"
    ]
  },
  "MCTSAgent": {
    "__init__": [
      "self",
      "player_id",
      "num_actions",
      "mcts_bot",
      "name"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ]
  },
  "PlaythroughTest": {
    "test_runs": [
      "self"
    ],
    "test_format_tensor_1d": [
      "self"
    ],
    "test_format_tensor_2d": [
      "self"
    ],
    "test_format_tensor_3d": [
      "self"
    ],
    "test_format_tensor_3d_linewrap": [
      "self"
    ]
  },
  "_transitions": [
    "state",
    "policies"
  ],
  "_tuples_from_policy": [
    "policy_vector"
  ],
  "_CalculatorReturn": [],
  "Calculator": {
    "__init__": [
      "self",
      "game"
    ],
    "__call__": [
      "self",
      "player",
      "player_policy",
      "info_states"
    ]
  },
  "_regret_matching": [
    "cumulative_regrets",
    "legal_actions"
  ],
  "_CFRSolver": {
    "evaluate_and_update_policy": [
      "self"
    ]
  },
  "CFRPlusSolver": {
    "__init__": [
      "self",
      "game"
    ]
  },
  "CFRSolver": {
    "__init__": [
      "self",
      "game"
    ]
  },
  "SFLPTest": {
    "test_rock_paper_scissors": [
      "self"
    ],
    "test_kuhn_poker": [
      "self"
    ],
    "test_kuhn_poker_efg": [
      "self"
    ],
    "test_leduc_poker": [
      "self"
    ],
    "test_iigoofspiel4": [
      "self"
    ],
    "test_exploitablity": [
      "self"
    ]
  },
  "BoltzmannQLearner": {
    "__init__": [
      "self",
      "player_id",
      "num_actions",
      "step_size",
      "discount_factor",
      "temperature_schedule",
      "centralized"
    ],
    "_softmax": [
      "self",
      "info_state",
      "legal_actions",
      "temperature"
    ],
    "_get_action_probs": [
      "self",
      "info_state",
      "legal_actions",
      "epsilon"
    ]
  },
  "compute_ratings_from_matrices": [
    "win_matrix",
    "draw_matrix",
    "smoothing_factor",
    "max_iterations",
    "convergence_delta"
  ],
  "GAME": [],
  "policy_bots": [],
  "EvaluateBotsTest": {
    "test_cpp_vs_python": [
      "self",
      "bots"
    ],
    "test_random_vs_stateful": [
      "self"
    ]
  },
  "_get_future_states": [
    "possibilities",
    "state",
    "reach"
  ],
  "_add_transition": [
    "transitions",
    "key",
    "state"
  ],
  "_initialize_maps": [
    "states",
    "values",
    "transitions"
  ],
  "value_iteration": [
    "game",
    "depth_limit",
    "threshold",
    "cyclic_game"
  ],
  "EFRTest": {
    "setUp": [
      "self"
    ],
    "test_policy_zero_is_uniform": [
      "self",
      "deviations_name"
    ],
    "test_efr_kuhn_poker": [
      "self",
      "deviations_name"
    ],
    "test_efr_kuhn_poker_3p": [
      "self",
      "deviations_name"
    ],
    "test_efr_cce_dist_sheriff": [
      "self",
      "deviations_name"
    ]
  },
  "valuedict": [],
  "JointActionSolver": {
    "__call__": [
      "self",
      "payoffs_array"
    ]
  },
  "TwoPlayerNashSolver": {
    "__call__": [
      "self",
      "payoffs_array"
    ]
  },
  "CorrelatedEqSolver": {
    "__init__": [
      "self",
      "is_cce"
    ],
    "__call__": [
      "self",
      "payoffs_array"
    ]
  },
  "StackelbergEqSolver": {
    "__init__": [
      "self",
      "is_first_leader"
    ],
    "__call__": [
      "self",
      "payoffs_array"
    ]
  },
  "MultiagentQLearner": {
    "__init__": [
      "self",
      "player_id",
      "num_players",
      "num_actions",
      "joint_action_solver",
      "step_size",
      "epsilon_schedule",
      "discount_factor"
    ],
    "_get_payoffs_array": [
      "self",
      "info_state"
    ],
    "_epsilon_greedy": [
      "self",
      "info_state",
      "legal_actions",
      "epsilon"
    ],
    "step": [
      "self",
      "time_step",
      "actions",
      "is_evaluation"
    ]
  },
  "MultiagentQTest": {
    "test_simple_pathfinding_run": [
      "self"
    ],
    "test_rps_run": [
      "self"
    ]
  },
  "ActionValuesTest": {
    "test_runs_with_uniform_policies": [
      "self",
      "game_name",
      "num_players"
    ],
    "test_kuhn_poker_always_pass_p0": [
      "self"
    ]
  },
  "NoisyPolicyTest": {
    "test_cpp_and_python_implementations_are_identical": [
      "self",
      "game_name"
    ],
    "test_simultaneous_game_noisy_policy": [
      "self",
      "game_name"
    ]
  },
  "_EXPECTED_EXPLOITABILITIES_CFRBR_KUHN": [],
  "_EXPECTED_EXPLOITABILITIES_CFRBR_LEDUC": [],
  "CFRBRTest": {
    "test_policy_zero_is_uniform": [
      "self",
      "linear_averaging",
      "regret_matching_plus"
    ],
    "test_policy_and_average_policy": [
      "self"
    ],
    "test_cpp_and_python_cfr_br": [
      "self",
      "game",
      "solver_cls",
      "expected_exploitability"
    ]
  },
  "to_fraction_str": [
    "x",
    "lrsnash_max_denom"
  ],
  "lrs_solve": [
    "row_payoffs",
    "col_payoffs",
    "lrsnash_max_denom",
    "lrsnash_path"
  ],
  "lemke_howson_solve": [
    "row_payoffs",
    "col_payoffs"
  ],
  "_DATA": [],
  "MMDDilatedTest": {
    "test_solution_fixed_point": [
      "self",
      "game",
      "inverse_alpha",
      "gambit_qre_sol"
    ],
    "test_gap": [
      "self",
      "game",
      "inverse_alpha",
      "gambit_qre_sol"
    ],
    "test_rps_update": [
      "self",
      "alpha"
    ]
  },
  "mip_nash": [
    "game",
    "objective",
    "solver"
  ],
  "max_social_welfare_two_player": [
    "variables"
  ],
  "min_social_welfare_two_player": [
    "variables"
  ],
  "max_support_two_player": [
    "variables"
  ],
  "min_support_two_player": [
    "variables"
  ],
  "max_gini_two_player": [
    "variables"
  ],
  "TWO_PLAYER_OBJECTIVE": [],
  "FictitiousPlayTest": {
    "test_xfp": [
      "self"
    ],
    "test_meta_game_kuhn2p": [
      "self"
    ],
    "test_meta_game_kuhn3p": [
      "self"
    ],
    "test_meta_game_kuhn4p": [
      "self"
    ],
    "test_meta_game_leduc2p": [
      "self"
    ],
    "test_matching_pennies_3p": [
      "self"
    ],
    "test_shapleys_game": [
      "self"
    ]
  },
  "OutcomeSamplingMCCFRTest": {
    "test_outcome_sampling_leduc_2p": [
      "self"
    ],
    "test_outcome_sampling_kuhn_2p": [
      "self"
    ],
    "test_outcome_sampling_kuhn_3p": [
      "self"
    ]
  },
  "_USE_ACTION_IDS": [],
  "_FLOAT_DECIMAL_PLACES": [],
  "_escape": [
    "x"
  ],
  "_format_value": [
    "v"
  ],
  "_format_vec": [
    "vec"
  ],
  "_format_matrix": [
    "mat"
  ],
  "_format_float": [
    "x"
  ],
  "_format_float_vector": [
    "v"
  ],
  "_format_chance_outcomes": [
    "chance_outcomes"
  ],
  "_format_tensor": [
    "tensor",
    "tensor_name",
    "max_cols"
  ],
  "playthrough": [
    "game_string",
    "action_sequence",
    "alsologtostdout",
    "observation_params_string",
    "seed"
  ],
  "format_shapes": [
    "d"
  ],
  "_format_params": [
    "d",
    "as_game"
  ],
  "ShouldDisplayStateTracker": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "state"
    ]
  },
  "playthrough_lines": [
    "game_string",
    "alsologtostdout",
    "action_sequence",
    "observation_params_string",
    "seed"
  ],
  "content_lines": [
    "lines"
  ],
  "_playthrough_params": [
    "lines"
  ],
  "_read_playthrough": [
    "filename"
  ],
  "replay": [
    "filename"
  ],
  "update_path": [
    "path",
    "shard_index",
    "num_shards"
  ],
  "OBJ_MAX": [],
  "OBJ_MIN": [],
  "CONS_TYPE_LEQ": [],
  "CONS_TYPE_GEQ": [],
  "CONS_TYPE_EQ": [],
  "DOMINANCE_STRICT": [],
  "DOMINANCE_VERY_WEAK": [],
  "DOMINANCE_WEAK": [],
  "_Variable": {
    "__init__": [
      "self",
      "vid",
      "lb",
      "ub"
    ]
  },
  "_Constraint": {
    "__init__": [
      "self",
      "cid",
      "ctype"
    ]
  },
  "LinearProgram": {
    "__init__": [
      "self",
      "objective"
    ],
    "add_or_reuse_variable": [
      "self",
      "label",
      "lb",
      "ub"
    ],
    "add_or_reuse_constraint": [
      "self",
      "label",
      "ctype"
    ],
    "set_obj_coeff": [
      "self",
      "var_label",
      "coeff"
    ],
    "set_cons_coeff": [
      "self",
      "cons_label",
      "var_label",
      "coeff"
    ],
    "add_to_cons_coeff": [
      "self",
      "cons_label",
      "var_label",
      "add_coeff"
    ],
    "set_cons_rhs": [
      "self",
      "cons_label",
      "value"
    ],
    "get_var_id": [
      "self",
      "label"
    ],
    "get_num_cons": [
      "self"
    ],
    "solve": [
      "self",
      "solver"
    ]
  },
  "solve_zero_sum_matrix_game": [
    "game"
  ],
  "is_dominated": [
    "action",
    "game_or_payoffs",
    "player",
    "mode",
    "tol",
    "return_mixture"
  ],
  "_pure_dominated_from_advantages": [
    "advantages",
    "mode",
    "tol"
  ],
  "iterated_dominance": [
    "game_or_payoffs",
    "mode",
    "tol"
  ],
  "ADIDAS": {
    "__init__": [
      "self",
      "seed"
    ],
    "estimate_exploitability_sym": [
      "self",
      "dist",
      "num_eval_samples",
      "num_ckpts",
      "num_players",
      "game",
      "policies"
    ],
    "estimate_exploitability_nonsym": [
      "self",
      "dist",
      "num_eval_samples",
      "num_ckpts",
      "num_players",
      "game",
      "policies"
    ],
    "update_payoff_matrices": [
      "self",
      "payoff_matrices",
      "payoff_matrices_new",
      "s"
    ],
    "construct_payoff_matrices_from_samples_sym": [
      "self",
      "game",
      "dist",
      "num_samples",
      "policies",
      "num_players",
      "num_ckpts"
    ],
    "construct_payoff_matrices_exactly_sym": [
      "self",
      "game",
      "dist",
      "num_players"
    ],
    "construct_payoff_matrices_from_samples_nonsym": [
      "self",
      "game",
      "dist",
      "num_samples",
      "policies",
      "num_players",
      "num_ckpts"
    ],
    "construct_payoff_matrices_exactly_nonsym": [
      "self",
      "game",
      "dist",
      "num_players"
    ],
    "approximate_nash": [
      "self",
      "game",
      "solver",
      "sym",
      "num_iterations",
      "num_samples",
      "num_eval_samples",
      "approx_eval",
      "exact_eval",
      "avg_trajectory",
      "return_trajectory"
    ]
  },
  "NfgUtilsTest": {
    "test_strategy_averager_len_smaller_than_window": [
      "self"
    ],
    "test_strategy_averager": [
      "self"
    ]
  },
  "_DELIMITER": [],
  "_EMPTY_INFOSET_KEYS": [],
  "_EMPTY_INFOSET_ACTION_KEYS": [],
  "_construct_lps": [
    "state",
    "infosets",
    "infoset_actions",
    "infoset_action_maps",
    "chance_reach",
    "lps",
    "parent_is_keys",
    "parent_isa_keys"
  ],
  "solve_zero_sum_game": [
    "game",
    "solver"
  ],
  "MIPNash": {
    "test_simple_games": [
      "self"
    ]
  },
  "UNLIMITED_NUM_WORLD_SAMPLES": [],
  "UNEXPANDED_VISIT_COUNT": [],
  "TIE_TOLERANCE": [],
  "ISMCTSFinalPolicyType": {
    "NORMALIZED_VISITED_COUNT": [],
    "MAX_VISIT_COUNT": [],
    "MAX_VALUE": []
  },
  "ChildSelectionPolicy": {
    "UCT": [],
    "PUCT": []
  },
  "ChildInfo": {
    "__init__": [
      "self",
      "visits",
      "return_sum",
      "prior"
    ],
    "value": [
      "self"
    ]
  },
  "ISMCTSNode": {
    "__init__": [
      "self"
    ]
  },
  "ISMCTSBot": {
    "__init__": [
      "self",
      "game",
      "evaluator",
      "uct_c",
      "max_simulations",
      "max_world_samples",
      "random_state",
      "final_policy_type",
      "use_observation_string",
      "allow_inconsistent_action_sets",
      "child_selection_policy"
    ],
    "random_number": [
      "self"
    ],
    "reset": [
      "self"
    ],
    "get_state_key": [
      "self",
      "state"
    ],
    "run_search": [
      "self",
      "state"
    ],
    "step": [
      "self",
      "state"
    ],
    "get_policy": [
      "self",
      "state"
    ],
    "step_with_policy": [
      "self",
      "state"
    ],
    "get_final_policy": [
      "self",
      "state",
      "node"
    ],
    "sample_root_state": [
      "self",
      "state"
    ],
    "resample_from_infostate": [
      "self",
      "state"
    ],
    "create_new_node": [
      "self",
      "state"
    ],
    "set_resampler": [
      "self",
      "cb"
    ],
    "lookup_node": [
      "self",
      "state"
    ],
    "lookup_or_create_node": [
      "self",
      "state"
    ],
    "filter_illegals": [
      "self",
      "node",
      "legal_actions"
    ],
    "repopulate_prior_map_inconsistent_action_sets": [
      "self",
      "state",
      "node"
    ],
    "expand_if_necessary": [
      "self",
      "state",
      "node",
      "action"
    ],
    "select_action_tree_policy": [
      "self",
      "state",
      "node",
      "legal_actions"
    ],
    "_action_value": [
      "self",
      "node",
      "child"
    ],
    "_select_candidate_actions": [
      "self",
      "node"
    ],
    "select_action": [
      "self",
      "node"
    ],
    "check_expand": [
      "self",
      "node",
      "legal_actions"
    ],
    "run_simulation": [
      "self",
      "state"
    ]
  },
  "StrategyAverager": {
    "__init__": [
      "self",
      "num_players",
      "action_space_shapes",
      "window_size"
    ],
    "append": [
      "self",
      "meta_strategies"
    ],
    "average_strategies": [
      "self"
    ]
  },
  "SequenceFormTest": {
    "test_sequence_to_policy": [
      "self",
      "game",
      "cfr_iters"
    ],
    "test_sequence_payoff": [
      "self",
      "game",
      "cfr_iters"
    ],
    "test_sequence_tangent_projection": [
      "self",
      "game",
      "seed",
      "step_size"
    ]
  },
  "TreeWalkCalculator": {
    "__init__": [
      "self",
      "game"
    ],
    "_get_action_values": [
      "self",
      "state",
      "policies",
      "reach_probabilities"
    ],
    "compute_all_states_action_values": [
      "self",
      "policies"
    ],
    "_get_tabular_statistics": [
      "self",
      "keys"
    ],
    "get_tabular_statistics": [
      "self",
      "tabular_policy"
    ],
    "__call__": [
      "self",
      "policies",
      "tabular_policy"
    ],
    "get_root_node_values": [
      "self",
      "policies"
    ],
    "_get_action_values_only": [
      "self",
      "state",
      "policies",
      "reach_probabilities"
    ]
  },
  "AdidasTest": {
    "test_adidas_on_prisoners_dilemma": [
      "self"
    ],
    "test_adidas_on_elfarol": [
      "self"
    ]
  },
  "LPSolversTest": {
    "test_rock_paper_scissors": [
      "self"
    ],
    "test_biased_rock_paper_scissors": [
      "self"
    ],
    "test_asymmetric_pure_nonzero_val": [
      "self"
    ],
    "test_solve_blotto": [
      "self"
    ],
    "_assert_dominated": [
      "self"
    ],
    "_assert_undominated": [
      "self"
    ],
    "test_dominance": [
      "self"
    ],
    "test_dominance_3player": [
      "self"
    ],
    "test_dominance_prisoners_dilemma": [
      "self"
    ],
    "test_dominance_mixture": [
      "self"
    ],
    "_checked_iterated_dominance": [
      "self"
    ],
    "test_iterated_dominance_prisoners_dilemma": [
      "self"
    ],
    "test_iterated_dominance_auction": [
      "self"
    ],
    "test_iterated_dominance_ordering": [
      "self"
    ],
    "test_iterated_dominance_strict_invariance": [
      "self"
    ]
  },
  "RegretMatchingTest": {
    "test_two_players": [
      "self"
    ],
    "test_three_players": [
      "self"
    ],
    "test_rps": [
      "self"
    ],
    "test_biased_rps": [
      "self"
    ]
  },
  "policy_value": [
    "state",
    "policies",
    "probability_threshold"
  ],
  "JointPolicyAggregatorTest": {
    "test_policy_aggregation_random": [
      "self",
      "game_name"
    ]
  },
  "policy_to_dict": [
    "player_policy",
    "game",
    "all_states",
    "state_to_information_state"
  ],
  "get_best_response_actions_as_string": [
    "best_response_actions"
  ],
  "tabular_policy_to_cpp_map": [
    "policy"
  ],
  "DiscountedCfrTest": {
    "test_discounted_cfr_on_kuhn": [
      "self"
    ],
    "test_discounted_cfr_runs_against_leduc": [
      "self"
    ]
  },
  "RandomAgentTest": {
    "test_step": [
      "self"
    ]
  },
  "AverageType": {
    "SIMPLE": [],
    "FULL": []
  },
  "ExternalSamplingSolver": {
    "__init__": [
      "self",
      "game",
      "average_type"
    ],
    "iteration": [
      "self"
    ],
    "_full_update_average": [
      "self",
      "state",
      "reach_probs"
    ],
    "_update_regrets": [
      "self",
      "state",
      "player"
    ]
  },
  "SampleSomeStatesTest": {
    "test_sampling_in_simple_games": [
      "self"
    ]
  },
  "RandomAgent": {
    "__init__": [
      "self",
      "player_id",
      "num_actions",
      "name"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ]
  },
  "sample_some_states": [
    "game",
    "max_states",
    "make_distribution_fn"
  ],
  "_alpha_beta": [
    "state",
    "depth",
    "alpha",
    "beta",
    "value_function",
    "maximizing_player_id"
  ],
  "alpha_beta_search": [
    "game",
    "state",
    "value_function",
    "maximum_depth",
    "maximizing_player_id"
  ],
  "expectiminimax": [
    "state",
    "depth",
    "value_function",
    "maximizing_player_id"
  ],
  "_get_subgames_states": [
    "state",
    "all_states",
    "depth_limit",
    "depth",
    "include_terminals",
    "include_chance_states",
    "include_mean_field_states",
    "to_string",
    "stop_if_encountered"
  ],
  "get_all_states": [
    "game",
    "depth_limit",
    "include_terminals",
    "include_chance_states",
    "include_mean_field_states",
    "to_string",
    "stop_if_encountered"
  ],
  "ExternalSamplingMCCFRTest": {
    "test_external_sampling_leduc_2p_simple": [
      "self"
    ],
    "test_external_sampling_leduc_2p_full": [
      "self"
    ],
    "test_external_sampling_kuhn_2p_simple": [
      "self"
    ],
    "test_external_sampling_kuhn_2p_full": [
      "self"
    ],
    "disabled_test_external_sampling_liars_dice_2p_simple": [
      "self"
    ],
    "test_external_sampling_kuhn_3p_simple": [
      "self"
    ],
    "test_external_sampling_kuhn_3p_full": [
      "self"
    ]
  },
  "solve_stackelberg": [
    "game",
    "is_first_leader"
  ],
  "GetAllStatesTest": {
    "test_tic_tac_toe_number_histories": [
      "self"
    ],
    "test_simultaneous_python_game_get_all_state": [
      "self"
    ],
    "test_simultaneous_game_get_all_state": [
      "self"
    ]
  },
  "get_isa_key": [
    "info_state",
    "action"
  ],
  "get_action_from_key": [
    "isa_key"
  ],
  "get_infostate_from_key": [
    "isa_key",
    "player"
  ],
  "is_root": [
    "key",
    "player"
  ],
  "construct_vars": [
    "game"
  ],
  "uniform_random_seq": [
    "game",
    "infoset_actions_to_seq"
  ],
  "_construct_vars": [
    "state",
    "infosets",
    "infoset_actions_to_seq",
    "infoset_action_maps",
    "infoset_parent_map",
    "chance_reach",
    "parent_is_keys",
    "parent_isa_keys",
    "payoff_dict",
    "infoset_actions_children"
  ],
  "_construct_numpy_vars": [
    "payoff_dict",
    "infoset_actions_to_seq"
  ],
  "construct_constraint_vars": [
    "infoset_parent_map",
    "infoset_actions_to_seq",
    "infoset_action_maps"
  ],
  "sequence_to_policy": [
    "sequences",
    "game",
    "infoset_actions_to_seq",
    "infoset_action_maps"
  ],
  "policy_to_sequence": [
    "game",
    "policies",
    "infoset_actions_to_seq"
  ],
  "_policy_to_sequence": [
    "state",
    "policies",
    "sequences",
    "infoset_actions_to_seq",
    "parent_seq_val"
  ],
  "_DCFRSolver": {
    "__init__": [
      "self",
      "game",
      "alternating_updates",
      "linear_averaging",
      "regret_matching_plus",
      "alpha",
      "beta",
      "gamma"
    ],
    "_initialize_info_state_nodes": [
      "self",
      "state"
    ],
    "_compute_counterfactual_regret_for_player": [
      "self",
      "state",
      "policies",
      "reach_probabilities",
      "player"
    ],
    "evaluate_and_update_policy": [
      "self"
    ]
  },
  "DCFRSolver": {
    "__init__": [
      "self",
      "game",
      "alpha",
      "beta",
      "gamma"
    ]
  },
  "LCFRSolver": {
    "__init__": [
      "self",
      "game"
    ]
  },
  "ExploitabilityTest": {
    "test_exploitability_on_kuhn_poker_uniform_random": [
      "self"
    ],
    "test_kuhn_poker_uniform_random_best_response_pid0": [
      "self"
    ],
    "test_kuhn_poker_uniform_random_best_response_pid1": [
      "self"
    ],
    "test_kuhn_poker_uniform_random": [
      "self"
    ],
    "test_kuhn_poker_always_fold": [
      "self"
    ],
    "test_kuhn_poker_optimal": [
      "self"
    ],
    "test_leduc_poker_uniform_random": [
      "self"
    ],
    "test_leduc_poker_always_fold": [
      "self"
    ],
    "test_2p_nash_conv": [
      "self",
      "game_name",
      "policy_func",
      "expected"
    ],
    "test_kuhn_poker_uniform_random_nash_conv": [
      "self",
      "num_players"
    ],
    "test_python_same_as_cpp_for_multiplayer_uniform_random_nash_conv": [
      "self",
      "game_name",
      "num_players"
    ],
    "test_cpp_python_cfr_kuhn": [
      "self"
    ]
  },
  "game_trans": [],
  "eq_trans": [],
  "value_trans": [],
  "game_rps": [],
  "eq_rps": [],
  "value_rps": [],
  "p_mat0": [],
  "dominated_idxs0": [],
  "p_mat1": [],
  "dominated_idxs1": [],
  "p_mat2": [],
  "dom_idxs2": [],
  "NashAveragingTest": {
    "test_simple_games": [
      "self",
      "game",
      "eq",
      "value"
    ],
    "test_ava_games_with_dominated_strategy": [
      "self",
      "game",
      "dominated_idxs"
    ],
    "test_avt_games_with_dominated_strategy": [
      "self",
      "game",
      "dominated_idxs"
    ],
    "test_avt_games_with_multiple_dominant_strategies": [
      "self",
      "game",
      "dom_idxs"
    ]
  },
  "blackjack_info_state_to_string": [
    "state"
  ],
  "QlearnerTest": {
    "test_simple_game": [
      "self"
    ],
    "test_blackjack": [
      "self"
    ]
  },
  "QLearner": {
    "__init__": [
      "self",
      "player_id",
      "num_actions",
      "step_size",
      "epsilon_schedule",
      "discount_factor",
      "centralized",
      "info_state_to_string_override"
    ],
    "_epsilon_greedy": [
      "self",
      "info_state",
      "legal_actions",
      "epsilon"
    ],
    "_get_action_probs": [
      "self",
      "info_state",
      "legal_actions",
      "epsilon"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ],
    "loss": [
      "self"
    ]
  },
  "NoisyPolicy": {
    "__init__": [
      "self",
      "policy",
      "player_id",
      "alpha",
      "beta"
    ],
    "_state_key": [
      "self",
      "state",
      "player"
    ],
    "get_or_create_noise": [
      "self",
      "state",
      "player_id"
    ],
    "mix_probs": [
      "self",
      "probs",
      "noise_probs"
    ],
    "policy": [
      "self"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "WoLFSchedule": {
    "__init__": [
      "self",
      "t0",
      "t1"
    ],
    "step": [
      "self"
    ],
    "value": [
      "self"
    ]
  },
  "WoLFPHC": {
    "__init__": [
      "self",
      "player_id",
      "num_actions",
      "step_size",
      "epsilon_schedule",
      "delta_w",
      "delta_l",
      "discount_factor"
    ],
    "_hill_climbing": [
      "self",
      "info_state",
      "legal_actions"
    ],
    "_get_action_probs": [
      "self",
      "info_state",
      "legal_actions",
      "epsilon"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ],
    "loss": [
      "self"
    ]
  },
  "_state_values": [
    "state",
    "num_players",
    "policy"
  ],
  "best_response": [
    "game",
    "policy",
    "player_id"
  ],
  "exploitability": [
    "game",
    "policy"
  ],
  "_NashConvReturn": [],
  "nash_conv": [
    "game",
    "policy",
    "return_only_nash_conv",
    "use_cpp_br"
  ],
  "_aggregate_at_state": [
    "joint_policies",
    "state",
    "player"
  ],
  "_DictPolicy": {
    "__init__": [
      "self",
      "game",
      "policies_as_dict"
    ],
    "_state_key": [
      "self",
      "state",
      "player_id"
    ],
    "policies": [
      "self"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "JointPolicyAggregator": {
    "__init__": [
      "self",
      "game",
      "epsilon"
    ],
    "_state_key": [
      "self",
      "state",
      "player_id"
    ],
    "aggregate": [
      "self",
      "pids",
      "joint_policies",
      "weights"
    ],
    "_sub_aggregate": [
      "self",
      "pid",
      "weights"
    ],
    "_rec_aggregate": [
      "self",
      "pid",
      "state",
      "my_reaches"
    ]
  },
  "robust_child_with_total_reward_tiebreaker": [
    "root"
  ],
  "robust_child": [
    "root"
  ],
  "max_child": [
    "root"
  ],
  "max_robust_child": [
    "root",
    "find_robust"
  ],
  "secure_child": [
    "root",
    "secure_c"
  ],
  "max_robust_secure_child": [
    "root",
    "secure_c",
    "find_secure"
  ],
  "MinimaxTest": {
    "test_compute_game_value": [
      "self"
    ],
    "test_compute_game_value_with_evaluation_function": [
      "self"
    ],
    "test_win": [
      "self"
    ],
    "test_loss": [
      "self"
    ]
  },
  "DoubleOracleTest": {
    "test_rock_paper_scissors": [
      "self"
    ],
    "test_single_step": [
      "self"
    ],
    "test_kuhn_poker": [
      "self"
    ]
  },
  "SmallTest": {
    "test_biased_game": [
      "self",
      "trials",
      "atol",
      "rtol",
      "seed"
    ],
    "simp_to_euc": [
      "a",
      "b",
      "center"
    ],
    "test_spiral_game": [
      "self",
      "dx_1",
      "dy_1",
      "dx_2",
      "dy_2",
      "expected_cos_sim",
      "trials",
      "eps",
      "seed"
    ]
  },
  "TensorGame": {
    "__init__": [
      "self",
      "pt",
      "seed"
    ],
    "num_players": [
      "self"
    ],
    "num_strategies": [
      "self"
    ],
    "payoff_tensor": [
      "self"
    ],
    "get_payoffs_for_strategies": [
      "self",
      "policies"
    ],
    "best_response": [
      "self",
      "mixed_strategy",
      "return_exp"
    ],
    "best_population_response": [
      "self",
      "dist",
      "policies"
    ]
  },
  "ElFarol": {
    "__init__": [
      "self",
      "n",
      "c",
      "B",
      "S",
      "G",
      "seed"
    ]
  },
  "GAMUT": {
    "__init__": [
      "self",
      "config_list",
      "java_path",
      "seed"
    ],
    "num_players": [
      "self"
    ],
    "num_strategies": [
      "self"
    ],
    "payoff_tensor": [
      "self"
    ],
    "get_payoffs_for_strategies": [
      "self",
      "policies"
    ]
  },
  "PyspielTensorGame": {
    "__init__": [
      "self",
      "string_specifier",
      "tensor_game",
      "seed"
    ],
    "num_players": [
      "self"
    ],
    "num_strategies": [
      "self"
    ],
    "payoff_tensor": [
      "self"
    ],
    "get_payoffs_for_strategies": [
      "self",
      "policies"
    ]
  },
  "MatrixGame": {
    "__init__": [
      "self",
      "pt",
      "seed"
    ],
    "num_players": [
      "self"
    ],
    "num_strategies": [
      "self"
    ],
    "payoff_tensor": [
      "self"
    ],
    "get_payoffs_for_strategies": [
      "self",
      "policies"
    ],
    "best_response": [
      "self",
      "mixed_strategy",
      "return_exp"
    ],
    "best_population_response": [
      "self",
      "dist",
      "policies"
    ]
  },
  "BiasedGame": {
    "__init__": [
      "self",
      "seed"
    ]
  },
  "PrisonersDilemma": {
    "__init__": [
      "self",
      "seed"
    ]
  },
  "RockPaperScissors": {
    "__init__": [
      "self",
      "weights",
      "seed"
    ]
  },
  "SpiralGame": {
    "__init__": [
      "self",
      "center",
      "seed"
    ]
  },
  "MatchingPennies": {
    "__init__": [
      "self",
      "bias",
      "seed"
    ]
  },
  "Shapleys": {
    "__init__": [
      "self",
      "beta",
      "seed"
    ]
  },
  "SimplexTest": {
    "test_euclidean_projection": [
      "self",
      "vector",
      "expected_projection"
    ],
    "test_tangent_projection": [
      "self",
      "vector",
      "expected_projection"
    ],
    "test_grad_norm": [
      "self",
      "dist",
      "grad",
      "expected_norm"
    ]
  },
  "uniform_dist": [
    "x"
  ],
  "argmax": [
    "random",
    "z"
  ],
  "pt_reduce": [
    "payoff_tensor",
    "strats",
    "remove_players"
  ],
  "isnan": [
    "x"
  ],
  "grad_norm": [
    "dist",
    "grad",
    "eps",
    "simplex_tol"
  ],
  "project_grad": [
    "g"
  ],
  "euclidean_projection_onto_simplex": [
    "y",
    "eps",
    "subset"
  ],
  "project_to_interior": [
    "x",
    "eps"
  ],
  "sym": [
    "pt"
  ],
  "Solver": {
    "__init__": [
      "self",
      "proj_grad",
      "euclidean",
      "rnd_init",
      "seed"
    ],
    "init_vars": [
      "self",
      "num_strats",
      "num_players"
    ],
    "compute_gradients": [
      "self",
      "params",
      "payoff_matrices"
    ],
    "exploitability": [
      "self",
      "params",
      "payoff_matrices"
    ],
    "euc_descent_step": [
      "self",
      "params",
      "grads",
      "t",
      "eps"
    ],
    "mirror_descent_step": [
      "self",
      "params",
      "grads",
      "t",
      "eps"
    ]
  },
  "construct_game_queries": [
    "base_profile",
    "num_checkpts"
  ],
  "construct_game_queries_for_exp": [
    "base_profile",
    "num_checkpts"
  ],
  "run_games_and_record_payoffs": [
    "game_queries",
    "evaluate_game",
    "ckpt_to_policy"
  ],
  "form_payoff_matrices": [
    "game_results",
    "num_checkpts"
  ],
  "test_seed": [],
  "pt_r": [],
  "pt_c": [],
  "pd": [],
  "pd_nash": [],
  "pd_non_nash_1": [],
  "pd_non_nash_exp_1": [],
  "pd_non_nash_ate_exp_1": [],
  "pd_non_nash_2": [],
  "pd_non_nash_exp_2": [],
  "pd_non_nash_ate_exp_2": [],
  "qre_br": [],
  "entr_br": [],
  "entr_non_nash_2": [],
  "u_br_minus_non_nash": [],
  "pd_non_nash_qre_exp_2": [],
  "rps": [],
  "rps_nash": [],
  "rps_non_nash_1": [],
  "rps_non_nash_exp_1": [],
  "rps_non_nash_2": [],
  "rps_non_nash_exp_2": [],
  "rps_non_nash_3": [],
  "rps_non_nash_exp_3": [],
  "unreg_exploitability": [
    "dist",
    "payoff_tensor"
  ],
  "ate_exploitability": [
    "dist",
    "payoff_tensor",
    "p"
  ],
  "qre_exploitability": [
    "dist",
    "payoff_tensor",
    "temperature"
  ],
  "grad_norm_exploitability": [
    "dist",
    "payoff_tensor",
    "eta",
    "temperature"
  ],
  "qre_br_1": [],
  "qre_br_2": [],
  "entr_br_1": [],
  "entr_br_2": [],
  "entr_non_nash_2_1": [],
  "entr_non_nash_2_2": [],
  "u_br_minus_non_nash_1": [],
  "u_br_minus_non_nash_2": [],
  "pd_non_nash_qre_exp_2_1": [],
  "pd_non_nash_qre_exp_2_2": [],
  "rect": [],
  "rect_unreg_nash": [],
  "rect_unreg_nash_ate_exp": [],
  "u_br_minus_dist_1": [],
  "u_br_minus_dist_2": [],
  "rect_qre_exp_1": [],
  "rect_qre_exp_2": [],
  "rect_unreg_nash_qre_exp": [],
  "logits_to_dist": [
    "logits"
  ],
  "dist_to_logits": [
    "dist",
    "eps"
  ],
  "gradients": [
    "dist",
    "payoff_matrices",
    "num_players",
    "temperature",
    "proj_grad"
  ],
  "euc_project": [
    "dist",
    "y"
  ],
  "mirror_project": [
    "dist",
    "y"
  ],
  "numerical_gradient": [
    "fun",
    "x",
    "eps"
  ],
  "prep_params": [
    "dist",
    "payoff_matrices",
    "num_params",
    "solver_tuple"
  ],
  "ExploitabilityDescentTest": {
    "test_exploitability_gradient_on_symmetric_two_player_matrix_games": [
      "self",
      "solver_tuple",
      "trials",
      "max_num_strats",
      "atol",
      "rtol",
      "seed"
    ]
  },
  "loss_gradients": [
    "dist",
    "payoff_matrices",
    "num_players",
    "temperature",
    "proj_grad"
  ],
  "cheap_gradients": [
    "random",
    "dist",
    "y",
    "payoff_matrices",
    "num_players",
    "p",
    "proj_grad"
  ],
  "cheap_gradients_vr": [
    "random",
    "dist",
    "y",
    "payoff_matrices",
    "num_players",
    "pm_vr",
    "p",
    "proj_grad",
    "version"
  ],
  "build_model": [
    "game"
  ],
  "EvaluatorTest": {
    "test_evaluator_caching": [
      "self"
    ],
    "test_works_with_mcts": [
      "self"
    ]
  },
  "AlphaZeroEvaluator": {
    "__init__": [
      "self",
      "game",
      "model",
      "cache_size"
    ],
    "cache_info": [
      "self"
    ],
    "clear_cache": [
      "self"
    ],
    "_inference": [
      "self",
      "state"
    ],
    "evaluate": [
      "self",
      "state"
    ],
    "prior": [
      "self",
      "state"
    ]
  },
  "X_AXIS": [],
  "MAX_WIDTH": [],
  "SMOOTHING_RATE": [],
  "SUBSAMPLING_MAX": [],
  "print_columns": [
    "strings",
    "max_width"
  ],
  "load_jsonl_data": [
    "filename"
  ],
  "sub_sample": [
    "data",
    "count"
  ],
  "smooth": [
    "data",
    "count"
  ],
  "subselect": [
    "row",
    "keys"
  ],
  "select": [
    "data",
    "keys"
  ],
  "prepare": [
    "data",
    "cols"
  ],
  "subplot": [
    "rows",
    "cols",
    "pos"
  ],
  "plot_avg_stddev": [
    "ax",
    "x",
    "data",
    "data_col"
  ],
  "plot_histogram_numbered": [
    "ax",
    "x",
    "data",
    "data_col"
  ],
  "plot_histogram_named": [
    "ax",
    "x",
    "data",
    "data_col",
    "normalized"
  ],
  "plot_zero": [
    "df",
    "ax",
    "x"
  ],
  "plot_data": [
    "config",
    "data"
  ],
  "solved": [],
  "solve_game": [
    "state"
  ],
  "ModelTest": {
    "test_model_learns_simple": [
      "self",
      "model_type"
    ],
    "test_model_learns_optimal": [
      "self",
      "model_type"
    ]
  },
  "cascade": [
    "x",
    "fns"
  ],
  "tfkl": [],
  "conv_2d": [],
  "batch_norm": [
    "training",
    "updates",
    "name"
  ],
  "residual_layer": [
    "inputs",
    "num_filters",
    "kernel_size",
    "training",
    "updates",
    "name"
  ],
  "TrainInput": {
    "stack": [
      "train_inputs"
    ]
  },
  "Losses": {
    "total": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "__add__": [
      "self",
      "other"
    ],
    "__truediv__": [
      "self",
      "n"
    ]
  },
  "Model": {
    "valid_model_types": [],
    "__init__": [
      "self",
      "session",
      "saver",
      "path"
    ],
    "build_model": [
      "cls",
      "model_type",
      "input_shape",
      "output_size",
      "nn_width",
      "nn_depth",
      "weight_decay",
      "learning_rate",
      "path"
    ],
    "from_checkpoint": [
      "cls",
      "checkpoint",
      "path"
    ],
    "from_graph": [
      "cls",
      "metagraph",
      "path"
    ],
    "__del__": [
      "self"
    ],
    "_define_graph": [
      "model_type",
      "input_shape",
      "output_size",
      "nn_width",
      "nn_depth",
      "weight_decay",
      "learning_rate"
    ],
    "num_trainable_variables": [
      "self"
    ],
    "print_trainable_variables": [
      "self"
    ],
    "write_graph": [
      "self",
      "filename"
    ],
    "inference": [
      "self",
      "observation",
      "legals_mask"
    ],
    "update": [
      "self",
      "train_inputs"
    ],
    "save_checkpoint": [
      "self",
      "step"
    ],
    "load_checkpoint": [
      "self",
      "path"
    ]
  },
  "JOIN_WAIT_DELAY": [],
  "TrajectoryState": {
    "__init__": [
      "self",
      "observation",
      "current_player",
      "legals_mask",
      "action",
      "policy",
      "value"
    ]
  },
  "Trajectory": {
    "__init__": [
      "self"
    ],
    "add": [
      "self",
      "information_state",
      "action",
      "policy"
    ]
  },
  "Buffer": {
    "__init__": [
      "self",
      "max_size"
    ],
    "__len__": [
      "self"
    ],
    "__bool__": [
      "self"
    ],
    "append": [
      "self",
      "val"
    ],
    "extend": [
      "self",
      "batch"
    ],
    "sample": [
      "self",
      "count"
    ]
  },
  "Config": {},
  "_init_model_from_config": [
    "config"
  ],
  "watcher": [
    "fn"
  ],
  "_init_bot": [
    "config",
    "game",
    "evaluator_",
    "evaluation"
  ],
  "_play_game": [
    "logger",
    "game_num",
    "game",
    "bots",
    "temperature",
    "temperature_drop"
  ],
  "update_checkpoint": [
    "logger",
    "queue",
    "model",
    "az_evaluator"
  ],
  "actor": [],
  "evaluator": [],
  "learner": [],
  "alpha_zero": [
    "config"
  ],
  "BestResponseOracle": {
    "__init__": [
      "self",
      "best_response_backend",
      "game",
      "all_states",
      "state_to_information_state",
      "prob_cut_threshold",
      "action_value_tolerance"
    ],
    "__call__": [
      "self",
      "game",
      "training_parameters",
      "strategy_sampler",
      "using_joint_strategies"
    ]
  },
  "DEFAULT_STRATEGY_SELECTION_METHOD": [],
  "EPSILON_MIN_POSITIVE_PROBA": [],
  "exhaustive": [
    "solver",
    "number_policies_selected"
  ],
  "filter_function_factory": [
    "filter_function"
  ],
  "rectified_filter": [
    "player_policies",
    "selection_probabilities",
    "player",
    "effective_number_to_select",
    "solver"
  ],
  "probabilistic_filter": [
    "player_policies",
    "selection_probabilities",
    "player",
    "effective_number_to_select",
    "solver"
  ],
  "top_k_probabilities_filter": [
    "player_policies",
    "selection_probabilities",
    "player",
    "effective_number_to_select",
    "solver"
  ],
  "uniform_filter": [
    "player_policies",
    "selection_probabilities",
    "player",
    "effective_number_to_select",
    "solver"
  ],
  "functional_probabilistic_filter": [
    "player_policies",
    "selection_probabilities",
    "player",
    "effective_number_to_select",
    "solver"
  ],
  "uniform": [],
  "rectified": [],
  "probabilistic": [],
  "top_k_probabilities": [],
  "functional_probabilistic": [],
  "get_current_and_average_payoffs": [
    "ps2ro_trainer",
    "current_player",
    "current_strategy"
  ],
  "rectified_selector": [
    "ps2ro_trainer",
    "current_player",
    "current_strategy"
  ],
  "empty_list_generator": [
    "number_dimensions"
  ],
  "get_indices_from_non_marginalized": [
    "policies"
  ],
  "rectified_non_marginalized": [
    "solver"
  ],
  "exhaustive_non_marginalized": [
    "solver"
  ],
  "probabilistic_non_marginalized": [
    "solver"
  ],
  "top_k_probabilites_non_marginalized": [
    "solver"
  ],
  "uniform_non_marginalized": [
    "solver"
  ],
  "compressed_lambda": [
    "x"
  ],
  "functional_probabilistic_non_marginalized": [
    "solver"
  ],
  "TRAINING_STRATEGY_SELECTORS": [],
  "TRAIN_TARGET_SELECTORS": [],
  "PSROSolver": {
    "__init__": [
      "self",
      "game",
      "oracle",
      "sims_per_entry",
      "initial_policies",
      "rectifier",
      "training_strategy_selector",
      "meta_strategy_method",
      "sample_from_marginals",
      "number_policies_selected",
      "n_noisy_copies",
      "alpha_noise",
      "beta_noise"
    ],
    "_initialize_policy": [
      "self",
      "initial_policies"
    ],
    "_initialize_game_state": [
      "self"
    ],
    "get_joint_policy_ids": [
      "self"
    ],
    "get_joint_policies_from_id_list": [
      "self",
      "selected_policy_ids"
    ],
    "update_meta_strategies": [
      "self"
    ],
    "get_policies_and_strategies": [
      "self"
    ],
    "_restrict_target_training": [
      "self",
      "current_player",
      "ind",
      "total_policies",
      "probabilities_of_playing_policies",
      "restrict_target_training_bool",
      "epsilon"
    ],
    "update_agents": [
      "self"
    ],
    "update_empirical_gamestate": [
      "self",
      "seed"
    ],
    "get_meta_game": [
      "self"
    ],
    "meta_games": [
      "self"
    ],
    "get_policies": [
      "self"
    ],
    "get_and_update_non_marginalized_meta_strategies": [
      "self",
      "update"
    ],
    "get_strategy_computation_and_selection_kwargs": [
      "self"
    ]
  },
  "random_choice": [
    "outcomes",
    "probabilities"
  ],
  "sample_strategy": [
    "total_policies",
    "probabilities_of_playing_policies",
    "probs_are_marginal"
  ],
  "sample_strategy_marginal": [
    "total_policies",
    "probabilities_of_playing_policies"
  ],
  "sample_random_tensor_index": [
    "probabilities_of_index_tensor"
  ],
  "sample_strategy_joint": [
    "total_policies",
    "probabilities_of_playing_policies"
  ],
  "round_maintain_sum": [
    "x"
  ],
  "get_alpharank_marginals": [
    "payoff_tables",
    "pi"
  ],
  "remove_epsilon_negative_probs": [
    "probs",
    "epsilon"
  ],
  "get_joint_strategy_from_marginals": [
    "probabilities"
  ],
  "alpharank_strategy": [
    "solver",
    "return_joint"
  ],
  "get_strategy_profile_ids": [
    "payoff_tables"
  ],
  "get_joint_policies_from_id_list": [
    "payoff_tables",
    "policies",
    "profile_id_list"
  ],
  "aggregate_policies": [
    "game",
    "total_policies",
    "probabilities_of_playing_policies"
  ],
  "marginal_to_joint": [
    "policies"
  ],
  "aggregate_joint_policies": [
    "game",
    "total_policies",
    "probabilities_of_playing_policies"
  ],
  "update_episodes_per_oracles": [
    "episodes_per_oracle",
    "played_policies_indexes"
  ],
  "freeze_all": [
    "policies_per_player"
  ],
  "random_count_weighted_choice": [
    "count_weight"
  ],
  "RLOracle": {
    "__init__": [
      "self",
      "env",
      "best_response_class",
      "best_response_kwargs",
      "number_training_episodes",
      "self_play_proportion"
    ],
    "sample_episode": [
      "self",
      "unused_time_step",
      "agents",
      "is_evaluation"
    ],
    "_has_terminated": [
      "self",
      "episodes_per_oracle"
    ],
    "sample_policies_for_episode": [
      "self",
      "new_policies",
      "training_parameters",
      "episodes_per_oracle",
      "strategy_sampler"
    ],
    "_rollout": [
      "self",
      "game",
      "agents"
    ],
    "generate_new_policies": [
      "self",
      "training_parameters"
    ],
    "__call__": [
      "self",
      "game",
      "training_parameters",
      "strategy_sampler"
    ]
  },
  "rl_policy_factory": [
    "rl_class"
  ],
  "PGPolicy": [],
  "DQNPolicy": [],
  "_DEFAULT_STRATEGY_SELECTION_METHOD": [],
  "_DEFAULT_META_STRATEGY_METHOD": [],
  "_process_string_or_callable": [
    "string_or_callable",
    "dictionary"
  ],
  "sample_episode": [
    "state",
    "policies"
  ],
  "AbstractMetaTrainer": {
    "__init__": [
      "self",
      "game",
      "oracle",
      "initial_policies",
      "meta_strategy_method",
      "training_strategy_selector",
      "symmetric_game",
      "number_policies_selected"
    ],
    "_initialize_policy": [
      "self",
      "initial_policies"
    ],
    "_initialize_game_state": [
      "self"
    ],
    "iteration": [
      "self",
      "seed"
    ],
    "update_meta_strategies": [
      "self"
    ],
    "update_agents": [
      "self"
    ],
    "update_empirical_gamestate": [
      "self",
      "seed"
    ],
    "sample_episodes": [
      "self",
      "policies",
      "num_episodes"
    ],
    "get_meta_strategies": [
      "self"
    ],
    "get_meta_game": [
      "self"
    ],
    "get_policies": [
      "self"
    ],
    "get_kwargs": [
      "self"
    ]
  },
  "strategy_sampler_fun": [
    "total_policies",
    "probabilities_of_playing_policies"
  ],
  "AbstractOracle": {
    "__init__": [
      "self",
      "number_policies_sampled"
    ],
    "set_iteration_numbers": [
      "self",
      "number_policies_sampled"
    ],
    "__call__": [
      "self",
      "game",
      "policy",
      "total_policies",
      "current_player",
      "probabilities_of_playing_policies"
    ],
    "sample_episode": [
      "self",
      "game",
      "policies_selected"
    ],
    "evaluate_policy": [
      "self",
      "game",
      "pol",
      "total_policies",
      "current_player",
      "probabilities_of_playing_policies",
      "strategy_sampler"
    ]
  },
  "uniform_strategy": [
    "solver",
    "return_joint"
  ],
  "softmax_on_range": [
    "number_policies"
  ],
  "uniform_biased_strategy": [
    "solver",
    "return_joint"
  ],
  "renormalize": [
    "probabilities"
  ],
  "nash_strategy": [
    "solver",
    "return_joint"
  ],
  "prd_strategy": [
    "solver",
    "return_joint"
  ],
  "rm_strategy": [
    "solver",
    "return_joint"
  ],
  "META_STRATEGY_METHODS": [],
  "BestResponseOracleTest": {
    "test_cpp_python_best_response_oracle": [
      "self",
      "game_name",
      "num_players"
    ]
  },
  "FakeSolver": {
    "__init__": [
      "self",
      "strategies",
      "policies"
    ],
    "get_policies": [
      "self"
    ],
    "get_meta_strategies": [
      "self"
    ]
  },
  "equal_to_transposition_lists": [
    "a",
    "b"
  ],
  "rectified_alias": [
    "solver",
    "number_policies_to_select"
  ],
  "probabilistic_alias": [
    "solver",
    "number_policies_to_select"
  ],
  "top_k_probabilities_alias": [
    "solver",
    "number_policies_to_select"
  ],
  "StrategySelectorsTest": {
    "test_vital": [
      "self"
    ]
  },
  "_NUM_PLAYERS": [],
  "_GAME_TYPE": [],
  "_GAME_INFO": [],
  "NoopResetEnv": {
    "__init__": [
      "self",
      "env",
      "noop_max"
    ],
    "reset": [
      "self"
    ]
  },
  "AtariGame": {
    "__init__": [
      "self",
      "params"
    ],
    "observation_tensor_shape": [
      "self"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "AtariState": {
    "__init__": [
      "self",
      "game"
    ],
    "current_player": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "AtariObserver": {
    "__init__": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "DerivedNPlayerPolicyFromMeanFieldPolicyTest": {
    "test_state_conversion_method": [
      "self"
    ],
    "test_uniform_mfg_policy_conversion_to_n_player_uniform_policy": [
      "self"
    ],
    "test_pigou_network_game_outcome_optimal_mfg_policy_in_n_player_game": [
      "self"
    ],
    "test_learning_and_applying_mfg_policy_in_n_player_game": [
      "self"
    ]
  },
  "Action": {
    "PASS": [],
    "BET": []
  },
  "_DECK": [],
  "KuhnPokerGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "KuhnPokerState": {
    "__init__": [
      "self",
      "game"
    ],
    "current_player": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "KuhnPokerObserver": {
    "__init__": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "_PIPS": [],
  "_EDGES": [],
  "create_possible_actions": [],
  "_ACTIONS": [],
  "_ACTIONS_STR": [],
  "_HAND_SIZE": [],
  "_MAX_GAME_LENGTH": [],
  "DominoesGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "DominoesState": {
    "__init__": [
      "self",
      "game"
    ],
    "current_player": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "get_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "update_open_edges": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "draw_board": [
      "self"
    ]
  },
  "DominoesObserver": {
    "__init__": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "copy_indices": [
      "self",
      "dest",
      "source",
      "index_list"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "IteratedPrisonersDilemmaTest": {
    "test_default_param": [
      "self"
    ],
    "test_non_default_param_from_string": [
      "self"
    ],
    "test_non_default_param_from_dict": [
      "self"
    ],
    "test_game_as_turn_based": [
      "self"
    ],
    "test_game_as_turn_based_via_string": [
      "self"
    ],
    "test_game_from_cc": [
      "self"
    ]
  },
  "_create_empty_mfg_state": [
    "game"
  ],
  "DerivedNPlayerPolicyFromMeanFieldPolicy": {
    "__init__": [
      "self",
      "game",
      "mfg_policy"
    ],
    "_convert_state_to_mean_field_state": [
      "self",
      "n_player_state",
      "player_id"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "LiarsPokerTest": {
    "test_can_create_game_and_state": [
      "self"
    ],
    "test_draw_hands": [
      "self"
    ],
    "_populate_game_hands": [
      "self",
      "game",
      "state"
    ],
    "test_basic_bid": [
      "self"
    ],
    "_verify_returns": [
      "self",
      "game",
      "state"
    ],
    "test_single_random_round": [
      "self"
    ],
    "test_single_deterministic_round": [
      "self"
    ],
    "test_single_rebid": [
      "self"
    ],
    "test_rebid_then_new_bid": [
      "self"
    ],
    "test_game_from_cc": [
      "self"
    ],
    "test_pickle": [
      "self"
    ],
    "test_cloned_state_matches_original_state": [
      "self"
    ]
  },
  "IMPORTED_ALL_LIBRARIES": [],
  "_DEFAULT_PARAMS": [],
  "DynamicRoutingGame": {
    "__init__": [
      "self",
      "params",
      "network",
      "vehicles",
      "perform_sanity_checks"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "DynamicRoutingGameState": {
    "__init__": [
      "self",
      "game",
      "vehicles",
      "time_step_length"
    ],
    "current_time_step": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "assert_valid_player": [
      "self",
      "vehicle"
    ],
    "_legal_actions": [
      "self",
      "vehicle"
    ],
    "_apply_actions": [
      "self",
      "actions"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "get_current_vehicle_locations": [
      "self"
    ],
    "get_location_as_int": [
      "self",
      "vehicle"
    ],
    "get_current_vehicle_locations_as_int": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "NetworkObserver": {
    "__init__": [
      "self",
      "num_vehicles",
      "num_time"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "DominoesBlockTest": {
    "test_game_from_cc": [
      "self"
    ],
    "test_single_deterministic_game_1": [
      "self"
    ],
    "test_single_deterministic_game_2": [
      "self"
    ],
    "apply_action": [
      "state",
      "action"
    ],
    "deal_hands": [
      "state",
      "hands"
    ]
  },
  "_NUM_ROWS": [],
  "_NUM_COLS": [],
  "_NUM_CELLS": [],
  "TicTacToeGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "TicTacToeState": {
    "__init__": [
      "self",
      "game"
    ],
    "current_player": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "BoardObserver": {
    "__init__": [
      "self",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "_line_value": [
    "line"
  ],
  "_line_exists": [
    "board"
  ],
  "_coord": [
    "move"
  ],
  "_board_to_string": [
    "board"
  ],
  "LINE_NETWORK": [],
  "LINE_NETWORK_VEHICLES_DEMAND": [],
  "LINE_NETWORK_OD_DEMAND": [],
  "BRAESS_NUM_PLAYER": [],
  "BRAESS_NETWORK": [],
  "BRAESS_NETWORK_VEHICLES_DEMAND": [],
  "BRAESS_NETWORK_OD_DEMAND": [],
  "__SIOUX_FALLS_ADJACENCY": [],
  "__SIOUX_FALLS_FREE_FLOW_TRAVEL_TIME": [],
  "__SIOUX_FALLS_BPR_A_COEFF": [],
  "__SIOUX_FALLS_NODES": [],
  "__SIOUX_FALLS_DEMAND_AUX": [],
  "create_sioux_falls_network": [],
  "SIOUX_FALLS_NETWORK": [],
  "SIOUX_FALLS_OD_DEMAND": [],
  "SIOUX_FALLS_DUMMY_OD_DEMAND": [],
  "CHALLENGE_ACTION": [],
  "BID_ACTION_OFFSET": [],
  "_MAX_NUM_PLAYERS": [],
  "_MIN_NUM_PLAYERS": [],
  "_HAND_LENGTH": [],
  "_NUM_DIGITS": [],
  "_FULL_DECK": [],
  "LiarsPoker": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "LiarsPokerState": {
    "__init__": [
      "self",
      "game"
    ],
    "current_player": [
      "self"
    ],
    "winner": [
      "self"
    ],
    "loser": [
      "self"
    ],
    "_is_challenge_possible": [
      "self"
    ],
    "_is_rebid_possible": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_decode_bid": [
      "self",
      "bid"
    ],
    "encode_bid": [
      "self",
      "count",
      "number"
    ],
    "_counts": [
      "self"
    ],
    "_update_bid_history": [
      "self",
      "bid",
      "player"
    ],
    "_update_challenge_history": [
      "self",
      "bid",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "LiarsPokerObserver": {
    "__init__": [
      "self",
      "iig_obs_type",
      "num_players",
      "hand_length",
      "num_digits",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "GLOBAL_TEST_LLM": [],
  "ChatGameTest": {
    "setUp": [
      "self"
    ],
    "test_game_from_cc": [
      "self",
      "fixed_scenario"
    ]
  },
  "DominoesTest": {
    "test_game_from_cc": [
      "self"
    ],
    "test_single_deterministic_game_1": [
      "self"
    ],
    "test_single_deterministic_game_2": [
      "self"
    ],
    "apply_action": [
      "self",
      "state",
      "action"
    ],
    "deal_hands": [
      "self",
      "state",
      "hands"
    ]
  },
  "_DEFAULT_AUTOMATIONS": [],
  "_SUPPORTED_VARIANT_CLASSES": [],
  "SUPPORTED_VARIANT_MAP": [],
  "_SHORT_DECK_VARIANTS": [],
  "_STANDARD_DECK_VARIANTS": [],
  "_VARIANTS_SUPPORTING_ACPC_STYLE": [],
  "_get_variants_supporting_param": [
    "param_name"
  ],
  "VARIANT_PARAM_USAGE": [],
  "_ALL_IN_FOR_ONE_CHIP_EDGECASE_STRING": [],
  "_GAME_TYPE_ACPC_STYLE": [],
  "ACTION_FOLD": [],
  "_ACTION_CHECK_OR_CALL_OR_POST_BRING_IN": [],
  "ACTION_CHECK_OR_CALL": [],
  "ACTION_POST_BRING_IN": [],
  "FOLD_AND_CHECK_OR_CALL_ACTIONS": [],
  "_parse_values": [
    "param_str"
  ],
  "PokerkitWrapper": {
    "__init__": [
      "self",
      "params"
    ],
    "_calculate_num_distinct_actions": [
      "self"
    ],
    "_calculate_min_utility": [
      "self"
    ],
    "_calculate_max_utility": [
      "self"
    ],
    "game_info": [
      "self"
    ],
    "_calculate_max_game_length": [
      "self"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "raise_error_if_player_out_of_range": [
      "self",
      "player"
    ],
    "information_state_tensor_size": [
      "self"
    ],
    "_wrapped_game_template_factory": [
      "self"
    ],
    "wrapped_state_factory": [
      "self"
    ],
    "_game_type": [
      "self"
    ]
  },
  "PokerkitWrapperState": {
    "__init__": [
      "self",
      "game"
    ],
    "current_player": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_legal_actions_base": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "_action_to_string_base": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "is_chance_node": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "clone": [
      "self"
    ],
    "deepcopy_wrapped_state": [
      "self"
    ],
    "_get_betting_history": [
      "self",
      "action_converter"
    ],
    "to_struct": [
      "self"
    ],
    "to_json": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "PokerkitWrapperObserver": {
    "__init__": [
      "self",
      "game",
      "iig_obs_types",
      "params"
    ],
    "poker_hand_history_actions": [
      "self",
      "state",
      "player"
    ],
    "_observation_string": [
      "self",
      "state",
      "player"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "PokerkitWrapperAcpcStyle": {
    "__init__": [
      "self",
      "params"
    ],
    "_game_type": [
      "self"
    ],
    "_calculate_max_game_length": [
      "self"
    ],
    "game_type": [
      "self"
    ],
    "new_initial_state": [
      "self"
    ]
  },
  "PokerkitWrapperAcpcStyleState": {
    "__init__": [
      "self",
      "game"
    ],
    "_refresh_action_mappings_if_player_node": [
      "self"
    ],
    "to_pokerkit_action": [
      "self",
      "acpc_style_action"
    ],
    "to_acpc_action": [
      "self",
      "pokerkit_style_action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "to_struct": [
      "self"
    ]
  },
  "ToAcpcActionConverter": {
    "__init__": [
      "self",
      "num_players",
      "initial_contributions"
    ],
    "get_per_street_contribution": [
      "self",
      "street",
      "player"
    ],
    "get_total_contribution_before_street": [
      "self",
      "street",
      "player"
    ],
    "track_chance_node": [
      "self"
    ],
    "track_pokerkit_style_player_action": [
      "self",
      "player",
      "pokerkit_style_action",
      "size"
    ],
    "create_player_action_map": [
      "self",
      "player",
      "pokerkit_style_actions",
      "pokerkit_style_action_strings"
    ],
    "_convert_action_to_acpc_style": [
      "self",
      "pokerkit_style_action",
      "pokerkit_style_action_string",
      "total_prior_street_contribution"
    ]
  },
  "BlockDominoesGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "BlockDominoesState": {
    "__init__": [
      "self",
      "game"
    ],
    "current_player": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "get_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "update_open_edges": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "BlockDominoesObserver": {
    "__init__": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "_PAYOFF": [],
  "Chance": {
    "CONTINUE": [],
    "STOP": []
  },
  "IteratedPrisonersDilemmaGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "IteratedPrisonersDilemmaState": {
    "__init__": [
      "self",
      "game",
      "termination_probability"
    ],
    "current_player": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_apply_actions": [
      "self",
      "actions"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "action_history_string": [
      "self",
      "player"
    ]
  },
  "IteratedPrisonersDilemmaObserver": {
    "__init__": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "_NUM_ITERATION_CFR_TEST": [],
  "DynamicRoutingGameTest": {
    "test_random_game": [
      "self"
    ],
    "test_game_as_turn_based": [
      "self"
    ],
    "test_game_as_turn_based_via_string": [
      "self"
    ],
    "test_non_default_param_from_string": [
      "self"
    ],
    "test_non_default_param_from_dict": [
      "self"
    ],
    "test_action_consistency_convert_to_turn_based": [
      "self"
    ],
    "test_cfr_on_turn_based_game_with_exploitability": [
      "self"
    ],
    "test_ext_mccfr_on_turn_based_game_with_exploitability": [
      "self"
    ],
    "test_int_mccfr_on_turn_based_game_with_exploitability": [
      "self"
    ],
    "test_creation_of_rl_environment": [
      "self"
    ],
    "test_vehicle_origin_outside_network": [
      "self"
    ],
    "test_vehicle_destination_outside_network": [
      "self"
    ],
    "test_multiple_departure_time_vehicle": [
      "self"
    ],
    "test_game_evolution_first_action_policy": [
      "self"
    ],
    "test_observer_correct": [
      "self"
    ],
    "test_apply_actions_error_no_movement_with_negative_waiting_time": [
      "self"
    ],
    "test_apply_actions_error_wrong_movement_with_negative_waiting_time": [
      "self"
    ],
    "test_apply_actions_error_movement_with_positive_waiting_time": [
      "self"
    ],
    "test_braess_paradox": [
      "self"
    ]
  },
  "kuhn_nash_equilibrium": [
    "alpha"
  ],
  "GAME_TYPE": [],
  "ChatGameObserver": {
    "_build_str_to_info_state": [
      "self"
    ],
    "_info_state": [
      "self",
      "input_text",
      "obs_size"
    ]
  },
  "ChatGame": {
    "__init__": [
      "self",
      "params"
    ],
    "load_chat_game": [
      "self",
      "llm_type",
      "observations",
      "vectorize",
      "header",
      "payoffs",
      "aggregate_payoffs",
      "given_names",
      "given_llm_seeds",
      "given_prompt_actions",
      "given_private_info",
      "initial_scenario",
      "num_names",
      "num_prompt_actions",
      "num_private_info",
      "examples_names",
      "examples_prompt_actions",
      "examples_private_info",
      "examples_scenarios",
      "llm_list_suffix",
      "llm_termination_prompt",
      "seed"
    ],
    "generate_response": [
      "self",
      "prompt",
      "seed",
      "num_output_tokens"
    ],
    "generate_bool": [
      "self",
      "prompt",
      "seed"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "new_initial_state": [
      "self"
    ]
  },
  "KuhnPokerTest": {
    "test_game_from_cc": [
      "self"
    ],
    "test_consistent": [
      "self"
    ],
    "test_nash_value_sequence_form_lp": [
      "self"
    ],
    "test_exploitability_uniform_random_py": [
      "self"
    ],
    "test_exploitability_uniform_random_cc": [
      "self"
    ],
    "test_cfr_cc": [
      "self"
    ]
  },
  "NetworkTest": {
    "setUp": [
      "self"
    ],
    "test_adjacency_list_init": [
      "self"
    ],
    "test_get_successors_with_wrong_node": [
      "self"
    ],
    "test_get_action_id_without_connected_nodes": [
      "self"
    ],
    "test_get_action_id_with_wrong_nodes": [
      "self"
    ],
    "test_is_location_at_sink_noded_with_wrong_road_section": [
      "self"
    ],
    "test_is_location_at_sink_noded_with_wrong_road_section_2": [
      "self"
    ],
    "test_is_location_at_sink_noded_with_wrong_arg": [
      "self"
    ],
    "test_get_road_section_with_action_id": [
      "self"
    ],
    "test_num_links_method": [
      "self"
    ],
    "test_num_actions_method": [
      "self"
    ],
    "test_links": [
      "self"
    ],
    "test_check_list_of_vehicles_is_correct_method": [
      "self"
    ],
    "test_check_list_of_od_demand_is_correct_method": [
      "self"
    ],
    "test_str_method": [
      "self"
    ],
    "test_get_travel_time_methods": [
      "self"
    ],
    "test_assert_valid_action_methods": [
      "self"
    ],
    "test_default_travel_time_methods": [
      "self"
    ],
    "test_customable_travel_time_methods": [
      "self"
    ]
  },
  "VehicleTest": {
    "test_vehicle_1": [
      "self"
    ],
    "test_vehicle_2": [
      "self"
    ]
  },
  "OriginDestinationDemandTest": {
    "test_od_demand_1": [
      "self"
    ],
    "test_od_demand_2": [
      "self"
    ]
  },
  "_DATA_DIR": [],
  "TicTacToeTest": {
    "test_can_create_game_and_state": [
      "self"
    ],
    "test_random_game": [
      "self"
    ],
    "test_game_from_cc": [
      "self"
    ],
    "test_playthoughs_consistent": [
      "self"
    ],
    "test_observation_tensors_same": [
      "self"
    ],
    "test_pickle": [
      "self"
    ],
    "test_cloned_state_matches_original_state": [
      "self"
    ],
    "test_consistent": [
      "self"
    ]
  },
  "dataclass": [],
  "DEFAULT_NUM_HANDS": [],
  "INACTIVE_PLAYER_SEAT": [],
  "INVALID_BLIND_BET_SIZE_OR_BRING_IN_VALUE": [],
  "BlindLevel": {},
  "BetSizeLevel": {},
  "BringInLevel": {},
  "parse_blind_schedule": [
    "blind_schedule_str"
  ],
  "parse_bet_size_schedule": [
    "bet_size_schedule_str"
  ],
  "parse_bring_in_schedule": [
    "bring_in_schedule_str"
  ],
  "RepeatedPokerkit": {
    "__init__": [
      "self",
      "params"
    ],
    "_calculate_num_distinct_actions": [
      "self"
    ],
    "_calculate_min_utility": [
      "self"
    ],
    "_calculate_max_utility": [
      "self"
    ],
    "game_info": [
      "self"
    ],
    "_calculate_max_game_length": [
      "self"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "information_state_tensor_size": [
      "self"
    ]
  },
  "RepeatedPokerkitState": {
    "__init__": [
      "self",
      "game"
    ],
    "update_stacks": [
      "self"
    ],
    "update_seat_assignments_unrotated": [
      "self"
    ],
    "update_dealer": [
      "self"
    ],
    "update_seat_rotation": [
      "self"
    ],
    "update_blinds_bet_sizes_and_or_bring_in": [
      "self"
    ],
    "update_pokerkit_wrapper": [
      "self"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "current_player": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "to_string": [
      "self"
    ],
    "clone": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "is_chance_node": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "to_struct": [
      "self"
    ],
    "to_json": [
      "self"
    ]
  },
  "RepeatedPokerkitObserver": {
    "__init__": [
      "self",
      "game",
      "iig_obs_types",
      "params"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "NashEquilibriumtest": {
    "test_exploitability_is_zero_on_nash": [
      "self",
      "alpha"
    ]
  },
  "NO_POSSIBLE_ACTION": [],
  "_road_section_from_nodes": [
    "origin",
    "destination"
  ],
  "_nodes_from_road_section": [
    "movement"
  ],
  "assign_dictionary_input_to_object": [
    "dict_object",
    "road_sections",
    "default_value"
  ],
  "Network": {
    "__init__": [
      "self",
      "adjacency_list",
      "node_position",
      "bpr_a_coefficient",
      "bpr_b_coefficient",
      "capacity",
      "free_flow_travel_time"
    ],
    "_create_action_by_road_section": [
      "self"
    ],
    "num_links": [
      "self"
    ],
    "num_actions": [
      "self"
    ],
    "links": [
      "self"
    ],
    "get_successors": [
      "self",
      "node"
    ],
    "get_action_id_from_movement": [
      "self",
      "origin",
      "destination"
    ],
    "get_road_section_from_action_id": [
      "self",
      "action_id"
    ],
    "is_location_at_sink_node": [
      "self",
      "road_section"
    ],
    "check_list_of_vehicles_is_correct": [
      "self",
      "vehicles"
    ],
    "check_list_of_od_demand_is_correct": [
      "self",
      "vehicles"
    ],
    "__str__": [
      "self"
    ],
    "get_travel_time": [
      "self",
      "road_section",
      "volume"
    ],
    "assert_valid_action": [
      "self",
      "action",
      "road_section"
    ],
    "return_position_of_road_section": [
      "self",
      "road_section"
    ],
    "return_list_for_matplotlib_quiver": [
      "self"
    ]
  },
  "Vehicle": {
    "__init__": [
      "self",
      "origin",
      "destination",
      "departure_time"
    ],
    "origin": [
      "self"
    ],
    "destination": [
      "self"
    ],
    "departure_time": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "OriginDestinationDemand": {
    "__init__": [
      "self",
      "origin",
      "destination",
      "departure_time",
      "counts"
    ],
    "counts": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "ct": [],
  "REWARD_MODEL": [],
  "ALL_PLAYERS": [],
  "MIN_RND_SEED": [],
  "MAX_RND_SEED": [],
  "DEFAULT_LLM_SEED": [],
  "LLM_LENGTH_MESSAGE_TOKENS": [],
  "LLM_LENGTH_MESSAGE_CHARS": [],
  "LLM_LENGTH_OBS_TOKENS": [],
  "LLM_LENGTH_OBS_CHARS": [],
  "LLM_LENGTH_PAYOFF_OBS_TOKENS": [],
  "LLM_LENGTH_PAYOFF_OBS_CHARS": [],
  "LLM_LENGTH_LIST_OF_WORDS_TOKENS": [],
  "LLM_LIST_GEN_ATTEMPTS": [],
  "LLM_LENGTH_SCORE_TOKENS": [],
  "ITEM_PREFIX": [],
  "MIN_PLAYERS": [],
  "MAX_PLAYERS": [],
  "MAX_NUM_REPLIES": [],
  "VEC_SIZE": [],
  "DEFAULT_PARAMS": [],
  "GAME_TYPE_KWARGS": [],
  "InitialStateConfiguration": {},
  "ChatGameState": {
    "__init__": [
      "self",
      "game",
      "init_state_configs"
    ],
    "_init_empty_game": [
      "self",
      "init_state_config"
    ],
    "_setup_game": [
      "self",
      "init_state_config"
    ],
    "__str__": [
      "self"
    ],
    "_unravel_flat_action": [
      "self",
      "action"
    ],
    "_build_payoff_query": [
      "self",
      "payoff_query",
      "msg",
      "player_str"
    ],
    "_llm_is_terminal": [
      "self"
    ],
    "_names_from_validated_receiver": [
      "self",
      "receiver",
      "speaker"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_apply_msg": [
      "self",
      "speaker_msg"
    ],
    "apply_msg": [
      "self",
      "speaker_msg"
    ],
    "action_to_prompt": [
      "self",
      "action",
      "seed",
      "header"
    ],
    "action_to_msg": [
      "self",
      "action",
      "seed"
    ],
    "unravel_flat_action_to_dict": [
      "self",
      "speaker",
      "action"
    ],
    "compute_rewards": [
      "self",
      "dialogue"
    ],
    "current_player": [
      "self"
    ],
    "is_terminal": [
      "self"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "returns": [
      "self"
    ],
    "dialogue": [
      "self"
    ],
    "dialogue_str": [
      "self"
    ],
    "private_info": [
      "self"
    ],
    "header": [
      "self"
    ],
    "vectorize": [
      "self"
    ],
    "obs": [
      "self"
    ],
    "names": [
      "self"
    ],
    "speakers": [
      "self"
    ],
    "played_actions": [
      "self"
    ],
    "num_actions": [
      "self"
    ],
    "prompt_actions": [
      "self"
    ]
  },
  "ChatGameObserverBase": {
    "__init__": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "_build_str_to_info_state": [
      "self"
    ],
    "_info_state": [
      "self",
      "input_text",
      "obs_size"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "BaseChatGame": {
    "__init__": [
      "self",
      "params"
    ],
    "_load_chat_game": [
      "self",
      "observations",
      "vectorize",
      "header",
      "payoffs",
      "aggregate_payoffs",
      "given_names",
      "given_llm_seeds",
      "given_prompt_actions",
      "given_private_info",
      "initial_scenario",
      "num_names",
      "num_prompt_actions",
      "num_private_info",
      "examples_names",
      "examples_prompt_actions",
      "examples_private_info",
      "examples_scenarios",
      "llm_list_suffix",
      "llm_termination_prompt",
      "seed"
    ],
    "_generate_response": [
      "self",
      "prompt",
      "seed",
      "num_output_tokens"
    ],
    "_generate_bool": [
      "self",
      "prompt",
      "seed"
    ],
    "_build_meta_query": [
      "self",
      "scenarios"
    ],
    "_initial_scenario_is_valid": [
      "self",
      "scenario"
    ],
    "generate_prompts": [
      "self",
      "key",
      "examples",
      "num_prompts",
      "retrieve_prompt"
    ],
    "generate_scenario": [
      "self"
    ],
    "new_initial_state_configs": [
      "self"
    ],
    "game_info": [
      "self"
    ],
    "obs": [
      "self"
    ],
    "vectorize": [
      "self"
    ],
    "header": [
      "self"
    ],
    "payoffs": [
      "self"
    ],
    "aggregate_payoffs": [
      "self"
    ],
    "reward_type": [
      "self"
    ],
    "rnd": [
      "self"
    ],
    "llm_termination_prompt": [
      "self"
    ],
    "llm_seeds": [
      "self"
    ],
    "num_llm_seeds": [
      "self"
    ],
    "num_init_states": [
      "self"
    ],
    "given_prompt_actions": [
      "self"
    ]
  },
  "TestLLM": {
    "MOCK": []
  },
  "MockScore": {},
  "MockModel": {
    "__init__": [
      "self",
      "name"
    ]
  },
  "MockResponse": {
    "__init__": [
      "self",
      "text"
    ]
  },
  "MockClient": {
    "__init__": [
      "self"
    ],
    "sample": [
      "self",
      "model",
      "length",
      "seed",
      "prompt"
    ],
    "score": [
      "self",
      "model",
      "prompt"
    ],
    "list_models": [
      "self"
    ]
  },
  "MockLLM": {
    "__init__": [
      "self"
    ],
    "generate_response": [
      "self",
      "prompt",
      "seed",
      "num_output_tokens"
    ],
    "generate_bool": [
      "self",
      "prompt",
      "seed"
    ]
  },
  "MockTokenizer": {
    "to_int": [
      "self",
      "text"
    ]
  },
  "MockVectorizer": {
    "__init__": [
      "self"
    ],
    "vectorize": [
      "self",
      "text",
      "obs_size"
    ]
  },
  "RESET": [],
  "BLACK": [],
  "RED": [],
  "GREEN": [],
  "YELLOW": [],
  "BLUE": [],
  "PURPLE": [],
  "CYAN": [],
  "WHITE": [],
  "BLACK2": [],
  "ColorText": {
    "__init__": [
      "self",
      "reset_color"
    ],
    "set_color": [
      "self",
      "color"
    ],
    "set_reset_color": [
      "self",
      "color"
    ],
    "reset": [
      "self"
    ],
    "color": [
      "self",
      "log_str",
      "color"
    ]
  },
  "get_config": [],
  "prefix": [],
  "PREFIX": [],
  "POSTFIX": [],
  "Observation": {},
  "BaseScenario": {},
  "Header": {},
  "plain_header_is_valid": [
    "header"
  ],
  "strip_msg": [
    "text",
    "block_msg",
    "block_opt",
    "terminal_str"
  ],
  "first_special_char": [
    "text",
    "max_idx",
    "special_chars"
  ],
  "retrieve_special_char_block": [
    "text",
    "special_chars",
    "useless_chars"
  ],
  "retrieve_alpha_block": [
    "text"
  ],
  "retrieve_numeric_block": [
    "text"
  ],
  "wrap": [
    "message"
  ],
  "CHAR_OPT": [],
  "CHAR_MSG": [],
  "BLOCK_LEN": [],
  "SPECIAL_CHARS": [],
  "BLOCK_OPT": [],
  "BLOCK_MSG": [],
  "PLAIN": [],
  "W_OPTS_PREFIX": [],
  "OOO_LIST_A": [],
  "OOO_A": [],
  "DAY_PREFS_LIST_A": [],
  "DAY_PREFS_A": [],
  "SCENARIO_A_LIST": [],
  "SCENARIO_A": [],
  "OOO_LIST_B": [],
  "OOO_B": [],
  "DAY_PREFS_LIST_B": [],
  "DAY_PREFS_B": [],
  "SCENARIO_B_LIST": [],
  "SCENARIO_B": [],
  "query": [],
  "LLM_TERMINATION_PROMPT": [],
  "SCENARIO_C_LIST": [],
  "SCENARIO_C": [],
  "TOPIC_A": [],
  "INFO_A": [],
  "TOPIC_B": [],
  "INFO_B": [],
  "ENDOWMENT_A_LIST": [],
  "ENDOWMENT_A": [],
  "VALUATION_A_LIST": [],
  "VALUATION_A": [],
  "ENDOWMENT_B_LIST": [],
  "ENDOWMENT_B": [],
  "VALUATION_B_LIST": [],
  "VALUATION_B": [],
  "STYLES": [],
  "TONES": [],
  "NAMES": [],
  "action_keys": [],
  "info_keys": [],
  "w_opts": [],
  "ab": [],
  "ba": [],
  "cd": [],
  "dc": [],
  "context": [],
  "HEADER": [],
  "action_defaults": [],
  "info_defaults": [],
  "email_1a": [],
  "email_2a": [],
  "email_3a": [],
  "example_a": [],
  "email_1b": [],
  "email_2b": [],
  "email_3b": [],
  "example_b": [],
  "email_1c": [],
  "email_2c": [],
  "email_3c": [],
  "example_c": [],
  "instr_a": [],
  "instr_b": [],
  "instr_c": [],
  "instr_d": [],
  "info": [],
  "instr_e": [],
  "instr_f": [],
  "BaseEnvsTest": {
    "test_give_me_a_name": [
      "self",
      "base_env"
    ]
  },
  "Termination": {},
  "Payoff": {},
  "MIN_PAYOFF": [],
  "MAX_PAYOFF": [],
  "PAYOFF_PROMPT": [],
  "PAYOFF_OBS_TRANS_PREFIX": [],
  "PAYOFF_OBS_TRANS_POSTFIX": [],
  "PAYOFF": [],
  "PAYOFF_PROMPT_a": [],
  "PAYOFF_PROMPT_b": [],
  "NOOP": [],
  "LEFT": [],
  "RIGHT": [],
  "_Point": [],
  "_select_random_legal_action": [
    "time_step"
  ],
  "CatchEnvTest": {
    "test_obs_spec": [
      "self"
    ],
    "test_action_spec": [
      "self"
    ],
    "test_action_interfaces": [
      "self"
    ],
    "test_many_runs": [
      "self"
    ]
  },
  "CliffWalkingEnvTest": {
    "test_obs_spec": [
      "self"
    ],
    "test_action_spec": [
      "self"
    ],
    "test_action_interfaces": [
      "self"
    ],
    "test_many_runs": [
      "self"
    ]
  },
  "IteratedMatrixGame": {
    "__init__": [
      "self",
      "payoff_matrix",
      "iterations",
      "batch_size",
      "include_remaining_iterations"
    ],
    "one_hot": [
      "self",
      "x",
      "n"
    ],
    "num_players": [
      "self"
    ],
    "observation_spec": [
      "self"
    ],
    "action_spec": [
      "self"
    ],
    "step": [
      "self",
      "actions"
    ],
    "_get_legal_actions": [
      "self"
    ],
    "reset": [
      "self"
    ]
  },
  "IteratedPrisonersDilemma": [
    "iterations",
    "batch_size"
  ],
  "IteratedMatchingPennies": [
    "iterations",
    "batch_size"
  ],
  "make_iterated_matrix_game": [
    "game",
    "iterations",
    "batch_size"
  ],
  "make_agent_networks": [
    "num_actions"
  ],
  "run_agents": [
    "agents",
    "env",
    "num_steps"
  ],
  "LolaPolicyGradientTest": {
    "test_run_game": [
      "self",
      "game_name"
    ]
  },
  "DicePolicyGradientTest": {
    "test_run_game": [
      "self",
      "game_name"
    ]
  },
  "BoltzmannDQN": {
    "__init__": [
      "self"
    ],
    "_create_networks": [
      "self",
      "rng",
      "state_representation_size"
    ],
    "_softmax_action_probs": [
      "self",
      "params",
      "info_state",
      "legal_actions",
      "coeff"
    ],
    "_get_action_probs": [
      "self",
      "info_state",
      "legal_actions",
      "is_evaluation"
    ],
    "update_prev_q_network": [
      "self"
    ]
  },
  "NetA2C": {
    "__init__": [
      "self",
      "num_actions",
      "hidden_layers_sizes"
    ],
    "__call__": [
      "self",
      "info_state"
    ]
  },
  "NetPG": {
    "__init__": [
      "self",
      "num_actions",
      "hidden_layers_sizes"
    ],
    "__call__": [
      "self",
      "info_state"
    ]
  },
  "generate_a2c_pi_loss": [
    "net_apply",
    "loss_class",
    "entropy_cost",
    "l2_actor_weight",
    "lambda_"
  ],
  "generate_a2c_critic_loss": [
    "net_apply",
    "l2_critic_weight",
    "lambda_"
  ],
  "generate_pg_pi_loss": [
    "net_apply",
    "loss_class",
    "entropy_cost",
    "l2_actor_weight"
  ],
  "generate_pg_critic_loss": [
    "net_apply",
    "l2_critic_weight",
    "lambda_"
  ],
  "generate_act_func": [
    "net_apply"
  ],
  "ADVANTAGE_TRAIN_SHUFFLE_SIZE": [],
  "STRATEGY_TRAIN_SHUFFLE_SIZE": [],
  "DeepCFRTest": {
    "test_deep_cfr_runs": [
      "self",
      "game_name"
    ],
    "test_matching_pennies_3p": [
      "self"
    ]
  },
  "TransitionBatch": {},
  "TrainState": {},
  "UpdateFn": [],
  "get_minibatches": [
    "batch",
    "num_minibatches"
  ],
  "get_critic_update_fn": [
    "agent_id",
    "critic_network",
    "optimizer",
    "num_minibatches",
    "gamma"
  ],
  "get_dice_update_fn": [
    "agent_id",
    "rng",
    "policy_network",
    "critic_network",
    "optimizer",
    "opp_pi_lr",
    "env",
    "n_lookaheads",
    "gamma"
  ],
  "get_lola_update_fn": [
    "agent_id",
    "policy_network",
    "optimizer",
    "pi_lr",
    "gamma",
    "lola_weight"
  ],
  "get_opponent_update_fn": [
    "agent_id",
    "policy_network",
    "optimizer",
    "num_minibatches"
  ],
  "OpponentShapingAgent": {
    "__init__": [
      "self",
      "player_id",
      "opponent_ids",
      "info_state_size",
      "num_actions",
      "policy",
      "critic",
      "batch_size",
      "critic_learning_rate",
      "pi_learning_rate",
      "opp_policy_learning_rate",
      "opponent_model_learning_rate",
      "clip_grad_norm",
      "policy_update_interval",
      "discount",
      "critic_discount",
      "seed",
      "fit_opponent_model",
      "correction_type",
      "use_jit",
      "n_lookaheads",
      "num_critic_mini_batches",
      "num_opponent_updates",
      "env"
    ],
    "train_state": [
      "self"
    ],
    "policy_network": [
      "self"
    ],
    "critic_network": [
      "self"
    ],
    "metrics": [
      "self",
      "return_last_only"
    ],
    "update_params": [
      "self",
      "state",
      "player_id"
    ],
    "get_value_fn": [
      "self"
    ],
    "get_policy": [
      "self",
      "return_probs"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ],
    "_init_train_state": [
      "self",
      "info_state_size"
    ],
    "_store_time_step": [
      "self",
      "time_step",
      "action"
    ],
    "_train_step": [
      "self"
    ],
    "_should_update": [
      "self"
    ],
    "_update_agent": [
      "self",
      "batch"
    ],
    "_construct_episode_batches": [
      "self",
      "transitions"
    ],
    "_update_policy": [
      "self",
      "batch"
    ],
    "_update_critic": [
      "self",
      "batch"
    ],
    "_update_opponents": [
      "self",
      "batch"
    ],
    "_make_transition": [
      "self",
      "time_step"
    ]
  },
  "NFSPTest": {
    "test_run_kuhn": [
      "self"
    ]
  },
  "ReservoirBufferTest": {
    "test_reservoir_buffer_add": [
      "self"
    ],
    "test_reservoir_buffer_max_capacity": [
      "self"
    ],
    "test_reservoir_buffer_sample": [
      "self"
    ]
  },
  "MODE": [],
  "NFSP": {
    "__init__": [
      "self",
      "player_id",
      "state_representation_size",
      "num_actions",
      "hidden_layers_sizes",
      "reservoir_buffer_capacity",
      "anticipatory_param",
      "batch_size",
      "rl_learning_rate",
      "sl_learning_rate",
      "min_buffer_size_to_learn",
      "learn_every",
      "optimizer_str"
    ],
    "_get_update_func": [
      "self",
      "opt_update"
    ],
    "get_step_counter": [
      "self"
    ],
    "temp_mode_as": [
      "self",
      "mode"
    ],
    "_sample_episode_policy": [
      "self"
    ],
    "_act": [
      "self",
      "info_state",
      "legal_actions"
    ],
    "mode": [
      "self"
    ],
    "loss": [
      "self"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ],
    "_add_transition": [
      "self",
      "time_step",
      "agent_output"
    ],
    "_loss_avg": [
      "self",
      "param_avg",
      "info_states",
      "action_probs"
    ],
    "get_update": [
      "self"
    ],
    "_learn": [
      "self"
    ],
    "_full_checkpoint_name": [
      "self",
      "checkpoint_dir",
      "name"
    ],
    "_latest_checkpoint_filename": [
      "self",
      "name"
    ],
    "save": [
      "self",
      "checkpoint_dir"
    ],
    "has_checkpoint": [
      "self",
      "checkpoint_dir"
    ],
    "restore": [
      "self",
      "checkpoint_dir"
    ]
  },
  "JAX_CFR_SIMULTANEOUS_UPDATE": [],
  "update_regrets_plus": [
    "regret"
  ],
  "update_regrets": [
    "regret"
  ],
  "JaxCFRConstants": {},
  "JaxCFR": {
    "__init__": [
      "self",
      "game",
      "regret_matching_plus",
      "alternating_updates",
      "linear_averaging"
    ],
    "init": [
      "self"
    ],
    "multiple_steps": [
      "self",
      "iterations"
    ],
    "evaluate_and_update_policy": [
      "self"
    ],
    "step": [
      "self"
    ],
    "propagate_strategy": [
      "self",
      "current_strategies"
    ],
    "jit_step": [
      "self",
      "regrets",
      "averages",
      "average_policy_update_coefficient",
      "player"
    ],
    "average_policy": [
      "self"
    ]
  },
  "compare_cfr_with_jax_cfr": [
    "game"
  ],
  "compare_leduc": [],
  "compare_battleship": [],
  "compare_goofspiel_descending": [],
  "compare_goofspiel_randomized": [],
  "DistributionDict": [],
  "TabularDistribution": {
    "__init__": [
      "self",
      "game"
    ],
    "value": [
      "self",
      "state"
    ],
    "value_str": [
      "self",
      "state_str",
      "default_value"
    ],
    "get_params": [
      "self"
    ],
    "set_params": [
      "self",
      "params"
    ],
    "state_to_str": [
      "self",
      "state"
    ],
    "distribution": [
      "self"
    ]
  },
  "save_parametric_distribution": [
    "dist",
    "filename"
  ],
  "ValueFunctionState": [],
  "ValueFunction": {
    "__init__": [
      "self",
      "game"
    ],
    "value": [
      "self",
      "state",
      "action"
    ],
    "__call__": [
      "self",
      "state",
      "action"
    ],
    "set_value": [
      "self",
      "state",
      "value",
      "action"
    ],
    "has": [
      "self",
      "state",
      "action"
    ],
    "add_value": [
      "self",
      "state",
      "value",
      "action"
    ]
  },
  "TabularValueFunction": {
    "__init__": [
      "self",
      "game"
    ],
    "value": [
      "self",
      "state",
      "action"
    ],
    "set_value": [
      "self",
      "state",
      "value",
      "action"
    ],
    "has": [
      "self",
      "state",
      "action"
    ]
  },
  "Distribution": {
    "__init__": [
      "self",
      "game"
    ],
    "value": [
      "self",
      "state"
    ],
    "value_str": [
      "self",
      "state_str",
      "default_value"
    ],
    "__call__": [
      "self",
      "state"
    ]
  },
  "ParametricDistribution": {
    "get_params": [
      "self"
    ],
    "set_params": [
      "self",
      "params"
    ]
  },
  "SoftmaxPolicyTest": {
    "test_softmax": [
      "self",
      "name"
    ]
  },
  "FixedPoint": {
    "__init__": [
      "self",
      "game",
      "temperature"
    ],
    "iteration": [
      "self"
    ],
    "get_policy": [
      "self"
    ],
    "distribution": [
      "self"
    ]
  },
  "get_joint_br": [
    "game",
    "weights",
    "mus"
  ],
  "compute_rewards": [
    "game",
    "policies",
    "mus"
  ],
  "compute_average_welfare": [
    "game",
    "policies",
    "mus",
    "rhos",
    "nus"
  ],
  "cce_br": [
    "game",
    "policies",
    "weights",
    "mus",
    "nus",
    "rewards"
  ],
  "ce_br": [
    "game",
    "policies",
    "weights",
    "mus",
    "nus",
    "rewards"
  ],
  "partial_ce_br": [
    "game",
    "policies",
    "weights",
    "mus",
    "nus",
    "rewards"
  ],
  "cce_gap": [
    "game",
    "policies",
    "weights",
    "mus",
    "nus",
    "rewards",
    "compute_true_rewards"
  ],
  "ce_gap": [
    "game",
    "policies",
    "weights",
    "mus",
    "nus",
    "rewards",
    "compute_true_rewards"
  ],
  "AverageNetworkFictitiousPlayTest": {
    "test_train": [
      "self",
      "name"
    ]
  },
  "softmax_projection": [
    "logits"
  ],
  "ProjectedPolicy": {
    "__init__": [
      "self",
      "game",
      "player_ids",
      "state_value",
      "coeff"
    ],
    "value": [
      "self",
      "state",
      "action"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "MirrorDescent": {
    "__init__": [
      "self",
      "game",
      "state_value",
      "lr",
      "root_state"
    ],
    "get_state_value": [
      "self",
      "state",
      "learning_rate"
    ],
    "eval_state": [
      "self",
      "state",
      "learning_rate"
    ],
    "get_projected_policy": [
      "self"
    ],
    "iteration": [
      "self",
      "learning_rate"
    ],
    "get_policy": [
      "self"
    ],
    "distribution": [
      "self"
    ]
  },
  "GreedyPolicy": {
    "__init__": [
      "self",
      "game",
      "player_ids",
      "state_action_value"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ],
    "action": [
      "self",
      "state",
      "player_id"
    ]
  },
  "DeepOnlineMirrorDescentTest": {
    "test_train": [
      "self",
      "name"
    ]
  },
  "MergedPolicy": {
    "__init__": [
      "self",
      "game",
      "player_ids",
      "policies",
      "distributions",
      "weights"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "FictitiousPlay": {
    "__init__": [
      "self",
      "game",
      "lr",
      "temperature"
    ],
    "get_policy": [
      "self"
    ],
    "get_correlating_policy": [
      "self"
    ],
    "get_correlating_distribution": [
      "self"
    ],
    "iteration": [
      "self",
      "br_policy",
      "learning_rate"
    ]
  },
  "MixedDistribution": {
    "__init__": [
      "self",
      "mus",
      "weights",
      "tol"
    ],
    "_prune": [
      "self"
    ],
    "value": [
      "self",
      "state"
    ],
    "value_str": [
      "self",
      "state_str",
      "default_value"
    ],
    "__call__": [
      "self",
      "state"
    ]
  },
  "get_exact_value": [
    "pi",
    "mu",
    "game"
  ],
  "sample_value": [
    "pi",
    "mu",
    "game"
  ],
  "get_nu_values": [
    "policies",
    "nu",
    "game"
  ],
  "ProjectedPolicyMunchausen": {
    "__init__": [
      "self",
      "game",
      "player_ids",
      "state_value",
      "learning_rate",
      "policy"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "MunchausenMirrorDescent": {
    "eval_state": [
      "self",
      "state",
      "learning_rate"
    ],
    "get_projected_policy": [
      "self"
    ]
  },
  "dict_equal": [
    "dic1",
    "dic2"
  ],
  "equal_policies": [
    "pol1",
    "pol2",
    "all_states"
  ],
  "filter_policies": [
    "policies",
    "new_policies",
    "all_states"
  ],
  "MeanFieldPSRO": {
    "__init__": [
      "self",
      "game",
      "regret_minimizer",
      "regret_steps_per_step",
      "best_responder",
      "filter_new_policies",
      "increase_precision_when_done_early"
    ],
    "step": [
      "self"
    ],
    "get_equilibrium": [
      "self"
    ]
  },
  "JointBestResponse": {
    "__init__": [
      "self",
      "game",
      "distributions",
      "weights",
      "root_state"
    ],
    "get_state_rewards": [
      "self",
      "mu_states"
    ],
    "get_new_mu_states": [
      "self",
      "mu_states"
    ],
    "eval_state": [
      "self",
      "mu_states"
    ],
    "evaluate": [
      "self"
    ],
    "value": [
      "self",
      "state",
      "action"
    ]
  },
  "ILLEGAL_ACTION_PENALTY": [],
  "MIN_ACTION_PROB": [],
  "_copy_params": [
    "params"
  ],
  "MunchausenDQN": {
    "__init__": [
      "self",
      "player_id",
      "state_representation_size",
      "num_actions",
      "batch_size",
      "learn_every",
      "epsilon_start",
      "epsilon_end",
      "epsilon_decay_duration",
      "epsilon_power",
      "discount_factor",
      "replay_buffer_capacity",
      "min_buffer_size_to_learn",
      "replay_buffer_class",
      "optimizer",
      "learning_rate",
      "loss",
      "huber_loss_parameter",
      "update_target_network_every",
      "hidden_layers_sizes",
      "qnn_params_init",
      "tau",
      "alpha",
      "reset_replay_buffer_on_update",
      "gradient_clipping",
      "with_munchausen",
      "seed"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation",
      "add_transition_record",
      "use_softmax",
      "tau"
    ],
    "add_transition": [
      "self",
      "prev_time_step",
      "prev_action",
      "prev_legal_actions",
      "time_step"
    ],
    "_get_action_probs": [
      "self",
      "params",
      "info_states",
      "legal_one_hots"
    ],
    "_loss": [
      "self",
      "params",
      "params_target",
      "params_prev",
      "info_states",
      "actions",
      "legal_one_hots",
      "rewards",
      "next_info_states",
      "are_final_steps",
      "next_legal_one_hots"
    ],
    "_get_update": [
      "self"
    ],
    "_to_one_hot": [
      "self",
      "a",
      "value"
    ],
    "learn": [
      "self"
    ],
    "_epsilon_greedy": [
      "self",
      "info_state",
      "legal_actions",
      "epsilon"
    ],
    "_get_epsilon": [
      "self",
      "is_evaluation"
    ],
    "_softmax": [
      "self",
      "info_state",
      "legal_actions",
      "tau"
    ],
    "update_prev_q_network": [
      "self"
    ],
    "loss": [
      "self"
    ]
  },
  "SoftMaxMunchausenDQN": {
    "__init__": [
      "self",
      "agent",
      "tau"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ]
  },
  "DeepOnlineMirrorDescent": {
    "__init__": [
      "self",
      "game",
      "envs",
      "agents",
      "eval_every",
      "num_episodes_per_iteration",
      "logging_fn"
    ],
    "_train_agents": [
      "self"
    ],
    "_update_policy_and_distribution": [
      "self"
    ],
    "get_softmax_policy": [
      "self",
      "tau"
    ],
    "iteration": [
      "self"
    ],
    "policy": [
      "self"
    ],
    "distribution": [
      "self"
    ]
  },
  "BestResponse": {
    "__init__": [
      "self",
      "game",
      "distribution",
      "state_value",
      "root_state"
    ],
    "eval_state": [
      "self",
      "state"
    ],
    "evaluate": [
      "self"
    ],
    "value": [
      "self",
      "state",
      "action"
    ]
  },
  "MunchausenMirrorDescentTest": {
    "test_run": [
      "self",
      "name"
    ]
  },
  "AverageNetworkFictitiousPlay": {
    "__init__": [
      "self",
      "game",
      "envs",
      "br_rl_agents",
      "num_episodes_per_iteration",
      "num_training_steps_per_iteration",
      "eval_every",
      "logging_fn"
    ],
    "_update_distribution": [
      "self"
    ],
    "policy": [
      "self"
    ],
    "iteration": [
      "self"
    ]
  },
  "GreedyPolicyTest": {
    "test_greedy": [
      "self",
      "name"
    ]
  },
  "MirrorDescentTest": {
    "test_fp": [
      "self",
      "name"
    ]
  },
  "NashConv": {
    "__init__": [
      "self",
      "game",
      "policy",
      "root_state"
    ],
    "nash_conv": [
      "self"
    ],
    "br_values": [
      "self"
    ],
    "distribution": [
      "self"
    ]
  },
  "FixedPointTest": {
    "test_run": [
      "self",
      "name"
    ],
    "test_softmax": [
      "self",
      "name"
    ]
  },
  "_check_distribution_sum": [
    "distribution",
    "expected_sum"
  ],
  "DistributionPolicy": {
    "__init__": [
      "self",
      "game",
      "policy",
      "root_state"
    ],
    "evaluate": [
      "self"
    ],
    "_forward_actions": [
      "self",
      "current_states",
      "distribution",
      "actions_and_probs_fn"
    ],
    "_one_forward_step": [
      "self",
      "current_states",
      "distribution",
      "policy"
    ]
  },
  "PolicyValue": {
    "__init__": [
      "self",
      "game",
      "distribution",
      "policy",
      "state_value",
      "root_state"
    ],
    "eval_state": [
      "self",
      "state"
    ],
    "evaluate": [
      "self"
    ],
    "value": [
      "self",
      "state",
      "action"
    ]
  },
  "SoftmaxPolicy": {
    "__init__": [
      "self",
      "game",
      "player_ids",
      "temperature",
      "state_action_value",
      "prior_policy"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "DistributionTest": {
    "test_basic": [
      "self"
    ],
    "test_state_support_outside_distrib": [
      "self"
    ],
    "test_multi_pop": [
      "self"
    ]
  },
  "get_proba_constraints_positivity": [
    "nus"
  ],
  "get_proba_constraint_sum_eq": [
    "nus"
  ],
  "compress_internal_weights": [
    "nus",
    "regrets"
  ],
  "compress_external_weights": [
    "nus",
    "regrets",
    "lbd"
  ],
  "power_method": [
    "w_nus"
  ],
  "RegretMinimizer": {
    "__init__": [
      "self",
      "game",
      "policies",
      "regret_steps_per_step",
      "rho_tol",
      "compress_nus",
      "compress_every",
      "compress_lbd",
      "stop_early",
      "stop_regret_threshold",
      "value_estimator",
      "value_estimation_n",
      "compute_internal_regret"
    ],
    "update_policy_mus": [
      "self"
    ],
    "get_nu": [
      "self"
    ],
    "step": [
      "self"
    ],
    "step_for": [
      "self",
      "T"
    ],
    "compute_average_regret": [
      "self"
    ],
    "compute_regrets": [
      "self"
    ],
    "reset": [
      "self",
      "policies"
    ]
  },
  "polynomial_weight_update": [
    "weights",
    "rewards",
    "eta"
  ],
  "PolynomialWeightAlgorithm": {
    "__init__": [
      "self",
      "game",
      "policies",
      "eta",
      "regret_steps_per_step",
      "rho_tol",
      "compress_nus",
      "compress_every",
      "compress_lbd",
      "stop_early",
      "stop_regret_threshold",
      "value_estimator",
      "value_estimation_n",
      "compute_internal_regret"
    ],
    "get_all_w_nus": [
      "self"
    ],
    "get_nu": [
      "self"
    ],
    "compute_p": [
      "self"
    ],
    "_update_weights": [
      "self",
      "rewards"
    ],
    "step": [
      "self"
    ],
    "step_for": [
      "self",
      "T"
    ],
    "get_post_compression_regret_and_weights": [
      "self"
    ],
    "compress_nus_and_weights": [
      "self",
      "nu_weights"
    ],
    "normalize_nu_weights": [
      "self"
    ],
    "get_normalized_nu_weights": [
      "self"
    ],
    "compute_regrets": [
      "self"
    ],
    "compute_average_regret": [
      "self"
    ],
    "get_nus": [
      "self"
    ],
    "get_mus": [
      "self"
    ],
    "get_rewards": [
      "self"
    ],
    "get_mus_and_weights": [
      "self"
    ],
    "compute_optimal_eta": [
      "self"
    ],
    "reset": [
      "self",
      "policies"
    ]
  },
  "Hedge": {
    "__init__": [
      "self",
      "game",
      "policies",
      "eta",
      "regret_steps_per_step",
      "rho_tol",
      "compress_nus",
      "compress_lbd",
      "compress_every",
      "stop_early",
      "stop_regret_threshold",
      "value_estimator",
      "value_estimation_n",
      "compute_internal_regret"
    ],
    "_update_weights": [
      "self",
      "rewards"
    ]
  },
  "convert_param_spec": [
    "param_spec"
  ],
  "BoltzmannPolicyIterationTest": {
    "test_run": [
      "self",
      "name"
    ]
  },
  "BoltzmannPolicyIteration": {
    "get_projected_policy": [
      "self"
    ]
  },
  "PolicyTest": {
    "test_train": [
      "self",
      "name",
      "setting"
    ]
  },
  "NashC": {
    "__init__": [
      "self",
      "game",
      "distrib",
      "pi_value",
      "root_state"
    ]
  },
  "Agent": {
    "__init__": [
      "self",
      "info_state_size",
      "num_actions"
    ],
    "layer_init": [
      "self",
      "layer",
      "bias_const"
    ],
    "get_value": [
      "self",
      "x"
    ],
    "get_action_and_value": [
      "self",
      "x",
      "action"
    ]
  },
  "rollout": [
    "env",
    "iter_agent",
    "eps_agent",
    "num_epsiodes",
    "steps",
    "device"
  ],
  "calculate_advantage": [
    "gamma",
    "norm",
    "rewards",
    "values",
    "dones",
    "device"
  ],
  "learn": [
    "history",
    "optimizer_actor",
    "optimize_critic",
    "agent",
    "num_minibatches",
    "update_epochs",
    "itr_eps",
    "eps_eps",
    "alpha",
    "ent_coef",
    "max_grad_norm"
  ],
  "calculate_explotability": [
    "game",
    "distrib",
    "policy"
  ],
  "Geometry": {
    "SQUARE": [],
    "TORUS": []
  },
  "_DEFAULT_SIZE": [],
  "_NUM_ACTIONS": [],
  "_NUM_CHANCE": [],
  "DEFAULT_REWARD_MATRIX_THREE_POPULATIONS": [],
  "DEFAULT_REWARD_MATRIX_FOUR_POPULATIONS": [],
  "DEFAULT_INIT_DISTRIB_THREE_POPULATIONS": [],
  "DEFAULT_INIT_DISTRIB_FOUR_POPULATIONS": [],
  "_DEFAULT_GEOMETRY": [],
  "_DEFAULT_NOISE_PROBABILITY": [],
  "_DEFAULT_CONGESTION_COEFF": [],
  "THREE_POPULATIONS": [],
  "FOUR_POPULATIONS": [],
  "get_param": [
    "param_name",
    "params"
  ],
  "_state_to_str": [
    "x",
    "y",
    "t",
    "population",
    "player_id"
  ],
  "MFGPredatorPreyGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "max_chance_nodes_in_history": [
      "self"
    ],
    "new_initial_state_for_population": [
      "self",
      "population"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "pos_to_merged": [
    "pos",
    "size"
  ],
  "merged_to_pos": [
    "merged_pos",
    "size"
  ],
  "MFGPredatorPreyState": {
    "_ACTION_TO_MOVE": [],
    "_NEUTRAL_ACTION": [],
    "__init__": [
      "self",
      "game",
      "population"
    ],
    "population": [
      "self"
    ],
    "pos": [
      "self"
    ],
    "t": [
      "self"
    ],
    "state_to_str": [
      "self",
      "pos",
      "t",
      "population",
      "player_id"
    ],
    "mean_field_population": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "update_pos": [
      "self",
      "action"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "distribution_support": [
      "self"
    ],
    "get_pos_proba": [
      "self",
      "pos",
      "population"
    ],
    "update_distribution": [
      "self",
      "distribution"
    ],
    "is_terminal": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "Observer": {
    "__init__": [
      "self",
      "params",
      "game"
    ],
    "set_from": [
      "self",
      "state",
      "player"
    ],
    "string_from": [
      "self",
      "state",
      "player"
    ]
  },
  "GAME_SETTINGS": [],
  "DYNAMIC_ROUTING_NETWORK": [],
  "create_game_with_setting": [
    "game_name",
    "setting"
  ],
  "get_l1_distribution_dist": [
    "mu1",
    "mu2"
  ],
  "LinearPolicy": {
    "__init__": [
      "self",
      "game",
      "player_ids"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "MFG_STR_CONST": [],
  "MFGLinearQuadraticGameTest": {
    "test_load": [
      "self"
    ],
    "test_create": [
      "self"
    ],
    "test_create_with_params": [
      "self"
    ],
    "check_cloning": [
      "self",
      "state"
    ],
    "test_random_game": [
      "self"
    ]
  },
  "MFGCrowdModellingGameTest": {
    "test_load": [
      "self"
    ],
    "test_create": [
      "self"
    ],
    "test_create_with_params": [
      "self"
    ],
    "test_random_game": [
      "self"
    ],
    "test_reward": [
      "self"
    ],
    "test_distribution": [
      "self"
    ],
    "test_compare_py_cpp": [
      "self"
    ]
  },
  "WAITING_TIME_NOT_ASSIGNED": [],
  "MeanFieldRoutingGame": {
    "__init__": [
      "self",
      "params",
      "network",
      "od_demand",
      "perform_sanity_checks"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "max_chance_nodes_in_history": [
      "self"
    ],
    "get_road_section_as_int": [
      "self",
      "section"
    ]
  },
  "MeanFieldRoutingGameState": {
    "__init__": [
      "self",
      "game",
      "time_step_length"
    ],
    "current_time_step": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "state_to_str": [
      "self",
      "location",
      "time_step",
      "player_id",
      "waiting_time",
      "destination"
    ],
    "distribution_support": [
      "self"
    ],
    "update_distribution": [
      "self",
      "distribution"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "is_terminal": [
      "self"
    ],
    "is_waiting": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "get_location_as_int": [
      "self"
    ],
    "get_destination_as_int": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "_SIZE": [],
  "_HORIZON": [],
  "_MEAN_REVERT": [],
  "_VOLATILITY": [],
  "_CROSS_Q": [],
  "_KAPPA": [],
  "_TERMINAL_COST": [],
  "_DELTA_T": [],
  "_N_ACTIONS_PER_SIDE": [],
  "_SPATIAL_BIAS": [],
  "MFGLinearQuadraticGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "max_chance_nodes_in_history": [
      "self"
    ]
  },
  "MFGLinearQuadraticState": {
    "__init__": [
      "self",
      "game"
    ],
    "to_string": [
      "self"
    ],
    "state_to_str": [
      "self",
      "x",
      "tick",
      "player_id"
    ],
    "n_actions": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "action_to_move": [
      "self",
      "action"
    ],
    "actions_to_position": [
      "self"
    ],
    "chance_outcomes": [
      "self"
    ],
    "distribution_support": [
      "self"
    ],
    "distribution_average": [
      "self"
    ],
    "update_distribution": [
      "self",
      "distribution"
    ],
    "t": [
      "self"
    ],
    "is_terminal": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "eta_t": [
      "self"
    ],
    "_rewards": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "_returns": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "grid_to_forbidden_states": [
    "grid"
  ],
  "FOUR_ROOMS_FORBIDDEN_STATES": [],
  "FOUR_ROOMS": [],
  "MAZE_FORBIDDEN_STATES": [],
  "MAZE": [],
  "MFGPredatorPreyGameTest": {
    "test_load": [
      "self"
    ],
    "test_dynamics": [
      "self",
      "geometry",
      "expected_pos"
    ],
    "test_create_with_params": [
      "self"
    ],
    "test_random_game": [
      "self",
      "population"
    ],
    "test_rewards": [
      "self",
      "reward_matrix",
      "players",
      "population",
      "initial_pos",
      "distributions",
      "expected_rewards",
      "init_distrib"
    ]
  },
  "MFGPeriodicAversionTest": {
    "test_load": [
      "self"
    ],
    "test_create": [
      "self"
    ],
    "test_create_with_params": [
      "self"
    ],
    "check_cloning": [
      "self",
      "state"
    ],
    "test_random_game": [
      "self"
    ]
  },
  "_DEFAULT_HORIZON": [],
  "_DEFAULT_CONGESTION_MATRIX": [],
  "_DEFAULT_NUM_PLAYERS": [],
  "_DEFAULT_INIT_DISTRIB": [],
  "pairs_string_to_list": [
    "positions"
  ],
  "forbidden_states_grid": [],
  "_DEFAULT_FORBIDDEN_STATES": [],
  "forbidden_states_indicator": [],
  "_DEFAULT_PROBA_NOISE": [],
  "_DEFAULT_COEF_CONGESTION": [],
  "_DEFAULT_COEF_TARGET": [],
  "MFGCrowdAvoidanceGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "max_chance_nodes_in_history": [
      "self"
    ],
    "new_initial_state_for_population": [
      "self",
      "population"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ]
  },
  "MFGCrowdAvoidanceState": {
    "_ACTION_TO_MOVE": [],
    "_NEUTRAL_ACTION": [],
    "__init__": [
      "self",
      "game",
      "population"
    ],
    "population": [
      "self"
    ],
    "pos": [
      "self"
    ],
    "t": [
      "self"
    ],
    "state_to_str": [
      "self",
      "pos",
      "t",
      "population",
      "player_id"
    ],
    "mean_field_population": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "update_pos": [
      "self",
      "action"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "distribution_support": [
      "self"
    ],
    "get_pos_proba": [
      "self",
      "pos",
      "population"
    ],
    "update_distribution": [
      "self",
      "distribution"
    ],
    "is_terminal": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "FactoryTest": {
    "test_smoke": [
      "self",
      "game_name",
      "setting"
    ]
  },
  "CrowdModelling2DTest": {
    "test_grid_to_forbidden_states": [
      "self"
    ]
  },
  "_NUMBER_OF_ITERATIONS_TESTS": [],
  "SocialOptimumBraess": {
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "NashEquilibriumBraess": {
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "MeanFieldRoutingGameTest": {
    "test_load": [
      "self"
    ],
    "test_create": [
      "self"
    ],
    "test_random_game": [
      "self"
    ],
    "test_evolving_trajectory_with_uniform_policy": [
      "self"
    ],
    "test_non_default_param_from_string": [
      "self"
    ],
    "test_non_default_param_from_dict": [
      "self"
    ],
    "test_online_mirror_descent": [
      "self"
    ],
    "test_online_mirror_descent_convergence": [
      "self"
    ],
    "test_vehicle_origin_outside_network": [
      "self"
    ],
    "test_vehicle_destination_outside_network": [
      "self"
    ],
    "test_multiple_departure_time_vehicle": [
      "self"
    ],
    "test_game_evolution_uniform_policy": [
      "self"
    ],
    "test_observer_correct": [
      "self"
    ],
    "test_apply_actions_error_no_movement_with_negative_waiting_time": [
      "self"
    ],
    "test_apply_actions_error_wrong_movement_with_negative_waiting_time": [
      "self"
    ],
    "test_apply_actions_error_movement_with_positive_waiting_time": [
      "self"
    ],
    "test_online_mirror_descent_sioux_falls_dummy": [
      "self"
    ]
  },
  "CppVsPythonMeanFieldRoutingGameTest": {
    "test_braess_paradox_game": [
      "self",
      "game_name"
    ]
  },
  "MFGNormalFormGameTest": {
    "test_load": [
      "self"
    ],
    "test_create": [
      "self"
    ],
    "test_create_with_params": [
      "self"
    ],
    "test_reward": [
      "self"
    ]
  },
  "_EPSILON": [],
  "MFGCrowdModellingGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "max_chance_nodes_in_history": [
      "self"
    ]
  },
  "MFGCrowdModellingState": {
    "_ACTION_TO_MOVE": [],
    "_NEUTRAL_ACTION": [],
    "__init__": [
      "self",
      "game"
    ],
    "x": [
      "self"
    ],
    "t": [
      "self"
    ],
    "state_to_str": [
      "self",
      "x",
      "t",
      "player_id"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "chance_outcomes": [
      "self"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "distribution_support": [
      "self"
    ],
    "update_distribution": [
      "self",
      "distribution"
    ],
    "is_terminal": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "_rewards": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "_returns": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "_COEF_AVERSION": [],
  "_X_MIN": [],
  "_X_MAX": [],
  "MFGPeriodicAversionGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "max_chance_nodes_in_history": [
      "self"
    ]
  },
  "MFGPeriodicAversionState": {
    "__init__": [
      "self",
      "game"
    ],
    "to_string": [
      "self"
    ],
    "state_to_str": [
      "self",
      "x",
      "tick",
      "player_id"
    ],
    "n_actions": [
      "self"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "action_to_move": [
      "self",
      "action"
    ],
    "state_to_position": [
      "self",
      "state"
    ],
    "position_to_state": [
      "self",
      "position"
    ],
    "chance_outcomes": [
      "self"
    ],
    "distribution_support": [
      "self"
    ],
    "get_state_proba": [
      "self",
      "state"
    ],
    "update_distribution": [
      "self",
      "distribution"
    ],
    "t": [
      "self"
    ],
    "is_terminal": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "_rewards": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "_returns": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "MFGCrowdAvoidanceGameTest": {
    "test_load": [
      "self"
    ],
    "test_dynamics": [
      "self",
      "geometry",
      "expected_pos"
    ],
    "test_create_with_params": [
      "self"
    ],
    "test_random_game": [
      "self",
      "population"
    ],
    "test_rewards": [
      "self",
      "coef_congestion",
      "coef_target",
      "congestion_matrix",
      "players",
      "population",
      "initial_pos",
      "distributions",
      "expected_rewards",
      "init_distrib"
    ]
  },
  "coop_reward": [
    "last_action",
    "distribution"
  ],
  "biased_indirect_rps": [
    "last_action",
    "distribution"
  ],
  "dominated_reward_source": [
    "last_action",
    "distribution"
  ],
  "MFGNormalFormGame": {
    "__init__": [
      "self",
      "params"
    ],
    "new_initial_state": [
      "self"
    ],
    "make_py_observer": [
      "self",
      "iig_obs_type",
      "params"
    ],
    "max_chance_nodes_in_history": [
      "self"
    ]
  },
  "MFGNormalFormState": {
    "__init__": [
      "self",
      "game",
      "last_action"
    ],
    "state_to_str": [
      "self",
      "player_id"
    ],
    "_legal_actions": [
      "self",
      "player"
    ],
    "_apply_action": [
      "self",
      "action"
    ],
    "_action_to_string": [
      "self",
      "player",
      "action"
    ],
    "distribution_support": [
      "self"
    ],
    "update_distribution": [
      "self",
      "distribution"
    ],
    "is_terminal": [
      "self"
    ],
    "current_player": [
      "self"
    ],
    "_rewards": [
      "self"
    ],
    "rewards": [
      "self"
    ],
    "_returns": [
      "self"
    ],
    "returns": [
      "self"
    ],
    "__str__": [
      "self"
    ]
  },
  "MODEL_FILE_NAME": [],
  "run_iterations": [
    "game",
    "solver",
    "start_iteration"
  ],
  "construct_is_mcts_policy": [
    "game",
    "state",
    "tabular_policy",
    "bot",
    "searched"
  ],
  "_run_once": [
    "state",
    "bots"
  ],
  "_WBridge5Client": {
    "__init__": [
      "self",
      "command"
    ],
    "start": [
      "self"
    ],
    "read_line": [
      "self"
    ],
    "send_line": [
      "self",
      "line"
    ]
  },
  "TestPolicy": {
    "__init__": [
      "self",
      "action_int"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "_KNOWN_PLAYERS": [],
  "_opt_print": [],
  "select_actions": [
    "observations",
    "cur_player"
  ],
  "print_iteration": [
    "time_step",
    "actions",
    "player_id"
  ],
  "turn_based_example": [
    "unused_arg"
  ],
  "_rollout_until_timeout": [
    "game_name",
    "time_limit",
    "give_up_after",
    "if_simultaneous_convert_to_turn_based"
  ],
  "_eval_agent": [
    "env",
    "agent",
    "num_episodes"
  ],
  "main_loop": [
    "unused_arg"
  ],
  "game": [],
  "NTupleNetwork": {
    "__init__": [
      "self",
      "n_tuple_size",
      "max_tuple_index",
      "n_tuples"
    ],
    "learn": [
      "self",
      "states"
    ],
    "update": [
      "self",
      "state",
      "adjust"
    ],
    "update_tuple": [
      "self",
      "idx",
      "n_tuple",
      "state",
      "adjust"
    ],
    "evaluator": [
      "self",
      "state",
      "action"
    ],
    "value": [
      "self",
      "state"
    ]
  },
  "_GAME_STRING": [],
  "GameStats": {
    "__str__": [
      "self"
    ]
  },
  "traverse_game_tree": [
    "game",
    "state",
    "game_stats"
  ],
  "eval_against_random_bots": [
    "env",
    "trained_agents",
    "random_agents",
    "num_episodes"
  ],
  "play_tic_tac_toe": [],
  "Instance": {
    "__init__": [
      "self",
      "pool",
      "p1values",
      "p2values"
    ],
    "__str__": [
      "self"
    ]
  },
  "Negotiation": {
    "__init__": [
      "self",
      "instance",
      "outcome",
      "rewards"
    ],
    "__str__": [
      "self"
    ]
  },
  "dialogue_matches_prev_line": [
    "line1",
    "line2"
  ],
  "parse_dataset": [
    "filename"
  ],
  "write_instances_file": [
    "negotiations",
    "filename"
  ],
  "compute_nbs_from_simulations": [
    "game",
    "num_games",
    "bots"
  ],
  "MaxBot": {
    "__init__": [
      "self"
    ],
    "step": [
      "self",
      "state"
    ]
  },
  "LoadAgent": [
    "agent_type",
    "game",
    "player_id",
    "rng"
  ],
  "eval_agent": [
    "env",
    "agent",
    "num_episodes"
  ],
  "universal_poker": [],
  "_ITERATIONS": [],
  "CUSTOM_LIMIT_HOLDEM_ACPC_GAMEDEF": [],
  "play_tarok_game": [],
  "print_info": [
    "unused_game",
    "state"
  ],
  "print_talon_exchange_info": [
    "state"
  ],
  "print_tricks_playing_info": [
    "state"
  ],
  "run_experiment": [
    "num_players",
    "env",
    "payoffs",
    "centralized"
  ],
  "NUM_ACTIONS": [],
  "MIN_ACTION": [],
  "net_fn": [
    "x"
  ],
  "load_model": [],
  "ai_action": [
    "state",
    "net",
    "params"
  ],
  "controller_factory": [],
  "eval_against_fixed_bots": [
    "env",
    "trained_agents",
    "fixed_agents",
    "num_episodes"
  ],
  "create_training_agents": [
    "num_players",
    "num_actions",
    "info_state_size",
    "hidden_layers_sizes"
  ],
  "FirstActionAgent": {
    "__init__": [
      "self",
      "player_id",
      "num_actions",
      "name"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ]
  },
  "RollingAverage": {
    "__init__": [
      "self",
      "size"
    ],
    "add": [
      "self",
      "value"
    ],
    "mean": [
      "self"
    ]
  },
  "_SAVE_PATH": [],
  "LLM_TYPE": [],
  "Domain": {
    "TRADE_FRUIT_W_TONE": [],
    "DEBATE_W_STYLE": [],
    "SCHEDULE_MEETING_W_DOW": [],
    "SCHEDULE_MEETING_W_TONE": []
  },
  "new_debate_scenario_config": [
    "config",
    "game_id"
  ],
  "same_scenario_config": [
    "config",
    "game_id"
  ],
  "get_config_debate": [
    "config"
  ],
  "get_config_trade_fruit_w_tone": [
    "config"
  ],
  "get_config_schedule_meeting_w_dow": [
    "config"
  ],
  "get_config_schedule_meeting_w_tone": [
    "config"
  ],
  "InfoStateRecord": {},
  "EqRecord": {},
  "record_info_state_data": [
    "state",
    "policy",
    "observer",
    "vectorize"
  ],
  "ImitationDatasetConstructor": {
    "__init__": [
      "self",
      "save_path",
      "config"
    ],
    "sample_to_dict": [
      "self",
      "info_state_string",
      "sample",
      "eq_record"
    ],
    "eval_vs_any": [
      "self",
      "game",
      "eq"
    ],
    "construct_dataset": [
      "self"
    ]
  },
  "ImitationDatasetConstructorReporting": {
    "__init__": [
      "self",
      "save_path",
      "experiment_name",
      "game_string",
      "game_id",
      "seed",
      "num_demos",
      "num_iters",
      "domain"
    ],
    "report": [
      "self",
      "demo",
      "results"
    ]
  },
  "setup_logging": [],
  "make_single_atari_env": [
    "gym_id",
    "seed",
    "idx",
    "capture_video",
    "run_name",
    "use_episodic_life_env"
  ],
  "make_single_env": [
    "game_name",
    "seed"
  ],
  "_PLAYER0_TYPE": [],
  "_PLAYER1_TYPE": [],
  "load_bot": [
    "bot_type",
    "pid"
  ],
  "play_game": [
    "state",
    "bots"
  ],
  "init_pg_responder": [
    "env"
  ],
  "init_br_responder": [
    "env"
  ],
  "init_dqn_responder": [
    "env"
  ],
  "print_policy_analysis": [
    "policies",
    "game",
    "verbose"
  ],
  "gpsro_looper": [
    "env",
    "oracle",
    "agents"
  ],
  "BotAgent": {
    "__init__": [
      "self",
      "num_actions",
      "bot",
      "name"
    ],
    "restart": [
      "self"
    ],
    "step": [
      "self",
      "time_step",
      "is_evaluation"
    ]
  },
  "eval_agents": [
    "env",
    "agents",
    "num_players",
    "num_episodes"
  ],
  "print_roshambo_bot_names_and_ids": [
    "roshambo_bot_names"
  ],
  "create_roshambo_bot_agent": [
    "player_id",
    "num_actions",
    "bot_names",
    "pop_id"
  ],
  "analyze_bot_table": [
    "filename"
  ],
  "create_epsilon_schedule": [
    "sched_str"
  ],
  "_zero_sum_node_decorator": [
    "state"
  ],
  "pretty_board": [
    "time_step"
  ],
  "command_line_action": [
    "time_step"
  ],
  "NFSPPolicies": {
    "__init__": [
      "self",
      "env",
      "nfsp_policies",
      "mode"
    ],
    "action_probabilities": [
      "self",
      "state",
      "player_id"
    ]
  },
  "random_policy": [
    "rnd",
    "state"
  ],
  "fixed_prompt_policy": [
    "rnd",
    "state",
    "prompt_action_dict"
  ],
  "mixed_prompt_policy": [
    "rnd",
    "state",
    "prompt_keys",
    "mixture"
  ],
  "build_player_policy": [
    "policies"
  ],
  "simulate_dialogue": [
    "game",
    "policy"
  ],
  "estimate_payoff_tensor": [
    "game",
    "rnd",
    "num_trials"
  ],
  "score_candidate_responses": [
    "game_str",
    "config",
    "load_dict",
    "rnd",
    "background_policies",
    "candidates",
    "player_ids",
    "num_trials"
  ],
  "compute_sym_eq": [
    "pt"
  ],
  "PSRO": {
    "__init__": [
      "self",
      "save_path",
      "config"
    ],
    "run": [
      "self"
    ]
  },
  "PSROReporting": {
    "__init__": [
      "self",
      "save_path",
      "experiment_name",
      "game_string",
      "seed",
      "num_iters",
      "num_trials",
      "num_candidates",
      "domain",
      "base_candidates"
    ],
    "report": [
      "self",
      "psro_iter",
      "payoff_tensor",
      "br",
      "mean_scores",
      "candidates",
      "eq"
    ]
  },
  "OptState": [],
  "Params": [],
  "NUM_CARDS": [],
  "NUM_PLAYERS": [],
  "TOP_K_ACTIONS": [],
  "DEFAULT_LAYER_SIZES": [],
  "_trajectory": [
    "line"
  ],
  "make_dataset": [
    "file"
  ],
  "batch": [
    "dataset",
    "batch_size"
  ],
  "one_hot": [
    "x",
    "k"
  ],
  "_no_play_trajectory": [
    "line"
  ],
  "marl_path_finding_example": [
    "_"
  ],
  "get_example_2x2_payoffs": [],
  "_manually_create_game": [],
  "_easy_create_game": [],
  "_even_easier_create_game": [],
  "_import_data_create_game": [],
  "_OPTIONAL_GAMES": [],
  "_AVAILABLE_GAMES": [],
  "_MISSING_GAMES": [],
  "_SHORTNAME": [],
  "_is_optional_game": [
    "basename"
  ],
  "_playthrough_match": [
    "filename",
    "regex"
  ],
  "_add_tests": [],
  "_ALL_GAMES": [],
  "_GAMES_TO_TEST": [],
  "_GAMES_NOT_UNDER_TEST": [],
  "_GAMES_TO_OMIT_LEGAL_ACTIONS_CHECK": [],
  "_GAMES_FULL_TREE_TRAVERSAL_TESTS": [],
  "_GAMES_FULL_TREE_TRAVERSAL_TESTS_NAMES": [],
  "TOTAL_NUM_STATES": [],
  "PERFECT_RECALL_NUM_STATES": [],
  "EnforceAPIOnFullTreeBase": {
    "setUpClass": [
      "cls"
    ],
    "test_legal_actions_empty": [
      "self"
    ],
    "test_number_of_nodes": [
      "self"
    ],
    "test_current_player_returns_terminal_player_on_terminal_nodes": [
      "self"
    ],
    "test_information_state_no_argument_raises_on_terminal_nodes": [
      "self"
    ],
    "test_game_is_perfect_recall": [
      "self"
    ],
    "test_constant_sum": [
      "self"
    ],
    "test_information_state_functions_raises_on_chance_nodes": [
      "self"
    ],
    "test_current_player_infosets_no_overlap_between_players": [
      "self"
    ]
  },
  "Relation": {
    "SUBSET_OR_EQUALS": [],
    "EQUALS": []
  },
  "EnforceAPIOnPartialTreeBase": {
    "setUpClass": [
      "cls"
    ],
    "test_sequence_lengths": [
      "self"
    ],
    "test_observations_raises_error_on_invalid_player": [
      "self"
    ],
    "test_legal_actions_returns_empty_list_on_opponent": [
      "self"
    ],
    "test_private_information_contents": [
      "self"
    ],
    "test_no_invalid_public_observations": [
      "self"
    ]
  },
  "_assert_properties_recursive": [
    "state",
    "assert_functions"
  ],
  "_assert_is_perfect_recall": [
    "game"
  ],
  "_assert_is_perfect_recall_recursive": [
    "state",
    "current_history",
    "infostate_player_to_history"
  ],
  "_create_test_case_classes": [],
  "load_tests": [
    "loader",
    "tests",
    "pattern"
  ]
}