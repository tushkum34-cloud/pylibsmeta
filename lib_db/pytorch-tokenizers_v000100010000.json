{
  "Llama2cTokenizer": {
    "__init__": [
      "self",
      "model_path"
    ],
    "encode": [
      "self",
      "s",
      "bos",
      "eos"
    ],
    "decode": [
      "self",
      "t"
    ],
    "decode_token": [
      "self",
      "t"
    ],
    "export": [
      "self",
      "output_path"
    ]
  },
  "logger": [],
  "TIKTOKEN_MAX_ENCODE_CHARS": [],
  "MAX_NO_WHITESPACES_CHARS": [],
  "_INSTANCE": [],
  "TiktokenTokenizer": {
    "get_instance": [
      "cls"
    ],
    "__init__": [
      "self",
      "model_path",
      "pat_str",
      "special_tokens"
    ],
    "encode": [
      "self",
      "s"
    ],
    "decode": [
      "self",
      "t"
    ],
    "decode_token": [
      "self",
      "t"
    ],
    "_split_whitespaces_or_nonwhitespaces": [
      "s",
      "max_consecutive_slice_len"
    ]
  },
  "__version__": [],
  "get_tokenizer": [
    "tokenizer_path",
    "tokenizer_config_path"
  ],
  "__all__": [],
  "HuggingFaceTokenizer": {
    "__init__": [
      "self",
      "model_path",
      "config_path"
    ],
    "encode": [
      "self",
      "s"
    ],
    "decode": [
      "self",
      "t"
    ],
    "decode_token": [
      "self",
      "t"
    ]
  },
  "CL100K_PAT_STR": [],
  "LLAMA_BASIC_SPECIAL_TOKENS": [],
  "LLAMA_NUM_RESERVED_SPECIAL_TOKENS": [],
  "LLAMA_RESERVED_SPECIAL_TOKENS": [],
  "LLAMA_SPECIAL_TOKENS": []
}