{
  "_old_torch_load_file": [],
  "_old_safe_open": [],
  "all_tensors_are_read_only": [],
  "mmm": [],
  "verboseLevel": [],
  "_map_to_dtype": [],
  "MmapTracker": {
    "__init__": [
      "self",
      "file_path"
    ],
    "register": [
      "self",
      "mmap_obj",
      "map_id",
      "start",
      "size"
    ],
    "get_active_maps": [
      "self"
    ]
  },
  "tensor_slice": {
    "catalog": [],
    "value": [],
    "name": [],
    "__init__": [
      "self",
      "catalog",
      "name",
      "value"
    ],
    "__getitem__": [
      "self",
      "s"
    ],
    "get_dtype": [
      "self"
    ],
    "get_shape": [
      "self"
    ]
  },
  "tensor_stub": {
    "dtype": [],
    "shape": [],
    "__init__": [
      "self",
      "dtype",
      "shape"
    ],
    "ndim": [
      "self"
    ],
    "numel": [
      "self"
    ],
    "device": [
      "self"
    ]
  },
  "cached_metadata": {
    "file_path": [],
    "file_length": [],
    "file_date": [],
    "catalog": [],
    "metadata": [],
    "skip_bytes": [],
    "__init__": [
      "self",
      "file_path",
      "catalog",
      "metadata",
      "skip_bytes"
    ],
    "get_metadata": [
      "self",
      "file_path"
    ]
  },
  "_cached_entry": [],
  "_parse_metadata": [
    "metadata"
  ],
  "_read_safetensors_header": [
    "path",
    "file"
  ],
  "load_metadata_state_dict": [
    "file_path"
  ],
  "torch_write_file": [
    "sd",
    "file_path",
    "quantization_map",
    "config",
    "extra_meta"
  ],
  "SafeTensorFile": {
    "__init__": [
      "self",
      "file_path",
      "metadata",
      "catalog",
      "skip_bytes",
      "lazy_loading",
      "writable_tensors",
      "device",
      "streaming"
    ],
    "load_metadata": [
      "cls",
      "file_path",
      "lazy_loading",
      "writable_tensors",
      "device",
      "streaming"
    ],
    "init_tensors": [
      "self",
      "lazyTensors",
      "writable_tensors"
    ],
    "create_tensors_with_mmap": [
      "self",
      "writable_tensors"
    ],
    "create_tensors_without_mmap": [
      "self"
    ],
    "_open_streaming_handle": [
      "self"
    ],
    "_close_streaming_handle": [
      "self"
    ],
    "_create_tensor_from_buffer": [
      "self",
      "shape",
      "dtype",
      "buffer"
    ],
    "create_tensor_with_streaming": [
      "self",
      "name"
    ],
    "reorder_for_streaming": [
      "self",
      "names"
    ],
    "get_slice": [
      "self",
      "name"
    ],
    "get_tensor": [
      "self",
      "name"
    ],
    "keys": [
      "self"
    ],
    "names": [
      "self"
    ],
    "tensors": [
      "self"
    ],
    "metadata": [
      "self"
    ],
    "__len__": [
      "self"
    ],
    "__contains__": [
      "self",
      "key"
    ],
    "__iter__": [
      "self"
    ],
    "_free_resources": [
      "self"
    ]
  },
  "_SafeTensorLoader": {
    "__init__": [
      "self",
      "filename",
      "writable_tensors",
      "device",
      "streaming"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_val",
      "exc_tb"
    ],
    "keys": [
      "self"
    ],
    "get_tensor": [
      "self",
      "name"
    ],
    "get_slice": [
      "self",
      "name"
    ],
    "close": [
      "self"
    ]
  },
  "safe_open": [
    "filename",
    "framework",
    "device",
    "writable_tensors",
    "streaming"
  ],
  "torch_load_file": [
    "filename",
    "device",
    "writable_tensors"
  ],
  "QEmbedding": {
    "bias": [],
    "qcreate": [
      "cls",
      "module",
      "weights",
      "activations",
      "optimizer",
      "device"
    ],
    "forward": [
      "self",
      "input"
    ]
  },
  "cudacontext": [
    "device"
  ],
  "shared_state": [],
  "get_cache": [
    "cache_name"
  ],
  "clear_caches": [],
  "default_verboseLevel": [],
  "ONE_MB": [],
  "sizeofhalffloat": [],
  "sizeofint8": [],
  "total_pinned_bytes": [],
  "max_pinnable_bytes": [],
  "physical_memory": [],
  "HEADER": [],
  "ENDC": [],
  "BOLD": [],
  "UNBOLD": [],
  "clock": {
    "__init__": [
      "self"
    ],
    "start": [
      "cls"
    ],
    "stop": [
      "self"
    ],
    "time_gap": [
      "self"
    ],
    "format_time_gap": [
      "self"
    ]
  },
  "move_tensors": [
    "obj",
    "device"
  ],
  "_get_module_name": [
    "v"
  ],
  "_compute_verbose_level": [
    "level"
  ],
  "_get_perc_reserved_mem_max": [
    "perc_reserved_mem_max"
  ],
  "_get_max_reservable_memory": [
    "perc_reserved_mem_max"
  ],
  "_detect_main_towers": [
    "model",
    "min_floors"
  ],
  "_get_model": [
    "model_path"
  ],
  "_remove_model_wrapper": [
    "model"
  ],
  "_move_to_pinned_tensor": [
    "source_tensor",
    "big_tensor",
    "offset",
    "length"
  ],
  "_safetensors_load_file": [
    "file_path",
    "writable_tensors"
  ],
  "_force_load_buffer": [
    "p"
  ],
  "_force_load_parameter": [
    "p"
  ],
  "_make_buffer": [
    "tensor"
  ],
  "_make_parameter": [
    "tensor",
    "requires_grad"
  ],
  "_unwrap_quantized_tensor": [
    "tensor"
  ],
  "_qtensor_get_quantized_subtensors": [
    "self"
  ],
  "_qtensor_set_quantized_subtensors": [
    "self",
    "sub_tensors"
  ],
  "_get_quantized_subtensors": [
    "p"
  ],
  "_set_quantized_subtensors": [
    "p",
    "sub_tensors"
  ],
  "_subtensors_nbytes": [
    "sub_tensors"
  ],
  "_subtensors_itemsize": [
    "sub_tensors",
    "fallback"
  ],
  "_get_tensor_ref": [
    "p"
  ],
  "BIG_TENSOR_MAX_SIZE": [],
  "BIG_TENSOR_MIN_SIZE": [],
  "RESERVED_RAM_MIN_AVAILABLE": [],
  "_extract_tie_weights_from_sd": [
    "sd",
    "sd_name",
    "verboseLevel"
  ],
  "_pin_sd_to_memory": [
    "sd",
    "sd_name",
    "tied_weights",
    "gig_tensor_size",
    "verboseLevel"
  ],
  "_pin_to_memory": [
    "model",
    "model_id",
    "partialPinning",
    "pinnedPEFTLora",
    "big_tensor_size",
    "perc_reserved_mem_max",
    "verboseLevel"
  ],
  "welcome_displayed": [],
  "_welcome": [],
  "change_dtype": [
    "model",
    "new_dtype",
    "exclude_buffers"
  ],
  "_extract_num_from_str": [
    "num_in_str"
  ],
  "_quantize_dirty_hack": [
    "model"
  ],
  "_quantization_map": [
    "model"
  ],
  "_set_module_by_name": [
    "parent_module",
    "name",
    "child_module"
  ],
  "_quantize_submodule": [
    "model",
    "name",
    "module",
    "weights",
    "activations",
    "optimizer"
  ],
  "_requantize": [
    "model",
    "state_dict",
    "quantization_map",
    "default_dtype"
  ],
  "_quantize": [
    "model_to_quantize",
    "weights",
    "verboseLevel",
    "threshold",
    "model_id"
  ],
  "load_loras_into_model": [
    "model",
    "lora_path",
    "lora_multi",
    "activate_all_loras",
    "check_only",
    "ignore_model_variations",
    "pinnedLora",
    "maxReservedLoras",
    "split_linear_modules_map",
    "preprocess_sd",
    "verboseLevel"
  ],
  "merge_dicts": [
    "A",
    "B"
  ],
  "sync_models_loras": [
    "model",
    "model2"
  ],
  "unload_loras_from_model": [
    "model"
  ],
  "set_step_no_for_lora": [
    "model",
    "step_no"
  ],
  "activate_loras": [
    "model",
    "lora_nos",
    "lora_multi"
  ],
  "move_loras_to_device": [
    "model",
    "device"
  ],
  "fast_load_transformers_model": [
    "model_path",
    "do_quantize",
    "quantizationType",
    "pinToMemory",
    "partialPinning",
    "forcedConfigPath",
    "defaultConfigPath",
    "modelClass",
    "modelPrefix",
    "writable_tensors",
    "verboseLevel",
    "preprocess_sd",
    "fused_split_map",
    "modules",
    "return_shared_modules",
    "default_dtype",
    "ignore_unused_weights",
    "configKwargs"
  ],
  "flush_torch_caches": [],
  "map_state_dict": [
    "state_dict",
    "rules"
  ],
  "filter_state_dict_basic": [
    "state_dict",
    "base_model_prefix",
    "keep_prefix"
  ],
  "load_sd": [
    "file_path",
    "filters",
    "keep_prefixes",
    "writable_tensors"
  ],
  "load_model_data": [
    "model",
    "file_path",
    "do_quantize",
    "quantizationType",
    "pinToMemory",
    "partialPinning",
    "modelPrefix",
    "writable_tensors",
    "preprocess_sd",
    "postprocess_sd",
    "fused_split_map",
    "modules",
    "return_shared_modules",
    "default_dtype",
    "ignore_unused_weights",
    "verboseLevel",
    "ignore_missing_keys"
  ],
  "save_model": [
    "model",
    "file_path",
    "do_quantize",
    "quantizationType",
    "verboseLevel",
    "config_file_path",
    "filter_sd"
  ],
  "extract_models": [
    "obj",
    "prefix"
  ],
  "get_model_name": [
    "model"
  ],
  "HfHook": {
    "__init__": [
      "self"
    ],
    "init_hook": [
      "self",
      "module"
    ],
    "detach_hook": [
      "self",
      "module"
    ]
  },
  "_mm_lora_linear_forward": [
    "module"
  ],
  "_mm_lora_generic_forward": [
    "module"
  ],
  "last_offload_obj": [],
  "offload": {
    "__init__": [
      "self"
    ],
    "add_module_to_blocks": [
      "self",
      "model_id",
      "blocks_name",
      "submodule",
      "prev_block_name",
      "submodule_name"
    ],
    "can_model_be_cotenant": [
      "self",
      "model_id"
    ],
    "_move_loras": [
      "self",
      "loras_active_adapters",
      "loras_modules",
      "to_GPU",
      "model"
    ],
    "gpu_load_blocks": [
      "self",
      "model_id",
      "blocks_name",
      "preload"
    ],
    "gpu_unload_blocks": [
      "self",
      "model_id",
      "blocks_name"
    ],
    "gpu_load": [
      "self",
      "model_id"
    ],
    "unload_all": [
      "self"
    ],
    "move_args_to_gpu": [
      "self",
      "dtype"
    ],
    "ready_to_check_mem": [
      "self"
    ],
    "empty_cache_if_needed": [
      "self"
    ],
    "any_param_or_buffer": [
      "self",
      "target_module"
    ],
    "_get_lora_scaling": [
      "self",
      "loras_scaling",
      "model",
      "active_adapter"
    ],
    "_lora_generic_forward": [
      "self",
      "model",
      "submodule",
      "loras_data",
      "func"
    ],
    "_dora_linear_forward": [
      "self",
      "model",
      "submodule",
      "adapters_data",
      "weight",
      "bias",
      "original_bias",
      "dora_mode"
    ],
    "_lokr_chunk_forward": [
      "self",
      "x_2d",
      "lokr_w1",
      "lokr_w2",
      "specs"
    ],
    "_lora_linear_forward": [
      "self",
      "model",
      "submodule",
      "loras_data",
      "x"
    ],
    "hook_lora": [
      "self",
      "submodule",
      "current_model",
      "model_id",
      "loras_model_data",
      "loras_model_shortcuts",
      "submodule_name"
    ],
    "ensure_model_loaded": [
      "self",
      "model_id"
    ],
    "hook_preload_blocks_for_compilation": [
      "self",
      "target_module",
      "model_id",
      "blocks_name",
      "context"
    ],
    "_pre_check": [
      "self",
      "module"
    ],
    "_get_wrapper_for_type": [
      "self",
      "mod_cls"
    ],
    "hook_check_load_into_GPU_if_needed": [
      "self",
      "target_module",
      "model",
      "model_id",
      "blocks_name",
      "previous_method",
      "context"
    ],
    "hook_check_load_into_GPU_if_needed_default": [
      "self",
      "target_module",
      "model",
      "model_id",
      "blocks_name",
      "previous_method",
      "context"
    ],
    "hook_change_module": [
      "self",
      "target_module",
      "model",
      "model_id",
      "module_id",
      "previous_method",
      "previous_method_name"
    ],
    "tune_preloading": [
      "self",
      "model_id",
      "current_budget",
      "towers_names"
    ],
    "release": [
      "self"
    ]
  },
  "all": [
    "pipe_or_dict_of_modules",
    "pinnedMemory",
    "pinnedPEFTLora",
    "partialPinning",
    "loras",
    "quantizeTransformer",
    "extraModelsToQuantize",
    "quantizationType",
    "budgets",
    "workingVRAM",
    "asyncTransfers",
    "compile",
    "convertWeightsFloatTo",
    "perc_reserved_mem_max",
    "coTenantsMap",
    "vram_safety_coefficient",
    "compile_mode",
    "verboseLevel"
  ],
  "profile": [
    "pipe_or_dict_of_modules",
    "profile_no",
    "verboseLevel"
  ],
  "_QTYPE_QMODULE_CACHE": [],
  "_QMODULE_BASE_ATTRS": [],
  "_DEFAULT_KIND_PRIORITIES": [],
  "_extract_qtypes": [
    "handler"
  ],
  "_extract_qmodule_classes": [
    "handler"
  ],
  "_build_qmodule_cache": [],
  "_get_qmodule_base_attrs": [],
  "_get_qmodule_for_qtype": [
    "qtype_obj"
  ],
  "_load_with_qmodule": [
    "module",
    "qmodule_cls",
    "state_dict",
    "prefix",
    "local_metadata",
    "strict",
    "missing_keys",
    "unexpected_keys",
    "error_msgs"
  ],
  "QLinearQuantoRouter": {
    "qcreate": [
      "cls",
      "module",
      "weights",
      "activations",
      "optimizer",
      "device"
    ],
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "bias",
      "device",
      "dtype",
      "weights",
      "activations",
      "optimizer",
      "quantize_input"
    ],
    "set_default_dtype": [
      "self",
      "dtype"
    ],
    "forward": [
      "self",
      "input"
    ],
    "_load_from_state_dict": [
      "self",
      "state_dict",
      "prefix",
      "local_metadata",
      "strict",
      "missing_keys",
      "unexpected_keys",
      "error_msgs"
    ]
  },
  "_FP8_QUANTO_BRIDGE_MODULE": [],
  "_HANDLER_MODULES": [],
  "_HANDLER_OBJECTS": [],
  "register_handler": [
    "handler"
  ],
  "unregister_handler": [
    "handler"
  ],
  "_load_handlers": [],
  "_handler_name": [
    "handler"
  ],
  "_FILE_EXTENSION_HANDLERS": [],
  "register_file_extension": [
    "extension",
    "handler"
  ],
  "get_extension_handler": [
    "file_path"
  ],
  "normalize_extension_path": [
    "file_path"
  ],
  "_normalize_kind_key": [
    "value"
  ],
  "_priority_for_kind": [
    "kind"
  ],
  "_get_handler_priority": [
    "handler"
  ],
  "_select_primary_kind": [
    "names",
    "priority_map"
  ],
  "_merge_quant_maps": [
    "target",
    "source"
  ],
  "_as_field_tuple": [
    "value"
  ],
  "_get_split_handler": [
    "info",
    "field",
    "default_handlers"
  ],
  "_get_split_base_fields": [
    "info",
    "split_fields"
  ],
  "_merge_share_fields": [
    "info",
    "share_fields"
  ],
  "_call_split_handler": [
    "handler"
  ],
  "_fill_sub_maps": [
    "sub_maps",
    "name",
    "value"
  ],
  "sd_split_linear": [
    "state_dict",
    "split_map",
    "split_fields",
    "share_fields",
    "verboseLevel",
    "split_handlers",
    "allowed_bases",
    "return_split_bases"
  ],
  "split_linear_modules": [
    "model",
    "map",
    "split_handlers",
    "share_fields"
  ],
  "detect_safetensors_format": [
    "state_dict",
    "verboseLevel"
  ],
  "detect_and_convert": [
    "state_dict",
    "default_dtype",
    "verboseLevel"
  ],
  "get_available_qtypes": [],
  "get_available_qtype_aliases": [],
  "get_quantization_tokens": [
    "quantization"
  ],
  "get_quantization_label": [
    "quantization"
  ],
  "_quantization_filename_cache": [],
  "_normalize_quant_file_key": [
    "file_path"
  ],
  "get_cached_quantization_for_file": [
    "file_path"
  ],
  "cache_quantization_for_file": [
    "file_path",
    "kind"
  ],
  "_detect_kind_from_handlers": [
    "file_path",
    "verboseLevel"
  ],
  "_detect_label_from_handlers": [
    "file_path",
    "verboseLevel"
  ],
  "_infer_qtype_from_quantization_map": [
    "quantization_map"
  ],
  "detect_quantization_kind_for_file": [
    "file_path",
    "verboseLevel"
  ],
  "detect_quantization_label_from_filename": [
    "filename"
  ],
  "_get_qtype_name_from_quant_map": [
    "entry"
  ],
  "_build_qtype_handler_map": [],
  "_collect_fused_bases": [
    "state_dict",
    "fused_split_map"
  ],
  "_remap_quantization_entries": [
    "quantization_map",
    "base",
    "targets"
  ],
  "split_fused_weights": [
    "state_dict",
    "quantization_map",
    "fused_split_map",
    "default_dtype",
    "verboseLevel"
  ],
  "apply_pre_quantization": [
    "model",
    "state_dict",
    "quantization_map",
    "default_dtype",
    "verboseLevel"
  ],
  "_patch_marlin_fp8_bias": [],
  "DATA_SUFFIX": [],
  "SCALE_SUFFIX": [],
  "IN_SCALE": [],
  "OUT_SCALE": [],
  "_QTYPE_NAME": [],
  "_SCALE_META_KEYS": [],
  "_DTYPE_ALIASES": [],
  "register_fp8_weight_debug_name": [
    "weight",
    "name"
  ],
  "get_fp8_weight_debug_name": [
    "weight"
  ],
  "_is_weight_key": [
    "k"
  ],
  "Accessor": {
    "keys": [
      "self"
    ],
    "get_tensor": [
      "self",
      "key"
    ],
    "metadata": [
      "self"
    ],
    "has": [
      "self",
      "key"
    ],
    "can_delete": [
      "self"
    ],
    "delete": [
      "self",
      "key"
    ]
  },
  "FileAccessor": {
    "__init__": [
      "self",
      "path"
    ],
    "keys": [
      "self"
    ],
    "has": [
      "self",
      "key"
    ],
    "get_tensor": [
      "self",
      "key"
    ],
    "metadata": [
      "self"
    ],
    "close": [
      "self"
    ]
  },
  "DictAccessor": {
    "__init__": [
      "self",
      "sd",
      "meta",
      "in_place",
      "free_cuda_cache",
      "cuda_cache_interval"
    ],
    "keys": [
      "self"
    ],
    "has": [
      "self",
      "key"
    ],
    "get_tensor": [
      "self",
      "key"
    ],
    "metadata": [
      "self"
    ],
    "can_delete": [
      "self"
    ],
    "delete": [
      "self",
      "key"
    ]
  },
  "_as_accessor": [
    "src"
  ],
  "_normalize_scale_dtype": [
    "scale_dtype"
  ],
  "_json_to_dict": [
    "s"
  ],
  "_maybe_parse_scale_map": [
    "meta"
  ],
  "_quick_fp8_variant_from_sentinel": [
    "acc"
  ],
  "_per_channel_reshape": [
    "vec",
    "weight"
  ],
  "ConvertResult": {
    "state_dict": [
      "self"
    ],
    "quant_map": [
      "self"
    ],
    "fp8_format": [
      "self"
    ],
    "patch_needed": [
      "self"
    ]
  },
  "_preferred_fp8_scale_dtype": [],
  "convert_scaled_fp8_to_quanto": [
    "src"
  ],
  "detect": [
    "state_dict",
    "verboseLevel"
  ],
  "convert_to_quanto": [
    "state_dict",
    "default_dtype",
    "verboseLevel",
    "detection"
  ],
  "_patch_state": [],
  "_fp8_kernel_patch_state": [],
  "enable_fp8_fp32_scale_support": [],
  "disable_fp8_fp32_scale_support": [],
  "_quant_map_has_fp8": [
    "quant_map"
  ],
  "enable_fp8_marlin_fallback": [],
  "disable_fp8_marlin_fallback": [],
  "maybe_enable_fp8_marlin_fallback": [
    "quantization_map"
  ],
  "enable_fp8_marlin_workspace_fix": [],
  "disable_fp8_marlin_workspace_fix": [],
  "_cli": [],
  "profile_type": {
    "tostr": [
      "v"
    ],
    "HighRAM_HighVRAM": [],
    "HighRAM_LowVRAM": [],
    "LowRAM_HighVRAM": [],
    "LowRAM_LowVRAM": [],
    "VerylowRAM_LowVRAM": []
  }
}