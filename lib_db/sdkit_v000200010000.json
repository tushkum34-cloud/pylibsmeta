{
  "Context": {
    "__init__": [
      "self"
    ],
    "device": [
      "self",
      "d"
    ],
    "torch_device": [
      "self"
    ],
    "half_precision": [
      "self",
      "h"
    ],
    "vram_usage_level": [
      "self",
      "level"
    ]
  },
  "SUPPORTED_DEVICES": [],
  "main": [],
  "load_tensor_file": [
    "path"
  ],
  "save_tensor_file": [
    "data",
    "path"
  ],
  "save_images": [
    "images",
    "dir_path",
    "file_name",
    "output_format",
    "output_quality",
    "output_lossless"
  ],
  "save_dicts": [
    "entries",
    "dir_path",
    "file_name",
    "output_format",
    "file_format"
  ],
  "tensor_ids_snapshot": [],
  "recorded_tensor_names": [],
  "gc": [
    "context"
  ],
  "get_device_usage": [
    "device",
    "log_info",
    "process_usage_only",
    "log_prefix"
  ],
  "get_vram_usage_slow": [],
  "get_object_id": [
    "o"
  ],
  "record_tensor_name": [
    "t",
    "name",
    "log_info"
  ],
  "print_tensor_info": [
    "t",
    "name"
  ],
  "get_tensors_in_memory": [
    "device"
  ],
  "print_largest_tensors_in_memory": [
    "device",
    "num"
  ],
  "take_memory_snapshot": [
    "device",
    "print_snapshot"
  ],
  "_get_tensor_entries": [
    "device",
    "sorted_by_size"
  ],
  "_fmt_tensors_summary": [
    "entries"
  ],
  "download_file": [
    "url",
    "out_path"
  ],
  "log": [],
  "LOG_FORMAT": [],
  "convert_pipeline_unet_to_onnx": [
    "pipeline",
    "save_path",
    "opset",
    "device",
    "fp16"
  ],
  "convert_pipeline_vae_to_onnx": [
    "pipeline",
    "save_path",
    "opset",
    "device",
    "fp16"
  ],
  "_convert_pipeline_model_to_onnx": [
    "pipeline",
    "model",
    "model_args",
    "input_names",
    "dynamic_axes",
    "use_external_data_format",
    "save_path",
    "opset",
    "device",
    "fp16"
  ],
  "onnx_export": [
    "model",
    "model_args",
    "output_path",
    "ordered_input_names",
    "output_names",
    "dynamic_axes",
    "opset",
    "use_external_data_format"
  ],
  "convert_onnx_unet_to_tensorrt": [
    "pipeline",
    "onnx_path",
    "trt_out_dir",
    "batch_size_range",
    "dimensions_range"
  ],
  "convert_onnx_vae_to_tensorrt": [
    "pipeline",
    "onnx_path",
    "trt_out_dir",
    "batch_size_range",
    "dimensions_range"
  ],
  "_convert_onnx_to_tensorrt": [
    "onnx_path",
    "trt_out_dir",
    "shape_fn",
    "name",
    "batch_size_range",
    "dimensions_range"
  ],
  "convert_pipeline_to_onnx": [
    "pipeline",
    "save_path",
    "opset",
    "device",
    "fp16"
  ],
  "convert_pipeline_to_tensorrt": [
    "pipeline",
    "trt_dir_path",
    "batch_size_range",
    "dimensions_range",
    "opset",
    "fp16"
  ],
  "hash_url_quick": [
    "url"
  ],
  "hash_file_quick": [
    "file_path"
  ],
  "compute_quick_hash": [
    "total_size_fn",
    "read_bytes_fn"
  ],
  "hash_bytes": [
    "bytes"
  ],
  "to_tensor": [
    "x",
    "device",
    "dtype"
  ],
  "img_to_tensor": [
    "img",
    "batch_size",
    "device",
    "half_precision",
    "shift_range",
    "unsqueeze"
  ],
  "get_image_latent_and_mask": [
    "context",
    "image",
    "mask",
    "desired_width",
    "desired_height",
    "batch_size"
  ],
  "latent_samples_to_images": [
    "context",
    "samples"
  ],
  "diffusers_latent_samples_to_images": [
    "context",
    "latent_samples"
  ],
  "tensor_to_bitmap": [
    "tensor"
  ],
  "NVIDIA_RE": [],
  "NVIDIA_HALF_PRECISION_BUG_RE": [],
  "AMD_HALF_PRECISION_BUG_RE": [],
  "has_amd_gpu": [],
  "mem_get_info": [
    "device"
  ],
  "memory_allocated": [
    "device"
  ],
  "memory_stats": [
    "device"
  ],
  "empty_cache": [],
  "ipc_collect": [],
  "is_cpu_device": [
    "device"
  ],
  "has_half_precision_bug": [
    "device_name"
  ],
  "make_sd_context": [
    "model_path",
    "vram_usage_level"
  ],
  "get_nested_attr": [
    "o",
    "key"
  ],
  "img_to_base64_str": [
    "img",
    "output_format",
    "output_quality",
    "output_lossless"
  ],
  "img_to_buffer": [
    "img",
    "output_format",
    "output_quality",
    "output_lossless"
  ],
  "buffer_to_base64_str": [
    "buffered",
    "output_format"
  ],
  "base64_str_to_buffer": [
    "img_str"
  ],
  "base64_str_to_img": [
    "img_str"
  ],
  "resize_img": [
    "img",
    "desired_width",
    "desired_height",
    "clamp_to_8"
  ],
  "apply_color_profile": [
    "orig_image",
    "image_to_modify"
  ],
  "black_to_transparent": [
    "img"
  ],
  "get_image": [
    "img"
  ],
  "Filter": {},
  "_get_module": [
    "model_type"
  ],
  "apply_filters": [
    "context",
    "filters",
    "images"
  ],
  "apply_filter_single_image": [
    "context",
    "filters",
    "image"
  ],
  "get_filter_module": [
    "filter_type"
  ],
  "gfpgan_temp_device_lock": [],
  "apply": [
    "context",
    "image"
  ],
  "dlib_model_url": [],
  "get_largest_face": [
    "det_faces",
    "h",
    "w"
  ],
  "get_center_face": [
    "det_faces",
    "h",
    "w",
    "center"
  ],
  "FaceRestoreHelper": {
    "__init__": [
      "self",
      "upscale_factor",
      "face_size",
      "crop_ratio",
      "det_model",
      "save_ext",
      "template_3points",
      "pad_blur",
      "use_parse",
      "device"
    ],
    "set_upscale_factor": [
      "self",
      "upscale_factor"
    ],
    "read_image": [
      "self",
      "img"
    ],
    "init_dlib": [
      "self",
      "detection_path",
      "landmark5_path"
    ],
    "get_face_landmarks_5_dlib": [
      "self",
      "only_keep_largest",
      "scale"
    ],
    "get_face_landmarks_5": [
      "self",
      "only_keep_largest",
      "only_center_face",
      "resize",
      "blur_ratio",
      "eye_dist_threshold"
    ],
    "align_warp_face": [
      "self",
      "save_cropped_path",
      "border_mode"
    ],
    "get_inverse_affine": [
      "self",
      "save_inverse_affine_path"
    ],
    "add_restored_face": [
      "self",
      "restored_face",
      "input_face"
    ],
    "paste_faces_to_input_image": [
      "self",
      "save_path",
      "upsample_img",
      "draw_box",
      "face_upsampler"
    ],
    "clean_all": [
      "self"
    ]
  },
  "codeformer_temp_device_lock": [],
  "inference": [
    "context",
    "image",
    "upscale_bg",
    "upscale_faces",
    "upscale_factor",
    "codeformer_fidelity",
    "codeformer_net"
  ],
  "ROOT_DIR": [],
  "download_pretrained_models": [
    "file_ids",
    "save_path_root"
  ],
  "imwrite": [
    "img",
    "file_path",
    "params",
    "auto_mkdir"
  ],
  "img2tensor": [
    "imgs",
    "bgr2rgb",
    "float32"
  ],
  "load_file_from_url": [
    "url",
    "model_dir",
    "progress",
    "file_name"
  ],
  "scandir": [
    "dir_path",
    "suffix",
    "recursive",
    "full_path"
  ],
  "is_gray": [
    "img",
    "threshold"
  ],
  "rgb2gray": [
    "img",
    "out_channel"
  ],
  "bgr2gray": [
    "img",
    "out_channel"
  ],
  "calc_mean_std": [
    "feat",
    "eps"
  ],
  "adain_npy": [
    "content_feat",
    "style_feat"
  ],
  "merge_models": [
    "model0_path",
    "model1_path",
    "ratio",
    "out_path",
    "use_fp16"
  ],
  "merge_two_models": [
    "model0",
    "model1",
    "alpha",
    "use_fp16"
  ],
  "merge_multiple_models": [
    "models",
    "alphas",
    "key_name_pattern",
    "normalize"
  ],
  "get_cond_and_uncond": [
    "prompt",
    "negative_prompt",
    "batch_size",
    "model"
  ],
  "parse_prompt": [
    "prompt",
    "batch_size",
    "model"
  ],
  "split_weighted_subprompts": [
    "text"
  ],
  "generate_images": [
    "context",
    "prompt",
    "negative_prompt",
    "seed",
    "width",
    "height",
    "num_outputs",
    "num_inference_steps",
    "guidance_scale",
    "init_image",
    "init_image_mask",
    "control_image",
    "control_alpha",
    "prompt_strength",
    "preserve_init_image_color_profile",
    "strict_mask_border",
    "sampler_name",
    "hypernetwork_strength",
    "tiling",
    "lora_alpha",
    "sampler_params",
    "callback"
  ],
  "txt2img": [
    "params",
    "context",
    "num_inference_steps"
  ],
  "img2img": [
    "params",
    "context",
    "num_inference_steps",
    "num_outputs",
    "width",
    "height",
    "init_image",
    "init_image_mask",
    "prompt_strength",
    "preserve_init_image_color_profile",
    "strict_mask_border"
  ],
  "make_with_diffusers": [
    "context",
    "prompt",
    "negative_prompt",
    "seed",
    "width",
    "height",
    "num_outputs",
    "num_inference_steps",
    "guidance_scale",
    "init_image",
    "init_image_mask",
    "control_image",
    "control_alpha",
    "prompt_strength",
    "sampler_name",
    "lora_alpha",
    "tiling",
    "strict_mask_border",
    "callback"
  ],
  "assert_controlnet_model": [
    "controlnet",
    "sd_context_dim"
  ],
  "get_sd_type_from_dim": [
    "dim"
  ],
  "blend_mask": [
    "images",
    "init_image",
    "init_image_mask",
    "width",
    "height"
  ],
  "samplers": [],
  "sample": [
    "context",
    "sampler_name",
    "noise",
    "batch_size",
    "shape",
    "steps",
    "cond",
    "uncond",
    "guidance_scale",
    "callback"
  ],
  "DenoiserWrap": {
    "__init__": [
      "self",
      "model"
    ],
    "forward": [
      "self",
      "x",
      "sigma",
      "uncond",
      "cond",
      "guidance_scale"
    ]
  },
  "_make_plms": [
    "ddim_scheduler_config"
  ],
  "_make_unipc": [
    "solver_type",
    "solver_order",
    "time_skip"
  ],
  "_samplers_init": [],
  "make_sampler": [
    "sampler_name",
    "ddim_scheduler_config"
  ],
  "make_samples": [
    "context",
    "sampler_name",
    "seed",
    "batch_size",
    "shape",
    "steps",
    "cond",
    "uncond",
    "guidance_scale",
    "callback"
  ],
  "make_some_noise": [
    "seed",
    "batch_size",
    "shape",
    "device"
  ],
  "_sample_txt2img": [
    "model",
    "sampler_name",
    "noise",
    "steps",
    "batch_size",
    "params"
  ],
  "_sample_img2img": [
    "model",
    "sampler_name",
    "noise",
    "steps",
    "batch_size",
    "params"
  ],
  "register_buffer_mps_aware": [
    "self",
    "name",
    "attr"
  ],
  "NoiseScheduleVP": {
    "__init__": [
      "self",
      "schedule",
      "betas",
      "alphas_cumprod",
      "continuous_beta_0",
      "continuous_beta_1"
    ],
    "marginal_log_mean_coeff": [
      "self",
      "t"
    ],
    "marginal_alpha": [
      "self",
      "t"
    ],
    "marginal_std": [
      "self",
      "t"
    ],
    "marginal_lambda": [
      "self",
      "t"
    ],
    "inverse_lambda": [
      "self",
      "lamb"
    ]
  },
  "model_wrapper": [
    "model",
    "noise_schedule",
    "model_type",
    "model_kwargs",
    "guidance_type",
    "condition",
    "unconditional_condition",
    "guidance_scale",
    "classifier_fn",
    "classifier_kwargs"
  ],
  "UniPC": {
    "__init__": [
      "self",
      "model_fn",
      "noise_schedule",
      "predict_x0",
      "thresholding",
      "max_val",
      "variant"
    ],
    "dynamic_thresholding_fn": [
      "self",
      "x0",
      "t"
    ],
    "noise_prediction_fn": [
      "self",
      "x",
      "t"
    ],
    "data_prediction_fn": [
      "self",
      "x",
      "t"
    ],
    "model_fn": [
      "self",
      "x",
      "t"
    ],
    "get_time_steps": [
      "self",
      "skip_type",
      "t_T",
      "t_0",
      "N",
      "device"
    ],
    "get_orders_and_timesteps_for_singlestep_solver": [
      "self",
      "steps",
      "order",
      "skip_type",
      "t_T",
      "t_0",
      "device"
    ],
    "denoise_to_zero_fn": [
      "self",
      "x",
      "s"
    ],
    "multistep_uni_pc_update": [
      "self",
      "x",
      "model_prev_list",
      "t_prev_list",
      "t",
      "order"
    ],
    "multistep_uni_pc_vary_update": [
      "self",
      "x",
      "model_prev_list",
      "t_prev_list",
      "t",
      "order",
      "use_corrector"
    ],
    "multistep_uni_pc_bh_update": [
      "self",
      "x",
      "model_prev_list",
      "t_prev_list",
      "t",
      "order",
      "x_t",
      "use_corrector"
    ],
    "sample": [
      "self",
      "x",
      "steps",
      "t_start",
      "t_end",
      "order",
      "skip_type",
      "method",
      "lower_order_final",
      "denoise_to_zero",
      "solver_type",
      "atol",
      "rtol",
      "corrector",
      "callback",
      "img_callback"
    ]
  },
  "interpolate_fn": [
    "x",
    "xp",
    "yp"
  ],
  "expand_dims": [
    "v",
    "dims"
  ],
  "UniPCSampler": {
    "__init__": [
      "self",
      "model"
    ],
    "register_buffer": [
      "self",
      "name",
      "attr"
    ],
    "sample": [
      "self",
      "S",
      "batch_size",
      "shape",
      "conditioning",
      "callback",
      "normals_sequence",
      "img_callback",
      "quantize_x0",
      "eta",
      "mask",
      "x0",
      "temperature",
      "noise_dropout",
      "score_corrector",
      "corrector_kwargs",
      "verbose",
      "x_T",
      "log_every_t",
      "unconditional_guidance_scale",
      "unconditional_conditioning",
      "variant",
      "time_skip",
      "order",
      "lower_order_final",
      "thresholding"
    ]
  },
  "download_models": [
    "models",
    "download_base_dir",
    "subdir_for_model_type",
    "download_config_if_available"
  ],
  "download_model": [
    "model_type",
    "model_id",
    "download_base_dir",
    "subdir_for_model_type",
    "download_config_if_available"
  ],
  "resolve_downloaded_model_path": [
    "model_type",
    "model_id",
    "download_base_dir",
    "subdir_for_model_type"
  ],
  "get_actual_base_dir": [
    "model_type",
    "download_base_dir",
    "subdir_for_model_type"
  ],
  "get_url_and_filename": [
    "model_type",
    "model_id",
    "url_key"
  ],
  "scan_model": [
    "file_path"
  ],
  "LoraBlock": {
    "rank": [
      "self"
    ],
    "apply": [
      "self",
      "alpha"
    ],
    "_get_weight": [
      "self"
    ]
  },
  "load_model": [
    "context"
  ],
  "get_lora_type": [
    "lora"
  ],
  "load_lora": [
    "pipe",
    "lora",
    "sd_type",
    "lora_path"
  ],
  "move_model_to_cpu": [
    "context"
  ],
  "unload_model": [
    "context"
  ],
  "apply_lora_model": [
    "context",
    "alphas"
  ],
  "_name": [
    "key",
    "unet_layers_per_block"
  ],
  "EMBEDDING_TYPES": [],
  "get_embeddings_to_load": [
    "pipe",
    "embeddings_paths",
    "sd_type"
  ],
  "load_embeddings": [
    "embeddings",
    "embedding_tokens",
    "components"
  ],
  "remove_hooks": [
    "components"
  ],
  "attach_hooks": [
    "context",
    "components"
  ],
  "get_embedding": [
    "embedding"
  ],
  "get_embedding_token": [
    "filename"
  ],
  "apply_compel_pooled_patch": [],
  "Loader": {},
  "model_load_lock": [],
  "get_loader_module": [
    "model_type"
  ],
  "load_controlnet": [
    "context",
    "controlnet_path"
  ],
  "filters": [],
  "make_load_model": [
    "model_type"
  ],
  "make_unload_model": [
    "model_type"
  ],
  "_set_vae": [
    "context",
    "vae"
  ],
  "_get_base_model_vae": [
    "context"
  ],
  "HypernetworkModule": {
    "multiplier": [],
    "activation_dict": [],
    "__init__": [
      "self",
      "dim",
      "state_dict",
      "layer_structure",
      "activation_func",
      "weight_init",
      "add_layer_norm",
      "use_dropout",
      "activate_output",
      "last_layer_dropout",
      "model",
      "device"
    ],
    "fix_old_state_dict": [
      "self",
      "state_dict"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "apply_hypernetwork": [
    "hypernetwork",
    "attention_context",
    "layer"
  ],
  "override_attention_context_kv": [
    "hypernetwork_model"
  ],
  "adaptive_instance_normalization": [
    "content_feat",
    "style_feat"
  ],
  "PositionEmbeddingSine": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "forward": [
      "self",
      "x",
      "mask"
    ]
  },
  "_get_activation_fn": [
    "activation"
  ],
  "TransformerSALayer": {
    "__init__": [
      "self",
      "embed_dim",
      "nhead",
      "dim_mlp",
      "dropout",
      "activation"
    ],
    "with_pos_embed": [
      "self",
      "tensor",
      "pos"
    ],
    "forward": [
      "self",
      "tgt",
      "tgt_mask",
      "tgt_key_padding_mask",
      "query_pos"
    ]
  },
  "Fuse_sft_block": {
    "__init__": [
      "self",
      "in_ch",
      "out_ch"
    ],
    "forward": [
      "self",
      "enc_feat",
      "dec_feat",
      "w"
    ]
  },
  "CodeFormer": {
    "__init__": [
      "self",
      "dim_embd",
      "n_head",
      "n_layers",
      "codebook_size",
      "latent_size",
      "connect_list",
      "fix_modules"
    ],
    "_init_weights": [
      "self",
      "module"
    ],
    "forward": [
      "self",
      "x",
      "w",
      "detach_16",
      "code_only",
      "adain"
    ]
  },
  "normalize": [
    "in_channels"
  ],
  "swish": [
    "x"
  ],
  "VectorQuantizer": {
    "__init__": [
      "self",
      "codebook_size",
      "emb_dim",
      "beta"
    ],
    "forward": [
      "self",
      "z"
    ],
    "get_codebook_feat": [
      "self",
      "indices",
      "shape"
    ]
  },
  "GumbelQuantizer": {
    "__init__": [
      "self",
      "codebook_size",
      "emb_dim",
      "num_hiddens",
      "straight_through",
      "kl_weight",
      "temp_init"
    ],
    "forward": [
      "self",
      "z"
    ]
  },
  "Downsample": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Upsample": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ResBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels"
    ],
    "forward": [
      "self",
      "x_in"
    ]
  },
  "AttnBlock": {
    "__init__": [
      "self",
      "in_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Encoder": {
    "__init__": [
      "self",
      "in_channels",
      "nf",
      "emb_dim",
      "ch_mult",
      "num_res_blocks",
      "resolution",
      "attn_resolutions"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Generator": {
    "__init__": [
      "self",
      "nf",
      "emb_dim",
      "ch_mult",
      "res_blocks",
      "img_size",
      "attn_resolutions"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VQAutoEncoder": {
    "__init__": [
      "self",
      "img_size",
      "nf",
      "ch_mult",
      "quantizer",
      "res_blocks",
      "attn_resolutions",
      "codebook_size",
      "emb_dim",
      "beta",
      "gumbel_straight_through",
      "gumbel_kl_weight",
      "model_path"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "VQGANDiscriminator": {
    "__init__": [
      "self",
      "nc",
      "ndf",
      "n_layers",
      "model_path"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "load_diffusers_model": [
    "context",
    "state_dict",
    "model_path",
    "config_file_path",
    "clip_skip",
    "convert_to_tensorrt",
    "trt_build_config"
  ],
  "test_and_fix_precision": [
    "context",
    "model",
    "config",
    "attn_precision"
  ],
  "get_model_config_file": [
    "context",
    "check_for_config_with_same_name"
  ],
  "resolve_model_config_file_path": [
    "model_info",
    "model_path"
  ],
  "is_lora": [
    "sd"
  ],
  "check_and_fix_nai_keys": [
    "state_dict"
  ],
  "SIZE_SPAN": [],
  "apply_directml_unet": [
    "pipeline",
    "onnx_path"
  ],
  "apply_tensorrt": [
    "pipeline",
    "trt_dir"
  ],
  "UnetDirectML": {
    "__init__": [
      "self",
      "onnx_path"
    ],
    "forward": [
      "self",
      "sample",
      "timestep",
      "encoder_hidden_states"
    ]
  },
  "TRTModel": {
    "ENGINE_TYPES": [],
    "__init__": [
      "self",
      "pipeline",
      "trt_dir"
    ],
    "load_engine": [
      "self",
      "engine_type",
      "engine_info"
    ],
    "get_engine_info_for_request": [
      "self",
      "engine_type",
      "batch_size",
      "width",
      "height"
    ],
    "allocate_buffers": [
      "self",
      "pipeline",
      "device",
      "dtype",
      "batch_size",
      "width",
      "height"
    ],
    "_allocate_buffers": [
      "self",
      "engine_type",
      "pipeline",
      "device",
      "dtype",
      "batch_size",
      "width",
      "height",
      "engine"
    ],
    "forward_unet": [
      "self",
      "sample",
      "timestep",
      "encoder_hidden_states"
    ],
    "forward_vae": [
      "self",
      "sample"
    ],
    "_forward": [
      "self",
      "engine_type",
      "feed_dict"
    ]
  },
  "send_to_device": [
    "context",
    "model"
  ],
  "get_context_kv": [
    "attention_context"
  ],
  "make_attn_forward": [
    "context",
    "attn_precision"
  ],
  "print_model_size_breakdown": [
    "model"
  ],
  "db": [],
  "index": [],
  "get_models_db": [],
  "get_model_info_from_db": [
    "quick_hash",
    "model_type",
    "model_id"
  ],
  "rebuild_index": []
}