{
  "BCELoss": {
    "__init__": [
      "self",
      "label_smooth",
      "eps",
      "reduction"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "SoftF1Loss": {
    "__init__": [
      "self",
      "beta",
      "eps"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "soft_dice_score": [
    "output",
    "target",
    "label_smooth",
    "eps",
    "dims"
  ],
  "DiceLoss": {
    "__init__": [
      "self",
      "mode",
      "classes",
      "log_loss",
      "from_logits",
      "label_smooth",
      "ignore_index",
      "eps"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ],
    "aggregate_loss": [
      "loss"
    ],
    "compute_score": [
      "output",
      "target",
      "label_smooth",
      "eps",
      "dims"
    ]
  },
  "TverskyLoss": {
    "__init__": [
      "self",
      "alpha",
      "beta",
      "smooth"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "log_t": [
    "u",
    "t"
  ],
  "exp_t": [
    "u",
    "t"
  ],
  "compute_normalization_fixed_point": [
    "activations",
    "t",
    "num_iters"
  ],
  "compute_normalization_binary_search": [
    "activations",
    "t",
    "num_iters"
  ],
  "ComputeNormalization": {
    "forward": [
      "ctx",
      "activations",
      "t",
      "num_iters"
    ],
    "backward": [
      "ctx",
      "grad_output"
    ]
  },
  "compute_normalization": [
    "activations",
    "t",
    "num_iters"
  ],
  "tempered_softmax": [
    "activations",
    "t",
    "num_iters"
  ],
  "bi_tempered_logistic_loss": [
    "activations",
    "labels",
    "t1",
    "t2",
    "label_smooth",
    "num_iters",
    "reduction"
  ],
  "BiTemperedLogisticLoss": {
    "__init__": [
      "self",
      "t1",
      "t2",
      "label_smooth",
      "ignore_index",
      "reduction"
    ],
    "forward": [
      "self",
      "predictions",
      "targets"
    ]
  },
  "BinaryBiTemperedLogisticLoss": {
    "__init__": [
      "self",
      "t1",
      "t2",
      "label_smooth",
      "ignore_index",
      "reduction"
    ],
    "forward": [
      "self",
      "predictions",
      "targets"
    ]
  },
  "LDAMLoss": {
    "__init__": [
      "self",
      "num_class_list",
      "max_m",
      "weight",
      "s"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "get_supported_loss_functions": [
    "filters"
  ],
  "lovasz_grad": [
    "gt_sorted"
  ],
  "lovasz_hinge_flat": [
    "y_pred",
    "y_true"
  ],
  "LovaszHingeLoss": {
    "__init__": [
      "self",
      "per_image"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "FocalLoss": {
    "__init__": [
      "self",
      "alpha",
      "gamma"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "FocalCosineLoss": {
    "__init__": [
      "self",
      "alpha",
      "gamma",
      "focal_weight",
      "reduction"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "BCEFocalLoss": {
    "__init__": [
      "self",
      "alpha",
      "gamma",
      "label_smooth",
      "eps",
      "reduction"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "FocalTverskyLoss": {
    "__init__": [
      "self",
      "alpha",
      "beta",
      "gamma",
      "smooth"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "soft_jaccard_score": [
    "output",
    "target",
    "label_smooth",
    "eps",
    "dims"
  ],
  "JaccardLoss": {
    "__init__": [
      "self",
      "mode",
      "classes",
      "log_loss",
      "from_logits",
      "label_smooth",
      "eps"
    ],
    "forward": [
      "self",
      "y_pred",
      "y_true"
    ]
  },
  "AdaPNM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SCALE_TYPE": [],
  "ApolloDQN": {
    "__init__": [
      "self",
      "params",
      "lr",
      "init_lr",
      "beta",
      "rebound",
      "weight_decay",
      "weight_decay_type",
      "warmup_steps",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "APOLLO": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "scale_type",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "correct_bias",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Ano": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "logarithmic_schedule",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "TAM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "decay_rate",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaTAM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "decay_rate",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "LARS": {
    "__init__": [
      "self",
      "params",
      "lr",
      "weight_decay",
      "momentum",
      "dampening",
      "trust_coefficient",
      "nesterov",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "FTRL": {
    "__init__": [
      "self",
      "params",
      "lr",
      "lr_power",
      "beta",
      "lambda_1",
      "lambda_2",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "closest_smaller_divisor_of_n_to_k": [
    "n",
    "k"
  ],
  "AdamWSN": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "subset_size",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaDelta": {
    "__init__": [
      "self",
      "params",
      "lr",
      "rho",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "polyval": [
    "x",
    "coef"
  ],
  "ERF1994": {
    "__init__": [
      "self",
      "num_coefs"
    ],
    "w_algorithm": [
      "self",
      "z"
    ],
    "forward": [
      "self",
      "z"
    ]
  },
  "TRAC": {
    "__init__": [
      "self",
      "optimizer",
      "betas",
      "num_coefs",
      "s_prev",
      "eps"
    ],
    "__str__": [
      "self"
    ],
    "param_groups": [
      "self"
    ],
    "state": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "erf_imag": [
      "self",
      "x"
    ],
    "backup_params_and_grads": [
      "self"
    ],
    "trac_step": [
      "self",
      "updates",
      "grads"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SPlus": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ema_rate",
      "inverse_steps",
      "nonstandard_constant",
      "max_dim",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "eval": [
      "self"
    ],
    "train": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_scaled_lr": [
      "shape",
      "lr",
      "nonstandard_constant",
      "max_dim"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaMax": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "QHAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "nus",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Kate": {
    "__init__": [
      "self",
      "params",
      "lr",
      "delta",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AvaGrad": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SRMM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "memory_length",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Lookahead": {
    "__init__": [
      "self",
      "optimizer",
      "k",
      "alpha",
      "pullback_momentum"
    ],
    "param_groups": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "backup_and_load_cache": [
      "self"
    ],
    "clear_and_load_backup": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state"
    ],
    "update": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "OrthoGrad": {
    "__init__": [
      "self",
      "optimizer"
    ],
    "__str__": [
      "self"
    ],
    "param_groups": [
      "self"
    ],
    "state": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "apply_orthogonal_gradients": [
      "self",
      "params"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "MEMORY_SAVE_MODE_TYPE": [],
  "precondition_update_prob_schedule": [
    "max_prob",
    "min_prob",
    "decay",
    "flat_start"
  ],
  "Kron": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "pre_conditioner_update_probability",
      "max_size_triangular",
      "min_ndim_triangular",
      "memory_save_mode",
      "momentum_into_precondition_update",
      "mu_dtype",
      "precondition_dtype",
      "balance_prob",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "initialize_q_expressions": [
    "t",
    "scale",
    "max_size",
    "min_ndim_triangular",
    "memory_save_mode",
    "dtype"
  ],
  "balance_q": [
    "q_in"
  ],
  "solve_triangular_right": [
    "x",
    "a"
  ],
  "get_a_and_conj_b": [
    "expr_a",
    "g",
    "qs",
    "v"
  ],
  "get_q_terms": [
    "expr_gs",
    "a",
    "conj_b"
  ],
  "update_precondition": [
    "qs",
    "expressions",
    "v",
    "g",
    "step",
    "eps"
  ],
  "get_precondition_grad": [
    "qs",
    "expressions",
    "g"
  ],
  "get_global_gradient_norm": [
    "param_groups",
    "device"
  ],
  "SAM": {
    "__init__": [
      "self",
      "params",
      "base_optimizer",
      "rho",
      "adaptive",
      "use_gc",
      "perturb_eps"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "first_step": [
      "self",
      "zero_grad"
    ],
    "second_step": [
      "self",
      "zero_grad"
    ],
    "step": [
      "self",
      "closure"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "GSAM": {
    "__init__": [
      "self",
      "params",
      "base_optimizer",
      "model",
      "rho_scheduler",
      "alpha",
      "adaptive",
      "perturb_eps"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "update_rho_t": [
      "self"
    ],
    "perturb_weights": [
      "self",
      "rho"
    ],
    "un_perturb": [
      "self"
    ],
    "gradient_decompose": [
      "self",
      "alpha"
    ],
    "sync_grad": [
      "self"
    ],
    "grad_norm": [
      "self",
      "by",
      "weight_adaptive"
    ],
    "maybe_no_sync": [
      "self"
    ],
    "set_closure": [
      "self",
      "loss_fn",
      "inputs",
      "targets"
    ],
    "step": [
      "self",
      "closure"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "WSAM": {
    "__init__": [
      "self",
      "model",
      "params",
      "base_optimizer",
      "rho",
      "gamma",
      "adaptive",
      "decouple",
      "max_norm",
      "eps"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "first_step": [
      "self",
      "zero_grad"
    ],
    "second_step": [
      "self",
      "zero_grad"
    ],
    "step": [
      "self",
      "closure"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "BSAM": {
    "__init__": [
      "self",
      "params",
      "num_data",
      "lr",
      "betas",
      "weight_decay",
      "rho",
      "adaptive",
      "damping"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "first_step": [
      "self"
    ],
    "second_step": [
      "self"
    ],
    "third_step": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "LookSAM": {
    "__init__": [
      "self",
      "params",
      "base_optimizer",
      "rho",
      "k",
      "alpha",
      "adaptive",
      "use_gc",
      "perturb_eps"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_step": [
      "self"
    ],
    "first_step": [
      "self",
      "zero_grad"
    ],
    "second_step": [
      "self",
      "zero_grad"
    ],
    "step": [
      "self",
      "closure"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "FriendlySAM": {
    "__init__": [
      "self",
      "params",
      "base_optimizer",
      "rho",
      "sigma",
      "lmbda",
      "adaptive",
      "perturb_eps"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "first_step": [
      "self",
      "zero_grad"
    ],
    "second_step": [
      "self",
      "zero_grad"
    ],
    "step": [
      "self",
      "closure"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "AdamC": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AccSGD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "kappa",
      "xi",
      "constant",
      "weight_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SGDW": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "dampening",
      "nesterov",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "ASGD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "amplifier",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "theta",
      "dampening",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_norms_by_group": [
      "group",
      "device"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SignSGD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SGDSaI": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "warmup_step": [
      "self",
      "closure"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "VSGD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "ghattg",
      "ps",
      "tau1",
      "tau2",
      "weight_decay",
      "weight_decouple",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Grams": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "parse_pytorch_version": [
    "version_string"
  ],
  "compare_versions": [
    "v1",
    "v2"
  ],
  "CPUOffloadOptimizer": {
    "__init__": [
      "self",
      "params",
      "optimizer_class"
    ],
    "step": [
      "self",
      "closure"
    ],
    "zero_grad": [
      "self",
      "_"
    ],
    "param_groups": [
      "self"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ]
  },
  "StochasticAccumulator": {
    "stochastic_grad_accum": [
      "p"
    ],
    "reassign_grad_buffer": [
      "model"
    ],
    "assign_hooks": [
      "model"
    ]
  },
  "is_valid_parameters": [
    "parameters"
  ],
  "has_overflow": [
    "grad_norm"
  ],
  "to_real": [
    "x"
  ],
  "normalize_gradient": [
    "x",
    "use_channels",
    "epsilon"
  ],
  "clip_grad_norm": [
    "parameters",
    "max_norm",
    "sync"
  ],
  "unit_norm": [
    "x",
    "norm"
  ],
  "disable_running_stats": [
    "model"
  ],
  "enable_running_stats": [
    "model"
  ],
  "reg_noise": [
    "network1",
    "network2",
    "num_data",
    "lr",
    "eta",
    "temperature"
  ],
  "copy_stochastic": [
    "target",
    "source"
  ],
  "AdamS": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaMod": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Lamb": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "rectify",
      "degenerated_to_sgd",
      "n_sma_threshold",
      "grad_averaging",
      "max_grad_norm",
      "adam",
      "pre_norm",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_global_gradient_norm": [
      "self"
    ],
    "update": [
      "self",
      "p",
      "group",
      "grad_norm",
      "n_sma",
      "step_size",
      "beta1",
      "beta2",
      "beta3"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "flatten_grad": [
    "grads"
  ],
  "un_flatten_grad": [
    "grads",
    "shapes"
  ],
  "PCGrad": {
    "__init__": [
      "self",
      "optimizer",
      "reduction"
    ],
    "init_group": [
      "self"
    ],
    "zero_grad": [
      "self"
    ],
    "step": [
      "self"
    ],
    "set_grad": [
      "self",
      "grads"
    ],
    "retrieve_grad": [
      "self"
    ],
    "pack_grad": [
      "self",
      "objectives"
    ],
    "project_conflicting": [
      "self",
      "grads",
      "has_grads"
    ],
    "pc_backward": [
      "self",
      "objectives"
    ]
  },
  "DynamicLossScaler": {
    "__init__": [
      "self",
      "init_scale",
      "scale_factor",
      "scale_window",
      "tolerance",
      "threshold"
    ],
    "update_scale": [
      "self",
      "overflow"
    ],
    "decrease_loss_scale": [
      "self"
    ]
  },
  "SafeFP16Optimizer": {
    "__init__": [
      "self",
      "optimizer",
      "aggregate_g_norms",
      "min_loss_scale"
    ],
    "get_parameters": [
      "cls",
      "optimizer"
    ],
    "build_fp32_params": [
      "cls",
      "parameters",
      "flatten"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state_dict"
    ],
    "backward": [
      "self",
      "loss",
      "update_main_grads"
    ],
    "sync_fp16_grads_to_fp32": [
      "self",
      "multiply_grads"
    ],
    "multiply_grads": [
      "self",
      "c"
    ],
    "update_main_grads": [
      "self"
    ],
    "clip_main_grads": [
      "self",
      "max_norm"
    ],
    "step": [
      "self",
      "closure"
    ],
    "zero_grad": [
      "self"
    ],
    "get_lr": [
      "self"
    ],
    "set_lr": [
      "self",
      "lr"
    ],
    "loss_scale": [
      "self"
    ]
  },
  "AdaBelief": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "rectify",
      "n_sma_threshold",
      "degenerated_to_sgd",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "CosineDecay": {
    "__init__": [
      "self",
      "death_rate",
      "t_max",
      "eta_min",
      "last_epoch"
    ],
    "step": [
      "self",
      "current_step"
    ],
    "get_death_rate": [
      "self",
      "current_step"
    ]
  },
  "SPAM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "density",
      "weight_decay",
      "warmup_epoch",
      "threshold",
      "grad_accu_steps",
      "update_proj_gap",
      "eps",
      "maximize"
    ],
    "initialize_random_rank_boolean_tensor": [
      "m",
      "n",
      "density",
      "device"
    ],
    "update_mask_random": [
      "self",
      "p",
      "old_mask"
    ],
    "update_masks": [
      "self"
    ],
    "init_masks": [
      "self"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "StableSPAM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "gamma1",
      "gamma2",
      "theta",
      "t_max",
      "eta_min",
      "weight_decay",
      "update_proj_gap",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "DAdaptAdaGrad": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "d0",
      "growth_rate",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "DAdaptAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "d0",
      "growth_rate",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "bias_correction",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "DAdaptSGD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "d0",
      "growth_rate",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "DAdaptAdan": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "d0",
      "growth_rate",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "DAdaptLion": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "d0",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Aida": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "k",
      "xi",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "rectify",
      "n_sma_threshold",
      "degenerated_to_sgd",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "PID": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "dampening",
      "derivative",
      "integral",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdamG": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "p",
      "q",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "s": [
      "self",
      "p"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "BNB_OPTIMIZERS": [],
  "load_bnb_optimizer": [
    "optimizer"
  ],
  "load_q_galore_optimizer": [
    "optimizer"
  ],
  "load_ao_optimizer": [
    "optimizer"
  ],
  "load_optimizer": [
    "optimizer"
  ],
  "create_optimizer": [
    "model",
    "optimizer_name",
    "lr",
    "weight_decay",
    "wd_ban_list",
    "use_lookahead",
    "use_orthograd"
  ],
  "get_optimizer_parameters": [
    "model_or_parameter",
    "weight_decay",
    "wd_ban_list"
  ],
  "get_supported_optimizers": [
    "filters"
  ],
  "AdaFactor": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "decay_rate",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "clip_threshold",
      "ams_bound",
      "scale_parameter",
      "relative_step",
      "warmup_init",
      "eps1",
      "eps2",
      "momentum_dtype",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_lr": [
      "self",
      "lr",
      "step",
      "rms",
      "relative_step",
      "warmup_init",
      "scale_parameter"
    ],
    "get_options": [
      "shape"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Conda": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "update_proj_gap",
      "scale",
      "projection_type",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Tiger": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "LayerWiseGrafting": {
    "NONE": [],
    "SGD": [],
    "ADAGRAD": [],
    "RMSPROP": [],
    "SQRTN": []
  },
  "Graft": {
    "__init__": [
      "self"
    ],
    "add_statistics": [
      "self",
      "grad",
      "beta2"
    ],
    "precondition_gradient": [
      "self",
      "grad"
    ],
    "update_momentum": [
      "self",
      "update",
      "beta1"
    ]
  },
  "SGDGraft": {
    "__init__": [
      "self",
      "var"
    ],
    "update_momentum": [
      "self",
      "update",
      "beta1"
    ]
  },
  "SQRTNGraft": {
    "__init__": [
      "self",
      "var"
    ],
    "precondition_gradient": [
      "self",
      "grad"
    ]
  },
  "AdaGradGraft": {
    "__init__": [
      "self",
      "var",
      "diagonal_eps"
    ],
    "add_statistics": [
      "self",
      "grad",
      "_"
    ],
    "precondition_gradient": [
      "self",
      "grad"
    ]
  },
  "RMSPropGraft": {
    "__init__": [
      "self",
      "var",
      "diagonal_eps"
    ],
    "add_statistics": [
      "self",
      "grad",
      "beta2"
    ],
    "precondition_gradient": [
      "self",
      "grad"
    ]
  },
  "BlockPartitioner": {
    "__init__": [
      "self",
      "var",
      "rank",
      "block_size",
      "pre_conditioner_type"
    ],
    "build_pre_conditioner_shapes": [
      "split_sizes",
      "pre_conditioner_type",
      "rank"
    ],
    "shapes_for_pre_conditioners": [
      "self"
    ],
    "partition": [
      "self",
      "x"
    ],
    "merge_partitions": [
      "self",
      "partitions"
    ]
  },
  "PreConditionerType": {
    "ALL": [],
    "INPUT": [],
    "OUTPUT": []
  },
  "PreConditioner": {
    "__init__": [
      "self",
      "var",
      "beta2",
      "inverse_exponent_override",
      "block_size",
      "skip_preconditioning_rank_lt",
      "no_preconditioning_for_layers_with_dim_gt",
      "shape_interpretation",
      "pre_conditioner_type",
      "matrix_eps",
      "use_svd"
    ],
    "get_should_precondition_dims": [
      "self"
    ],
    "skip_precondition": [
      "self",
      "x"
    ],
    "add_statistics": [
      "self",
      "grad"
    ],
    "compute_pre_conditioners": [
      "self"
    ],
    "precondition_block": [
      "partitioned_grad",
      "should_preconditioned_dims",
      "pre_conditioners_for_grad"
    ],
    "preconditioned_grad": [
      "self",
      "grad"
    ]
  },
  "build_graft": [
    "p",
    "graft_type",
    "diagonal_eps"
  ],
  "power_iteration": [
    "mat_g",
    "num_iters"
  ],
  "compute_power_schur_newton": [
    "mat_g",
    "p",
    "max_iters",
    "error_tolerance",
    "ridge_epsilon",
    "max_error_ratio"
  ],
  "compute_power_svd": [
    "matrix",
    "power"
  ],
  "merge_small_dims": [
    "shape_to_merge",
    "max_dim"
  ],
  "zero_power_via_newton_schulz_5": [
    "g",
    "num_steps",
    "eps",
    "safety_factor",
    "weights"
  ],
  "centralize_gradient": [
    "grad",
    "gc_conv_only"
  ],
  "ADOPT": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "clip_lambda",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Adai": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "stable_weight_decay",
      "dampening",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "reduce_max_except_dim": [
    "x",
    "dim"
  ],
  "SM3": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "beta",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "make_sparse": [
      "grad",
      "values"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SWATS": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "nesterov",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "get_adjusted_lr": [
    "lr",
    "param_shape",
    "use_adjusted_lr"
  ],
  "Muon": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "nesterov",
      "ns_steps",
      "use_adjusted_lr",
      "adamw_lr",
      "adamw_betas",
      "adamw_wd",
      "adamw_eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "DistributedMuon": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "nesterov",
      "ns_steps",
      "use_adjusted_lr",
      "adamw_lr",
      "adamw_betas",
      "adamw_wd",
      "adamw_eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaMuon": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "ns_steps",
      "use_adjusted_lr",
      "adamw_lr",
      "adamw_betas",
      "adamw_wd",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaGO": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "gamma",
      "eps",
      "v",
      "nesterov",
      "ns_steps",
      "use_adjusted_lr",
      "adamw_lr",
      "adamw_betas",
      "adamw_wd",
      "adamw_eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "prepare_muon_parameters": [
    "model",
    "optimizer_name",
    "lr",
    "weight_decay",
    "adamw_lr",
    "adamw_wd"
  ],
  "FILTER_TYPE": [],
  "gradfilter_ma": [
    "model",
    "grads",
    "window_size",
    "lamb",
    "filter_type",
    "warmup"
  ],
  "gradfilter_ema": [
    "model",
    "grads",
    "alpha",
    "lamb"
  ],
  "GrokFastAdamW": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "grokfast",
      "grokfast_alpha",
      "grokfast_lamb",
      "grokfast_after_step",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "normalize_lr",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Ranger21": {
    "__init__": [
      "self",
      "params",
      "num_iterations",
      "lr",
      "beta0",
      "betas",
      "use_softplus",
      "beta_softplus",
      "disable_lr_scheduler",
      "num_warm_up_iterations",
      "num_warm_down_iterations",
      "warm_down_min_lr",
      "agc_clipping_value",
      "agc_eps",
      "centralize_gradients",
      "normalize_gradients",
      "lookahead_merge_time",
      "lookahead_blending_alpha",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "norm_loss_factor",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "build_warm_up_iterations": [
      "total_iterations",
      "beta2",
      "warm_up_pct"
    ],
    "build_warm_down_iterations": [
      "total_iterations",
      "warm_down_pct"
    ],
    "warm_up_dampening": [
      "self",
      "lr",
      "step"
    ],
    "warm_down": [
      "self",
      "lr",
      "iteration"
    ],
    "step": [
      "self",
      "closure"
    ],
    "lookahead_process_step": [
      "self"
    ]
  },
  "PNM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaHessian": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "hessian_power",
      "update_period",
      "num_samples",
      "hessian_distribution",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure",
      "hessian"
    ]
  },
  "SophiaH": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "p",
      "update_period",
      "num_samples",
      "hessian_distribution",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure",
      "hessian"
    ]
  },
  "Fromage": {
    "__init__": [
      "self",
      "params",
      "lr",
      "p_bound",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "MARS_TYPE": [],
  "MARS": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "gamma",
      "mars_type",
      "optimize_1d",
      "lr_1d",
      "betas_1d",
      "weight_decay",
      "weight_decay_1d",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "cautious",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "optimize_mixed": [
      "self",
      "grad",
      "last_grad",
      "exp_avg",
      "exp_avg_sq",
      "max_exp_avg_sq",
      "betas",
      "gamma",
      "mars_type",
      "is_grad_2d",
      "step",
      "ams_bound",
      "cautious",
      "eps"
    ],
    "optimize_1d": [
      "self",
      "grad",
      "exp_avg",
      "exp_avg_sq",
      "max_exp_avg_sq",
      "betas",
      "step",
      "ams_bound",
      "cautious",
      "eps"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdamMini": {
    "__init__": [
      "self",
      "model",
      "lr",
      "betas",
      "weight_decay",
      "model_sharding",
      "num_embeds",
      "num_heads",
      "num_query_groups",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "get_optimizer_groups": [
      "self",
      "weight_decay"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step_embed": [
      "p",
      "grad",
      "state",
      "lr",
      "beta1",
      "beta2",
      "bias_correction1",
      "bias_correction2_sq",
      "eps"
    ],
    "step_attn_proj": [
      "p",
      "grad",
      "state",
      "parameter_per_head",
      "lr",
      "beta1",
      "beta2",
      "bias_correction1",
      "bias_correction2_sq",
      "eps"
    ],
    "step_attn": [
      "p",
      "grad",
      "state",
      "num_heads",
      "q_per_kv",
      "lr",
      "beta1",
      "beta2",
      "bias_correction1",
      "bias_correction2_sq",
      "eps"
    ],
    "step_lefts": [
      "self",
      "p",
      "grad",
      "state",
      "lr",
      "beta1",
      "beta2",
      "bias_correction1",
      "bias_correction2_sq",
      "eps"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Prodigy": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "beta3",
      "d0",
      "d_coef",
      "growth_rate",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "bias_correction",
      "safeguard_warmup",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "MSVAG": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_rho": [
      "beta_power",
      "beta"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "LMONorm": {
    "NONE": [],
    "AUTO": [],
    "SPECTRAL": [],
    "SPECTRALCONV": [],
    "SIGN": [],
    "BIAS": [],
    "COL": [],
    "ROW": []
  },
  "Norm": {
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad"
    ]
  },
  "Col": {
    "__init__": [
      "self",
      "normalized",
      "transpose"
    ],
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad",
      "eps"
    ]
  },
  "Row": {
    "__init__": [
      "self",
      "normalized",
      "transpose"
    ],
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad",
      "eps"
    ]
  },
  "BiasRMS": {
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad",
      "eps"
    ]
  },
  "SpectralConv": {
    "__init__": [
      "self",
      "num_steps"
    ],
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad",
      "eps"
    ]
  },
  "Spectral": {
    "__init__": [
      "self",
      "max_scale",
      "normalize",
      "num_steps"
    ],
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad",
      "eps"
    ]
  },
  "Sign": {
    "__init__": [
      "self",
      "zero_init",
      "normalize"
    ],
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad",
      "eps"
    ]
  },
  "Auto": {
    "init": [
      "self",
      "x"
    ],
    "lmo": [
      "self",
      "grad"
    ]
  },
  "build_lmo_norm": [
    "norm_type"
  ],
  "SCION": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "constraint",
      "norm_type",
      "norm_kwargs",
      "scale",
      "weight_decay",
      "weight_decouple",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "init": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SCIONLight": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "constraint",
      "norm_type",
      "norm_kwargs",
      "scale",
      "weight_decay",
      "weight_decouple",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "init": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Mode": [],
  "BCOS": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "beta2",
      "mode",
      "simple_cond",
      "weight_decay",
      "weight_decouple",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "compute_v": [
      "self",
      "grad",
      "m",
      "beta",
      "beta2"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "VARIANTS": [],
  "A2Grad": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "lips",
      "rho",
      "variant",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdEMAMix": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "alpha",
      "t_alpha_beta3",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "schedule_alpha": [
      "t_alpha_beta3",
      "step",
      "alpha"
    ],
    "schedule_beta3": [
      "t_alpha_beta3",
      "step",
      "beta1",
      "beta3"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SimplifiedAdEMAMix": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "alpha",
      "beta1_warmup",
      "min_beta1",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "linear_hl_warmup_scheduler": [
      "step",
      "beta_end",
      "beta_start",
      "warmup"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "NovoGrad": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "grad_averaging",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "DiffGrad": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "rectify",
      "n_sma_threshold",
      "degenerated_to_sgd",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Adan": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "max_grad_norm",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_global_gradient_norm": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "update_ema": [
    "state",
    "loss"
  ],
  "compute_scalar": [
    "ema"
  ],
  "get_coef": [
    "scalar"
  ],
  "get_scalar_ratio": [
    "scalar",
    "use_shadow"
  ],
  "EmoNavi": {
    "__init__": [
      "self",
      "params",
      "lr",
      "lr_max",
      "lr_min",
      "betas",
      "use_shadow",
      "shadow_weight",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "EmoLynx": {
    "__init__": [
      "self",
      "params",
      "lr",
      "lr_max",
      "lr_min",
      "betas",
      "use_shadow",
      "shadow_weight",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "EmoFact": {
    "__init__": [
      "self",
      "params",
      "lr",
      "lr_max",
      "lr_min",
      "betas",
      "use_shadow",
      "shadow_weight",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "EmoNeco": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "use_shadow",
      "shadow_weight",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "EmoZeal": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "use_shadow",
      "shadow_weight",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "FAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "clip",
      "p",
      "eps",
      "momentum_dtype",
      "fim_dtype",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "SOAP": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "shampoo_beta",
      "weight_decay",
      "precondition_frequency",
      "max_precondition_dim",
      "merge_dims",
      "precondition_1d",
      "correct_bias",
      "normalize_gradient",
      "data_format",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "project": [
      "self",
      "grad",
      "state",
      "merge_dims",
      "max_precondition_dim",
      "project_type"
    ],
    "get_orthogonal_matrix": [
      "mat"
    ],
    "get_orthogonal_matrix_qr": [
      "self",
      "state",
      "max_precondition_dim",
      "merge_dims"
    ],
    "init_pre_conditioner": [
      "grad",
      "state",
      "precondition_frequency",
      "shampoo_beta",
      "max_precondition_dim",
      "precondition_1d",
      "merge_dims"
    ],
    "update_pre_conditioner": [
      "self",
      "grad",
      "state",
      "step",
      "max_precondition_dim",
      "precondition_1d",
      "merge_dims"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "LOMO": {
    "__init__": [
      "self",
      "model",
      "lr",
      "clip_grad_norm",
      "clip_grad_value",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "fuse_update": [
      "self"
    ],
    "fuse_update_zero3": [
      "self"
    ],
    "fused_backward": [
      "self",
      "loss",
      "lr"
    ],
    "grad_norm": [
      "self",
      "loss"
    ]
  },
  "AdaLOMO": {
    "__init__": [
      "self",
      "model",
      "lr",
      "weight_decay",
      "loss_scale",
      "clip_threshold",
      "decay_rate",
      "clip_grad_norm",
      "clip_grad_value",
      "eps1",
      "eps2"
    ],
    "__str__": [
      "self"
    ],
    "initialize_states": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "fuse_update": [
      "self"
    ],
    "fuse_update_zero3": [
      "self"
    ],
    "fused_backward": [
      "self",
      "loss",
      "lr"
    ],
    "grad_norm": [
      "self",
      "loss"
    ]
  },
  "Ranger": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "alpha",
      "k",
      "n_sma_threshold",
      "degenerated_to_sgd",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "use_gc",
      "gc_conv_only",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "channel_view": [
    "x"
  ],
  "layer_view": [
    "x"
  ],
  "cosine_similarity_by_view": [
    "x",
    "y",
    "eps",
    "view_func"
  ],
  "projection": [
    "p",
    "grad",
    "perturb",
    "delta",
    "wd_ratio",
    "eps"
  ],
  "SGDP": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "dampening",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "delta",
      "wd_ratio",
      "nesterov",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdamP": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "delta",
      "wd_ratio",
      "nesterov",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "MADGRAD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Gravity": {
    "__init__": [
      "self",
      "params",
      "lr",
      "alpha",
      "beta",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AggMo": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "FOCUS": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "gamma",
      "weight_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "TransformDCT": {
    "__init__": [
      "self",
      "param_groups",
      "target_chunk",
      "norm"
    ],
    "einsum_2d": [
      "self",
      "x",
      "b",
      "d"
    ],
    "einsum_2d_t": [
      "self",
      "x",
      "b",
      "d"
    ],
    "encode": [
      "self",
      "x"
    ],
    "decode": [
      "self",
      "x"
    ]
  },
  "CompressDCT": {
    "__init__": [
      "self"
    ],
    "clamp_top_k": [
      "x",
      "top_k"
    ],
    "compress": [
      "self",
      "x",
      "top_k"
    ],
    "decompress": [
      "self",
      "p",
      "idx",
      "val",
      "shape"
    ],
    "batch_decompress": [
      "self",
      "p",
      "idx",
      "val",
      "shape"
    ]
  },
  "dct": [
    "x",
    "norm"
  ],
  "inverse_dct": [
    "x",
    "norm"
  ],
  "get_prime_divisors": [
    "n"
  ],
  "get_divisors": [
    "n"
  ],
  "get_smaller_split": [
    "n",
    "close_to"
  ],
  "DeMo": {
    "__init__": [
      "self",
      "params",
      "lr",
      "compression_decay",
      "compression_top_k",
      "compression_chunk",
      "weight_decay",
      "process_group",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "find_dtype": [
      "self"
    ],
    "init_demo_states": [
      "self"
    ],
    "init_parameters": [
      "self"
    ],
    "demo_all_gather": [
      "self",
      "sparse_idx",
      "sparse_val"
    ],
    "init_group": [
      "self"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Lion": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaGC": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "beta",
      "lambda_abs",
      "lambda_rel",
      "warmup_steps",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "GaLore": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "PROJECTION_TYPE": [],
  "GaLoreProjector": {
    "__init__": [
      "self",
      "rank",
      "update_proj_gap",
      "scale",
      "projection_type"
    ],
    "get_orthogonal_matrix": [
      "self",
      "weights",
      "projection_type",
      "from_random_matrix"
    ],
    "get_low_rank_grad_std": [
      "self",
      "grad"
    ],
    "get_low_rank_grad_reverse_std": [
      "self",
      "grad"
    ],
    "get_low_rank_grad_right": [
      "self",
      "grad"
    ],
    "get_low_rank_grad_left": [
      "self",
      "grad"
    ],
    "get_low_rank_grad_full": [
      "self",
      "grad"
    ],
    "get_low_rank_grad_random": [
      "self",
      "grad"
    ],
    "update_ortho_matrix": [
      "self",
      "x",
      "from_random_matrix"
    ],
    "project": [
      "self",
      "grad",
      "num_steps",
      "svd_basis_matrix",
      "from_random_matrix"
    ],
    "project_back": [
      "self",
      "low_rank_grad"
    ]
  },
  "agc": [
    "p",
    "grad",
    "agc_eps",
    "agc_clip_val",
    "eps"
  ],
  "LaProp": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "centered",
      "steps_before_using_centered",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "neuron_norm": [
    "x"
  ],
  "neuron_mean": [
    "x"
  ],
  "Nero": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "constraints",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "StableAdamW": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "kahan_sum",
      "weight_decay",
      "weight_decouple",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "l2_projection": [
    "parameters",
    "max_norm"
  ],
  "AliG": {
    "__init__": [
      "self",
      "params",
      "max_lr",
      "projection_fn",
      "momentum",
      "adjusted_momentum",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "compute_step_size": [
      "self",
      "loss"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaShift": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "keep_num",
      "reduce_func",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Yogi": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "initial_accumulator",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Adalite": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "g_norm_min",
      "ratio_min",
      "tau",
      "eps1",
      "eps2",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "RACS": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "alpha",
      "gamma",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Alice": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "alpha",
      "alpha_c",
      "update_interval",
      "rank",
      "gamma",
      "leading_basis",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "subspace_iteration": [
      "a",
      "mat",
      "num_steps"
    ],
    "switch": [
      "self",
      "q",
      "u_prev",
      "rank",
      "leading_basis"
    ],
    "compensation": [
      "grad",
      "u",
      "p",
      "phi",
      "gamma",
      "decay_rate",
      "rank"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "AdaNorm": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "r",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "CAME": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "clip_threshold",
      "ams_bound",
      "eps1",
      "eps2",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_options": [
      "shape"
    ],
    "get_rms": [
      "x"
    ],
    "approximate_sq_grad": [
      "exp_avg_sq_row",
      "exp_avg_sq_col",
      "output"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "divide": [
    "numer",
    "de_nom",
    "eps"
  ],
  "VanillaMTL": {
    "__init__": [
      "self",
      "backbone",
      "heads"
    ],
    "backbone": [
      "self"
    ],
    "train": [
      "self",
      "mode"
    ],
    "to": [
      "self"
    ],
    "_hook": [
      "self",
      "index"
    ],
    "forward": [
      "self",
      "x"
    ],
    "backward": [
      "self",
      "losses",
      "backbone_loss"
    ],
    "mtl_parameters": [
      "self",
      "recurse"
    ],
    "model_parameters": [
      "self",
      "recurse"
    ]
  },
  "rotate": [
    "points",
    "rotation",
    "total_size"
  ],
  "rotate_back": [
    "points",
    "rotation",
    "total_size"
  ],
  "RotateModule": {
    "__init__": [
      "self",
      "parent",
      "item"
    ],
    "hook": [
      "self",
      "grad"
    ],
    "p": [
      "self"
    ],
    "r": [
      "self"
    ],
    "weight": [
      "self"
    ],
    "rotate": [
      "self",
      "z"
    ],
    "rotate_back": [
      "self",
      "z"
    ],
    "forward": [
      "self",
      "z"
    ]
  },
  "RotateOnly": {
    "__init__": [
      "self",
      "backbone",
      "heads",
      "latent_size"
    ],
    "rotation": [
      "self"
    ],
    "backbone": [
      "self"
    ],
    "to": [
      "self"
    ],
    "train": [
      "self",
      "mode"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "item"
    ],
    "_hook": [
      "self",
      "index"
    ],
    "forward": [
      "self",
      "x"
    ],
    "backward": [
      "self",
      "losses",
      "backbone_loss"
    ],
    "_rep_grad": [
      "self"
    ],
    "mtl_parameters": [
      "self",
      "recurse"
    ],
    "model_parameters": [
      "self",
      "recurse"
    ]
  },
  "RotoGrad": {
    "__init__": [
      "self",
      "backbone",
      "heads",
      "latent_size"
    ],
    "_rep_grad": [
      "self"
    ]
  },
  "RotoGradNorm": {
    "__init__": [
      "self",
      "backbone",
      "heads",
      "latent_size"
    ],
    "weight": [
      "self"
    ],
    "_rep_grad": [
      "self"
    ]
  },
  "AdaBound": {
    "__init__": [
      "self",
      "params",
      "lr",
      "final_lr",
      "betas",
      "gamma",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Fira": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "ScheduleFreeSGD": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "r",
      "weight_lr_power",
      "warmup_steps",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "eval": [
      "self"
    ],
    "train": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "ScheduleFreeAdamW": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "r",
      "weight_lr_power",
      "warmup_steps",
      "decoupling_c",
      "ams_bound",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "eval": [
      "self"
    ],
    "train": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "ScheduleFreeRAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "r",
      "weight_lr_power",
      "silent_sgd_phase",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "eval": [
      "self"
    ],
    "train": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "ScheduleFreeWrapper": {
    "__init__": [
      "self",
      "optimizer",
      "momentum",
      "weight_decay",
      "r",
      "weight_lr_power",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "param_groups": [
      "self"
    ],
    "__getstate__": [
      "self"
    ],
    "add_param_group": [
      "self",
      "param_group"
    ],
    "state_dict": [
      "self"
    ],
    "load_state_dict": [
      "self",
      "state"
    ],
    "zero_grad": [
      "self",
      "set_to_none"
    ],
    "eval": [
      "self"
    ],
    "train": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "swap": [
      "x",
      "y"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "damped_pair_vg": [
    "g",
    "damp"
  ],
  "norm_lower_bound": [
    "a"
  ],
  "woodbury_identity": [
    "inv_a",
    "u",
    "v"
  ],
  "triu_with_diagonal_and_above": [
    "a"
  ],
  "update_precondition_dense": [
    "q",
    "dxs",
    "dgs",
    "step",
    "eps"
  ],
  "AdaSmooth": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "RAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "n_sma_threshold",
      "degenerated_to_sgd",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Shampoo": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "preconditioning_compute_steps",
      "matrix_eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "ScalableShampoo": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "moving_average_for_momentum",
      "weight_decay",
      "decoupled_weight_decay",
      "decoupled_learning_rate",
      "inverse_exponent_override",
      "start_preconditioning_step",
      "preconditioning_compute_steps",
      "statistics_compute_steps",
      "block_size",
      "skip_preconditioning_rank_lt",
      "no_preconditioning_for_layers_with_dim_gt",
      "shape_interpretation",
      "graft_type",
      "pre_conditioner_type",
      "nesterov",
      "diagonal_eps",
      "matrix_eps",
      "use_svd",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "is_precondition_step": [
      "self",
      "step"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "PAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "partial",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Amos": {
    "__init__": [
      "self",
      "params",
      "lr",
      "beta",
      "momentum",
      "extra_l2",
      "c_coef",
      "d_coef",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "get_scale": [
      "p"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "QHM": {
    "__init__": [
      "self",
      "params",
      "lr",
      "momentum",
      "nu",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "EXAdam": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "Ranger25": {
    "__init__": [
      "self",
      "params",
      "lr",
      "betas",
      "weight_decay",
      "alpha",
      "t_alpha_beta3",
      "lookahead_merge_time",
      "lookahead_blending_alpha",
      "cautious",
      "stable_adamw",
      "orthograd",
      "eps",
      "maximize"
    ],
    "__str__": [
      "self"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "schedule_alpha": [
      "t_alpha_beta3",
      "step",
      "alpha"
    ],
    "schedule_beta3": [
      "t_alpha_beta3",
      "step",
      "beta1",
      "beta3"
    ],
    "apply_orthogonal_gradients": [
      "self",
      "params",
      "eps"
    ],
    "step": [
      "self",
      "closure"
    ]
  },
  "LinearScheduler": {
    "_step": [
      "self"
    ]
  },
  "CosineScheduler": {
    "_step": [
      "self"
    ]
  },
  "PolyScheduler": {
    "__init__": [
      "self",
      "optimizer",
      "poly_order"
    ],
    "_step": [
      "self"
    ]
  },
  "ProportionScheduler": {
    "__init__": [
      "self",
      "lr_scheduler",
      "max_lr",
      "min_lr",
      "max_value",
      "min_value"
    ],
    "get_lr": [
      "self"
    ],
    "step": [
      "self"
    ]
  },
  "CosineAnnealingWarmupRestarts": {
    "__init__": [
      "self",
      "optimizer",
      "first_cycle_steps",
      "cycle_mult",
      "max_lr",
      "min_lr",
      "warmup_steps",
      "gamma",
      "last_epoch"
    ],
    "init_lr": [
      "self"
    ],
    "get_lr": [
      "self"
    ],
    "step": [
      "self",
      "epoch"
    ]
  },
  "SchedulerType": {
    "CONSTANT": [],
    "LINEAR": [],
    "PROPORTION": [],
    "STEP": [],
    "MULTI_STEP": [],
    "MULTIPLICATIVE": [],
    "CYCLIC": [],
    "ONE_CYCLE": [],
    "COSINE": [],
    "POLY": [],
    "COSINE_ANNEALING": [],
    "COSINE_ANNEALING_WITH_WARM_RESTART": [],
    "COSINE_ANNEALING_WITH_WARMUP": [],
    "CHEBYSHEV": [],
    "REX": [],
    "WARMUP_STABLE_DECAY": [],
    "__str__": [
      "self"
    ]
  },
  "load_lr_scheduler": [
    "lr_scheduler_name"
  ],
  "get_supported_lr_schedulers": [
    "filters"
  ],
  "get_chebyshev_steps": [
    "num_epochs",
    "small_m",
    "big_m"
  ],
  "get_chebyshev_permutation": [
    "num_epochs"
  ],
  "get_chebyshev_perm_steps": [
    "num_epochs"
  ],
  "get_chebyshev_lr_lambda": [
    "epoch",
    "num_epochs",
    "is_warmup"
  ],
  "get_chebyshev_schedule": [
    "optimizer",
    "num_epochs",
    "is_warmup",
    "last_epoch"
  ],
  "REXScheduler": {
    "__init__": [
      "self",
      "optimizer",
      "total_steps",
      "max_lr",
      "min_lr"
    ],
    "init_lr": [
      "self"
    ],
    "get_lr": [
      "self"
    ],
    "get_linear_lr": [
      "self"
    ],
    "step": [
      "self",
      "epoch"
    ]
  },
  "COOLDOWN_TYPE": [],
  "get_cosine_cooldown_lr_ratio": [
    "current_step",
    "num_warmup_steps",
    "num_stable_steps",
    "num_decay_steps",
    "min_lr_ratio",
    "num_cycles"
  ],
  "get_1sqrt_cooldown_lr_ratio": [
    "current_step",
    "num_warmup_steps",
    "num_stable_steps",
    "num_decay_steps"
  ],
  "get_1square_cooldown_lr_ratio": [
    "current_step",
    "num_warmup_steps",
    "num_stable_steps",
    "num_decay_steps"
  ],
  "get_linear_cooldown_lr_ratio": [
    "current_step",
    "num_warmup_steps",
    "num_stable_steps",
    "num_decay_steps"
  ],
  "get_wsd_scheduler_lambda": [
    "current_step"
  ],
  "get_wsd_schedule": [
    "optimizer",
    "num_warmup_steps",
    "num_stable_steps",
    "num_decay_steps",
    "min_lr_ratio",
    "num_cycles",
    "cooldown_type",
    "last_epoch"
  ],
  "deberta_v3_large_lr_scheduler": [
    "model",
    "layer_low_threshold",
    "layer_middle_threshold",
    "head_param_start",
    "base_lr",
    "head_lr",
    "wd"
  ],
  "OPTIMIZER": [],
  "OPTIMIZER_INSTANCE_OR_CLASS": [],
  "SCHEDULER": [],
  "Defaults": [],
  "ParamGroup": [],
  "State": [],
  "Parameters": [],
  "Closure": [],
  "Loss": [],
  "Betas": [],
  "HUTCHINSON_G": [],
  "CLASS_MODE": [],
  "DATA_FORMAT": [],
  "NoSparseGradientError": {
    "__init__": [
      "self",
      "optimizer_name",
      "note"
    ]
  },
  "ZeroParameterSizeError": {
    "__init__": [
      "self"
    ]
  },
  "NoClosureError": {
    "__init__": [
      "self",
      "optimizer_name",
      "note"
    ]
  },
  "NegativeLRError": {
    "__init__": [
      "self",
      "lr",
      "lr_type"
    ]
  },
  "NegativeStepError": {
    "__init__": [
      "self",
      "num_steps",
      "step_type"
    ]
  },
  "NoComplexParameterError": {
    "__init__": [
      "self",
      "optimizer_name",
      "note"
    ]
  },
  "BaseLinearWarmupScheduler": {
    "__init__": [
      "self",
      "optimizer",
      "t_max",
      "max_lr",
      "min_lr",
      "init_lr",
      "warmup_steps"
    ],
    "validate_parameters": [
      "self"
    ],
    "_init_lr": [
      "self"
    ],
    "step": [
      "self"
    ],
    "_step": [
      "self"
    ],
    "get_lr": [
      "self"
    ]
  },
  "BaseOptimizer": {
    "__init__": [
      "self",
      "params",
      "defaults"
    ],
    "load_optimizer": [
      "optimizer"
    ],
    "set_hessian": [
      "param_groups",
      "state",
      "hessian"
    ],
    "zero_hessian": [
      "param_groups",
      "state",
      "pre_zero"
    ],
    "compute_hutchinson_hessian": [
      "param_groups",
      "state",
      "num_samples",
      "alpha",
      "distribution"
    ],
    "apply_weight_decay": [
      "p",
      "grad",
      "lr",
      "weight_decay",
      "weight_decouple",
      "fixed_decay",
      "ratio"
    ],
    "apply_cautious_weight_decay": [
      "p",
      "update",
      "lr",
      "weight_decay"
    ],
    "apply_ams_bound": [
      "ams_bound",
      "exp_avg_sq",
      "max_exp_avg_sq",
      "eps",
      "exp_avg_sq_eps"
    ],
    "debias": [
      "beta",
      "step"
    ],
    "debias_beta": [
      "beta",
      "step"
    ],
    "apply_adam_debias": [
      "adam_debias",
      "step_size",
      "bias_correction1"
    ],
    "get_rectify_step_size": [
      "is_rectify",
      "step",
      "lr",
      "beta2",
      "n_sma_threshold",
      "degenerated_to_sgd"
    ],
    "get_adanorm_gradient": [
      "grad",
      "adanorm",
      "exp_grad_norm",
      "r"
    ],
    "get_rms": [
      "x"
    ],
    "approximate_sq_grad": [
      "exp_avg_sq_row",
      "exp_avg_sq_col",
      "output"
    ],
    "apply_cautious": [
      "update",
      "grad"
    ],
    "get_stable_adamw_rms": [
      "grad",
      "exp_avg_sq",
      "eps"
    ],
    "validate_range": [
      "x",
      "name",
      "low",
      "high",
      "range_type"
    ],
    "validate_non_negative": [
      "x",
      "name"
    ],
    "validate_non_positive": [
      "x",
      "name"
    ],
    "validate_positive": [
      "x",
      "name"
    ],
    "validate_boundary": [
      "constant",
      "boundary",
      "bound_type"
    ],
    "validate_step": [
      "step",
      "step_type"
    ],
    "validate_options": [
      "x",
      "name",
      "options"
    ],
    "validate_learning_rate": [
      "learning_rate"
    ],
    "validate_mod": [
      "x",
      "y"
    ],
    "validate_betas": [
      "self",
      "betas",
      "beta_range_type",
      "beta3_range_type"
    ],
    "validate_nus": [
      "self",
      "nus"
    ],
    "init_group": [
      "self",
      "group"
    ],
    "view_as_real": [
      "param"
    ],
    "maximize_gradient": [
      "grad",
      "maximize"
    ],
    "step": [
      "self",
      "closure"
    ]
  }
}