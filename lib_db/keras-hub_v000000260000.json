{
  "__version__": [],
  "version": [],
  "maybe_register_serializable": [
    "path",
    "symbol"
  ],
  "convert_to_comparible_type": [
    "x"
  ],
  "TestCase": {
    "assertAllClose": [
      "self",
      "x1",
      "x2",
      "atol",
      "rtol",
      "msg"
    ],
    "assertEqual": [
      "self",
      "x1",
      "x2",
      "msg"
    ],
    "assertAllEqual": [
      "self",
      "x1",
      "x2",
      "msg"
    ],
    "assertDTypeEqual": [
      "self",
      "x",
      "expected_dtype",
      "msg"
    ],
    "run_layer_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data",
      "expected_output_shape",
      "expected_output_data",
      "expected_num_trainable_weights",
      "expected_num_non_trainable_weights",
      "expected_num_non_trainable_variables",
      "run_training_check",
      "run_precision_checks"
    ],
    "run_preprocessing_layer_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data",
      "expected_output",
      "expected_detokenize_output",
      "return_output"
    ],
    "run_preprocessor_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data",
      "expected_output",
      "expected_detokenize_output",
      "token_id_key"
    ],
    "run_serialization_test": [
      "self",
      "instance"
    ],
    "run_precision_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data"
    ],
    "run_quantization_test": [
      "self",
      "instance",
      "cls",
      "init_kwargs",
      "input_data"
    ],
    "run_model_saving_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data",
      "atol",
      "rtol"
    ],
    "_verify_litert_outputs": [
      "self",
      "keras_output",
      "litert_output",
      "sig_outputs",
      "expected_output_shape",
      "verify_numerics",
      "comparison_mode",
      "output_thresholds"
    ],
    "_verify_litert_numerics": [
      "self",
      "keras_output",
      "litert_output",
      "sig_outputs",
      "output_thresholds",
      "comparison_mode"
    ],
    "run_litert_export_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data",
      "expected_output_shape",
      "model",
      "verify_numerics",
      "output_thresholds"
    ],
    "_compare_outputs": [
      "self",
      "keras_val",
      "litert_val",
      "comparison_mode",
      "key",
      "max_threshold",
      "mean_threshold"
    ],
    "run_backbone_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data",
      "expected_output_shape",
      "variable_length_data",
      "run_mixed_precision_check",
      "run_quantization_check"
    ],
    "run_vision_backbone_test": [
      "self",
      "cls",
      "init_kwargs",
      "input_data",
      "expected_output_shape",
      "spatial_output_keys",
      "expected_pyramid_output_keys",
      "expected_pyramid_image_sizes",
      "variable_length_data",
      "run_mixed_precision_check",
      "run_quantization_check",
      "run_data_format_check"
    ],
    "run_task_test": [
      "self",
      "cls",
      "init_kwargs",
      "train_data",
      "expected_output_shape",
      "batch_size"
    ],
    "run_preset_test": [
      "self",
      "cls",
      "preset",
      "input_data",
      "init_kwargs",
      "expected_output",
      "expected_output_shape",
      "expected_partial_output",
      "expected_labels"
    ],
    "get_test_data_dir": [
      "self"
    ],
    "load_test_image": [
      "self",
      "target_size"
    ]
  },
  "MockGemma3Tokenizer": {
    "__init__": [
      "self",
      "proto",
      "sequence_length",
      "dtype",
      "add_bos",
      "add_eos"
    ],
    "vocabulary_size": [
      "self"
    ],
    "get_vocabulary": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "RougeN": {
    "__init__": [
      "self",
      "order",
      "use_stemmer",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "REPLACE_SUBSTRINGS": [],
  "REGEX_PATTERNS": [],
  "Bleu": {
    "__init__": [
      "self",
      "tokenizer",
      "max_order",
      "smooth",
      "dtype",
      "name"
    ],
    "_tokenizer": [
      "self",
      "inputs"
    ],
    "_get_ngrams": [
      "self",
      "segment",
      "max_order"
    ],
    "_corpus_bleu": [
      "self",
      "reference_corpus",
      "translation_corpus",
      "matches_by_order",
      "possible_matches_by_order",
      "translation_length",
      "reference_length",
      "max_order",
      "smooth"
    ],
    "_calculate_bleu_score": [
      "self",
      "references",
      "translation"
    ],
    "update_state": [
      "self",
      "y_true",
      "y_pred",
      "sample_weight"
    ],
    "result": [
      "self"
    ],
    "reset_state": [
      "self"
    ],
    "get_config": [
      "self"
    ]
  },
  "RougeL": {
    "__init__": [
      "self",
      "use_stemmer",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "EditDistance": {
    "__init__": [
      "self",
      "normalize",
      "dtype",
      "name"
    ],
    "update_state": [
      "self",
      "y_true",
      "y_pred",
      "sample_weight"
    ],
    "result": [
      "self"
    ],
    "reset_state": [
      "self"
    ],
    "get_config": [
      "self"
    ]
  },
  "RougeBase": {
    "__init__": [
      "self",
      "variant",
      "use_stemmer",
      "dtype",
      "name"
    ],
    "update_state": [
      "self",
      "y_true",
      "y_pred",
      "sample_weight"
    ],
    "result": [
      "self"
    ],
    "reset_state": [
      "self"
    ],
    "get_config": [
      "self"
    ]
  },
  "Perplexity": {
    "__init__": [
      "self",
      "from_logits",
      "mask_token_id",
      "dtype",
      "name"
    ],
    "update_state": [
      "self",
      "y_true",
      "y_pred",
      "sample_weight"
    ],
    "result": [
      "self"
    ],
    "reset_state": [
      "self"
    ],
    "get_config": [
      "self"
    ]
  },
  "compute_sentence_piece_proto": [
    "data",
    "vocabulary_size",
    "model_type",
    "proto_output_file",
    "lowercase"
  ],
  "compute_word_piece_vocabulary": [
    "data",
    "vocabulary_size",
    "vocabulary_output_file",
    "lowercase",
    "strip_accents",
    "split",
    "split_on_cjk",
    "suffix_indicator",
    "reserved_tokens"
  ],
  "VOCAB_FILENAME": [],
  "WHITESPACE_REGEX": [],
  "PUNCTUATION_REGEX": [],
  "CJK_REGEX": [],
  "WHITESPACE_AND_PUNCTUATION_REGEX": [],
  "PUNCTUATION_AND_CJK_REGEX": [],
  "WHITESPACE_PUNCTUATION_AND_CJK_REGEX": [],
  "get_special_tokens_pattern": [
    "special_tokens"
  ],
  "pretokenize": [
    "text",
    "lowercase",
    "strip_accents",
    "split",
    "split_on_cjk",
    "special_tokens_pattern"
  ],
  "WordPieceTokenizer": {
    "__init__": [
      "self",
      "vocabulary",
      "sequence_length",
      "lowercase",
      "strip_accents",
      "split",
      "split_on_cjk",
      "suffix_indicator",
      "oov_token",
      "special_tokens",
      "special_tokens_in_strings",
      "dtype"
    ],
    "save_assets": [
      "self",
      "dir_path"
    ],
    "load_assets": [
      "self",
      "dir_path"
    ],
    "set_vocabulary": [
      "self",
      "vocabulary"
    ],
    "get_vocabulary": [
      "self"
    ],
    "vocabulary_size": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "get_config": [
      "self"
    ],
    "_check_vocabulary": [
      "self"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "compute_output_spec": [
      "self",
      "input_spec"
    ]
  },
  "Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "get_vocabulary": [
      "self"
    ],
    "vocabulary_size": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "special_tokens": [
      "self"
    ],
    "special_token_ids": [
      "self"
    ],
    "_add_special_token": [
      "self",
      "token",
      "name"
    ],
    "_update_special_token_ids": [
      "self"
    ],
    "get_config": [
      "self"
    ],
    "save_to_preset": [
      "self",
      "preset_dir"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "load_preset_assets": [
      "self",
      "preset"
    ],
    "presets": [
      "cls"
    ],
    "from_preset": [
      "cls",
      "preset",
      "config_file"
    ],
    "export_to_transformers": [
      "self",
      "path"
    ]
  },
  "SentencePieceTokenizer": {
    "__init__": [
      "self",
      "proto",
      "sequence_length",
      "dtype",
      "add_bos",
      "add_eos"
    ],
    "save_assets": [
      "self",
      "dir_path"
    ],
    "load_assets": [
      "self",
      "dir_path"
    ],
    "set_proto": [
      "self",
      "proto"
    ],
    "vocabulary_size": [
      "self"
    ],
    "get_vocabulary": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "get_config": [
      "self"
    ],
    "_check_vocabulary": [
      "self"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "compute_output_spec": [
      "self",
      "input_spec"
    ]
  },
  "ByteTokenizer": {
    "__init__": [
      "self",
      "lowercase",
      "sequence_length",
      "normalization_form",
      "errors",
      "replacement_char",
      "dtype"
    ],
    "vocabulary_size": [
      "self"
    ],
    "get_vocabulary": [
      "self"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "get_config": [
      "self"
    ]
  },
  "UnicodeCodepointTokenizer": {
    "__init__": [
      "self",
      "sequence_length",
      "lowercase",
      "normalization_form",
      "errors",
      "replacement_char",
      "input_encoding",
      "output_encoding",
      "vocabulary_size",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "vocabulary_size": [
      "self"
    ],
    "get_vocabulary": [
      "self"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ]
  },
  "MERGES_FILENAME": [],
  "SPECIAL_WHITESPACES": [],
  "SPLIT_PATTERN_1": [],
  "SPLIT_PATTERN_2": [],
  "create_alts_for_unsplittable_tokens": [
    "unsplittable_tokens"
  ],
  "bytes_to_unicode": [],
  "remove_strings_from_inputs": [
    "tensor",
    "string_to_remove"
  ],
  "split_strings_for_bpe": [
    "inputs",
    "unsplittable_tokens"
  ],
  "BytePairTokenizerCache": {
    "__init__": [
      "self"
    ],
    "_get_key": [
      "self",
      "keys"
    ],
    "lookup": [
      "self",
      "keys"
    ],
    "insert": [
      "self",
      "keys",
      "values"
    ]
  },
  "create_static_hashtable": [
    "keys",
    "values",
    "default"
  ],
  "BytePairTokenizer": {
    "__init__": [
      "self",
      "vocabulary",
      "merges",
      "sequence_length",
      "add_prefix_space",
      "unsplittable_tokens",
      "dtype"
    ],
    "save_assets": [
      "self",
      "dir_path"
    ],
    "load_assets": [
      "self",
      "dir_path"
    ],
    "set_vocabulary_and_merges": [
      "self",
      "vocabulary",
      "merges"
    ],
    "get_vocabulary": [
      "self"
    ],
    "vocabulary_size": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "_bpe_merge_one_step": [
      "self",
      "words",
      "mask"
    ],
    "_bpe_merge": [
      "self",
      "inputs"
    ],
    "_check_vocabulary": [
      "self"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "compute_output_spec": [
      "self",
      "input_spec"
    ],
    "_transform_bytes": [
      "self",
      "tokens"
    ],
    "_bpe_merge_and_update_cache": [
      "self",
      "tokens"
    ],
    "get_config": [
      "self"
    ]
  },
  "_core": [],
  "get_core": [],
  "get_device": [],
  "compile_model": [
    "struct_params",
    "struct_outputs",
    "device",
    "model_dtype"
  ],
  "get_outputs": [
    "inputs",
    "struct_outputs",
    "compiled_ov_model",
    "unpack_singleton"
  ],
  "ov_infer": [
    "model",
    "inputs",
    "stop_token_ids",
    "fn"
  ],
  "clone_initializer": [
    "initializer"
  ],
  "print_msg": [
    "message",
    "line_break"
  ],
  "gelu_approximate": [
    "x"
  ],
  "standardize_data_format": [
    "data_format"
  ],
  "fused_attention_op_available": [],
  "running_on_tpu": [],
  "running_on_gpu": [],
  "gpu_supports_fused_attention_op": [],
  "get_gpu_names": [],
  "sharded_weights_available": [],
  "KAGGLE_PREFIX": [],
  "GS_PREFIX": [],
  "HF_PREFIX": [],
  "MODELSCOPE_PREFIX": [],
  "KAGGLE_SCHEME": [],
  "GS_SCHEME": [],
  "HF_SCHEME": [],
  "MODELSCOPE_SCHEME": [],
  "ASSET_DIR": [],
  "TOKENIZER_ASSET_DIR": [],
  "CONFIG_FILE": [],
  "TOKENIZER_CONFIG_FILE": [],
  "AUDIO_CONVERTER_CONFIG_FILE": [],
  "IMAGE_CONVERTER_CONFIG_FILE": [],
  "TASK_CONFIG_FILE": [],
  "PREPROCESSOR_CONFIG_FILE": [],
  "METADATA_FILE": [],
  "MODEL_WEIGHTS_FILE": [],
  "TASK_WEIGHTS_FILE": [],
  "SHARDED_MODEL_WEIGHTS_CONFIG_FILE": [],
  "README_FILE": [],
  "HF_CONFIG_FILE": [],
  "HF_TOKENIZER_CONFIG_FILE": [],
  "SAFETENSOR_CONFIG_FILE": [],
  "SAFETENSOR_FILE": [],
  "BUILTIN_PRESETS": [],
  "BUILTIN_PRESETS_FOR_BACKBONE": [],
  "register_presets": [
    "presets",
    "backbone_cls"
  ],
  "builtin_presets": [
    "cls"
  ],
  "list_subclasses": [
    "cls"
  ],
  "find_subclass": [
    "preset",
    "cls",
    "backbone_cls"
  ],
  "get_file": [
    "preset",
    "path"
  ],
  "tf_registered_schemes": [],
  "tf_copy_gfile_to_cache": [
    "preset",
    "path"
  ],
  "check_file_exists": [
    "preset",
    "path"
  ],
  "_validate_backbone": [
    "preset"
  ],
  "to_snake_case": [
    "name"
  ],
  "create_model_card": [
    "preset"
  ],
  "delete_model_card": [
    "preset"
  ],
  "upload_preset": [
    "uri",
    "preset"
  ],
  "load_json": [
    "preset",
    "config_file"
  ],
  "check_config_class": [
    "config"
  ],
  "jax_memory_cleanup": [
    "layer"
  ],
  "set_dtype_in_config": [
    "config",
    "dtype"
  ],
  "get_preset_loader": [
    "preset"
  ],
  "get_preset_saver": [
    "preset"
  ],
  "PresetLoader": {
    "__init__": [
      "self",
      "preset",
      "config"
    ],
    "get_backbone_kwargs": [
      "self"
    ],
    "check_backbone_class": [
      "self"
    ],
    "load_backbone": [
      "self",
      "cls",
      "load_weights"
    ],
    "load_tokenizer": [
      "self",
      "cls",
      "config_file"
    ],
    "load_audio_converter": [
      "self",
      "cls"
    ],
    "load_image_converter": [
      "self",
      "cls"
    ],
    "load_task": [
      "self",
      "cls",
      "load_weights",
      "load_task_weights"
    ],
    "load_preprocessor": [
      "self",
      "cls",
      "config_file"
    ]
  },
  "KerasPresetLoader": {
    "check_backbone_class": [
      "self"
    ],
    "load_backbone": [
      "self",
      "cls",
      "load_weights"
    ],
    "load_tokenizer": [
      "self",
      "cls",
      "config_file"
    ],
    "load_audio_converter": [
      "self",
      "cls"
    ],
    "load_image_converter": [
      "self",
      "cls"
    ],
    "load_task": [
      "self",
      "cls",
      "load_weights",
      "load_task_weights"
    ],
    "_resolve_dtype": [
      "self",
      "config",
      "kwargs"
    ],
    "load_preprocessor": [
      "self",
      "cls",
      "config_file"
    ],
    "_load_serialized_object": [
      "self",
      "config"
    ],
    "_get_sharded_filenames": [
      "self",
      "config_path"
    ],
    "_load_backbone_weights": [
      "self",
      "backbone"
    ]
  },
  "KerasPresetSaver": {
    "__init__": [
      "self",
      "preset_dir"
    ],
    "save_backbone": [
      "self",
      "backbone",
      "max_shard_size"
    ],
    "save_tokenizer": [
      "self",
      "tokenizer"
    ],
    "save_audio_converter": [
      "self",
      "converter"
    ],
    "save_image_converter": [
      "self",
      "converter"
    ],
    "save_task": [
      "self",
      "task",
      "max_shard_size"
    ],
    "save_preprocessor": [
      "self",
      "preprocessor"
    ],
    "_recursive_pop": [
      "self",
      "config",
      "key"
    ],
    "_save_serialized_object": [
      "self",
      "layer",
      "config_file"
    ],
    "_save_metadata": [
      "self",
      "layer"
    ],
    "_get_variables_size_in_bytes": [
      "self",
      "variables"
    ]
  },
  "classproperty": {
    "__get__": [
      "self",
      "_",
      "owner_cls"
    ]
  },
  "_convert_inputs_to_dataset": [
    "x",
    "y",
    "sample_weight",
    "batch_size"
  ],
  "_train_validation_split": [
    "arrays",
    "validation_split"
  ],
  "PipelineModel": {
    "__init__": [
      "self"
    ],
    "preprocess_samples": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "fit": [
      "self",
      "x",
      "y",
      "batch_size",
      "sample_weight",
      "validation_data",
      "validation_split"
    ],
    "evaluate": [
      "self",
      "x",
      "y",
      "batch_size",
      "sample_weight"
    ],
    "predict": [
      "self",
      "x",
      "batch_size"
    ],
    "train_on_batch": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "test_on_batch": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "predict_on_batch": [
      "self",
      "x"
    ]
  },
  "NO_CONVERT_COUNTER": [],
  "pad": [
    "x",
    "shape",
    "padding_side",
    "pad_value"
  ],
  "no_convert_scope": [],
  "in_tf_function": [],
  "in_no_convert_scope": [],
  "preprocessing_function": [
    "fn"
  ],
  "convert_preprocessing_inputs": [
    "x"
  ],
  "convert_preprocessing_outputs": [
    "x"
  ],
  "_decode_strings_to_utf8": [
    "inputs"
  ],
  "tensor_to_list": [
    "inputs"
  ],
  "convert_to_ragged_batch": [
    "inputs"
  ],
  "truncate_at_token": [
    "inputs",
    "token",
    "mask"
  ],
  "strip_to_ragged": [
    "token_ids",
    "mask",
    "ids_to_strip"
  ],
  "assert_tf_installed": [
    "symbol_name"
  ],
  "assert_tf_libs_installed": [
    "symbol_name"
  ],
  "check_bounding_box_support": [],
  "assert_bounding_box_support": [
    "symbol_name"
  ],
  "assert_tf_backend": [
    "symbol_name"
  ],
  "is_tensor_type": [
    "x"
  ],
  "is_float_dtype": [
    "dtype"
  ],
  "is_int_dtype": [
    "dtype"
  ],
  "is_string_dtype": [
    "dtype"
  ],
  "get_dtype_size_in_bits": [
    "dtype"
  ],
  "get_tensor_size_in_bits": [
    "shape",
    "dtype"
  ],
  "any_equal": [
    "inputs",
    "values",
    "padding_mask"
  ],
  "target_gather": [
    "targets",
    "indices",
    "mask",
    "mask_val"
  ],
  "backbone_cls": [],
  "convert_backbone_config": [
    "transformers_config"
  ],
  "convert_weights": [
    "backbone",
    "loader",
    "transformers_config"
  ],
  "convert_tokenizer": [
    "cls",
    "preset"
  ],
  "SafetensorLoader": {
    "__init__": [
      "self",
      "preset",
      "prefix",
      "fname"
    ],
    "get_prefixed_key": [
      "self",
      "hf_weight_key",
      "dict_like"
    ],
    "get_tensor": [
      "self",
      "hf_weight_key"
    ],
    "port_weight": [
      "self",
      "keras_variable",
      "hf_weight_key",
      "hook_fn"
    ]
  },
  "convert_head": [
    "task",
    "loader",
    "transformers_config"
  ],
  "load_image_converter_config": [
    "preset",
    "transformers_config"
  ],
  "_resolve_multimodal_prefix": [
    "loader"
  ],
  "TransformersPresetLoader": {
    "__init__": [
      "self",
      "preset",
      "config"
    ],
    "check_backbone_class": [
      "self"
    ],
    "load_backbone": [
      "self",
      "cls",
      "load_weights"
    ],
    "load_task": [
      "self",
      "cls",
      "load_weights",
      "load_task_weights"
    ],
    "load_tokenizer": [
      "self",
      "cls",
      "config_name"
    ],
    "load_image_converter": [
      "self",
      "cls"
    ]
  },
  "transpose_and_reshape": [
    "x",
    "shape"
  ],
  "get_gemma_config": [
    "backbone"
  ],
  "get_gemma_weights_map": [
    "backbone",
    "include_lm_head"
  ],
  "get_gemma_tokenizer_config": [
    "tokenizer"
  ],
  "get_gemma3_config": [
    "backbone"
  ],
  "get_gemma3_weights_map": [
    "backbone",
    "include_lm_head"
  ],
  "get_gemma3_tokenizer_config": [
    "tokenizer"
  ],
  "get_qwen_config": [
    "backbone"
  ],
  "get_qwen_weights_map": [
    "backbone",
    "include_lm_head"
  ],
  "get_qwen_tokenizer_config": [
    "tokenizer"
  ],
  "MODEL_CONFIGS": [],
  "MODEL_EXPORTERS": [],
  "MODEL_TOKENIZER_CONFIGS": [],
  "export_backbone": [
    "backbone",
    "path",
    "include_lm_head"
  ],
  "export_tokenizer": [
    "tokenizer",
    "path"
  ],
  "export_to_safetensors": [
    "keras_model",
    "path"
  ],
  "VARIANT_MAP": [],
  "TimmPresetLoader": {
    "__init__": [
      "self",
      "preset",
      "config"
    ],
    "check_backbone_class": [
      "self"
    ],
    "load_backbone": [
      "self",
      "cls",
      "load_weights"
    ],
    "load_task": [
      "self",
      "cls",
      "load_weights",
      "load_task_weights"
    ],
    "load_image_converter": [
      "self",
      "cls"
    ]
  },
  "REPEATS_BY_SIZE": [],
  "convert_conv2d": [
    "model",
    "loader",
    "keras_layer_name",
    "hf_layer_name"
  ],
  "imagenet_id_to_name": [
    "id"
  ],
  "imagenet_name_to_id": [
    "name"
  ],
  "decode_imagenet_predictions": [
    "preds",
    "top",
    "include_synset_ids"
  ],
  "IMAGENET_NAMES": [],
  "IMAGENET_IDS": [],
  "coco_id_to_name": [
    "id"
  ],
  "coco_name_to_id": [
    "name"
  ],
  "COCO_NAMES": [],
  "COCO_IDS": [],
  "BeamSampler": {
    "__init__": [
      "self",
      "num_beams",
      "return_all_beams"
    ],
    "__call__": [
      "self",
      "next",
      "prompt",
      "cache",
      "index",
      "mask",
      "stop_token_ids",
      "hidden_states",
      "model"
    ],
    "get_config": [
      "self"
    ]
  },
  "ContrastiveSampler": {
    "__init__": [
      "self",
      "k",
      "alpha"
    ],
    "__call__": [
      "self",
      "next",
      "prompt",
      "cache",
      "index",
      "mask",
      "stop_token_ids",
      "hidden_states",
      "model"
    ],
    "similarity": [
      "self",
      "h1",
      "h2"
    ],
    "get_config": [
      "self"
    ]
  },
  "GreedySampler": {
    "__init__": [
      "self"
    ],
    "get_next_token": [
      "self",
      "probabilities"
    ]
  },
  "RandomSampler": {
    "__init__": [
      "self",
      "seed"
    ],
    "get_next_token": [
      "self",
      "probabilities"
    ],
    "get_config": [
      "self"
    ]
  },
  "TopPSampler": {
    "__init__": [
      "self",
      "p",
      "k",
      "seed"
    ],
    "get_next_token": [
      "self",
      "probabilities"
    ],
    "get_config": [
      "self"
    ]
  },
  "Sampler": {
    "__init__": [
      "self",
      "temperature"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "variables": [
      "self"
    ],
    "__call__": [
      "self",
      "next",
      "prompt",
      "cache",
      "index",
      "mask",
      "stop_token_ids",
      "hidden_states",
      "model"
    ],
    "compute_probabilities": [
      "self",
      "logits"
    ],
    "run_loop": [
      "self",
      "cond",
      "body",
      "model",
      "loop_vars",
      "maximum_iterations"
    ],
    "get_next_token": [
      "self",
      "probabilities"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "get_config": [
      "self"
    ]
  },
  "TopKSampler": {
    "__init__": [
      "self",
      "k",
      "seed"
    ],
    "get_next_token": [
      "self",
      "probabilities"
    ],
    "get_config": [
      "self"
    ]
  },
  "serialize": [
    "sampler"
  ],
  "deserialize": [
    "config",
    "custom_objects"
  ],
  "get": [
    "identifier"
  ],
  "SinePositionEncoding": {
    "__init__": [
      "self",
      "max_wavelength"
    ],
    "call": [
      "self",
      "inputs",
      "start_index",
      "positions"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "AnchorGenerator": {
    "__init__": [
      "self",
      "bounding_box_format",
      "min_level",
      "max_level",
      "num_scales",
      "aspect_ratios",
      "anchor_size"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "generate_base_anchors": [
      "self",
      "sizes",
      "aspect_ratios"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "_check_masks_shapes": [
    "inputs",
    "padding_mask",
    "attention_mask"
  ],
  "compute_causal_mask": [
    "batch_size",
    "input_length",
    "output_length",
    "cache_index"
  ],
  "merge_padding_and_attention_mask": [
    "inputs",
    "padding_mask",
    "attention_mask"
  ],
  "RMSNormalization": {
    "__init__": [
      "self",
      "input_dim"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "FNetEncoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "dropout",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "bias_initializer"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "CachedMultiHeadAttention": {
    "call": [
      "self",
      "query",
      "value",
      "key",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ]
  },
  "TokenAndPositionEmbedding": {
    "__init__": [
      "self",
      "vocabulary_size",
      "sequence_length",
      "embedding_dim",
      "tie_weights",
      "embeddings_initializer",
      "mask_zero"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ],
    "call": [
      "self",
      "inputs",
      "start_index",
      "positions"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "BoxMatcher": {
    "__init__": [
      "self",
      "thresholds",
      "match_values",
      "force_match_for_each_col"
    ],
    "call": [
      "self",
      "similarity_matrix"
    ],
    "_set_values_using_indicator": [
      "self",
      "x",
      "indicator",
      "val"
    ],
    "get_config": [
      "self"
    ]
  },
  "RotaryEmbedding": {
    "__init__": [
      "self",
      "max_wavelength",
      "scaling_factor",
      "rope_type",
      "beta_fast",
      "beta_slow",
      "original_max_position_embeddings",
      "truncate",
      "sequence_axis",
      "feature_axis"
    ],
    "_normalize_axes": [
      "self",
      "input_shape"
    ],
    "_validate_rotary_dimension": [
      "self",
      "rotary_dim"
    ],
    "call": [
      "self",
      "inputs",
      "start_index",
      "positions"
    ],
    "_apply_rotary_pos_emb": [
      "self",
      "tensor",
      "cos_emb",
      "sin_emb"
    ],
    "_compute_positions": [
      "self",
      "inputs",
      "start_index"
    ],
    "_compute_cos_sin_embedding": [
      "self",
      "inputs",
      "start_index",
      "positions"
    ],
    "_get_inverse_freq": [
      "self",
      "rotary_dim"
    ],
    "_get_yarn_inverse_freq": [
      "self",
      "rotary_dim"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "AlibiBias": {
    "__init__": [
      "self",
      "alibi_bias_max"
    ],
    "call": [
      "self",
      "attention_scores"
    ],
    "_get_alibi_bias": [
      "self",
      "num_heads",
      "key_length"
    ],
    "_get_slopes": [
      "self",
      "num_heads"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "TransformerEncoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_heads",
      "dropout",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "bias_initializer",
      "normalize_first"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "padding_mask",
      "attention_mask",
      "training",
      "return_attention_scores"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "PositionEmbedding": {
    "__init__": [
      "self",
      "sequence_length",
      "initializer"
    ],
    "get_config": [
      "self"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "start_index",
      "positions"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "EPSILON": [],
  "NonMaxSuppression": {
    "__init__": [
      "self",
      "bounding_box_format",
      "from_logits",
      "iou_threshold",
      "confidence_threshold",
      "max_detections"
    ],
    "call": [
      "self",
      "box_prediction",
      "class_prediction",
      "images"
    ],
    "get_config": [
      "self"
    ]
  },
  "non_max_suppression": [
    "boxes",
    "scores",
    "max_output_size",
    "iou_threshold",
    "score_threshold",
    "tile_size"
  ],
  "_bbox_overlap": [
    "boxes_a",
    "boxes_b"
  ],
  "_self_suppression": [
    "iou",
    "_",
    "iou_sum",
    "iou_threshold"
  ],
  "_cross_suppression": [
    "boxes",
    "box_slice",
    "iou_threshold",
    "inner_idx",
    "tile_size"
  ],
  "_suppression_loop_body": [
    "boxes",
    "iou_threshold",
    "output_size",
    "idx",
    "tile_size"
  ],
  "mask_invalid_detections": [
    "bounding_boxes"
  ],
  "MaskedLMHead": {
    "__init__": [
      "self",
      "vocabulary_size",
      "token_embedding",
      "intermediate_activation",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "bias_initializer"
    ],
    "build": [
      "self",
      "inputs_shape",
      "mask_positions_shape"
    ],
    "call": [
      "self",
      "inputs",
      "mask_positions"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape",
      "mask_positions_shape"
    ]
  },
  "TransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_heads",
      "dropout",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "bias_initializer",
      "normalize_first"
    ],
    "build": [
      "self",
      "decoder_sequence_shape",
      "encoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "encoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "encoder_padding_mask",
      "encoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "cross_attention_cache",
      "cross_attention_cache_update_index",
      "use_causal_mask",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "use_causal_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ]
  },
  "ReversibleEmbedding": {},
  "AudioConverter": {
    "backbone_cls": [],
    "audio_shape": [
      "self"
    ],
    "presets": [
      "cls"
    ],
    "from_preset": [
      "cls",
      "preset"
    ],
    "save_to_preset": [
      "self",
      "preset_dir"
    ]
  },
  "MultiSegmentPacker": {
    "__init__": [
      "self",
      "sequence_length",
      "start_value",
      "end_value",
      "sep_value",
      "pad_value",
      "truncate",
      "padding_side"
    ],
    "get_config": [
      "self"
    ],
    "_sanitize_inputs": [
      "self",
      "inputs"
    ],
    "_trim_inputs": [
      "self",
      "inputs"
    ],
    "_combine_inputs": [
      "self",
      "segments",
      "add_start_value",
      "add_end_value"
    ],
    "call": [
      "self",
      "inputs",
      "sequence_length",
      "add_start_value",
      "add_end_value"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "StartEndPacker": {
    "__init__": [
      "self",
      "sequence_length",
      "start_value",
      "end_value",
      "pad_value",
      "return_padding_mask",
      "name",
      "padding_side"
    ],
    "call": [
      "self",
      "inputs",
      "sequence_length",
      "add_start_value",
      "add_end_value"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "PreprocessingLayer": {
    "__init__": [
      "self"
    ],
    "get_build_config": [
      "self"
    ]
  },
  "_saturate_cast": [
    "x",
    "dtype",
    "backend_module"
  ],
  "ResizingAntialiasConfigurable": {
    "__init__": [
      "self",
      "height",
      "width",
      "interpolation",
      "antialias",
      "crop_to_aspect_ratio",
      "pad_to_aspect_ratio",
      "fill_mode",
      "fill_value",
      "data_format"
    ],
    "transform_images": [
      "self",
      "images",
      "transformation",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "ImageConverter": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "image_size",
      "scale",
      "offset",
      "crop_to_aspect_ratio",
      "pad_to_aspect_ratio",
      "interpolation",
      "antialias",
      "bounding_box_format",
      "data_format"
    ],
    "image_size": [
      "self",
      "value"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "_expand_non_channel_dims": [
      "self",
      "value",
      "inputs"
    ],
    "_convert_types": [
      "self",
      "x",
      "y",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "presets": [
      "cls"
    ],
    "from_preset": [
      "cls",
      "preset"
    ],
    "save_to_preset": [
      "self",
      "preset_dir"
    ]
  },
  "RandomSwap": {
    "__init__": [
      "self",
      "rate",
      "max_swaps",
      "skip_list",
      "skip_fn",
      "skip_py_fn",
      "seed",
      "name",
      "dtype"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "MaskedLMMaskGenerator": {
    "__init__": [
      "self",
      "vocabulary_size",
      "mask_selection_rate",
      "mask_token_id",
      "mask_selection_length",
      "unselectable_token_ids",
      "mask_token_rate",
      "random_token_rate"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "RandomDeletion": {
    "__init__": [
      "self",
      "rate",
      "max_deletions",
      "skip_list",
      "skip_fn",
      "skip_py_fn",
      "seed",
      "name",
      "dtype"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "Inpaint": {
    "__init__": [
      "self"
    ],
    "support_negative_prompts": [
      "self"
    ],
    "image_shape": [
      "self"
    ],
    "latent_shape": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "generate_step": [
      "self"
    ],
    "make_generate_function": [
      "self"
    ],
    "_normalize_generate_images": [
      "self",
      "inputs"
    ],
    "_normalize_generate_masks": [
      "self",
      "inputs"
    ],
    "_normalize_generate_inputs": [
      "self",
      "inputs"
    ],
    "_normalize_generate_outputs": [
      "self",
      "outputs",
      "input_is_scalar"
    ],
    "generate": [
      "self",
      "inputs",
      "num_steps",
      "strength",
      "guidance_scale",
      "seed"
    ],
    "_post_quantize": [
      "self",
      "mode"
    ]
  },
  "ObjectDetector": {
    "compile": [
      "self",
      "optimizer",
      "box_loss",
      "classification_loss",
      "metrics"
    ]
  },
  "Task": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self"
    ],
    "preprocess_samples": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "backbone": [
      "self",
      "value"
    ],
    "preprocessor": [
      "self",
      "value"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "presets": [
      "cls"
    ],
    "from_preset": [
      "cls",
      "preset",
      "load_weights"
    ],
    "load_task_weights": [
      "self",
      "filepath"
    ],
    "has_task_weights": [
      "self"
    ],
    "save_task_weights": [
      "self",
      "filepath"
    ],
    "save_to_preset": [
      "self",
      "preset_dir",
      "max_shard_size"
    ],
    "layers": [
      "self"
    ],
    "summary": [
      "self",
      "line_length",
      "positions",
      "print_fn"
    ]
  },
  "MaskedLM": {
    "__init__": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "get_quantization_layer_structure": [
      "self",
      "mode"
    ]
  },
  "FeaturePyramidBackbone": {
    "pyramid_outputs": [
      "self",
      "value"
    ]
  },
  "Seq2SeqLMPreprocessor": {
    "__init__": [
      "self",
      "tokenizer",
      "encoder_sequence_length",
      "decoder_sequence_length"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "generate_preprocess": [
      "self",
      "x"
    ],
    "generate_postprocess": [
      "self",
      "x"
    ],
    "encoder_sequence_length": [
      "self",
      "value"
    ],
    "decoder_sequence_length": [
      "self",
      "value"
    ],
    "sequence_length": [
      "self",
      "value"
    ]
  },
  "ImageSegmenter": {
    "compile": [
      "self",
      "optimizer",
      "loss"
    ]
  },
  "TextToImagePreprocessor": {},
  "AudioToTextPreprocessor": {},
  "CausalLMPreprocessor": {
    "__init__": [
      "self",
      "tokenizer",
      "sequence_length",
      "add_start_token",
      "add_end_token"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "generate_preprocess": [
      "self",
      "x",
      "sequence_length"
    ],
    "generate_postprocess": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ],
    "sequence_length": [
      "self",
      "value"
    ],
    "export_to_transformers": [
      "self",
      "path"
    ]
  },
  "DepthEstimatorPreprocessor": {
    "__init__": [
      "self",
      "image_converter"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "TextToImage": {
    "__init__": [
      "self"
    ],
    "support_negative_prompts": [
      "self"
    ],
    "latent_shape": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "generate_step": [
      "self"
    ],
    "make_generate_function": [
      "self"
    ],
    "_normalize_generate_inputs": [
      "self",
      "inputs"
    ],
    "_normalize_generate_outputs": [
      "self",
      "outputs",
      "input_is_scalar"
    ],
    "generate": [
      "self",
      "inputs",
      "num_steps",
      "guidance_scale",
      "seed"
    ],
    "_post_quantize": [
      "self",
      "mode"
    ]
  },
  "AudioToText": {},
  "Seq2SeqLM": {},
  "ImageToImage": {
    "__init__": [
      "self"
    ],
    "support_negative_prompts": [
      "self"
    ],
    "image_shape": [
      "self"
    ],
    "latent_shape": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "generate_step": [
      "self"
    ],
    "make_generate_function": [
      "self"
    ],
    "_normalize_generate_inputs": [
      "self",
      "inputs"
    ],
    "_normalize_generate_outputs": [
      "self",
      "outputs",
      "input_is_scalar"
    ],
    "generate": [
      "self",
      "inputs",
      "num_steps",
      "strength",
      "guidance_scale",
      "seed"
    ],
    "_post_quantize": [
      "self",
      "mode"
    ]
  },
  "ImageClassifier": {
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "pooling",
      "activation",
      "dropout",
      "head_dtype"
    ],
    "get_config": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ]
  },
  "ImageSegmenterPreprocessor": {
    "__init__": [
      "self",
      "image_converter",
      "resize_output_mask"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "Multiplier": {
    "__init__": [
      "self",
      "multiplier"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "DepthEstimator": {
    "__init__": [
      "self",
      "backbone",
      "depth_estimation_type",
      "min_depth",
      "max_depth",
      "preprocessor"
    ],
    "get_config": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ]
  },
  "Preprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "audio_converter_cls": [],
    "image_converter_cls": [],
    "__init__": [
      "self"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "tokenizer": [
      "self",
      "value"
    ],
    "audio_converter": [
      "self",
      "value"
    ],
    "image_converter": [
      "self",
      "value"
    ],
    "image_size": [
      "self",
      "value"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "presets": [
      "cls"
    ],
    "from_preset": [
      "cls",
      "preset",
      "config_file"
    ],
    "_add_missing_kwargs": [
      "cls",
      "loader",
      "kwargs"
    ],
    "load_preset_assets": [
      "self",
      "preset"
    ],
    "save_to_preset": [
      "self",
      "preset_dir"
    ]
  },
  "TextClassifier": {
    "compile": [
      "self",
      "optimizer",
      "loss"
    ]
  },
  "ImageClassifierPreprocessor": {
    "__init__": [
      "self",
      "image_converter"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "CausalLM": {
    "__init__": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "generate_step": [
      "self"
    ],
    "make_generate_function": [
      "self"
    ],
    "_normalize_generate_inputs": [
      "self",
      "inputs"
    ],
    "_normalize_generate_outputs": [
      "self",
      "outputs",
      "input_is_scalar"
    ],
    "generate": [
      "self",
      "inputs",
      "max_length",
      "stop_token_ids",
      "strip_prompt"
    ],
    "export_to_transformers": [
      "self",
      "path"
    ],
    "_post_quantize": [
      "self",
      "mode"
    ],
    "get_quantization_layer_structure": [
      "self",
      "mode"
    ]
  },
  "Backbone": {
    "__init__": [
      "self"
    ],
    "__setattr__": [
      "self",
      "name",
      "value"
    ],
    "token_embedding": [
      "self",
      "value"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "presets": [
      "cls"
    ],
    "from_preset": [
      "cls",
      "preset",
      "load_weights"
    ],
    "save_to_preset": [
      "self",
      "preset_dir",
      "max_shard_size"
    ],
    "default_lora_layer_names": [
      "self"
    ],
    "enable_lora": [
      "self",
      "rank",
      "target_layer_names"
    ],
    "save_lora_weights": [
      "self",
      "filepath"
    ],
    "load_lora_weights": [
      "self",
      "filepath"
    ],
    "export_to_transformers": [
      "self",
      "path"
    ]
  },
  "MaskedLMPreprocessor": {
    "__init__": [
      "self",
      "tokenizer",
      "sequence_length",
      "truncate",
      "mask_selection_rate",
      "mask_selection_length",
      "mask_token_rate",
      "random_token_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "get_config": [
      "self"
    ],
    "sequence_length": [
      "self",
      "value"
    ]
  },
  "TextClassifierPreprocessor": {
    "__init__": [
      "self",
      "tokenizer",
      "sequence_length",
      "truncate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "get_config": [
      "self"
    ],
    "sequence_length": [
      "self",
      "value"
    ]
  },
  "ObjectDetectorPreprocessor": {
    "__init__": [
      "self",
      "image_converter"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "ElectraTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "lowercase"
    ]
  },
  "electra_kernel_initializer": [
    "stddev"
  ],
  "ElectraBackbone": {
    "__init__": [
      "self",
      "vocab_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "embedding_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "num_segments",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "backbone_presets": [],
  "EdRecRMSNormalization": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "EdRecGatedFeedForward": {
    "__init__": [
      "self",
      "intermediate_dim",
      "hidden_dim",
      "dropout_rate",
      "activation",
      "kernel_initializer",
      "bias_initializer"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "EdRecEncoderBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "dropout_rate",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "padding_mask",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "EdRecDecoderBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "dropout_rate",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "encoder_outputs",
      "decoder_padding_mask",
      "encoder_padding_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "cross_attention_cache",
      "cross_attention_cache_update_index",
      "use_causal_mask",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "EdRecSeq2SeqLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_decoder_with_cache": [
      "self",
      "encoder_hidden_states",
      "encoder_padding_mask",
      "decoder_token_ids",
      "decoder_padding_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "cross_attention_cache",
      "cross_attention_cache_update_index"
    ],
    "call_encoder": [
      "self",
      "token_ids",
      "padding_mask"
    ],
    "_initialize_cache": [
      "self",
      "encoder_token_ids",
      "decoder_token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "EdRecBackbone": {
    "__init__": [
      "self",
      "vocab_size",
      "num_layers_enc",
      "num_layers_dec",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "dropout",
      "epsilon",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "token_embedding": [
      "self"
    ]
  },
  "SegFormerImageSegmenter": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "SegFormerImageConverter": {
    "backbone_cls": []
  },
  "presets": [],
  "SegFormerImageSegmenterPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "SegFormerBackbone": {
    "__init__": [
      "self",
      "image_encoder",
      "projection_filters"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "Gemma3CausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "preprocessor",
      "backbone"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "_normalize_generate_inputs": [
      "self",
      "inputs"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index",
      "img_embeddings",
      "vision_mask",
      "padding_mask",
      "vision_indices",
      "cache_update_mask"
    ],
    "_build_cache": [
      "self",
      "token_ids",
      "img_embeddings",
      "vision_mask",
      "padding_mask",
      "vision_indices"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "generate": [
      "self",
      "inputs",
      "max_length",
      "stop_token_ids",
      "strip_prompt"
    ]
  },
  "Gemma3CausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "image_converter_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "image_converter",
      "sequence_length",
      "add_start_token",
      "add_end_token",
      "max_images_per_prompt",
      "num_vision_tokens_per_image"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_get_vision_indices": [
      "self",
      "vision_mask"
    ],
    "_format_output": [
      "self",
      "images",
      "token_ids",
      "vision_mask",
      "response_mask",
      "padding_mask",
      "return_labels",
      "text_only_input",
      "batched"
    ],
    "_preprocess_images": [
      "self",
      "images",
      "batched"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "generate_preprocess": [
      "self",
      "x",
      "sequence_length"
    ],
    "get_config": [
      "self"
    ],
    "generate_postprocess": [
      "self",
      "x"
    ],
    "max_images_per_prompt": [
      "self",
      "value"
    ]
  },
  "START_OF_IMAGE_TOKEN": [],
  "IMAGE_PLACEHOLDER_TOKEN": [],
  "END_OF_IMAGE_TOKEN": [],
  "Gemma3Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto",
      "has_vision_tokens"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3MeanPooling": {
    "__init__": [
      "self"
    ],
    "call": [
      "self",
      "inputs",
      "padding_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3InterleaveEmbeddings": {
    "__init__": [
      "self",
      "num_vision_tokens_per_image",
      "dtype"
    ],
    "call": [
      "self",
      "image_embeddings",
      "text_embeddings",
      "vision_indices"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3ImageConverter": {
    "backbone_cls": [],
    "__init__": [
      "self"
    ]
  },
  "Gemma3Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "image_size",
      "num_layers",
      "num_query_heads",
      "num_key_value_heads",
      "hidden_dim",
      "intermediate_dim",
      "head_dim",
      "query_head_dim_normalize",
      "use_query_key_norm",
      "use_post_ffw_norm",
      "use_post_attention_norm",
      "attention_logit_soft_cap",
      "final_logit_soft_cap",
      "use_sliding_window_attention",
      "sliding_window_size",
      "local_rope_scaling_factor",
      "global_rope_scaling_factor",
      "vision_encoder",
      "layer_norm_epsilon",
      "use_bidirectional_attention",
      "dropout",
      "is_embedding_model",
      "pooling_intermediate_dim",
      "embedding_dim",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "default_lora_layer_names": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "Gemma3VisionEncoder": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_heads",
      "hidden_dim",
      "num_layers",
      "intermediate_dim",
      "output_dim",
      "pool_size",
      "layer_norm_epsilon",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3VisionEmbedding": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "hidden_dim",
      "num_channels",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "input_tokens"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "Gemma3VisionAttention": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "dropout",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_transpose_for_scores": [
      "self",
      "tensor",
      "batch_size"
    ],
    "call": [
      "self",
      "x",
      "attention_mask",
      "return_attention_scores",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3VisionEncoderLayer": {
    "__init__": [
      "self",
      "num_heads",
      "intermediate_dim",
      "layer_norm_epsilon"
    ],
    "compute_attention": [
      "self",
      "x",
      "mask"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "mask"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3VisionEncoderBlock": {
    "__init__": [
      "self",
      "patch_size",
      "image_size",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "intermediate_dim",
      "layer_norm_epsilon",
      "dtype"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "mask"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3VisionAveragePooling": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "pool_size"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "Gemma3VisionOutput": {
    "__init__": [
      "self",
      "output_dim",
      "layer_norm_epsilon",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "Gemma3DecoderBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "head_dim",
      "num_query_heads",
      "num_key_value_heads",
      "query_head_dim_normalize",
      "use_query_key_norm",
      "use_post_ffw_norm",
      "use_post_attention_norm",
      "gate_dim_reduction",
      "logit_soft_cap",
      "use_sliding_window_attention",
      "sliding_window_size",
      "layer_norm_epsilon",
      "rope_wavelength",
      "rope_scaling_factor",
      "use_bidirectional_attention",
      "dropout"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "_compute_image_bidirectional_attention_mask": [
      "self",
      "vision_mask"
    ],
    "_compute_attention_mask": [
      "self",
      "x",
      "padding_mask",
      "vision_mask",
      "cache",
      "cache_update_index"
    ],
    "call": [
      "self",
      "x",
      "padding_mask",
      "vision_mask",
      "cache",
      "cache_update_index",
      "cache_update_mask"
    ],
    "get_config": [
      "self"
    ]
  },
  "CachedGemma3Attention": {
    "__init__": [
      "self",
      "head_dim",
      "num_query_heads",
      "num_key_value_heads",
      "kernel_initializer",
      "logit_soft_cap",
      "use_sliding_window_attention",
      "sliding_window_size",
      "query_head_dim_normalize",
      "use_query_key_norm",
      "layer_norm_epsilon",
      "rope_wavelength",
      "rope_scaling_factor",
      "use_bidirectional_attention",
      "dropout"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "_apply_rope": [
      "self",
      "x",
      "start_index"
    ],
    "_use_fused_attention_op": [
      "self"
    ],
    "_compute_attention": [
      "self",
      "q",
      "k",
      "v",
      "attention_mask",
      "training",
      "cache_update_index"
    ],
    "_compute_bidirectional_sliding_mask": [
      "self",
      "batch_size",
      "sequence_length"
    ],
    "_mask_sliding_window": [
      "self",
      "attention_mask",
      "cache_update_index"
    ],
    "call": [
      "self",
      "x",
      "attention_mask",
      "cache",
      "cache_update_index",
      "cache_update_mask",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "ResNetImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "ResNetImageConverter": {
    "backbone_cls": []
  },
  "ResNetImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "ResNetBackbone": {
    "__init__": [
      "self",
      "input_conv_filters",
      "input_conv_kernel_sizes",
      "stackwise_num_filters",
      "stackwise_num_blocks",
      "stackwise_num_strides",
      "block_type",
      "use_pre_activation",
      "image_shape",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "apply_basic_block": [
    "x",
    "filters",
    "kernel_size",
    "stride",
    "conv_shortcut",
    "use_pre_activation",
    "data_format",
    "dtype",
    "name"
  ],
  "apply_bottleneck_block": [
    "x",
    "filters",
    "kernel_size",
    "stride",
    "conv_shortcut",
    "use_pre_activation",
    "data_format",
    "dtype",
    "name"
  ],
  "apply_basic_block_vd": [
    "x",
    "filters",
    "kernel_size",
    "stride",
    "conv_shortcut",
    "use_pre_activation",
    "data_format",
    "dtype",
    "name"
  ],
  "apply_bottleneck_block_vd": [
    "x",
    "filters",
    "kernel_size",
    "stride",
    "conv_shortcut",
    "use_pre_activation",
    "data_format",
    "dtype",
    "name"
  ],
  "apply_stack": [
    "x",
    "filters",
    "blocks",
    "stride",
    "block_type",
    "use_pre_activation",
    "first_shortcut",
    "data_format",
    "dtype",
    "name"
  ],
  "deberta_kernel_initializer": [
    "stddev"
  ],
  "DebertaV3Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "bucket_size",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "DebertaV3TextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "DebertaV3MaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "DebertaV3Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ],
    "special_tokens": [
      "self"
    ],
    "special_token_ids": [
      "self"
    ],
    "set_proto": [
      "self",
      "proto"
    ],
    "vocabulary_size": [
      "self"
    ],
    "get_vocabulary": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "detokenize": [
      "self",
      "ids"
    ]
  },
  "RelativeEmbedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "bucket_size",
      "layer_norm_epsilon",
      "kernel_initializer"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DisentangledAttentionEncoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_heads",
      "max_position_embeddings",
      "bucket_size",
      "dropout",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "bias_initializer"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "rel_embeddings",
      "padding_mask",
      "attention_mask"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "DebertaV3TextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "hidden_dim",
      "dropout"
    ],
    "get_config": [
      "self"
    ]
  },
  "DebertaV3MaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "DisentangledSelfAttention": {
    "__init__": [
      "self",
      "num_heads",
      "hidden_dim",
      "max_position_embeddings",
      "bucket_size",
      "dropout",
      "kernel_initializer",
      "bias_initializer"
    ],
    "build": [
      "self",
      "inputs_shape",
      "rel_embeddings_shape"
    ],
    "_get_common_kwargs_for_sublayer": [
      "self",
      "use_bias"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "rel_embeddings",
      "attention_mask",
      "training"
    ],
    "_make_log_bucket_position": [
      "self",
      "rel_pos"
    ],
    "_get_rel_pos": [
      "self",
      "num_positions"
    ],
    "_compute_disentangled_attention": [
      "self",
      "query",
      "key",
      "rel_embeddings"
    ],
    "call": [
      "self",
      "inputs",
      "rel_embeddings",
      "attention_mask",
      "return_attention_scores",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "SAM3PromptableConceptBackbone": {
    "__init__": [
      "self",
      "vision_encoder",
      "text_encoder",
      "geometry_encoder",
      "detr_encoder",
      "detr_decoder",
      "mask_decoder",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "window_partition": [
    "x",
    "height",
    "width",
    "window_size",
    "hidden_dim"
  ],
  "window_unpartition": [
    "x",
    "height",
    "width",
    "window_size",
    "hidden_dim"
  ],
  "box_cxcywh_to_xyxy": [
    "boxes"
  ],
  "concatenate_padded_sequences": [
    "sequence1",
    "mask1",
    "sequence_len1",
    "sequence2",
    "mask2",
    "sequence_len2",
    "hidden_dim"
  ],
  "create_bidirectional_mask": [
    "input_embeds",
    "attention_mask"
  ],
  "inverse_sigmoid": [
    "x",
    "eps"
  ],
  "SAM3PromptableConceptImageSegmenterPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "image_converter_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "image_converter",
      "sequence_length",
      "add_start_token",
      "add_end_token",
      "point_pad_value"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_preprocess_boxes": [
      "self",
      "boxes",
      "box_labels",
      "height",
      "width"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "get_config": [
      "self"
    ],
    "sequence_length": [
      "self",
      "value"
    ],
    "image_size": [
      "self",
      "value"
    ]
  },
  "SAM3PromptableConceptImageSegmenter": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "fit": [
      "self"
    ],
    "post_process_prediction": [
      "self",
      "predictions"
    ],
    "predict_step": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "SAM3ImageConverter": {
    "backbone_cls": []
  },
  "SAM3Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ],
    "get_config": [
      "self"
    ]
  },
  "CLIPEncoderLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "intermediate_activation",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "inputs_shape",
      "attention_mask_shape"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape",
      "attention_mask_shape"
    ],
    "call": [
      "self",
      "inputs",
      "attention_mask",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "SAM3TextEncoder": {
    "__init__": [
      "self",
      "vocabulary_size",
      "embedding_dim",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "intermediate_dim",
      "intermediate_activation",
      "max_sequence_length",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "token_ids_shape",
      "padding_masks_shape"
    ],
    "call": [
      "self",
      "token_ids",
      "padding_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "token_ids_shape",
      "padding_masks_shape"
    ],
    "compute_output_spec": [
      "self",
      "token_ids",
      "padding_masks"
    ]
  },
  "SAM3GeometryEncoderLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "hidden_activation",
      "dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "prompt_feats_shape",
      "vision_feats_shape",
      "vision_pos_encodings_shape",
      "prompt_masks_shape"
    ],
    "call": [
      "self",
      "prompt_feats",
      "vision_feats",
      "vision_pos_encodings",
      "prompt_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "prompt_feats_shape",
      "vision_feats_shape",
      "vision_pos_encodings_shape",
      "prompt_masks_shape"
    ]
  },
  "SAM3GeometryEncoder": {
    "__init__": [
      "self",
      "num_layers",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "roi_size",
      "hidden_activation",
      "dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "box_embeddings_shape",
      "box_masks_shape",
      "box_labels_shape",
      "fpn_hidden_states_shape",
      "fpn_position_encodings_shape"
    ],
    "_encode_box_coordinates": [
      "self",
      "center_x",
      "center_y",
      "width",
      "height"
    ],
    "_encode_boxes": [
      "self",
      "boxes",
      "boxes_mask",
      "boxes_labels",
      "vision_features"
    ],
    "_no_box_embeddings": [
      "self",
      "box_embeddings",
      "box_masks"
    ],
    "call": [
      "self",
      "box_embeddings",
      "box_masks",
      "box_labels",
      "fpn_hidden_states",
      "fpn_position_encodings",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "box_embeddings_shape",
      "box_masks_shape",
      "box_labels_shape",
      "fpn_hidden_states_shape",
      "fpn_position_encodings_shape"
    ],
    "compute_output_spec": [
      "self",
      "box_embeddings",
      "box_masks",
      "box_labels",
      "fpn_hidden_states",
      "fpn_position_encodings"
    ]
  },
  "SAM3MaskEmbedder": {
    "__init__": [
      "self",
      "hidden_dim"
    ],
    "build": [
      "self",
      "queries_shape"
    ],
    "call": [
      "self",
      "queries",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "queries_shape"
    ]
  },
  "SAM3PixelDecoder": {
    "__init__": [
      "self",
      "num_upsampling_stages",
      "hidden_dim",
      "data_format"
    ],
    "build": [
      "self",
      "backbone_features_shapes"
    ],
    "call": [
      "self",
      "backbone_features",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "backbone_features_shapes"
    ]
  },
  "SAM3MaskDecoder": {
    "__init__": [
      "self",
      "num_upsampling_stages",
      "hidden_dim",
      "num_heads",
      "dropout_rate",
      "layer_norm_epsilon",
      "data_format"
    ],
    "build": [
      "self",
      "decoder_queries_shape",
      "backbone_features_shape",
      "encoder_hidden_states_shape",
      "prompt_features_shape",
      "prompt_masks_shape"
    ],
    "_embed_pixels": [
      "self",
      "backbone_features",
      "encoder_hidden_states"
    ],
    "call": [
      "self",
      "decoder_queries",
      "backbone_features",
      "encoder_hidden_states",
      "prompt_features",
      "prompt_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "decoder_queries_shape",
      "backbone_features_shape",
      "encoder_hidden_states_shape",
      "prompt_features_shape",
      "prompt_masks_shape"
    ]
  },
  "_bilinear_interpolate": [
    "feature_maps",
    "roi_batch_ind",
    "y",
    "x",
    "ymask",
    "xmask",
    "height",
    "width",
    "hidden_dim"
  ],
  "roi_align_torch": [
    "feature_maps",
    "rois",
    "output_size",
    "spatial_scale",
    "aligned"
  ],
  "roi_align": [
    "feature_maps",
    "rois",
    "output_size",
    "height",
    "width",
    "hidden_dim",
    "spatial_scale",
    "aligned"
  ],
  "SAM3DetrEncoderLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "hidden_activation",
      "dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "vision_feats_shape",
      "prompt_feats_shape",
      "vision_pos_encodings_shape",
      "prompt_cross_attn_masks_shape"
    ],
    "call": [
      "self",
      "vision_feats",
      "prompt_feats",
      "vision_pos_encodings",
      "prompt_cross_attn_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "vision_feats_shape",
      "prompt_feats_shape",
      "vision_pos_encodings_shape",
      "prompt_cross_attn_masks_shape"
    ]
  },
  "SAM3DetrEncoder": {
    "__init__": [
      "self",
      "num_layers",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "hidden_activation",
      "dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "vision_features_shape",
      "text_features_shape",
      "vision_pos_embeds_shape",
      "text_masks_shape"
    ],
    "call": [
      "self",
      "vision_features",
      "text_features",
      "vision_pos_embeds",
      "text_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "vision_features_shape",
      "text_features_shape",
      "vision_pos_embeds_shape",
      "text_masks_shape"
    ]
  },
  "SAM3MLP": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "activation",
      "dropout_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "SAM3Attention": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads"
    ],
    "build": [
      "self",
      "query_shape",
      "key_shape",
      "value_shape"
    ],
    "call": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "attention_bias",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3RoPEAttention": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "attention_dropout_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "apply_rotary_pos_emb_2d": [
      "self",
      "query",
      "key",
      "cos",
      "sin"
    ],
    "call": [
      "self",
      "hidden_states",
      "position_embeddings",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3PatchEmbedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3Embedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "image_shape",
      "dropout_rate",
      "pretrain_image_shape",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "_tile_position_embeddings": [
      "position_embeddings",
      "patch_size",
      "source_shape",
      "target_shape"
    ],
    "_is_tiled_position_embeddings_updated": [
      "self"
    ],
    "save_own_variables": [
      "self",
      "store"
    ],
    "load_own_variables": [
      "self",
      "store"
    ]
  },
  "SAM3SinePositionEmbedding": {
    "__init__": [
      "self",
      "num_pos_feats",
      "temperature",
      "normalize",
      "scale"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "encode_1d_positions": [
      "self",
      "x",
      "y"
    ],
    "encode_boxes": [
      "self",
      "boxes"
    ],
    "call": [
      "self",
      "inputs",
      "height",
      "width",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3DecoderMLP": {
    "__init__": [
      "self",
      "num_layers",
      "hidden_dim",
      "output_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3BoxDecoder": {
    "build": [
      "self",
      "box_offsets_shape",
      "reference_boxes_shape",
      "pred_logits_shape",
      "presence_logits_shape"
    ],
    "call": [
      "self",
      "box_offsets",
      "reference_boxes",
      "pred_logits",
      "presence_logits",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "box_offsets_shape",
      "reference_boxes_shape",
      "pred_logits_shape",
      "presence_logits_shape"
    ]
  },
  "SAM3DetrDecoderLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "hidden_activation",
      "dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "hidden_states_shape",
      "query_pos_shape",
      "text_features_shape",
      "vision_features_shape",
      "vision_pos_encodings_shape",
      "text_cross_attn_masks_shape",
      "vision_cross_attn_masks_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "query_pos",
      "text_features",
      "vision_features",
      "vision_pos_encodings",
      "text_cross_attn_masks",
      "vision_cross_attn_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "hidden_states_shape",
      "query_pos_shape",
      "text_features_shape",
      "vision_features_shape",
      "vision_pos_encodings_shape",
      "text_cross_attn_masks_shape",
      "vision_cross_attn_masks_shape"
    ]
  },
  "SAM3DetrDecoder": {
    "__init__": [
      "self",
      "image_shape",
      "patch_size",
      "num_layers",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "num_queries",
      "hidden_activation",
      "dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "vision_features_shape",
      "text_features_shape",
      "vision_pos_encodings_shape",
      "text_masks_shape"
    ],
    "_get_coords": [
      "self",
      "height",
      "width",
      "dtype"
    ],
    "_get_rpb_matrix": [
      "self",
      "reference_boxes"
    ],
    "call": [
      "self",
      "vision_features",
      "text_features",
      "vision_pos_encodings",
      "text_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "vision_features_shape",
      "text_features_shape",
      "vision_pos_encodings_shape",
      "text_masks_shape"
    ]
  },
  "SAM3DotProductScoring": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "decoder_hidden_states_shape",
      "text_features_shape",
      "text_masks_shape"
    ],
    "_pool_text_features": [
      "self",
      "text_features",
      "text_mask"
    ],
    "call": [
      "self",
      "decoder_hidden_states",
      "text_features",
      "text_masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "decoder_hidden_states_shape",
      "text_features_shape",
      "text_masks_shape"
    ]
  },
  "SAM3ViTRotaryEmbedding": {
    "__init__": [
      "self",
      "rope_theta",
      "head_dim",
      "end_x",
      "end_y",
      "scale"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "load_own_variables": [
      "self",
      "store"
    ]
  },
  "SAM3ViTLayer": {
    "__init__": [
      "self",
      "image_shape",
      "patch_size",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "hidden_activation",
      "rope_theta",
      "window_size",
      "rotary_scale",
      "attention_dropout_rate",
      "hidden_dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3ViTEncoder": {
    "__init__": [
      "self",
      "image_shape",
      "patch_size",
      "num_layers",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "pretrain_image_shape",
      "hidden_activation",
      "rope_theta",
      "window_size",
      "global_attn_indexes",
      "attention_dropout_rate",
      "hidden_dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "pixel_values",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3FPNLayer": {
    "__init__": [
      "self",
      "input_dim",
      "fpn_dim",
      "scale_factor"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3VisionNeck": {
    "__init__": [
      "self",
      "hidden_dim",
      "fpn_hidden_dim",
      "scale_factors"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SAM3VisionEncoder": {
    "__init__": [
      "self",
      "image_shape",
      "patch_size",
      "num_layers",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "fpn_hidden_dim",
      "fpn_scale_factors",
      "pretrain_image_shape",
      "hidden_activation",
      "rope_theta",
      "window_size",
      "global_attn_indexes",
      "attention_dropout_rate",
      "hidden_dropout_rate",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "pixel_values",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "GPT2Preprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "sequence_length",
      "add_start_token",
      "add_end_token"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "get_config": [
      "self"
    ],
    "sequence_length": [
      "self",
      "value"
    ]
  },
  "GPT2Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "GPT2CausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "_gpt_2_kernel_initializer": [
    "stddev"
  ],
  "GPT2Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "GPT2CausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ],
    "get_quantization_layer_structure": [
      "self",
      "mode"
    ]
  },
  "DINOV3PatchEmbedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3Embedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "num_register_tokens",
      "use_mask_token",
      "initializer_range",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3RopePositionEmbedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "rope_theta",
      "patch_size",
      "data_format"
    ],
    "_get_patches_center_coordinates": [
      "self",
      "num_patches_h",
      "num_patches_w",
      "dtype"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3Attention": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "dropout_rate",
      "use_query_bias",
      "use_key_bias",
      "use_value_bias",
      "use_proj_bias"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_apply_rotary": [
      "self",
      "q",
      "k",
      "cos",
      "sin",
      "num_prefix_tokens"
    ],
    "call": [
      "self",
      "inputs",
      "attention_mask",
      "position_embeddings",
      "num_prefix_tokens",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3LayerScale": {
    "__init__": [
      "self",
      "hidden_dim",
      "init_values"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DINOV3DropPath": {
    "__init__": [
      "self",
      "rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3MLP": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "activation",
      "use_bias"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3GatedMLP": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "activation",
      "use_bias"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3Layer": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "layer_scale_init_value",
      "hidden_activation",
      "use_gated_mlp",
      "use_query_bias",
      "use_key_bias",
      "use_value_bias",
      "use_proj_bias",
      "use_mlp_bias",
      "attention_dropout",
      "drop_path_rate",
      "layer_norm_eps"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "attention_mask",
      "position_embeddings",
      "num_prefix_tokens",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3Encoder": {
    "__init__": [
      "self",
      "num_layers",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "layer_scale_init_value",
      "hidden_activation",
      "use_gated_mlp",
      "use_query_bias",
      "use_key_bias",
      "use_value_bias",
      "use_proj_bias",
      "use_mlp_bias",
      "attention_dropout",
      "drop_path_rate",
      "layer_norm_eps"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "attention_mask",
      "position_embeddings",
      "num_prefix_tokens",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV3ImageConverter": {
    "backbone_cls": []
  },
  "DINOV3Backbone": {
    "__init__": [
      "self",
      "patch_size",
      "num_layers",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "layer_scale_init_value",
      "num_register_tokens",
      "use_mask_token",
      "hidden_activation",
      "use_gated_mlp",
      "use_query_bias",
      "use_key_bias",
      "use_value_bias",
      "use_proj_bias",
      "use_mlp_bias",
      "attention_dropout",
      "drop_path_rate",
      "layer_norm_eps",
      "image_shape",
      "rope_theta",
      "apply_layernorm",
      "data_format",
      "dtype",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "CSPNetBackbone": {
    "__init__": [
      "self",
      "stem_filters",
      "stem_kernel_size",
      "stem_strides",
      "stackwise_depth",
      "stackwise_strides",
      "stackwise_num_filters",
      "block_type",
      "groups",
      "stage_type",
      "activation",
      "output_strides",
      "bottle_ratio",
      "block_ratio",
      "expand_ratio",
      "stem_padding",
      "stem_pooling",
      "avg_down",
      "down_growth",
      "cross_linear",
      "image_shape",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "bottleneck_block": [
    "filters",
    "channel_axis",
    "data_format",
    "bottle_ratio",
    "dilation",
    "groups",
    "activation",
    "dtype",
    "name"
  ],
  "dark_block": [
    "filters",
    "data_format",
    "channel_axis",
    "dilation",
    "bottle_ratio",
    "groups",
    "activation",
    "dtype",
    "name"
  ],
  "edge_block": [
    "filters",
    "data_format",
    "channel_axis",
    "dilation",
    "bottle_ratio",
    "groups",
    "activation",
    "dtype",
    "name"
  ],
  "cross_stage": [
    "filters",
    "strides",
    "dilation",
    "depth",
    "data_format",
    "channel_axis",
    "block_ratio",
    "bottle_ratio",
    "expand_ratio",
    "groups",
    "first_dilation",
    "avg_down",
    "activation",
    "down_growth",
    "cross_linear",
    "block_fn",
    "dtype",
    "name"
  ],
  "cross_stage3": [
    "data_format",
    "channel_axis",
    "filters",
    "strides",
    "dilation",
    "depth",
    "block_ratio",
    "bottle_ratio",
    "expand_ratio",
    "avg_down",
    "activation",
    "first_dilation",
    "down_growth",
    "cross_linear",
    "block_fn",
    "groups",
    "name",
    "dtype"
  ],
  "dark_stage": [
    "data_format",
    "channel_axis",
    "filters",
    "strides",
    "dilation",
    "depth",
    "block_ratio",
    "bottle_ratio",
    "avg_down",
    "activation",
    "first_dilation",
    "block_fn",
    "groups",
    "expand_ratio",
    "down_growth",
    "cross_linear",
    "name",
    "dtype"
  ],
  "create_csp_stem": [
    "data_format",
    "channel_axis",
    "activation",
    "padding",
    "filters",
    "kernel_size",
    "strides",
    "pooling",
    "dtype"
  ],
  "create_csp_stages": [
    "inputs",
    "filters",
    "data_format",
    "channel_axis",
    "stackwise_depth",
    "reduction",
    "block_ratio",
    "bottle_ratio",
    "expand_ratio",
    "strides",
    "groups",
    "avg_down",
    "down_growth",
    "cross_linear",
    "activation",
    "output_strides",
    "stage_type",
    "block_type",
    "dtype",
    "name"
  ],
  "_pad_arg": [
    "x",
    "n"
  ],
  "CSPNetImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "CSPNetImageConverter": {
    "backbone_cls": []
  },
  "CSPNetImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "SmolLM3Attention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "num_key_value_heads",
      "attention_bias",
      "attention_dropout",
      "rope_layer_enabled_list",
      "layer_types",
      "layer_idx",
      "max_position_embeddings",
      "rope_theta",
      "partial_rotary_factor"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "training",
      "attention_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "cache_update_index"
    ]
  },
  "SmolLM3MLP": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "mlp_bias"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SmolLM3DecoderLayer": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "num_key_value_heads",
      "attention_bias",
      "attention_dropout",
      "rope_layer_enabled_list",
      "layer_types",
      "layer_idx",
      "intermediate_size",
      "mlp_bias",
      "layer_norm_epsilon",
      "max_position_embeddings",
      "rope_theta",
      "partial_rotary_factor"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "training",
      "decoder_padding_mask",
      "decoder_attention_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SmolLM3RotaryEmbedding": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "max_position_embeddings",
      "rope_theta",
      "partial_rotary_factor"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "start_index"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "rotate_half": [
    "x"
  ],
  "apply_rotary_pos_emb": [
    "q",
    "k",
    "cos",
    "sin",
    "expansion_axis"
  ],
  "apply_rotary_pos_single": [
    "tensor",
    "cos",
    "sin",
    "expansion_axis"
  ],
  "repeat_kv": [
    "hidden_states",
    "n_rep"
  ],
  "rope_init": [
    "rope_theta",
    "partial_rotary_factor",
    "head_dim"
  ],
  "SmolLM3Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "SmolLM3CausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "__init__": [
      "self"
    ]
  },
  "SmolLM3Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "hidden_dim",
      "intermediate_dim",
      "num_layers",
      "num_attention_heads",
      "num_key_value_heads",
      "attention_bias",
      "attention_dropout",
      "rope_layer_enabled_list",
      "layer_types",
      "mlp_bias",
      "layer_norm_epsilon",
      "max_position_embeddings",
      "rope_theta",
      "partial_rotary_factor"
    ],
    "get_config": [
      "self"
    ]
  },
  "SmolLM3CausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "BertTextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "bert_kernel_initializer": [
    "stddev"
  ],
  "BertBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "num_segments",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "BertMaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "BertMaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "BertTextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "dropout"
    ],
    "get_config": [
      "self"
    ]
  },
  "BertTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "lowercase"
    ]
  },
  "XLMRobertaTextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "hidden_dim",
      "dropout"
    ],
    "get_config": [
      "self"
    ]
  },
  "XLMRobertaTextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "XLMRobertaTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ],
    "special_tokens": [
      "self"
    ],
    "special_token_ids": [
      "self"
    ],
    "set_proto": [
      "self",
      "proto"
    ],
    "vocabulary_size": [
      "self"
    ],
    "get_vocabulary": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ]
  },
  "XLMRobertaMaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "XLMRobertaBackbone": {},
  "XLMRobertaMaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "_gpt_neo_x_kernel_initializer": [
    "stddev"
  ],
  "GPTNeoXBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "rotary_percentage",
      "rotary_max_wavelength",
      "layer_norm_epsilon",
      "max_sequence_length",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "GPTNeoXDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_heads",
      "dropout",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "bias_initializer",
      "rotary_percentage",
      "rotary_max_wavelength",
      "max_sequence_length"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ]
  },
  "GPTNeoXTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "GPTNeoXAttention": {
    "__init__": [
      "self",
      "num_heads",
      "hidden_dim",
      "dropout",
      "kernel_initializer",
      "bias_initializer",
      "rotary_percentage",
      "rotary_max_wavelength",
      "max_sequence_length"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_get_common_kwargs_for_sublayer": [
      "self",
      "use_bias"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "training"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "GPTNeoXCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "GPTNeoXCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "DeepLabV3ImageConverter": {
    "backbone_cls": []
  },
  "SpatialPyramidPooling": {
    "__init__": [
      "self",
      "dilation_rates",
      "num_channels",
      "activation",
      "dropout"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeepLabV3ImageSegmenter": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "activation",
      "preprocessor"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeepLabV3ImageSegmenterPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "DeepLabV3Backbone": {
    "__init__": [
      "self",
      "image_encoder",
      "spatial_pyramid_pooling_key",
      "upsampling_size",
      "dilation_rates",
      "low_level_feature_key",
      "projection_filters",
      "image_shape"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "apply_low_level_feature_network": [
    "input_tensor",
    "projection_filters",
    "channel_axis"
  ],
  "RoformerV2TextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "RoformerV2Tokenizer": {
    "backbone_cls": []
  },
  "RoformerV2MaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "RoformerV2MaskedLMHead": {
    "__init__": [
      "self",
      "vocabulary_size",
      "token_embedding",
      "activation"
    ],
    "call": [
      "self",
      "inputs",
      "mask_positions"
    ],
    "get_config": [
      "self"
    ]
  },
  "RoformerV2MaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "RoformerV2TextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "roformer_kernel_initializer": [
    "stddev"
  ],
  "RoformerV2Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "head_size",
      "use_bias",
      "activation",
      "dropout",
      "num_segments",
      "dtype",
      "max_wavelength"
    ],
    "get_config": [
      "self"
    ]
  },
  "RoformerV2Encoder": {
    "__init__": [
      "self",
      "heads",
      "head_size",
      "intermediate_size",
      "max_wavelength",
      "dropout",
      "activation",
      "use_bias",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "attention_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "RoformerNorm": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "RoformrPositionalEmbedding": {
    "__init__": [
      "self",
      "output_dim",
      "max_wavelength"
    ],
    "call": [
      "self",
      "tensors"
    ],
    "align": [
      "self",
      "tensor",
      "axes",
      "ndim"
    ],
    "sinusoidal_embeddings": [
      "self",
      "pos",
      "dim",
      "base"
    ],
    "get_config": [
      "self"
    ]
  },
  "RoformerAttention": {
    "__init__": [
      "self",
      "heads",
      "head_size",
      "out_dim",
      "use_bias",
      "max_wavelength",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "attention_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2Encoder": {
    "__init__": [
      "self",
      "stage_in_channels",
      "stage_mid_channels",
      "stage_out_channels",
      "stage_num_blocks",
      "stage_num_of_layers",
      "apply_downsample",
      "use_lightweight_conv_block",
      "stage_kernel_size",
      "use_learnable_affine_block",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_state",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2Backbone": {
    "__init__": [
      "self",
      "depths",
      "embedding_size",
      "hidden_sizes",
      "stem_channels",
      "hidden_act",
      "use_learnable_affine_block",
      "stackwise_stage_filters",
      "apply_downsample",
      "use_lightweight_conv_block",
      "image_shape",
      "data_format",
      "out_features",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2ImageConverter": {
    "backbone_cls": []
  },
  "HGNetV2ImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor",
      "num_classes",
      "head_filters",
      "pooling",
      "activation",
      "dropout",
      "head_dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2LearnableAffineBlock": {
    "__init__": [
      "self",
      "scale_value",
      "bias_value",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_state"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2ConvLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "activation",
      "use_learnable_affine_block",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2ConvLayerLight": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "use_learnable_affine_block",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_state",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "HGNetV2Embeddings": {
    "__init__": [
      "self",
      "stem_channels",
      "hidden_act",
      "use_learnable_affine_block",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "pixel_values",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2BasicLayer": {
    "__init__": [
      "self",
      "in_channels",
      "middle_channels",
      "out_channels",
      "layer_num",
      "kernel_size",
      "residual",
      "light_block",
      "drop_path",
      "use_learnable_affine_block",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_state",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2Stage": {
    "__init__": [
      "self",
      "stage_in_channels",
      "stage_mid_channels",
      "stage_out_channels",
      "stage_num_blocks",
      "stage_num_of_layers",
      "apply_downsample",
      "use_lightweight_conv_block",
      "stage_kernel_size",
      "use_learnable_affine_block",
      "stage_index",
      "drop_path",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_state",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "HGNetV2ImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "ViTImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "ViTImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "pooling",
      "intermediate_dim",
      "activation",
      "dropout",
      "head_dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "hidden_dim",
      "mlp_dim",
      "use_bias",
      "dropout_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ]
  },
  "ViTPatchingAndEmbedding": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "hidden_dim",
      "num_channels",
      "use_class_token",
      "use_patch_bias",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "ViTEncoderBlock": {
    "__init__": [
      "self",
      "num_heads",
      "hidden_dim",
      "mlp_dim",
      "use_mha_bias",
      "use_mlp_bias",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "ViTEncoder": {
    "__init__": [
      "self",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "mlp_dim",
      "use_mha_bias",
      "use_mlp_bias",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "ViTImageConverter": {
    "backbone_cls": []
  },
  "ViTBackbone": {
    "__init__": [
      "self",
      "image_shape",
      "patch_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "mlp_dim",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon",
      "use_mha_bias",
      "use_mlp_bias",
      "use_class_token",
      "use_patch_bias",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "AlbertTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "AlbertMaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "AlbertTextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "dropout"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "get_config": [
      "self"
    ]
  },
  "AlbertTextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "AlbertMaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "albert_kernel_initializer": [
    "stddev"
  ],
  "AlbertBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "embedding_dim",
      "hidden_dim",
      "intermediate_dim",
      "num_groups",
      "num_inner_repetitions",
      "dropout",
      "max_sequence_length",
      "num_segments",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "T5LayerNorm": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states"
    ]
  },
  "T5MultiHeadAttention": {
    "__init__": [
      "self",
      "is_decoder",
      "hidden_dim",
      "key_value_dim",
      "num_heads",
      "dropout",
      "use_relative_attention_bias"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_bias": [
      "self",
      "query_length",
      "key_length"
    ],
    "call": [
      "self",
      "hidden_states",
      "mask",
      "key_value_states",
      "position_bias",
      "past_key_value",
      "layer_head_mask",
      "query_length",
      "training"
    ]
  },
  "T5Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "T5TransformerLayer": {
    "__init__": [
      "self",
      "is_decoder",
      "hidden_dim",
      "intermediate_dim",
      "key_value_dim",
      "dropout",
      "activation",
      "layer_norm_epsilon",
      "num_heads",
      "use_gated_activation",
      "use_relative_attention_bias"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "use_causal_mask",
      "training"
    ]
  },
  "T5Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "key_value_dim",
      "dropout",
      "activation",
      "use_gated_activation",
      "layer_norm_epsilon",
      "tie_embedding_weights",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "T5Preprocessor": {
    "tokenizer_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "sequence_length",
      "add_start_token",
      "add_end_token"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "get_config": [
      "self"
    ]
  },
  "PARSEQ_VOCAB": [],
  "PARSeqTokenizer": {
    "__init__": [
      "self",
      "vocabulary",
      "remove_whitespace",
      "normalize_unicode",
      "max_label_length",
      "dtype"
    ],
    "save_assets": [
      "self",
      "dir_path"
    ],
    "load_assets": [
      "self",
      "dir_path"
    ],
    "set_vocabulary": [
      "self",
      "vocabulary"
    ],
    "get_vocabulary": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "_preprocess": [
      "self",
      "inputs"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "vocabulary_size": [
      "self"
    ],
    "compute_output_spec": [
      "self",
      "input_spec"
    ]
  },
  "PARSeqDecoderBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "mlp_dim",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "forward_stream": [
      "self",
      "target",
      "target_norm",
      "target_kv",
      "memory",
      "padding_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "train_attention_mask"
    ],
    "call": [
      "self",
      "query",
      "content",
      "memory",
      "padding_mask",
      "update_content",
      "query_self_attention_cache",
      "query_self_attention_cache_update_index",
      "content_self_attention_cache",
      "content_self_attention_cache_update_index",
      "query_mask",
      "content_mask"
    ],
    "_compute_attention_mask": [
      "self",
      "x",
      "padding_mask",
      "cache",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "PARSeqDecoder": {
    "__init__": [
      "self",
      "vocabulary_size",
      "max_label_length",
      "num_layers",
      "hidden_dim",
      "mlp_dim",
      "num_heads",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "token_ids",
      "memory",
      "padding_mask",
      "query_mask",
      "content_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "PARSeqImageConverter": {
    "backbone_cls": []
  },
  "PARSeqBackbone": {
    "__init__": [
      "self",
      "image_encoder",
      "vocabulary_size",
      "max_label_length",
      "decoder_hidden_dim",
      "num_decoder_layers",
      "num_decoder_heads",
      "decoder_mlp_dim",
      "dropout_rate",
      "attention_dropout",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "PARSeqCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "preprocessor",
      "backbone",
      "num_perms",
      "add_forward_perms",
      "add_mirrored_perms",
      "seed",
      "end_token_id"
    ],
    "get_config": [
      "self"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "compute_loss": [
      "self",
      "x",
      "y",
      "y_pred",
      "sample_weight",
      "training"
    ],
    "generate_training_permutations": [
      "self",
      "max_num_chars"
    ],
    "generate_attention_masks": [
      "self",
      "perm"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index",
      "img_embeddings",
      "padding_mask"
    ],
    "_build_cache": [
      "self",
      "token_ids",
      "img_embeddings",
      "padding_mask"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "PARSeqCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "image_converter_cls": [],
    "__init__": [
      "self",
      "image_converter",
      "tokenizer",
      "sequence_length",
      "add_start_token",
      "add_end_token"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "generate_preprocess": [
      "self",
      "x",
      "sequence_length"
    ],
    "generate_postprocess": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "_qwen_kernel_initializer": [
    "stddev"
  ],
  "QwenBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "num_key_value_heads",
      "hidden_dim",
      "intermediate_dim",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "dropout",
      "dtype",
      "tie_word_embeddings",
      "use_sliding_window_attention",
      "sliding_window_size"
    ],
    "get_config": [
      "self"
    ],
    "get_layout_map": [
      "device_mesh",
      "model_parallel_dim_name",
      "data_parallel_dim_name"
    ]
  },
  "QwenTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "QwenCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "QwenLayerNorm": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "QwenCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "__init__": [
      "self"
    ]
  },
  "QwenTransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "dropout",
      "use_sliding_window_attention",
      "sliding_window_size"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "QwenAttention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "kernel_initializer",
      "bias_initializer",
      "dropout",
      "use_sliding_window_attention",
      "sliding_window_size"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "cache_update_index"
    ],
    "_mask_sliding_window": [
      "self",
      "attention_mask",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "AddRelativePositionalEmbedding": {
    "__init__": [
      "self",
      "input_size",
      "key_dim"
    ],
    "_get_rel_pos": [
      "self",
      "query_size",
      "key_size",
      "rel_pos"
    ],
    "call": [
      "self",
      "attention_map",
      "queries",
      "query_size",
      "key_size"
    ],
    "get_config": [
      "self"
    ]
  },
  "MultiHeadAttentionWithRelativePE": {
    "__init__": [
      "self",
      "num_heads",
      "key_dim",
      "use_bias",
      "use_rel_pos",
      "input_size"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "WindowPartitioning": {
    "__init__": [
      "self",
      "window_size"
    ],
    "partition": [
      "self",
      "x"
    ],
    "unpartition": [
      "self",
      "windows",
      "height_width_padded",
      "height_width"
    ],
    "get_config": [
      "self"
    ]
  },
  "WindowedTransformerEncoder": {
    "__init__": [
      "self",
      "project_dim",
      "intermediate_dim",
      "num_heads",
      "use_bias",
      "use_rel_pos",
      "window_size",
      "input_size",
      "activation",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "ViTDetPatchingAndEmbedding": {
    "__init__": [
      "self",
      "kernel_size",
      "strides",
      "embed_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "AddPositionalEmbedding": {
    "__init__": [
      "self",
      "img_size",
      "patch_size",
      "embed_dim"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_confg": [
      "self"
    ]
  },
  "ViTDetBackbone": {
    "__init__": [
      "self",
      "hidden_size",
      "num_layers",
      "intermediate_dim",
      "num_heads",
      "global_attention_layer_indices",
      "image_shape",
      "patch_size",
      "num_output_channels",
      "use_bias",
      "use_abs_pos",
      "use_rel_pos",
      "window_size",
      "layer_norm_epsilon"
    ],
    "get_config": [
      "self"
    ]
  },
  "Qwen3MoeLayerNorm": {
    "__init__": [
      "self",
      "head_dim",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "Qwen3MoeTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "Qwen3MoeCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "Qwen3MoeCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "Qwen3MoeAttention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "head_dim",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "kernel_initializer",
      "dropout",
      "layer_norm_epsilon",
      "sliding_window_size"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "cache_update_index"
    ],
    "_mask_sliding_window": [
      "self",
      "attention_mask",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "_qwen3_moe_kernel_initializer": [
    "stddev"
  ],
  "Qwen3MoeBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "num_key_value_heads",
      "hidden_dim",
      "intermediate_dim",
      "moe_intermediate_dim",
      "num_experts",
      "head_dim",
      "top_k",
      "norm_top_k_prob",
      "decoder_sparse_step",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "dropout",
      "dtype",
      "tie_word_embeddings",
      "sliding_window_size",
      "router_aux_loss_coefficient",
      "mlp_only_layers",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "get_layout_map": [
      "device_mesh",
      "model_parallel_dim_name",
      "data_parallel_dim_name"
    ]
  },
  "compute_load_balancing_loss": [
    "router_logits",
    "num_experts",
    "top_k",
    "attention_mask"
  ],
  "Qwen3MoeMLP": {
    "__init__": [
      "self",
      "intermediate_dim",
      "hidden_dim",
      "activation_fn",
      "layer_norm_epsilon",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "Qwen3MoeExperts": {
    "__init__": [
      "self",
      "num_experts",
      "hidden_dim",
      "intermediate_dim",
      "activation_fn",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "_"
    ],
    "call": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen3SparseMoeBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "moe_intermediate_dim",
      "num_experts",
      "top_k",
      "norm_top_k_prob",
      "kernel_initializer",
      "layer_norm_epsilon",
      "router_aux_loss_coefficient"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "training"
    ]
  },
  "Qwen3MoeTransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "moe_intermediate_dim",
      "num_experts",
      "top_k",
      "norm_top_k_prob",
      "head_dim",
      "is_sparse_mlp",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "dropout",
      "sliding_window_size",
      "router_aux_loss_coefficient"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "GemmaTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "GemmaCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "GemmaDecoderBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "head_dim",
      "num_query_heads",
      "num_key_value_heads",
      "query_head_dim_normalize",
      "use_post_ffw_norm",
      "use_post_attention_norm",
      "logit_soft_cap",
      "use_sliding_window_attention",
      "sliding_window_size",
      "layer_norm_epsilon",
      "dropout"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "_compute_attention_mask": [
      "self",
      "x",
      "padding_mask",
      "cache",
      "cache_update_index"
    ],
    "call": [
      "self",
      "x",
      "padding_mask",
      "cache",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "GemmaBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "num_key_value_heads",
      "hidden_dim",
      "intermediate_dim",
      "head_dim",
      "query_head_dim_normalize",
      "use_post_ffw_norm",
      "use_post_attention_norm",
      "attention_logit_soft_cap",
      "final_logit_soft_cap",
      "use_sliding_window_attention",
      "sliding_window_size",
      "layer_norm_epsilon",
      "dropout",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "get_layout_map": [
      "device_mesh",
      "model_parallel_dim_name",
      "data_parallel_dim_name"
    ]
  },
  "CachedGemmaAttention": {
    "__init__": [
      "self",
      "head_dim",
      "num_query_heads",
      "num_key_value_heads",
      "kernel_initializer",
      "logit_soft_cap",
      "use_sliding_window_attention",
      "sliding_window_size",
      "query_head_dim_normalize",
      "dropout"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "_apply_rope": [
      "self",
      "x",
      "start_index"
    ],
    "_use_fused_attention_op": [
      "self"
    ],
    "_compute_attention": [
      "self",
      "q",
      "k",
      "v",
      "attention_mask",
      "training",
      "cache_update_index"
    ],
    "_mask_sliding_window": [
      "self",
      "attention_mask",
      "cache_update_index"
    ],
    "call": [
      "self",
      "x",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "GemmaCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ],
    "get_quantization_layer_structure": [
      "self",
      "mode"
    ]
  },
  "RQVAEBackbone": {
    "__init__": [
      "self",
      "input_dim",
      "encoder_layer_dims",
      "output_dim",
      "decoder_layer_dims",
      "num_embeddings",
      "num_quantizers",
      "decay",
      "data_variance",
      "commitment_cost",
      "dtype"
    ],
    "compute_loss": [
      "self",
      "x",
      "y",
      "y_pred",
      "sample_weight"
    ],
    "get_config": [
      "self"
    ]
  },
  "Encoder": {
    "__init__": [
      "self",
      "layer_dims",
      "output_dim"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "Decoder": {
    "__init__": [
      "self",
      "layer_dims",
      "output_dim"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "VectorQuantizerEMA": {
    "__init__": [
      "self",
      "num_embeddings",
      "embedding_dim",
      "decay",
      "eps"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_codebook_usage": [
      "self",
      "encodings"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "ResidualVectorQuantizer": {
    "__init__": [
      "self",
      "quantizers"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "StableDiffusion3Inpaint": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "fit": [
      "self"
    ],
    "generate_step": [
      "self",
      "images",
      "masks",
      "noises",
      "token_ids",
      "starting_step",
      "num_steps",
      "guidance_scale"
    ],
    "generate": [
      "self",
      "inputs",
      "num_steps",
      "strength",
      "guidance_scale",
      "seed"
    ]
  },
  "StableDiffusion3TextToImagePreprocessor": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "clip_l_preprocessor",
      "clip_g_preprocessor",
      "t5_preprocessor"
    ],
    "sequence_length": [
      "self"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "generate_preprocess": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "StableDiffusion3TextToImage": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "fit": [
      "self"
    ],
    "generate_step": [
      "self",
      "latents",
      "token_ids",
      "num_steps",
      "guidance_scale"
    ],
    "generate": [
      "self",
      "inputs",
      "num_steps",
      "guidance_scale",
      "seed"
    ]
  },
  "T5Encoder": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "key_value_dim",
      "dropout",
      "activation",
      "use_gated_activation",
      "layer_norm_epsilon",
      "tie_embedding_weights",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "StableDiffusion3ImageToImage": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "fit": [
      "self"
    ],
    "generate_step": [
      "self",
      "images",
      "noises",
      "token_ids",
      "starting_step",
      "num_steps",
      "guidance_scale"
    ],
    "generate": [
      "self",
      "inputs",
      "num_steps",
      "strength",
      "guidance_scale",
      "seed"
    ]
  },
  "CLIPProjection": {
    "__init__": [
      "self",
      "hidden_dim"
    ],
    "build": [
      "self",
      "inputs_shape",
      "token_ids_shape"
    ],
    "call": [
      "self",
      "inputs",
      "token_ids"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "CLIPConcatenate": {
    "call": [
      "self",
      "clip_l_projection",
      "clip_g_projection",
      "clip_l_intermediate_output",
      "clip_g_intermediate_output",
      "padding"
    ]
  },
  "ImageRescaling": {
    "call": [
      "self",
      "inputs"
    ]
  },
  "LatentRescaling": {
    "call": [
      "self",
      "inputs"
    ]
  },
  "TimestepBroadcastTo": {
    "call": [
      "self",
      "latents",
      "timestep"
    ]
  },
  "ClassifierFreeGuidance": {
    "call": [
      "self",
      "inputs",
      "guidance_scale"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "EulerStep": {
    "call": [
      "self",
      "latents",
      "noise_residual",
      "sigma",
      "sigma_next"
    ],
    "compute_output_shape": [
      "self",
      "latents_shape"
    ]
  },
  "StableDiffusion3Backbone": {
    "__init__": [
      "self",
      "mmdit_patch_size",
      "mmdit_hidden_dim",
      "mmdit_num_layers",
      "mmdit_num_heads",
      "mmdit_position_size",
      "mmdit_qk_norm",
      "mmdit_dual_attention_indices",
      "vae",
      "clip_l",
      "clip_g",
      "t5",
      "latent_channels",
      "output_channels",
      "num_train_timesteps",
      "shift",
      "image_shape",
      "data_format",
      "dtype"
    ],
    "latent_shape": [
      "self"
    ],
    "clip_hidden_dim": [
      "self"
    ],
    "t5_hidden_dim": [
      "self"
    ],
    "encode_text_step": [
      "self",
      "token_ids",
      "negative_token_ids"
    ],
    "encode_image_step": [
      "self",
      "images"
    ],
    "configure_scheduler": [
      "self",
      "num_steps"
    ],
    "add_noise_step": [
      "self",
      "latents",
      "noises",
      "step",
      "num_steps"
    ],
    "denoise_step": [
      "self",
      "latents",
      "embeddings",
      "step",
      "num_steps",
      "guidance_scale"
    ],
    "decode_step": [
      "self",
      "latents"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config",
      "custom_objects"
    ]
  },
  "AdaptiveLayerNormalization": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_modulations"
    ],
    "build": [
      "self",
      "inputs_shape",
      "embeddings_shape"
    ],
    "call": [
      "self",
      "inputs",
      "embeddings",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape",
      "embeddings_shape"
    ]
  },
  "PatchEmbedding": {
    "__init__": [
      "self",
      "patch_size",
      "hidden_dim",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "AdjustablePositionEmbedding": {
    "__init__": [
      "self",
      "height",
      "width",
      "initializer"
    ],
    "call": [
      "self",
      "inputs",
      "height",
      "width"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "TimestepEmbedding": {
    "__init__": [
      "self",
      "embedding_dim",
      "frequency_dim",
      "max_period"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "_create_timestep_embedding": [
      "self",
      "inputs"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "get_qk_norm": [
    "qk_norm",
    "q_norm_name",
    "k_norm_name"
  ],
  "DismantledBlock": {
    "__init__": [
      "self",
      "num_heads",
      "hidden_dim",
      "mlp_ratio",
      "use_projection",
      "qk_norm",
      "use_dual_attention"
    ],
    "build": [
      "self",
      "inputs_shape",
      "timestep_embedding"
    ],
    "_modulate": [
      "self",
      "inputs",
      "shift",
      "scale"
    ],
    "_compute_pre_attention": [
      "self",
      "inputs",
      "timestep_embedding",
      "training"
    ],
    "_compute_post_attention": [
      "self",
      "inputs",
      "inputs_intermediates",
      "training"
    ],
    "_compute_pre_attention_with_dual_attention": [
      "self",
      "inputs",
      "timestep_embedding",
      "training"
    ],
    "_compute_post_attention_with_dual_attention": [
      "self",
      "inputs",
      "inputs2",
      "inputs_intermediates",
      "training"
    ],
    "call": [
      "self",
      "inputs",
      "timestep_embedding",
      "inputs_intermediates",
      "inputs2",
      "pre_attention",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "MMDiTBlock": {
    "__init__": [
      "self",
      "num_heads",
      "hidden_dim",
      "mlp_ratio",
      "use_context_projection",
      "qk_norm",
      "use_dual_attention"
    ],
    "build": [
      "self",
      "inputs_shape",
      "context_shape",
      "timestep_embedding_shape"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value"
    ],
    "call": [
      "self",
      "inputs",
      "context",
      "timestep_embedding",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape",
      "context_shape",
      "timestep_embedding_shape"
    ]
  },
  "Unpatch": {
    "__init__": [
      "self",
      "patch_size",
      "output_dim"
    ],
    "call": [
      "self",
      "inputs",
      "height",
      "width"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "MMDiT": {
    "__init__": [
      "self",
      "patch_size",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "position_size",
      "mlp_ratio",
      "latent_shape",
      "context_shape",
      "pooled_projection_shape",
      "qk_norm",
      "dual_attention_indices",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "FlowMatchEulerDiscreteScheduler": {
    "__init__": [
      "self",
      "num_train_timesteps",
      "shift"
    ],
    "_sigma_to_timestep": [
      "self",
      "sigma"
    ],
    "_timestep_to_sigma": [
      "self",
      "timestep"
    ],
    "set_sigmas": [
      "self",
      "num_steps"
    ],
    "call": [
      "self",
      "inputs",
      "num_steps"
    ],
    "add_noise": [
      "self",
      "inputs",
      "noises",
      "step",
      "num_steps"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self"
    ]
  },
  "MobileNetV5ImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "UniversalInvertedResidual": {
    "__init__": [
      "self",
      "filters",
      "dw_kernel_size_start",
      "dw_kernel_size_mid",
      "dw_kernel_size_end",
      "stride",
      "dilation",
      "pad_type",
      "noskip",
      "exp_ratio",
      "act_layer",
      "norm_layer",
      "se_layer",
      "drop_path_rate",
      "layer_scale_init_value",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "EdgeResidual": {
    "__init__": [
      "self",
      "filters",
      "exp_kernel_size",
      "stride",
      "dilation",
      "group_size",
      "pad_type",
      "expansion_in_chs",
      "noskip",
      "exp_ratio",
      "pw_kernel_size",
      "act_layer",
      "norm_layer",
      "se_layer",
      "drop_path_rate",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "CondConvResidual": {
    "__init__": [
      "self",
      "filters",
      "dw_kernel_size",
      "stride",
      "dilation",
      "pad_type",
      "noskip",
      "exp_ratio",
      "exp_kernel_size",
      "pw_kernel_size",
      "act_layer",
      "se_layer",
      "num_experts",
      "drop_path_rate",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_apply_cond_conv": [
      "self",
      "x",
      "experts",
      "routing_weights"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "MobileNetV5MultiScaleFusionAdapter": {
    "__init__": [
      "self",
      "in_chs",
      "filters",
      "output_resolution",
      "expansion_ratio",
      "interpolation_mode",
      "layer_scale_init_value",
      "noskip",
      "act_layer",
      "norm_layer",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "MobileNetV5ImageConverter": {
    "backbone_cls": []
  },
  "decode_block_str": [
    "block_str"
  ],
  "decode_arch_def": [
    "arch_def"
  ],
  "convert_arch_def_to_stackwise": [
    "arch_def"
  ],
  "MobileNetV5Builder": {
    "__init__": [
      "self",
      "output_stride",
      "pad_type",
      "round_chs_fn",
      "se_from_exp",
      "act_layer",
      "norm_layer",
      "aa_layer",
      "se_layer",
      "drop_path_rate",
      "layer_scale_init_value",
      "feature_location",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "_make_block": [
      "self",
      "ba",
      "block_idx",
      "block_count"
    ],
    "__call__": [
      "self",
      "in_chs",
      "model_block_args"
    ]
  },
  "DropPath": {
    "__init__": [
      "self",
      "drop_prob",
      "scale_by_keep",
      "dtype"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "LayerScale2d": {
    "__init__": [
      "self",
      "dim",
      "init_values",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "RmsNorm2d": {
    "__init__": [
      "self",
      "dim",
      "eps",
      "data_format",
      "channel_axis",
      "gamma_initializer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "ConvNormAct": {
    "__init__": [
      "self",
      "out_chs",
      "kernel_size",
      "stride",
      "dilation",
      "groups",
      "bias",
      "pad_type",
      "apply_act",
      "act_layer",
      "norm_layer",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "SEModule": {
    "__init__": [
      "self",
      "channels",
      "rd_ratio",
      "rd_channels",
      "rd_divisor",
      "add_maxpool",
      "bias",
      "act_layer",
      "norm_layer",
      "data_format",
      "channel_axis",
      "gate_layer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "num_groups": [
    "group_size",
    "channels"
  ],
  "parse_ksize": [
    "ss"
  ],
  "round_channels": [
    "channels",
    "multiplier",
    "divisor",
    "channel_min",
    "round_limit"
  ],
  "feature_take_indices": [
    "num_stages",
    "indices"
  ],
  "SelectAdaptivePool2d": {
    "__init__": [
      "self",
      "pool_type",
      "flatten",
      "data_format",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "feat_mult": [
      "self"
    ],
    "get_config": [
      "self"
    ]
  },
  "MobileNetV5Backbone": {
    "__init__": [
      "self",
      "stackwise_block_types",
      "stackwise_num_blocks",
      "stackwise_num_filters",
      "stackwise_strides",
      "stackwise_act_layers",
      "stackwise_exp_ratios",
      "stackwise_se_ratios",
      "stackwise_dw_kernel_sizes",
      "stackwise_dw_start_kernel_sizes",
      "stackwise_dw_end_kernel_sizes",
      "stackwise_exp_kernel_sizes",
      "stackwise_pw_kernel_sizes",
      "stackwise_num_heads",
      "stackwise_key_dims",
      "stackwise_value_dims",
      "stackwise_kv_strides",
      "stackwise_use_cpe",
      "filters",
      "stem_size",
      "stem_bias",
      "fix_stem",
      "num_features",
      "pad_type",
      "use_msfa",
      "msfa_indices",
      "msfa_output_resolution",
      "act_layer",
      "norm_layer",
      "se_layer",
      "se_from_exp",
      "round_chs_fn",
      "drop_path_rate",
      "layer_scale_init_value",
      "image_shape",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "MultiQueryAttention2d": {
    "__init__": [
      "self",
      "filters",
      "num_heads",
      "key_dim",
      "value_dim",
      "query_strides",
      "kv_stride",
      "dw_kernel_size",
      "dilation",
      "padding",
      "attn_drop",
      "proj_drop",
      "norm_layer",
      "use_bias",
      "channel_axis",
      "data_format",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "Attention2d": {
    "__init__": [
      "self",
      "filters",
      "num_heads",
      "bias",
      "attn_drop",
      "proj_drop",
      "channel_axis",
      "data_format",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "attn_mask",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "MobileAttention": {
    "__init__": [
      "self",
      "filters",
      "stride",
      "dw_kernel_size",
      "dilation",
      "pad_type",
      "num_heads",
      "key_dim",
      "value_dim",
      "use_multi_query",
      "query_strides",
      "kv_stride",
      "cpe_dw_kernel_size",
      "noskip",
      "norm_layer",
      "drop_path_rate",
      "attn_drop",
      "proj_drop",
      "layer_scale_init_value",
      "use_bias",
      "use_cpe",
      "channel_axis",
      "data_format",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "MobileNetV5ImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "head_hidden_size",
      "global_pool",
      "drop_rate",
      "head_dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "quick_gelu": [
    "x"
  ],
  "CLIPVisionEmbedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "image_size",
      "data_format",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "CLIPVisionPooler": {
    "call": [
      "self",
      "vision_embeddings"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "CLIPTextPooler": {
    "call": [
      "self",
      "text_embeddings",
      "token_ids"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "CLIPHead": {
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "vision_embedding",
      "text_embedding"
    ],
    "compute_output_shape": [
      "self",
      "vision_embedding_shape",
      "text_embedding_shape"
    ]
  },
  "CLIPVisionEncoder": {
    "__init__": [
      "self",
      "patch_size",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "intermediate_dim",
      "intermediate_activation",
      "intermediate_output_index",
      "image_shape",
      "data_format",
      "dtype",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "CLIPTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges",
      "pad_with_end_token"
    ],
    "set_vocabulary_and_merges": [
      "self",
      "vocabulary",
      "merges"
    ],
    "_bpe_merge_and_update_cache": [
      "self",
      "tokens"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "CLIPTextEncoder": {
    "__init__": [
      "self",
      "vocabulary_size",
      "embedding_dim",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "intermediate_dim",
      "intermediate_activation",
      "intermediate_output_index",
      "max_sequence_length",
      "dtype",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "CLIPImageConverter": {
    "backbone_cls": []
  },
  "CLIPBackbone": {
    "__init__": [
      "self",
      "vision_encoder",
      "text_encoder",
      "projection_dim",
      "dtype",
      "name"
    ],
    "get_vision_embeddings": [
      "self",
      "images"
    ],
    "get_text_embeddings": [
      "self",
      "token_ids"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config",
      "custom_objects"
    ]
  },
  "CLIPPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "image_converter_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "image_converter",
      "sequence_length",
      "add_start_token",
      "add_end_token",
      "to_lower"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "get_config": [
      "self"
    ]
  },
  "TwoWayTransformer": {
    "__init__": [
      "self"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "image_embedding",
      "image_positional_embeddings",
      "point_embedding"
    ],
    "get_config": [
      "self"
    ]
  },
  "SAMImageConverter": {
    "backbone_cls": []
  },
  "MultiHeadAttentionWithDownsampling": {
    "__init__": [
      "self",
      "num_heads",
      "key_dim",
      "downsample_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_separate_heads": [
      "self",
      "x"
    ],
    "_recombine_heads": [
      "self",
      "x"
    ],
    "call": [
      "self",
      "query",
      "value",
      "key"
    ],
    "get_config": [
      "self"
    ]
  },
  "TwoWayMultiHeadAttention": {
    "__init__": [
      "self",
      "num_heads",
      "key_dim",
      "intermediate_dim",
      "skip_first_layer_pos_embedding",
      "attention_downsample_rate",
      "activation"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "queries",
      "keys",
      "query_pos_embedding",
      "key_pos_embedding"
    ],
    "get_config": [
      "self"
    ]
  },
  "RandomFrequencyPositionalEmbeddings": {
    "__init__": [
      "self",
      "num_positional_features",
      "scale"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_positional_encodings": [
      "self",
      "coords"
    ],
    "call": [
      "self",
      "size"
    ],
    "encode_image": [
      "self",
      "size"
    ],
    "encode_coordinates": [
      "self",
      "coords_input",
      "image_size"
    ],
    "get_config": [
      "self"
    ]
  },
  "SAMBackbone": {
    "__init__": [
      "self",
      "image_encoder",
      "prompt_encoder",
      "mask_decoder",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "SAMPromptEncoder": {
    "__init__": [
      "self"
    ],
    "build": [
      "self",
      "points_shape",
      "labels_shape",
      "boxes_shape",
      "masks_shape"
    ],
    "compute_output_shape": [
      "self",
      "points_shape",
      "labels_shape",
      "boxes_shape",
      "masks_shape"
    ],
    "_embed_points": [
      "self",
      "points",
      "labels"
    ],
    "_embed_box": [
      "self",
      "box"
    ],
    "_embed_mask": [
      "self",
      "mask"
    ],
    "call": [
      "self",
      "images",
      "points",
      "labels",
      "boxes",
      "masks"
    ],
    "get_config": [
      "self"
    ]
  },
  "SAMImageSegmenterPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "SAMMaskDecoder": {
    "__init__": [
      "self"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "image_embeddings",
      "prompt_dense_positional_embeddings",
      "prompt_sparse_embeddings",
      "prompt_dense_embeddings"
    ],
    "_predict_masks": [
      "self",
      "image_embeddings",
      "image_positional_embeddings",
      "prompt_sparse_embeddings",
      "prompt_dense_embeddings"
    ],
    "get_config": [
      "self"
    ]
  },
  "SAMImageSegmenter": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "predict_step": [
      "self"
    ],
    "fit": [
      "self"
    ],
    "_add_placeholder_prompts": [
      "self",
      "inputs"
    ]
  },
  "roberta_kernel_initializer": [
    "stddev"
  ],
  "RobertaBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "RobertaTextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "hidden_dim",
      "dropout"
    ],
    "get_config": [
      "self"
    ]
  },
  "RobertaTextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "RobertaTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "RobertaMaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "RobertaMaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "OverlappingPatchingAndEmbedding": {
    "__init__": [
      "self",
      "project_dim",
      "patch_size",
      "stride"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "HierarchicalTransformerEncoder": {
    "__init__": [
      "self",
      "project_dim",
      "num_heads",
      "sr_ratio",
      "drop_prob",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "MixFFN": {
    "__init__": [
      "self",
      "channels",
      "mid_channels"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "SegFormerMultiheadAttention": {
    "__init__": [
      "self",
      "project_dim",
      "num_heads",
      "sr_ratio"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "MiTImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "MiTBackbone": {
    "__init__": [
      "self",
      "layerwise_depths",
      "num_layers",
      "layerwise_num_heads",
      "layerwise_sr_ratios",
      "max_drop_path_rate",
      "layerwise_patch_sizes",
      "layerwise_strides",
      "image_shape",
      "hidden_dims"
    ],
    "get_config": [
      "self"
    ]
  },
  "MiTImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "MiTImageConverter": {
    "backbone_cls": []
  },
  "backbone_presets_with_weights": [],
  "MoonshineAudioToTextPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "__init__": [
      "self",
      "audio_converter",
      "tokenizer",
      "decoder_sequence_length"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "decoder_sequence_length",
      "sequence_length"
    ],
    "generate_preprocess": [
      "self",
      "x",
      "decoder_sequence_length",
      "sequence_length"
    ],
    "generate_postprocess": [
      "self",
      "x"
    ]
  },
  "MoonshineAudioToText": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_decoder_with_cache": [
      "self",
      "encoder_hidden_states",
      "encoder_padding_mask",
      "decoder_token_ids",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "cross_attention_cache"
    ],
    "_build_cache": [
      "self",
      "encoder_input_values",
      "encoder_padding_mask",
      "decoder_token_ids",
      "decoder_padding_mask"
    ],
    "call_encoder": [
      "self",
      "encoder_input_values",
      "padding_mask"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "_rotate_half": [
    "x"
  ],
  "_apply_rotary_pos_emb": [
    "t",
    "freqs"
  ],
  "MoonshineMultiHeadAttention": {
    "__init__": [
      "self",
      "num_heads",
      "key_dim",
      "value_dim",
      "attention_bias",
      "attention_dropout",
      "use_causal_mask",
      "apply_rotary_embedding"
    ],
    "build": [
      "self",
      "query_shape",
      "value_shape",
      "key_shape"
    ],
    "_compute_causal_mask": [
      "self",
      "query",
      "value",
      "for_cache"
    ],
    "call": [
      "self",
      "query",
      "value",
      "key",
      "rotary_embedding",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ]
  },
  "compute_output_lengths": [
    "input_lengths"
  ],
  "ComputeAttentionMask": {
    "call": [
      "self",
      "features_for_shape",
      "output_lengths"
    ],
    "compute_output_shape": [
      "self",
      "input_shapes"
    ]
  },
  "Arange": {
    "call": [
      "self",
      "inputs"
    ]
  },
  "MoonshineBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "filter_dim",
      "encoder_num_layers",
      "decoder_num_layers",
      "hidden_dim",
      "intermediate_dim",
      "encoder_num_heads",
      "decoder_num_heads",
      "feedforward_expansion_factor",
      "encoder_use_swiglu_activation",
      "decoder_use_swiglu_activation",
      "max_position_embeddings",
      "pad_head_dim_to_multiple_of",
      "partial_rotary_factor",
      "dropout",
      "initializer_range",
      "rope_theta",
      "attention_bias",
      "attention_dropout",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "MoonshineTokenizer": {},
  "MoonshineDecoderBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "feedforward_expansion_factor",
      "use_swiglu_activation",
      "pad_head_dim_to_multiple_of",
      "initializer_range",
      "attention_bias",
      "attention_dropout",
      "dtype"
    ],
    "build": [
      "self",
      "decoder_sequence_shape",
      "encoder_sequence_shape"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "encoder_sequence",
      "rotary_embedding",
      "encoder_attention_mask",
      "decoder_padding_mask",
      "encoder_padding_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "cross_attention_cache",
      "cross_attention_cache_update_index",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape",
      "encoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "MoonshineAudioConverter": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "sampling_rate",
      "padding_value",
      "do_normalize"
    ],
    "call": [
      "self",
      "inputs",
      "sampling_rate",
      "padding",
      "max_length",
      "pad_to_multiple_of"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "MoonshineEncoderBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "feedforward_expansion_factor",
      "use_swiglu_activation",
      "pad_head_dim_to_multiple_of",
      "dtype",
      "initializer_range",
      "attention_bias",
      "attention_dropout"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "rotary_embedding",
      "attention_mask",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "moonshine_kernel_initializer": [
    "initializer_range"
  ],
  "MoonshineRotaryEmbedding": {
    "__init__": [
      "self",
      "head_dim",
      "max_position_embeddings",
      "base_value",
      "partial_rotary_factor",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "t"
    ],
    "get_config": [
      "self"
    ]
  },
  "MoonshineMLP": {
    "__init__": [
      "self",
      "hidden_dim",
      "feedforward_expansion_factor",
      "use_swiglu_activation",
      "initializer_range",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "PaliGemmaVitEmbeddings": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "hidden_dim",
      "num_channels",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "input_tokens"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "PaliGemmaVitAttention": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "dropout",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_transpose_for_scores": [
      "self",
      "tensor",
      "batch_size"
    ],
    "call": [
      "self",
      "x",
      "attention_mask",
      "return_attention_scores",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "PaliGemmaVitEncoderBlock": {
    "__init__": [
      "self",
      "num_heads",
      "intermediate_dim"
    ],
    "compute_attention": [
      "self",
      "x",
      "mask"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "mask"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "PaliGemmaVitEncoder": {
    "__init__": [
      "self",
      "patch_size",
      "image_size",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "intermediate_dim",
      "dtype"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "mask"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "MultiHeadAttentionPooling": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "dropout"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "PaliGemmaVit": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "num_heads",
      "hidden_dim",
      "num_layers",
      "intermediate_dim",
      "num_classes",
      "pooling",
      "classifier_activation",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "PaliGemmaDecoderBlock": {
    "call": [
      "self",
      "x",
      "padding_mask",
      "response_mask",
      "cache",
      "cache_update_index"
    ],
    "_compute_attention_mask": [
      "self",
      "x",
      "padding_mask",
      "cache",
      "cache_update_index",
      "response_mask"
    ]
  },
  "PaliGemmaBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "image_size",
      "num_layers",
      "num_query_heads",
      "num_key_value_heads",
      "hidden_dim",
      "intermediate_dim",
      "head_dim",
      "vit_patch_size",
      "vit_num_heads",
      "vit_hidden_dim",
      "vit_num_layers",
      "vit_intermediate_dim",
      "vit_pooling",
      "vit_classifier_activation",
      "vit_name",
      "query_head_dim_normalize",
      "use_post_ffw_norm",
      "use_post_attention_norm",
      "attention_logit_soft_cap",
      "final_logit_soft_cap",
      "use_sliding_window_attention",
      "sliding_window_size",
      "layer_norm_epsilon",
      "dropout",
      "dtype"
    ],
    "default_lora_layer_names": [
      "self"
    ],
    "get_config": [
      "self"
    ]
  },
  "PaliGemmaImageConverter": {
    "backbone_cls": []
  },
  "PaliGemmaTokenizer": {
    "backbone_cls": []
  },
  "PaliGemmaCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "preprocessor",
      "backbone"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index",
      "img_embeddings",
      "padding_mask"
    ],
    "_build_cache": [
      "self",
      "token_ids",
      "img_embeddings",
      "padding_mask"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "PaliGemmaCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "image_converter_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "image_converter",
      "sequence_length",
      "add_start_token",
      "add_end_token"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "generate_preprocess": [
      "self",
      "x",
      "sequence_length"
    ]
  },
  "BloomTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "BloomAttention": {
    "__init__": [
      "self",
      "num_heads",
      "dropout",
      "kernel_initializer",
      "bias_initializer"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "BloomCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "_bloom_kernel_initializer": [
    "stddev"
  ],
  "BloomBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "layer_norm_epsilon",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "BloomCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "BloomDecoder": {
    "__init__": [
      "self",
      "num_heads",
      "intermediate_dim",
      "dropout",
      "layer_norm_epsilon",
      "kernel_initializer",
      "bias_initializer"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "cache",
      "cache_update_index",
      "use_causal_mask"
    ],
    "_compute_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "use_causal_mask",
      "cache",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ]
  },
  "MobileNetImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "num_features",
      "preprocessor",
      "head_dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "MobileNetImageConverter": {
    "backbone_cls": []
  },
  "MobileNetImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "adjust_channels": [
    "x",
    "divisor",
    "min_value"
  ],
  "BN_EPSILON": [],
  "BN_MOMENTUM": [],
  "SqueezeAndExcite2D": {
    "__init__": [
      "self",
      "filters",
      "bottleneck_filters",
      "squeeze_activation",
      "excite_activation",
      "name",
      "dtype"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "DepthwiseConvBlock": {
    "__init__": [
      "self",
      "infilters",
      "filters",
      "kernel_size",
      "stride",
      "squeeze_excite_ratio",
      "residual",
      "name",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "InvertedResidualBlock": {
    "__init__": [
      "self",
      "expansion",
      "infilters",
      "filters",
      "kernel_size",
      "stride",
      "squeeze_excite_ratio",
      "activation",
      "padding",
      "name",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "ConvBnActBlock": {
    "__init__": [
      "self",
      "filter",
      "activation",
      "name",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "MobileNetBackbone": {
    "__init__": [
      "self",
      "stackwise_expansion",
      "stackwise_num_blocks",
      "stackwise_num_filters",
      "stackwise_kernel_size",
      "stackwise_num_strides",
      "stackwise_se_ratio",
      "stackwise_activation",
      "stackwise_padding",
      "output_num_filters",
      "depthwise_filters",
      "depthwise_stride",
      "depthwise_residual",
      "last_layer_filter",
      "squeeze_and_excite",
      "image_shape",
      "input_activation",
      "output_activation",
      "input_num_filters",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "Qwen3Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "_qwen3_kernel_initializer": [
    "stddev"
  ],
  "Qwen3Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "num_key_value_heads",
      "head_dim",
      "hidden_dim",
      "intermediate_dim",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "dropout",
      "tie_word_embeddings",
      "sliding_window_size",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "Qwen3CausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "Qwen3CausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "Qwen3LayerNorm": {
    "__init__": [
      "self",
      "head_dim",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "Qwen3Attention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "head_dim",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "kernel_initializer",
      "dropout",
      "layer_norm_epsilon",
      "sliding_window_size"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "cache_update_index"
    ],
    "_mask_sliding_window": [
      "self",
      "attention_mask",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "Qwen3TransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "head_dim",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "dropout",
      "sliding_window_size"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTBackbone": {
    "__init__": [
      "self",
      "image_shape",
      "patch_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon",
      "use_mha_bias",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "pooling",
      "activation",
      "dropout",
      "head_dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTEmbeddings": {
    "__init__": [
      "self",
      "image_size",
      "patch_size",
      "hidden_dim",
      "num_channels",
      "data_format",
      "use_mask_token",
      "dropout_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "bool_masked_pos"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTIntermediate": {
    "__init__": [
      "self",
      "intermediate_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTOutput": {
    "__init__": [
      "self",
      "hidden_dim",
      "dropout_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "input_tensor"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTEncoderBlock": {
    "__init__": [
      "self",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "use_mha_bias",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "return_attention_scores"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTEncoder": {
    "__init__": [
      "self",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "use_mha_bias",
      "dropout_rate",
      "attention_dropout",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_masks",
      "output_hidden_states",
      "return_attention_scores"
    ],
    "get_config": [
      "self"
    ]
  },
  "DeiTImageConverter": {
    "backbone_cls": []
  },
  "DeiTImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "MistralCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "MistralTransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "sliding_window",
      "dropout"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "_mistral_kernel_initializer": [
    "stddev"
  ],
  "MistralBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "hidden_dim",
      "intermediate_dim",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "sliding_window",
      "dropout",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "CachedMistralAttention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "kernel_initializer",
      "sliding_window",
      "dropout"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "get_config": [
      "self"
    ]
  },
  "MistralCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "MistralLayerNormalization": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "MistralTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "WhisperAudioConverter": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "num_mels",
      "num_fft_bins",
      "stride",
      "sampling_rate",
      "max_audio_length"
    ],
    "audio_shape": [
      "self"
    ],
    "_get_mel_filters": [
      "self"
    ],
    "_extract_audio_features": [
      "self",
      "audio"
    ],
    "call": [
      "self",
      "audio"
    ],
    "get_config": [
      "self"
    ]
  },
  "_load_dict": [
    "dict_or_path"
  ],
  "WhisperTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges",
      "special_tokens",
      "language_tokens"
    ],
    "special_tokens": [
      "self"
    ],
    "special_token_ids": [
      "self"
    ],
    "save_assets": [
      "self",
      "dir_path"
    ],
    "set_vocabulary_and_merges": [
      "self",
      "vocabulary",
      "merges"
    ],
    "get_config": [
      "self"
    ]
  },
  "WhisperEncoder": {
    "build": [
      "self",
      "inputs_shape"
    ]
  },
  "whisper_kernel_initializer": [
    "stddev"
  ],
  "Padder": {
    "call": [
      "self",
      "x"
    ]
  },
  "WhisperBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "num_mels",
      "dropout",
      "max_encoder_sequence_length",
      "max_decoder_sequence_length",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "_index_to_einsum_variable": [
    "i"
  ],
  "_build_proj_equation": [
    "free_dims",
    "bound_dims",
    "output_dims"
  ],
  "_get_output_shape": [
    "output_rank",
    "known_last_dims"
  ],
  "WhisperCachedMultiHeadAttention": {
    "build": [
      "self",
      "query_shape",
      "value_shape",
      "key_shape"
    ]
  },
  "WhisperDecoder": {
    "build": [
      "self",
      "decoder_sequence_shape",
      "encoder_sequence_shape"
    ]
  },
  "f_net_kernel_initializer": [
    "stddev"
  ],
  "f_net_bias_initializer": [
    "stddev"
  ],
  "FNetBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "num_segments",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "FNetTextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "dropout"
    ],
    "get_config": [
      "self"
    ]
  },
  "FNetMaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "FNetTextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "FNetMaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "FNetTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "BartSeq2SeqLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "build": [
      "self",
      "input_shape"
    ]
  },
  "bart_kernel_initializer": [
    "stddev"
  ],
  "BartBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "BartTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "BartSeq2SeqLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_decoder_with_cache": [
      "self",
      "encoder_hidden_states",
      "encoder_padding_mask",
      "decoder_token_ids",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "cross_attention_cache",
      "cross_attention_cache_update_index"
    ],
    "call_encoder": [
      "self",
      "token_ids",
      "padding_mask"
    ],
    "_initialize_cache": [
      "self",
      "encoder_token_ids",
      "decoder_token_ids"
    ],
    "_build_cache": [
      "self",
      "encoder_token_ids",
      "encoder_padding_mask",
      "decoder_token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "VAEBackbone": {
    "__init__": [
      "self",
      "encoder_num_filters",
      "encoder_num_blocks",
      "decoder_num_filters",
      "decoder_num_blocks",
      "sampler_method",
      "input_channels",
      "sample_channels",
      "output_channels",
      "scale",
      "shift",
      "data_format",
      "dtype"
    ],
    "scale": [
      "self"
    ],
    "shift": [
      "self"
    ],
    "encode": [
      "self",
      "inputs"
    ],
    "decode": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "Conv2DMultiHeadAttention": {
    "__init__": [
      "self",
      "filters",
      "groups",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "ResNetBlock": {
    "__init__": [
      "self",
      "filters",
      "has_residual_projection",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "VAEEncoder": {
    "__init__": [
      "self",
      "stackwise_num_filters",
      "stackwise_num_blocks",
      "output_channels",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "VAEDecoder": {
    "__init__": [
      "self",
      "stackwise_num_filters",
      "stackwise_num_blocks",
      "output_channels",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DiagonalGaussianDistributionSampler": {
    "__init__": [
      "self",
      "method",
      "axis",
      "seed"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "ESMProteinClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "hidden_dim",
      "dropout"
    ],
    "get_config": [
      "self"
    ]
  },
  "ESMEncoder": {
    "__init__": [
      "self",
      "heads",
      "head_size",
      "intermediate_size",
      "max_wavelength",
      "dropout",
      "activation",
      "use_bias",
      "kernel_initializer",
      "layer_norm_eps",
      "use_rotary"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "attention_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "ESMMaskedPLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "ESMRotaryEmbedding": {
    "_compute_cos_sin_embedding": [
      "self",
      "x",
      "position"
    ],
    "call": [
      "self",
      "q",
      "k",
      "position"
    ],
    "rotate_half": [
      "self",
      "x"
    ],
    "apply_rotary_pos_emb": [
      "self",
      "x",
      "cos",
      "sin"
    ]
  },
  "EsmSelfAttention": {
    "__init__": [
      "self",
      "use_rotary"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "attention_mask"
    ],
    "get_config": [
      "self"
    ]
  },
  "esm2_kernel_initializer": [
    "stddev"
  ],
  "ESMBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "use_bias",
      "activation",
      "dropout",
      "dtype",
      "max_sequence_length",
      "max_wavelength",
      "layer_norm_eps",
      "use_pre_layer_norm",
      "position_embedding_type",
      "pad_token_id"
    ],
    "get_config": [
      "self"
    ]
  },
  "ESMMaskedPLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "ESMProteinClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "ESMTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "lowercase",
      "oov_token"
    ]
  },
  "XceptionImageConverter": {
    "backbone_cls": []
  },
  "XceptionImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "XceptionImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "XceptionBackbone": {
    "__init__": [
      "self",
      "stackwise_conv_filters",
      "stackwise_pooling",
      "image_shape",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "Llama3CausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "Llama3CausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "Llama3Backbone": {},
  "Llama3Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges",
      "bos_token",
      "eos_token",
      "misc_special_tokens"
    ]
  },
  "DistilBertTextClassifierPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "DistilBertTextClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "activation",
      "hidden_dim",
      "dropout"
    ],
    "get_config": [
      "self"
    ]
  },
  "distilbert_kernel_initializer": [
    "stddev"
  ],
  "DistilBertBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "DistilBertTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "lowercase"
    ]
  },
  "DistilBertMaskedLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ]
  },
  "DistilBertMaskedLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ]
  },
  "ContentAndQueryEmbedding": {
    "__init__": [
      "self",
      "vocabulary_size",
      "hidden_dim",
      "dropout",
      "name"
    ],
    "positional_embedding": [
      "self",
      "pos_seq",
      "inv_freq",
      "bsz"
    ],
    "relative_positional_encoding": [
      "self",
      "qlen",
      "klen",
      "bsz",
      "clamp_len"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "token_id_input",
      "mlen"
    ],
    "compute_output_shape": [
      "self",
      "token_id_input_shape"
    ]
  },
  "xlnet_kernel_initializer": [
    "stddev"
  ],
  "XLNetEncoder": {
    "__init__": [
      "self",
      "num_heads",
      "hidden_dim",
      "head_dim",
      "intermediate_dim",
      "dropout",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer_range",
      "bias_initializer",
      "name"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "output_content",
      "attn_mask_content",
      "attn_mask_query",
      "pos_emb",
      "seg_mat",
      "output_query",
      "mems",
      "target_mapping"
    ],
    "compute_output_shape": [
      "self",
      "output_content_shape",
      "pos_emb_shape",
      "attn_mask_content_shape",
      "attn_mask_query_shape",
      "seg_mat_shape",
      "output_query_shape"
    ]
  },
  "XLNetAttentionMaskLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "kernel_initializer_range"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "mlen"
    ],
    "compute_output_shape": [
      "self",
      "padding_mask_shape"
    ]
  },
  "XLNetSegmentMatrixLayer": {
    "call": [
      "self",
      "segment_ids",
      "mlen"
    ],
    "compute_output_shape": [
      "self",
      "segment_ids_shape"
    ]
  },
  "XLNetBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "activation",
      "kernel_initializer_range",
      "bias_initializer",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "token_embedding": [
      "self"
    ]
  },
  "_CHR_IDX": [],
  "_rel_shift": [
    "x",
    "klen"
  ],
  "TwoStreamRelativeAttention": {
    "__init__": [
      "self",
      "kernel_initializer"
    ],
    "_get_common_kwargs_for_sublayer": [
      "self"
    ],
    "build": [
      "self",
      "content_stream_shape"
    ],
    "compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "position",
      "content_attention_bias",
      "positional_attention_bias",
      "segment_matrix",
      "segment_encoding",
      "segment_attention_bias",
      "attention_mask"
    ],
    "call": [
      "self",
      "content_stream",
      "content_attention_bias",
      "positional_attention_bias",
      "relative_position_encoding",
      "query_stream",
      "target_mapping",
      "segment_matrix",
      "segment_encoding",
      "segment_attention_bias",
      "state",
      "content_attention_mask",
      "query_attention_mask"
    ]
  },
  "FalconAttention": {
    "__init__": [
      "self",
      "num_heads",
      "attention_dropout_rate"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "alibi",
      "attention_mask",
      "cache",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "FalconBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_attention_heads",
      "hidden_dim",
      "intermediate_dim",
      "layer_norm_epsilon",
      "attention_dropout_rate",
      "feedforward_dropout_rate",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "FalconCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "FalconTransformerDecoder": {
    "__init__": [
      "self",
      "num_attention_heads",
      "intermediate_dim",
      "layer_norm_epsilon",
      "attention_dropout_rate",
      "feedforward_dropout_rate"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "inputs",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "attention_cache",
      "attention_cache_update_index",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "_compute_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "attention_cache",
      "attention_cache_update_index"
    ],
    "_build_alibi_tensor": [
      "self",
      "num_heads",
      "mask"
    ],
    "_get_slopes": [
      "self",
      "num_heads"
    ]
  },
  "FalconCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "FalconTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "SigLIPBackbone": {
    "__init__": [
      "self",
      "vision_encoder",
      "text_encoder",
      "dtype"
    ],
    "compute_loss": [
      "self",
      "x",
      "y",
      "y_pred",
      "sample_weight"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss",
      "metrics"
    ],
    "get_vision_embeddings": [
      "self",
      "images"
    ],
    "get_text_embeddings": [
      "self",
      "token_ids"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config",
      "custom_objects"
    ]
  },
  "SigLIPLoss": {
    "call": [
      "self",
      "y_true",
      "y_pred"
    ]
  },
  "SigLIPImageConverter": {
    "backbone_cls": []
  },
  "SigLIPPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "image_converter_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "image_converter",
      "sequence_length",
      "add_start_token",
      "add_end_token",
      "canonicalize_text"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "canonicalize_inputs": [
      "self",
      "inputs"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "get_config": [
      "self"
    ]
  },
  "SigLIPTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "SigLIPTextEncoder": {
    "__init__": [
      "self",
      "vocabulary_size",
      "embedding_dim",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "intermediate_dim",
      "intermediate_activation",
      "layer_norm_epsilon",
      "max_sequence_length",
      "projection_dim",
      "dtype",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "SigLIPVisionEmbedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "image_size",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SigLIPTextEmbedding": {
    "__init__": [
      "self",
      "vocabulary_size",
      "sequence_length",
      "embedding_dim",
      "tie_weights",
      "embeddings_initializer",
      "mask_zero"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "compute_mask": [
      "self",
      "inputs",
      "mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "SigLIPMLP": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "activation"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "SigLIPEncoderLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "intermediate_activation",
      "use_causal_mask",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "SigLIPMultiHeadAttentionPooling": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_heads",
      "activation",
      "layer_norm_epsilon"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "inputs_shape"
    ]
  },
  "SigLIPHead": {
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "vision_embedding",
      "text_embedding"
    ],
    "compute_output_shape": [
      "self",
      "vision_embedding_shape",
      "text_embedding_shape"
    ]
  },
  "SigLIPVisionEncoder": {
    "__init__": [
      "self",
      "patch_size",
      "hidden_dim",
      "num_layers",
      "num_heads",
      "intermediate_dim",
      "intermediate_activation",
      "layer_norm_epsilon",
      "image_shape",
      "data_format",
      "dtype",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "interpolate": [
    "x",
    "size",
    "data_format"
  ],
  "DepthAnythingBackbone": {
    "__init__": [
      "self",
      "image_encoder",
      "reassemble_factors",
      "neck_hidden_dims",
      "fusion_hidden_dim",
      "head_hidden_dim",
      "head_in_index",
      "feature_keys",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config",
      "custom_objects"
    ]
  },
  "DepthAnythingDepthEstimator": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "compile": [
      "self",
      "optimizer",
      "loss"
    ]
  },
  "DepthAnythingImageConverter": {
    "backbone_cls": []
  },
  "DepthAnythingLoss": {
    "__init__": [
      "self",
      "lambd",
      "min_depth",
      "max_depth",
      "reduction",
      "name",
      "dtype"
    ]
  },
  "silog": [
    "y_true",
    "y_pred",
    "lambd",
    "min_depth",
    "max_depth"
  ],
  "DepthAnythingDepthEstimatorPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "DepthAnythingTokenToImage": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_height",
      "patch_width",
      "num_cls_tokens",
      "num_register_tokens",
      "data_format"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DepthAnythingReassembleLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "factor",
      "data_format"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DepthAnythingPreActResidualLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "data_format"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DepthAnythingFeatureFusionLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "size",
      "data_format"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "residual",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DepthAnythingNeck": {
    "__init__": [
      "self",
      "patch_size",
      "image_size",
      "backbone_hidden_dim",
      "neck_hidden_dims",
      "reassemble_factors",
      "fusion_hidden_dim",
      "num_cls_tokens",
      "num_register_tokens",
      "data_format"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DepthAnythingDepthEstimationHead": {
    "__init__": [
      "self",
      "patch_size",
      "patch_height",
      "patch_width",
      "fusion_hidden_dim",
      "head_hidden_dim",
      "head_in_index",
      "data_format"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "VGGImageConverter": {
    "backbone_cls": []
  },
  "VGGBackbone": {
    "__init__": [
      "self",
      "stackwise_num_repeats",
      "stackwise_num_filters",
      "image_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "apply_vgg_block": [
    "x",
    "num_layers",
    "filters",
    "kernel_size",
    "activation",
    "padding",
    "max_pool",
    "name"
  ],
  "VGGImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "preprocessor",
      "pooling",
      "pooling_hidden_dim",
      "activation",
      "dropout",
      "head_dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "VGGImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "DFineObjectDetectorPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "DFineEncoderLayer": {
    "__init__": [
      "self",
      "normalize_before",
      "encoder_hidden_dim",
      "num_attention_heads",
      "dropout",
      "layer_norm_eps",
      "encoder_activation_function",
      "activation_dropout",
      "encoder_ffn_dim",
      "kernel_initializer",
      "bias_initializer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "output_attentions",
      "training"
    ],
    "compute_output_spec": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_embeddings",
      "output_attentions",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineEncoder": {
    "__init__": [
      "self",
      "normalize_before",
      "encoder_hidden_dim",
      "num_attention_heads",
      "dropout",
      "layer_norm_eps",
      "encoder_activation_function",
      "activation_dropout",
      "encoder_ffn_dim",
      "num_encoder_layers",
      "kernel_initializer",
      "bias_initializer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_spec": [
      "self",
      "src",
      "src_mask",
      "pos_embed",
      "output_attentions"
    ],
    "call": [
      "self",
      "src",
      "src_mask",
      "pos_embed",
      "output_attentions",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineMultiscaleDeformableAttention": {
    "__init__": [
      "self",
      "hidden_dim",
      "decoder_attention_heads",
      "num_feature_levels",
      "decoder_offset_scale",
      "decoder_method",
      "decoder_n_points",
      "num_queries",
      "spatial_shapes",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_attention": [
      "self",
      "hidden_states",
      "reference_points",
      "spatial_shapes"
    ],
    "call": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "reference_points",
      "spatial_shapes"
    ],
    "compute_output_spec": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "reference_points",
      "spatial_shapes"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineMultiheadAttention": {
    "__init__": [
      "self",
      "embedding_dim",
      "num_heads",
      "dropout",
      "bias",
      "kernel_initializer",
      "bias_initializer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "output_attentions",
      "training"
    ],
    "compute_output_spec": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "output_attentions",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "gather_along_first_two_dims": [
    "tensor",
    "batch_idx",
    "src_idx"
  ],
  "hungarian_matcher": [
    "outputs",
    "targets",
    "num_targets_per_image",
    "use_focal_loss",
    "matcher_alpha",
    "matcher_gamma",
    "matcher_bbox_cost",
    "matcher_class_cost",
    "matcher_ciou_cost",
    "backbone"
  ],
  "compute_vfl_loss": [
    "outputs",
    "targets",
    "indices",
    "num_boxes",
    "num_classes",
    "matcher_alpha",
    "matcher_gamma"
  ],
  "compute_box_losses": [
    "outputs",
    "targets",
    "indices",
    "num_boxes"
  ],
  "compute_local_losses": [
    "outputs",
    "targets",
    "indices",
    "num_boxes",
    "backbone",
    "ddf_temperature",
    "compute_ddf"
  ],
  "_translate_gt_valid_case": [
    "gt_flat",
    "valid_idx_mask",
    "function_values",
    "max_num_bins",
    "mask"
  ],
  "translate_gt": [
    "gt",
    "max_num_bins",
    "reg_scale",
    "up"
  ],
  "_compute_bbox2distance": [
    "points",
    "bbox",
    "max_num_bins",
    "reg_scale",
    "up",
    "eps"
  ],
  "bbox2distance": [
    "points",
    "bbox",
    "max_num_bins",
    "reg_scale",
    "up",
    "eps"
  ],
  "unimodal_distribution_focal_loss": [
    "pred",
    "label",
    "weight_right",
    "weight_left",
    "weight",
    "reduction",
    "avg_factor"
  ],
  "_get_source_permutation_idx": [
    "indices"
  ],
  "get_cdn_matched_indices": [
    "dn_meta"
  ],
  "DFineGate": {
    "__init__": [
      "self",
      "hidden_dim",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "second_residual",
      "hidden_states",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineMLP": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "activation_function",
      "kernel_initializer",
      "bias_initializer",
      "last_layer_initializer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "stat_features",
      "training"
    ],
    "compute_output_spec": [
      "self",
      "stat_features_spec"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineSourceFlattener": {
    "__init__": [
      "self",
      "channel_axis",
      "data_format",
      "dtype"
    ],
    "call": [
      "self",
      "sources",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "sources_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineContrastiveDenoisingGroupGenerator": {
    "__init__": [
      "self",
      "num_labels",
      "num_denoising",
      "label_noise_ratio",
      "box_noise_scale",
      "seed",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "targets",
      "num_queries"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineAnchorGenerator": {
    "__init__": [
      "self",
      "anchor_image_size",
      "feat_strides",
      "data_format",
      "dtype"
    ],
    "call": [
      "self",
      "sources_for_shape_derivation",
      "grid_size"
    ],
    "compute_output_shape": [
      "self",
      "sources_for_shape_derivation_shape",
      "grid_size_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineSpatialShapesExtractor": {
    "__init__": [
      "self",
      "data_format",
      "dtype"
    ],
    "call": [
      "self",
      "sources"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineInitialQueryAndReferenceGenerator": {
    "__init__": [
      "self",
      "num_queries",
      "hidden_dim",
      "learn_initial_query",
      "dtype"
    ],
    "call": [
      "self",
      "inputs",
      "denoising_bbox_unact",
      "denoising_class",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_spec": [
      "self",
      "inputs",
      "denoising_bbox_unact",
      "denoising_class",
      "training"
    ]
  },
  "DFineIntegral": {
    "__init__": [
      "self",
      "max_num_bins",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "pred_corners",
      "project",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineLQE": {
    "__init__": [
      "self",
      "top_prob_values",
      "max_num_bins",
      "lqe_hidden_dim",
      "num_lqe_layers",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "scores",
      "pred_corners",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineConvNormLayer": {
    "__init__": [
      "self",
      "filters",
      "kernel_size",
      "batch_norm_eps",
      "stride",
      "groups",
      "padding",
      "activation_function",
      "kernel_initializer",
      "bias_initializer",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_state",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineRepVggBlock": {
    "__init__": [
      "self",
      "activation_function",
      "filters",
      "batch_norm_eps",
      "kernel_initializer",
      "bias_initializer",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineCSPRepLayer": {
    "__init__": [
      "self",
      "activation_function",
      "batch_norm_eps",
      "filters",
      "num_blocks",
      "expansion",
      "kernel_initializer",
      "bias_initializer",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_state",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineFeatureAggregationBlock": {
    "__init__": [
      "self",
      "encoder_hidden_dim",
      "hidden_expansion",
      "batch_norm_eps",
      "activation_function",
      "num_blocks",
      "kernel_initializer",
      "bias_initializer",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "input_features",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineSCDown": {
    "__init__": [
      "self",
      "encoder_hidden_dim",
      "batch_norm_eps",
      "kernel_size",
      "stride",
      "kernel_initializer",
      "bias_initializer",
      "channel_axis",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "input_features",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineMLPPredictionHead": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim",
      "num_layers",
      "kernel_initializer",
      "bias_initializer",
      "last_layer_initializer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "compute_output_spec": [
      "self",
      "x_spec"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineObjectDetector": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "bounding_box_format",
      "preprocessor",
      "matcher_class_cost",
      "matcher_bbox_cost",
      "matcher_ciou_cost",
      "use_focal_loss",
      "matcher_alpha",
      "matcher_gamma",
      "weight_loss_vfl",
      "weight_loss_bbox",
      "weight_loss_ciou",
      "weight_loss_fgl",
      "weight_loss_ddf",
      "ddf_temperature",
      "prediction_decoder",
      "activation"
    ],
    "compute_loss": [
      "self",
      "x",
      "y",
      "y_pred",
      "sample_weight"
    ],
    "prediction_decoder": [
      "self",
      "prediction_decoder"
    ],
    "decode_predictions": [
      "self",
      "predictions",
      "data"
    ],
    "get_config": [
      "self"
    ],
    "predict_step": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "DFineHybridEncoder": {
    "__init__": [
      "self",
      "encoder_in_channels",
      "feat_strides",
      "encoder_hidden_dim",
      "encode_proj_layers",
      "positional_encoding_temperature",
      "eval_size",
      "normalize_before",
      "num_attention_heads",
      "dropout",
      "layer_norm_eps",
      "encoder_activation_function",
      "activation_dropout",
      "encoder_ffn_dim",
      "num_encoder_layers",
      "batch_norm_eps",
      "hidden_expansion",
      "depth_multiplier",
      "kernel_initializer",
      "bias_initializer",
      "channel_axis",
      "data_format",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "output_attentions",
      "output_hidden_states",
      "training"
    ],
    "build_2d_sincos_position_embedding": [
      "width",
      "height",
      "embedding_dim",
      "temperature",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_spec": [
      "self",
      "inputs_embeds",
      "attention_mask_spec",
      "output_attentions",
      "output_hidden_states",
      "training"
    ]
  },
  "DFineDecoderLayer": {
    "__init__": [
      "self",
      "hidden_dim",
      "decoder_attention_heads",
      "attention_dropout",
      "decoder_activation_function",
      "dropout",
      "activation_dropout",
      "layer_norm_eps",
      "decoder_ffn_dim",
      "num_feature_levels",
      "decoder_offset_scale",
      "decoder_method",
      "decoder_n_points",
      "spatial_shapes",
      "num_queries",
      "kernel_initializer",
      "bias_initializer",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "encoder_hidden_states",
      "attention_mask",
      "output_attentions",
      "training"
    ],
    "compute_output_spec": [
      "self",
      "hidden_states",
      "position_embeddings",
      "reference_points",
      "spatial_shapes",
      "encoder_hidden_states",
      "attention_mask",
      "output_attentions",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineDecoder": {
    "__init__": [
      "self",
      "eval_idx",
      "num_decoder_layers",
      "dropout",
      "hidden_dim",
      "reg_scale",
      "max_num_bins",
      "upsampling_factor",
      "decoder_attention_heads",
      "attention_dropout",
      "decoder_activation_function",
      "activation_dropout",
      "layer_norm_eps",
      "decoder_ffn_dim",
      "num_feature_levels",
      "decoder_offset_scale",
      "decoder_method",
      "decoder_n_points",
      "top_prob_values",
      "lqe_hidden_dim",
      "num_lqe_layers",
      "num_labels",
      "spatial_shapes",
      "layer_scale",
      "num_queries",
      "initializer_bias_prior_prob",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_spec": [
      "self",
      "inputs_embeds",
      "encoder_hidden_states",
      "reference_points",
      "spatial_shapes",
      "attention_mask",
      "output_hidden_states",
      "output_attentions",
      "training"
    ],
    "call": [
      "self",
      "inputs_embeds",
      "encoder_hidden_states",
      "reference_points",
      "spatial_shapes",
      "attention_mask",
      "output_hidden_states",
      "output_attentions",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "DFineImageConverter": {
    "backbone_cls": []
  },
  "d_fine_kernel_initializer": [
    "initializer_range",
    "name"
  ],
  "grid_sample": [
    "data",
    "grid",
    "align_corners",
    "height",
    "width"
  ],
  "multi_scale_deformable_attention_v2": [
    "value",
    "dynamic_spatial_shapes",
    "sampling_locations",
    "attention_weights",
    "num_points",
    "slice_sizes",
    "spatial_shapes",
    "num_levels",
    "num_queries",
    "method"
  ],
  "weighting_function": [
    "max_num_bins",
    "upsampling_factor",
    "reg_scale"
  ],
  "distance2bbox": [
    "points",
    "distance",
    "reg_scale"
  ],
  "hungarian_assignment": [
    "cost_matrix",
    "num_queries"
  ],
  "DFineDenoisingPreprocessorLayer": {
    "__init__": [
      "self",
      "dtype"
    ],
    "call": [
      "self",
      "inputs",
      "denoising_meta_values"
    ]
  },
  "DFineBackbone": {
    "__init__": [
      "self",
      "backbone",
      "decoder_in_channels",
      "encoder_hidden_dim",
      "num_labels",
      "num_denoising",
      "learn_initial_query",
      "num_queries",
      "anchor_image_size",
      "feat_strides",
      "num_feature_levels",
      "hidden_dim",
      "encoder_in_channels",
      "encode_proj_layers",
      "num_attention_heads",
      "encoder_ffn_dim",
      "num_encoder_layers",
      "hidden_expansion",
      "depth_multiplier",
      "eval_idx",
      "num_decoder_layers",
      "decoder_attention_heads",
      "decoder_ffn_dim",
      "decoder_n_points",
      "lqe_hidden_dim",
      "num_lqe_layers",
      "decoder_method",
      "label_noise_ratio",
      "box_noise_scale",
      "labels",
      "seed",
      "image_shape",
      "out_features",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config",
      "custom_objects"
    ]
  },
  "BN_AXIS": [],
  "CBABlock": {
    "__init__": [
      "self",
      "input_filters",
      "output_filters",
      "kernel_size",
      "strides",
      "data_format",
      "batch_norm_momentum",
      "batch_norm_epsilon",
      "activation",
      "dropout",
      "nores"
    ],
    "_conv_kernel_initializer": [
      "self",
      "scale",
      "mode",
      "distribution",
      "seed"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "EfficientNetImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "MBConvBlock": {
    "__init__": [
      "self",
      "input_filters",
      "output_filters",
      "expand_ratio",
      "kernel_size",
      "strides",
      "data_format",
      "se_ratio",
      "batch_norm_momentum",
      "batch_norm_epsilon",
      "activation",
      "dropout",
      "nores"
    ],
    "_conv_kernel_initializer": [
      "self",
      "scale",
      "mode",
      "distribution",
      "seed"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "EfficientNetImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "FusedMBConvBlock": {
    "__init__": [
      "self",
      "input_filters",
      "output_filters",
      "expand_ratio",
      "kernel_size",
      "strides",
      "data_format",
      "se_ratio",
      "batch_norm_momentum",
      "batch_norm_epsilon",
      "activation",
      "projection_activation",
      "dropout",
      "nores",
      "projection_kernel_size"
    ],
    "_conv_kernel_initializer": [
      "self",
      "scale",
      "mode",
      "distribution",
      "seed"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ]
  },
  "EfficientNetBackbone": {
    "__init__": [
      "self"
    ],
    "get_config": [
      "self"
    ],
    "_correct_pad_downsample": [
      "self",
      "inputs",
      "kernel_size"
    ],
    "_apply_efficientnet_block": [
      "self",
      "inputs",
      "filters_in",
      "filters_out",
      "kernel_size",
      "strides",
      "activation",
      "projection_activation",
      "expand_ratio",
      "se_ratio",
      "dropout",
      "batch_norm_epsilon",
      "name",
      "data_format"
    ]
  },
  "conv_kernel_initializer": [
    "scale"
  ],
  "round_filters": [
    "filters",
    "width_coefficient",
    "min_depth",
    "depth_divisor",
    "use_depth_divisor_as_min_depth",
    "cap_round_filter_decrease"
  ],
  "round_repeats": [
    "repeats",
    "depth_coefficient"
  ],
  "get_conv_constructor": [
    "conv_type"
  ],
  "EfficientNetImageConverter": {
    "backbone_cls": []
  },
  "RotaryPositionalEmbedding": {
    "call": [
      "self",
      "pos",
      "dim",
      "theta"
    ]
  },
  "ApplyRoPE": {
    "call": [
      "self",
      "xq",
      "xk",
      "freqs_cis"
    ]
  },
  "FluxRoPEAttention": {
    "__init__": [
      "self",
      "dropout_p",
      "is_causal"
    ],
    "call": [
      "self",
      "q",
      "k",
      "v",
      "positional_encoding"
    ]
  },
  "scaled_dot_product_attention": [
    "query",
    "key",
    "value",
    "attn_mask",
    "dropout_p",
    "is_causal",
    "scale"
  ],
  "rearrange_symbolic_tensors": [
    "qkv",
    "K",
    "H"
  ],
  "FluxBackbone": {
    "__init__": [
      "self",
      "input_channels",
      "hidden_size",
      "mlp_ratio",
      "num_heads",
      "depth",
      "depth_single_blocks",
      "axes_dim",
      "theta",
      "use_bias",
      "guidance_embed",
      "image_shape",
      "text_shape",
      "image_ids_shape",
      "text_ids_shape",
      "y_shape"
    ],
    "get_config": [
      "self"
    ],
    "encode_text_step": [
      "self",
      "token_ids",
      "negative_token_ids"
    ],
    "encode_image_step": [
      "self",
      "images"
    ],
    "add_noise_step": [
      "self",
      "latents",
      "noises",
      "step",
      "num_steps"
    ],
    "denoise_step": [
      "self"
    ],
    "decode_step": [
      "self",
      "latents"
    ]
  },
  "EmbedND": {
    "__init__": [
      "self",
      "theta",
      "axes_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "ids"
    ]
  },
  "MLPEmbedder": {
    "__init__": [
      "self",
      "hidden_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "QKNorm": {
    "__init__": [
      "self",
      "input_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "q",
      "k"
    ]
  },
  "SelfAttention": {
    "__init__": [
      "self",
      "dim",
      "num_heads",
      "use_bias"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "positional_encoding"
    ]
  },
  "Modulation": {
    "__init__": [
      "self",
      "dim",
      "double"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "DoubleStreamBlock": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "mlp_ratio",
      "use_bias"
    ],
    "call": [
      "self",
      "image",
      "text",
      "modulation_encoding",
      "positional_encoding"
    ]
  },
  "SingleStreamBlock": {
    "__init__": [
      "self",
      "hidden_size",
      "num_heads",
      "mlp_ratio",
      "qk_scale"
    ],
    "build": [
      "self",
      "x_shape",
      "modulation_encoding_shape",
      "positional_encoding_shape"
    ],
    "call": [
      "self",
      "x",
      "modulation_encoding",
      "positional_encoding"
    ]
  },
  "LastLayer": {
    "__init__": [
      "self",
      "hidden_size",
      "patch_size",
      "output_channels"
    ],
    "call": [
      "self",
      "x",
      "modulation_encoding"
    ]
  },
  "FluxTextToImagePreprocessor": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "clip_l_preprocessor",
      "t5_preprocessor"
    ],
    "sequence_length": [
      "self"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "generate_preprocess": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "FluxTextToImage": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "fit": [
      "self"
    ],
    "generate_step": [
      "self",
      "latents",
      "token_ids",
      "num_steps",
      "guidance_scale"
    ],
    "generate": [
      "self",
      "inputs",
      "num_steps",
      "guidance_scale",
      "seed"
    ]
  },
  "opt_kernel_initializer": [
    "stddev"
  ],
  "OPTBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_heads",
      "hidden_dim",
      "intermediate_dim",
      "dropout",
      "max_sequence_length",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "OPTTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "OPTCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "OPTCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "Phi3CausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "generate": [
      "self",
      "inputs",
      "max_length",
      "stop_token_ids"
    ]
  },
  "Phi3Decoder": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "dropout",
      "max_sequence_length",
      "pretraining_sequence_length",
      "rope_max_wavelength",
      "rope_scaling_type",
      "rope_scaling_short_factor",
      "rope_scaling_long_factor"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "attention_cache",
      "attention_cache_update_index"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "attention_cache",
      "attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "_phi3_kernel_initializer": [
    "stddev"
  ],
  "Phi3Backbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "hidden_dim",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "layer_norm_epsilon",
      "dropout",
      "max_sequence_length",
      "pretraining_sequence_length",
      "rope_max_wavelength",
      "rope_scaling_type",
      "rope_scaling_short_factor",
      "rope_scaling_long_factor",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "Phi3SuScaledRotaryEmbedding": {
    "__init__": [
      "self",
      "inverese_freq_short_factor",
      "inverese_freq_long_factor",
      "max_sequence_length",
      "pretraining_sequence_length",
      "max_wavelength"
    ],
    "_compute_cos_sin_embedding": [
      "self",
      "inputs",
      "start_index",
      "positions"
    ],
    "get_config": [
      "self"
    ]
  },
  "Phi3Attention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "kernel_initializer",
      "dropout",
      "max_sequence_length",
      "pretraining_sequence_length",
      "rope_max_wavelength",
      "rope_scaling_type",
      "rope_scaling_short_factor",
      "rope_scaling_long_factor"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "get_config": [
      "self"
    ]
  },
  "Phi3Tokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "Phi3LayerNorm": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "Phi3CausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "rwkv7_kernel_initializer": [
    "stddev"
  ],
  "RWKV7Backbone": {
    "__init__": [
      "self",
      "hidden_size",
      "head_size",
      "num_layers",
      "vocabulary_size",
      "intermediate_dim",
      "gate_lora",
      "mv_lora",
      "aaa_lora",
      "decay_lora",
      "dtype",
      "dropout_rate"
    ],
    "get_config": [
      "self"
    ]
  },
  "TRIE": {
    "__slots__": [],
    "__init__": [
      "self",
      "parent",
      "ch"
    ],
    "__repr__": [
      "self"
    ],
    "add": [
      "self",
      "key",
      "idx",
      "val"
    ],
    "find_longest": [
      "self",
      "key",
      "idx"
    ]
  },
  "RWKVTokenizerBase": {
    "__init__": [
      "self",
      "vocabs"
    ],
    "encodeBytes": [
      "self",
      "src"
    ],
    "decodeBytes": [
      "self",
      "tokens"
    ],
    "encode": [
      "self",
      "src"
    ],
    "decode": [
      "self",
      "tokens"
    ],
    "printTokens": [
      "self",
      "tokens"
    ]
  },
  "RWKVTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "dtype",
      "pad_token_id",
      "start_token_id",
      "end_token_id"
    ],
    "set_vocabulary": [
      "self",
      "vocabulary"
    ],
    "save_assets": [
      "self",
      "dir_path"
    ],
    "load_assets": [
      "self",
      "dir_path"
    ],
    "_check_vocabulary": [
      "self"
    ],
    "vocabulary_size": [
      "self"
    ],
    "get_vocabulary": [
      "self"
    ],
    "id_to_token": [
      "self",
      "id"
    ],
    "token_to_id": [
      "self",
      "token"
    ],
    "get_config": [
      "self"
    ],
    "tokenize": [
      "self",
      "inputs"
    ],
    "detokenize": [
      "self",
      "inputs"
    ],
    "compute_output_spec": [
      "self",
      "input_spec"
    ],
    "call": [
      "self",
      "inputs"
    ]
  },
  "RWKV7CausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "add_start_token"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight",
      "sequence_length"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "generate_preprocess": [
      "self",
      "x",
      "sequence_length"
    ],
    "generate_postprocess": [
      "self",
      "x"
    ]
  },
  "RWKV7CausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "compute_head",
      "padding_mask",
      "rnn_mode"
    ],
    "_build_cache": [
      "self",
      "token_ids",
      "padding_mask"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "transpose_head": [
    "x",
    "head_first"
  ],
  "rnn_generalized_delta_rule": [
    "r",
    "w",
    "k",
    "v",
    "a",
    "b",
    "initial_state",
    "output_final_state",
    "head_first"
  ],
  "TimeShift": {
    "call": [
      "self",
      "inputs",
      "cache_x"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "RWKV7ChannelMix": {
    "__init__": [
      "self",
      "dim_ffn",
      "kernel_initializer"
    ],
    "call": [
      "self",
      "x",
      "last_cache_x",
      "not_generation_mode"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "RWKV7TimeMix": {
    "__init__": [
      "self",
      "hidden_size",
      "head_size",
      "gate_lora",
      "mv_lora",
      "aaa_lora",
      "decay_lora",
      "kernel_initializer",
      "add_v_first"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "v_first",
      "padding_mask",
      "last_cache_x",
      "cache_state",
      "rnn_mode",
      "not_generation_mode",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "normalize": [
      "self",
      "x",
      "eps"
    ],
    "get_config": [
      "self"
    ]
  },
  "RWKV7_Block": {
    "__init__": [
      "self",
      "hidden_size",
      "head_size",
      "intermediate_dim",
      "gate_lora",
      "mv_lora",
      "aaa_lora",
      "decay_lora",
      "use_initial_norm",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "generate_call": [
      "self",
      "x",
      "v_first",
      "padding_mask",
      "cache_state",
      "cache_tmix_x",
      "cache_cmix_x",
      "rnn_mode"
    ],
    "call": [
      "self",
      "x",
      "v_first",
      "padding_mask"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "MixtralLayerNormalization": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "CachedMixtralAttention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "kernel_initializer",
      "sliding_window",
      "dropout"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_use_fused_attention_op": [
      "self"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "get_config": [
      "self"
    ]
  },
  "MixtralTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "MixtralCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "MixtralCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "_mixtral_kernel_initializer": [
    "stddev"
  ],
  "MixtralBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "hidden_dim",
      "intermediate_dim",
      "num_key_value_heads",
      "num_experts",
      "top_k",
      "router_jitter_noise",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "router_aux_loss_coef",
      "sliding_window",
      "dropout",
      "dtype",
      "output_router_logits"
    ],
    "get_config": [
      "self"
    ]
  },
  "MixtralMoeExperts": {
    "__init__": [
      "self",
      "num_experts",
      "hidden_dim",
      "intermediate_dim",
      "activation_fn",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "_"
    ],
    "call": [
      "self",
      "hidden_states"
    ]
  },
  "MixtralSparseMoeBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_experts",
      "top_k",
      "router_jitter_noise",
      "layer_norm_epsilon",
      "router_aux_loss_coef",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "training"
    ]
  },
  "MixtralTransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "num_experts",
      "top_k",
      "router_jitter_noise",
      "output_router_logits",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "activation",
      "layer_norm_epsilon",
      "router_aux_loss_coef",
      "kernel_initializer",
      "sliding_window",
      "dropout"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "RetinaNetObjectDetectorPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "RetinaNetBackbone": {
    "__init__": [
      "self",
      "image_encoder",
      "min_level",
      "max_level",
      "use_p5",
      "use_fpn_batch_norm",
      "image_shape",
      "data_format",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "RetinaNetImageConverter": {
    "backbone_cls": []
  },
  "RetinaNetLabelEncoder": {
    "__init__": [
      "self",
      "anchor_generator",
      "bounding_box_format",
      "encoding_format",
      "positive_threshold",
      "negative_threshold",
      "box_variance",
      "background_class",
      "ignore_class",
      "box_matcher_match_values",
      "box_matcher_force_match_for_each_col"
    ],
    "build": [
      "self",
      "images_shape",
      "gt_boxes_shape",
      "gt_classes_shape"
    ],
    "call": [
      "self",
      "images",
      "gt_boxes",
      "gt_classes"
    ],
    "_encode_sample": [
      "self",
      "gt_boxes",
      "gt_classes",
      "anchor_boxes",
      "height",
      "width",
      "channels"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ],
    "compute_output_shape": [
      "self",
      "images_shape",
      "gt_boxes_shape",
      "gt_classes_shape"
    ]
  },
  "PredictionHead": {
    "__init__": [
      "self",
      "output_filters",
      "num_filters",
      "num_conv_layers",
      "use_prior_probability",
      "prior_probability",
      "activation",
      "kernel_initializer",
      "bias_initializer",
      "kernel_regularizer",
      "bias_regularizer",
      "use_group_norm",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "input"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "FeaturePyramid": {
    "__init__": [
      "self",
      "min_level",
      "max_level",
      "use_p5",
      "num_filters",
      "activation",
      "kernel_initializer",
      "bias_initializer",
      "batch_norm_momentum",
      "batch_norm_epsilon",
      "kernel_regularizer",
      "bias_regularizer",
      "use_batch_norm",
      "data_format"
    ],
    "build": [
      "self",
      "input_shapes"
    ],
    "call": [
      "self",
      "inputs"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shapes"
    ]
  },
  "RetinaNetObjectDetector": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "num_classes",
      "bounding_box_format",
      "anchor_generator",
      "label_encoder",
      "use_prediction_head_norm",
      "classification_head_prior_probability",
      "pre_logits_num_conv_layers",
      "preprocessor",
      "activation",
      "dtype",
      "prediction_decoder"
    ],
    "compute_loss": [
      "self",
      "x",
      "y",
      "y_pred",
      "sample_weight"
    ],
    "predict_step": [
      "self"
    ],
    "prediction_decoder": [
      "self",
      "prediction_decoder"
    ],
    "decode_predictions": [
      "self",
      "predictions",
      "data"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "T5GemmaTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "T5GemmaAttention": {
    "__init__": [
      "self",
      "hidden_size",
      "num_attention_heads",
      "num_key_value_heads",
      "query_pre_attn_scalar",
      "attention_bias",
      "head_dim",
      "attention_type",
      "cross_attention_hidden_size",
      "initializer_range",
      "attention_dropout",
      "attn_logit_softcapping",
      "rope_max_wavelength",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_compute_attention_without_fused_op": [
      "self",
      "query_states",
      "key_states",
      "value_states",
      "attention_mask",
      "training"
    ],
    "_compute_attention": [
      "self",
      "query_states",
      "key_states",
      "value_states",
      "attention_mask",
      "training"
    ],
    "call": [
      "self",
      "inputs",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "T5GemmaDecoderLayer": {
    "__init__": [
      "self",
      "hidden_size",
      "rms_norm_eps",
      "num_attention_heads",
      "num_key_value_heads",
      "query_pre_attn_scalar",
      "attention_bias",
      "intermediate_size",
      "hidden_activation",
      "dropout_rate",
      "head_dim",
      "initializer_range",
      "attention_dropout",
      "layer_type",
      "cross_attention_hidden_size",
      "attn_logit_softcapping",
      "sliding_window",
      "rope_max_wavelength",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_make_self_attention_mask": [
      "self",
      "hidden_states",
      "padding_mask",
      "cache",
      "cache_update_index"
    ],
    "_make_cross_attention_mask": [
      "self",
      "hidden_states",
      "padding_mask"
    ],
    "call": [
      "self",
      "inputs",
      "self_attention_padding_mask",
      "cross_attention_padding_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "T5GemmaBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "encoder_hidden_dim",
      "encoder_intermediate_dim",
      "encoder_num_layers",
      "encoder_num_attention_heads",
      "encoder_num_key_value_heads",
      "encoder_head_dim",
      "encoder_layer_types",
      "decoder_hidden_dim",
      "decoder_intermediate_dim",
      "decoder_num_layers",
      "decoder_num_attention_heads",
      "decoder_num_key_value_heads",
      "decoder_head_dim",
      "decoder_layer_types",
      "dropout_rate",
      "rms_norm_eps",
      "query_pre_attn_scalar",
      "attention_bias",
      "hidden_activation",
      "tie_word_embeddings",
      "initializer_range",
      "attention_dropout",
      "sliding_window",
      "cross_attention_hidden_size",
      "attn_logit_softcapping",
      "final_logit_softcapping",
      "rope_max_wavelength",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "T5GemmaEncoderLayer": {
    "__init__": [
      "self",
      "hidden_size",
      "rms_norm_eps",
      "num_attention_heads",
      "num_key_value_heads",
      "query_pre_attn_scalar",
      "attention_bias",
      "intermediate_size",
      "hidden_activation",
      "dropout_rate",
      "initializer_range",
      "attention_dropout",
      "layer_type",
      "head_dim",
      "attn_logit_softcapping",
      "sliding_window",
      "rope_max_wavelength",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "_make_attention_mask": [
      "self",
      "hidden_states",
      "padding_mask"
    ],
    "call": [
      "self",
      "hidden_states",
      "padding_mask",
      "training"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "T5GemmaSeq2SeqLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_encoder": [
      "self",
      "token_ids",
      "padding_mask"
    ],
    "call_decoder_with_cache": [
      "self",
      "decoder_token_ids",
      "decoder_padding_mask",
      "cache",
      "cache_update_index",
      "encoder_output",
      "encoder_padding_mask"
    ],
    "_build_cache": [
      "self",
      "encoder_token_ids",
      "encoder_padding_mask",
      "decoder_token_ids",
      "decoder_padding_mask"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ]
  },
  "T5GemmaSeq2SeqLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "__init__": [
      "self",
      "tokenizer",
      "encoder_sequence_length",
      "decoder_sequence_length",
      "add_start_token",
      "add_end_token"
    ],
    "call": [
      "self",
      "x",
      "y",
      "sample_weight"
    ],
    "generate_preprocess": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "t5gemma_kernel_initializer": [
    "initializer_range"
  ],
  "T5GemmaMLP": {
    "__init__": [
      "self",
      "hidden_size",
      "intermediate_size",
      "hidden_activation",
      "dropout_rate",
      "initializer_range",
      "dtype"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x",
      "training"
    ],
    "get_config": [
      "self"
    ]
  },
  "GptOssExperts": {
    "__init__": [
      "self",
      "num_experts",
      "hidden_dim",
      "intermediate_dim",
      "kernel_initializer",
      "alpha",
      "limit"
    ],
    "build": [
      "self",
      "_"
    ],
    "call": [
      "self",
      "hidden_states"
    ]
  },
  "GptOssTopKRouter": {
    "__init__": [
      "self",
      "num_experts",
      "top_k",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "hidden_states_shape"
    ],
    "call": [
      "self",
      "hidden_states"
    ]
  },
  "GptOssSparseMoeBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "num_experts",
      "top_k",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "hidden_states"
    ]
  },
  "GptOssTransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "num_experts",
      "top_k",
      "output_router_logits",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "kernel_initializer",
      "sliding_window",
      "dropout",
      "head_dim"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "GptOssCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "GptOssCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "_gpt_oss_kernel_initializer": [
    "stddev"
  ],
  "GptOssBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "hidden_dim",
      "intermediate_dim",
      "num_key_value_heads",
      "num_experts",
      "top_k",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "sliding_window",
      "head_dim",
      "dropout",
      "output_router_logits",
      "dtype"
    ],
    "get_config": [
      "self"
    ]
  },
  "GptOssAttention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "kernel_initializer",
      "sliding_window",
      "dropout",
      "head_dim"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "start_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "GptOssTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "GptOssLayerNormalization": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "QwenMoeAttention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "kernel_initializer",
      "bias_initializer",
      "dropout",
      "use_sliding_window_attention",
      "sliding_window_size"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_use_fused_attention_op": [
      "self"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask",
      "cache_update_index"
    ],
    "_mask_sliding_window": [
      "self",
      "attention_mask",
      "cache_update_index"
    ],
    "get_config": [
      "self"
    ]
  },
  "QwenMoeMLP": {
    "__init__": [
      "self",
      "intermediate_dim",
      "hidden_dim",
      "activation_fn",
      "layer_norm_epsilon",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "x"
    ]
  },
  "QwenMoeExperts": {
    "__init__": [
      "self",
      "num_experts",
      "hidden_dim",
      "intermediate_dim",
      "activation_fn",
      "kernel_initializer"
    ],
    "build": [
      "self",
      "_"
    ],
    "call": [
      "self",
      "hidden_states"
    ]
  },
  "QwenSparseMoeBlock": {
    "__init__": [
      "self",
      "hidden_dim",
      "moe_intermediate_dim",
      "shared_expert_intermediate_dim",
      "num_experts",
      "top_k",
      "norm_top_k_prob",
      "kernel_initializer",
      "layer_norm_epsilon",
      "router_aux_loss_coefficient"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "training"
    ]
  },
  "QwenMoeTransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "moe_intermediate_dim",
      "shared_expert_intermediate_dim",
      "num_experts",
      "top_k",
      "norm_top_k_prob",
      "decoder_sparse_step",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "dropout",
      "use_sliding_window_attention",
      "sliding_window_size",
      "layer_index",
      "mlp_only_layers",
      "output_router_logits",
      "router_aux_loss_coefficient"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "QwenMoeLayerNorm": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "QwenMoeCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": [],
    "__init__": [
      "self"
    ]
  },
  "QwenMoeCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "_qwen_moe_kernel_initializer": [
    "stddev"
  ],
  "QwenMoeBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "num_key_value_heads",
      "hidden_dim",
      "intermediate_dim",
      "moe_intermediate_dim",
      "shared_expert_intermediate_dim",
      "num_experts",
      "top_k",
      "norm_top_k_prob",
      "decoder_sparse_step",
      "rope_max_wavelength",
      "rope_scaling_factor",
      "layer_norm_epsilon",
      "dropout",
      "dtype",
      "tie_word_embeddings",
      "use_sliding_window_attention",
      "sliding_window_size",
      "output_router_logits",
      "router_aux_loss_coefficient",
      "mlp_only_layers",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "get_layout_map": [
      "device_mesh",
      "model_parallel_dim_name",
      "data_parallel_dim_name"
    ]
  },
  "QwenMoeTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "vocabulary",
      "merges"
    ]
  },
  "LlamaAttention": {
    "__init__": [
      "self",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_position_scaling_factor",
      "rope_frequency_adjustment_factor",
      "rope_low_freq_factor",
      "rope_high_freq_factor",
      "rope_pretraining_sequence_length",
      "kernel_initializer",
      "dropout"
    ],
    "build": [
      "self",
      "inputs_shape"
    ],
    "call": [
      "self",
      "hidden_states",
      "attention_mask",
      "cache",
      "cache_update_index",
      "training"
    ],
    "_masked_softmax": [
      "self",
      "attention_scores",
      "attention_mask"
    ],
    "_compute_attention": [
      "self",
      "query",
      "key",
      "value",
      "attention_mask"
    ],
    "get_config": [
      "self"
    ]
  },
  "LlamaCausalLMPreprocessor": {
    "backbone_cls": [],
    "tokenizer_cls": []
  },
  "_llama_kernel_initializer": [
    "stddev"
  ],
  "LlamaBackbone": {
    "__init__": [
      "self",
      "vocabulary_size",
      "num_layers",
      "num_query_heads",
      "hidden_dim",
      "intermediate_dim",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_position_scaling_factor",
      "rope_frequency_adjustment_factor",
      "rope_low_freq_factor",
      "rope_high_freq_factor",
      "rope_pretraining_sequence_length",
      "layer_norm_epsilon",
      "dropout",
      "dtype",
      "tie_word_embeddings"
    ],
    "get_config": [
      "self"
    ],
    "get_layout_map": [
      "device_mesh",
      "model_parallel_dim_name",
      "data_parallel_dim_name"
    ]
  },
  "LlamaLayerNorm": {
    "__init__": [
      "self",
      "epsilon"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "x"
    ],
    "get_config": [
      "self"
    ]
  },
  "LlamaTransformerDecoder": {
    "__init__": [
      "self",
      "intermediate_dim",
      "num_query_heads",
      "num_key_value_heads",
      "rope_max_wavelength",
      "rope_position_scaling_factor",
      "rope_frequency_adjustment_factor",
      "rope_low_freq_factor",
      "rope_high_freq_factor",
      "rope_pretraining_sequence_length",
      "activation",
      "layer_norm_epsilon",
      "kernel_initializer",
      "dropout"
    ],
    "build": [
      "self",
      "decoder_sequence_shape"
    ],
    "call": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index",
      "training"
    ],
    "_compute_self_attention_mask": [
      "self",
      "decoder_sequence",
      "decoder_padding_mask",
      "decoder_attention_mask",
      "self_attention_cache",
      "self_attention_cache_update_index"
    ],
    "compute_output_shape": [
      "self",
      "decoder_sequence_shape"
    ],
    "get_config": [
      "self"
    ]
  },
  "LlamaRotaryEmbedding": {
    "__init__": [
      "self",
      "max_wavelength",
      "position_scaling_factor",
      "sequence_axis",
      "feature_axis",
      "frequency_adjustment_factor",
      "low_freq_factor",
      "high_freq_factor",
      "pretraining_sequence_length"
    ],
    "_get_inverse_freq": [
      "self",
      "rotary_dim"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "LlamaTokenizer": {
    "backbone_cls": [],
    "__init__": [
      "self",
      "proto"
    ]
  },
  "LlamaCausalLM": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "call_with_cache": [
      "self",
      "token_ids",
      "cache",
      "cache_update_index"
    ],
    "_build_cache": [
      "self",
      "token_ids"
    ],
    "generate_step": [
      "self",
      "inputs",
      "stop_token_ids"
    ],
    "score": [
      "self",
      "token_ids",
      "padding_mask",
      "scoring_mode",
      "layer_intercept_fn",
      "target_ids"
    ]
  },
  "BASNetPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "BASNetImageConverter": {
    "backbone_cls": []
  },
  "BASNetBackbone": {
    "__init__": [
      "self",
      "image_encoder",
      "num_classes",
      "image_shape",
      "projection_filters",
      "prediction_heads",
      "refinement_head",
      "dtype"
    ],
    "get_config": [
      "self"
    ],
    "from_config": [
      "cls",
      "config"
    ]
  },
  "convolution_block": [
    "x_input",
    "filters",
    "dilation",
    "dtype"
  ],
  "get_resnet_block": [
    "_resnet",
    "block_num"
  ],
  "basnet_predict": [
    "x_input",
    "backbone",
    "filters",
    "segmentation_heads",
    "dtype"
  ],
  "basnet_rrm": [
    "base_model",
    "filters",
    "segmentation_head",
    "dtype"
  ],
  "basnet_presets": [],
  "BASNetImageSegmenter": {
    "backbone_cls": [],
    "preprocessor_cls": [],
    "__init__": [
      "self",
      "backbone",
      "preprocessor"
    ],
    "compute_loss": [
      "self",
      "x",
      "y",
      "y_pred"
    ],
    "compile": [
      "self",
      "optimizer",
      "loss",
      "metrics"
    ]
  },
  "DenseNetBackbone": {
    "__init__": [
      "self",
      "stackwise_num_repeats",
      "image_shape",
      "compression_ratio",
      "growth_rate"
    ],
    "get_config": [
      "self"
    ]
  },
  "apply_dense_block": [
    "x",
    "channel_axis",
    "num_repeats",
    "growth_rate",
    "name"
  ],
  "apply_transition_block": [
    "x",
    "channel_axis",
    "compression_ratio",
    "name"
  ],
  "apply_conv_block": [
    "x",
    "channel_axis",
    "growth_rate",
    "name"
  ],
  "DenseNetImageConverter": {
    "backbone_cls": []
  },
  "DenseNetImageClassifierPreprocessor": {
    "backbone_cls": [],
    "image_converter_cls": []
  },
  "DenseNetImageClassifier": {
    "backbone_cls": [],
    "preprocessor_cls": []
  },
  "DINOV2Backbone": {
    "__init__": [
      "self",
      "patch_size",
      "num_layers",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "layer_scale_init_value",
      "num_register_tokens",
      "use_mask_token",
      "use_swiglu_ffn",
      "dropout_rate",
      "drop_path_rate",
      "image_shape",
      "position_embedding_shape",
      "antialias_in_interpolation",
      "apply_layernorm",
      "data_format",
      "dtype",
      "name"
    ],
    "get_config": [
      "self"
    ]
  },
  "DINOV2PatchEmbedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2Embedding": {
    "__init__": [
      "self",
      "hidden_dim",
      "patch_size",
      "image_shape",
      "num_register_tokens",
      "use_mask_token",
      "dropout_rate",
      "position_embedding_shape",
      "antialias_in_interpolation",
      "data_format"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "masks",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ],
    "compute_output_spec": [
      "self",
      "inputs"
    ],
    "_interpolate_position_embeddings": [
      "position_embeddings",
      "patch_size",
      "source_shape",
      "target_shape",
      "antialias"
    ],
    "_is_interpolated_position_embeddings_updated": [
      "self"
    ],
    "save_own_variables": [
      "self",
      "store"
    ],
    "load_own_variables": [
      "self",
      "store"
    ]
  },
  "DINOV2Attention": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "dropout_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2LayerScale": {
    "__init__": [
      "self",
      "hidden_dim",
      "init_values"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2DropPath": {
    "__init__": [
      "self",
      "rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2MLP": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim",
      "activation"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2SwiGLUFFN": {
    "__init__": [
      "self",
      "hidden_dim",
      "intermediate_dim"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2Layer": {
    "__init__": [
      "self",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "layer_scale_init_value",
      "use_swiglu_ffn",
      "dropout_rate",
      "drop_path_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2Encoder": {
    "__init__": [
      "self",
      "num_layers",
      "hidden_dim",
      "num_heads",
      "intermediate_dim",
      "layer_scale_init_value",
      "use_swiglu_ffn",
      "dropout_rate",
      "drop_path_rate"
    ],
    "build": [
      "self",
      "input_shape"
    ],
    "call": [
      "self",
      "inputs",
      "training"
    ],
    "get_config": [
      "self"
    ],
    "compute_output_shape": [
      "self",
      "input_shape"
    ]
  },
  "DINOV2ImageConverter": {
    "backbone_cls": []
  }
}