{
  "parse_replace_normalizer": [
    "normalizer_dict"
  ],
  "parse_bert_normalizer": [
    "normalizer_dict"
  ],
  "parse_split_step": [
    "pretokenizer_dict"
  ],
  "parse_byte_level_pretokenization_step": [
    "pretokenizer_dict"
  ],
  "parse_metaspace": [
    "pretokenizer_dict"
  ],
  "TransformersTokenizerPipelineParser": {
    "__init__": [
      "self",
      "tokenizer_object",
      "params"
    ],
    "parse": [
      "self"
    ],
    "special_tokens_split": [
      "self"
    ],
    "parse_normalizer_step": [
      "self",
      "step_dict"
    ],
    "normalization": [
      "self"
    ],
    "parse_pre_tokenization_step": [
      "self",
      "step_dict"
    ],
    "pre_tokenization": [
      "self"
    ],
    "tokenization_model": [
      "self"
    ],
    "post_tokenization": [
      "self"
    ],
    "add_truncation": [
      "self"
    ],
    "add_padding": [
      "self",
      "use_max_padding"
    ],
    "decoding": [
      "self"
    ]
  },
  "parse_special_tokens": [
    "hf_tokenizer",
    "only_special_tokens"
  ],
  "convert_fast_tokenizer": [
    "hf_tokenizer",
    "params",
    "number_of_inputs"
  ],
  "is_sentencepiece_model": [
    "hf_tokenizer"
  ],
  "is_sentencepiece_bpe_model": [
    "hf_tokenizer"
  ],
  "align_model_file": [
    "model",
    "hf_tokenizer",
    "added_tokens"
  ],
  "modify_sentencepiece_model": [
    "sp_model_path",
    "add_tokens",
    "hf_tokenizer",
    "skip_special_tokens",
    "add_prefix_space",
    "byte_fallback"
  ],
  "convert_sentencepiece_model_tokenizer": [
    "hf_tokenizer",
    "params",
    "add_attention_mask"
  ],
  "add_prefix_tokens": [
    "prefix_tokens",
    "dense_shape",
    "indices",
    "values",
    "attention_mask",
    "do_left_padding"
  ],
  "get_sp_detokenizer": [
    "sp_model_node",
    "params",
    "prepend_scheme"
  ],
  "is_tiktoken_model": [
    "hf_tokenizer"
  ],
  "convert_tiktoken_model_tokenizer": [
    "hf_tokenizer",
    "params"
  ],
  "build_rwkv_tokenizer": [
    "rwkv_vocab",
    "clean_up_tokenization_spaces",
    "tokenizer_output_type",
    "detokenizer_input_type"
  ],
  "TokenzierConversionParams": {},
  "logger": [],
  "connect_models": [
    "first",
    "second",
    "name_map",
    "by_indices",
    "keep_second_model_unaligned_inputs",
    "keep_remaining_first_model_outputs"
  ],
  "greedy_decoder": [
    "input"
  ],
  "add_greedy_decoding": [
    "text_generation_model",
    "logits_output",
    "output_type"
  ],
  "change_inputs_type": [
    "model",
    "input_type"
  ],
  "change_outputs_type": [
    "model",
    "output_type"
  ],
  "unicode_to_bytes": [],
  "apply_unicode_to_bytes": [
    "token",
    "return_corrupted_tokens"
  ],
  "transform_unigram_token_to_bytes": [
    "token",
    "byte_fallback"
  ],
  "get_hf_tokenizer_attribute": [
    "hf_tokenizer",
    "attributes"
  ],
  "get_package_version": [
    "name"
  ],
  "update_rt_info_with_environment": [
    "ov_tokenizer"
  ],
  "get_processor_template": [
    "hf_tokenizer"
  ],
  "parse_template_processing": [
    "post_processor_json",
    "hf_tokenizer"
  ],
  "parse_roberta_processing": [
    "post_processor_json",
    "hf_tokenizer"
  ],
  "parse_bert_processing": [
    "post_processor_json",
    "hf_tokenizer"
  ],
  "processor_parsers": [],
  "parse_processor_template": [
    "post_processor_json",
    "hf_tokenizer"
  ],
  "update_rt_info_with_processor_template": [
    "ov_tokenizer",
    "hf_tokenizer"
  ],
  "update_rt_info_with_params": [
    "ov_tokenizer",
    "hf_tokenizer",
    "params"
  ],
  "quote_meta": [
    "unquoted"
  ],
  "to_bytes": [
    "number"
  ],
  "create_unpacked_string": [
    "strings"
  ],
  "create_string_constant_node": [
    "value"
  ],
  "bpe": [
    "mergeable_ranks",
    "token",
    "max_rank"
  ],
  "generate_vocab_and_merges": [
    "encoding"
  ],
  "ModifyCombineSegmentsForPairInput": {
    "parse_inputs": [
      "self",
      "combine_seg"
    ],
    "insert_splits": [
      "self"
    ],
    "get_new_inputs": [
      "self"
    ],
    "assert_and_get_postprocessor": [
      "self",
      "model"
    ],
    "run_on_model": [
      "self",
      "model"
    ]
  },
  "add_second_input": [
    "model"
  ],
  "_ext_name": [],
  "_extension_path": [],
  "_openvino_dir": [],
  "is_openvino_tokenizers_compatible": [],
  "_compatibility_message": [],
  "_check_openvino_binary_compatibility": [],
  "old_core_init": [],
  "old_factory_init": [],
  "old_fe_init": [],
  "new_core_init": [
    "self"
  ],
  "new_factory_init": [
    "self"
  ],
  "new_fe_init": [
    "self"
  ],
  "get_create_wrapper": [
    "old_create"
  ],
  "_get_factory_callable": [],
  "_get_opset_factory_callable": [],
  "_get_factory": [],
  "_get_opset_factory": [],
  "StringToTypeAction": {
    "string_to_type_dict": [],
    "__call__": [
      "self",
      "parser",
      "namespace",
      "values",
      "option_string"
    ]
  },
  "check_positive_int": [
    "value"
  ],
  "TrueOrPositiveIntAction": {
    "__call__": [
      "self",
      "parser",
      "namespace",
      "values",
      "option_string"
    ]
  },
  "get_parser": [],
  "convert_hf_tokenizer": [],
  "capture_arg": [
    "func"
  ],
  "convert_tokenizer": [
    "tokenizer_object",
    "params"
  ],
  "__version__": [],
  "ATTENTION_MASK_INPUT_NAME": [],
  "TOKEN_IDS_INPUT_NAME": [],
  "TOKEN_TYPE_IDS_INPUT_NAME": [],
  "LOGITS_OUTPUT_NAME": [],
  "TOKEN_IDS_OUTPUT_NAME": [],
  "STRING_OUTPUT_NAME": [],
  "BOS_TOKEN_ID_NAME": [],
  "EOS_TOKEN_ID_NAME": [],
  "PAD_TOKEN_ID_NAME": [],
  "CHAT_TEMPLATE_NAME": [],
  "ORIGINAL_TOKENIZER_CLASS_NAME": [],
  "ORIGINAL_POST_PROCESSOR_NAME": [],
  "PROCESSED_POST_PROCESSOR_NAME": [],
  "rt_info_to_hf_attribute_map": [],
  "GREEDY_DECODER_NAME": [],
  "TOKENIZER_NAME": [],
  "DETOKENIZER_NAME": [],
  "MIN_CACHE_CAPACITY": [],
  "VOCAB_SIZE_CACHE_PROPORTION": [],
  "UTF8ReplaceMode": {
    "__str__": [
      "self"
    ],
    "__eq__": [
      "self",
      "other"
    ]
  },
  "BasePipelineStep": {
    "__str__": [
      "self"
    ],
    "get_config": [
      "self"
    ],
    "get_pipeline": [
      "self"
    ],
    "set_pipeline": [
      "self",
      "pipeline"
    ],
    "get_ov_subgraph": [
      "self"
    ],
    "finalize": [
      "self"
    ]
  },
  "SpecialToken": {
    "regex_repr": [
      "self"
    ]
  },
  "SpecialTokensSplit": {
    "__post_init__": [
      "self"
    ],
    "from_string_iter": [
      "cls",
      "strings"
    ],
    "from_hf_tokenizer": [
      "cls",
      "hf_tokenizer"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "NormalizationStep": {},
  "NormalizeUnicode": {
    "__post_init__": [
      "self"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "CaseFoldStep": {
    "__post_init__": [
      "self"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "RegexNormalizationStep": {
    "strip_accents_regex": [
      "cls"
    ],
    "add_prefix_whitespace_regex": [
      "cls"
    ],
    "add_prefix_whitespace_to_not_whitespace_regex": [
      "cls"
    ],
    "replace_spaces_metaspace": [
      "cls",
      "replace_term"
    ],
    "prepend_regex": [
      "cls",
      "string"
    ],
    "prepend_with_check_regex": [
      "cls",
      "string",
      "check_string"
    ],
    "del_control_chars_regex": [
      "cls"
    ],
    "strip_regex": [
      "cls",
      "left",
      "right"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "CharsmapStep": {
    "__add__": [
      "self",
      "other"
    ],
    "from_hf_step_json": [
      "cls",
      "step_json"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "PreTokenizatinStep": {},
  "RegexSplitStep": {
    "__post_init__": [
      "self"
    ],
    "__add__": [
      "self",
      "other"
    ],
    "split_by_chars": [
      "cls"
    ],
    "bert_whitespace_splitter": [
      "cls"
    ],
    "bert_keep_delimeters_splitter": [
      "cls"
    ],
    "bert_splitter": [
      "cls"
    ],
    "whitespace_splitter": [
      "cls"
    ],
    "metaspace_splitter": [
      "cls",
      "metaspace"
    ],
    "byte_level_splitter": [
      "cls"
    ],
    "digits_splitter": [
      "cls",
      "behaviour"
    ],
    "punctuation_splitter": [
      "cls",
      "behaviour"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "WhitespaceSplitStep": {
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "BytesToCharsStep": {
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "TokenizationModelStep": {},
  "VocabEncoderStep": {
    "__post_init__": [
      "self"
    ],
    "from_hf_json": [
      "cls",
      "tokenizer_json"
    ],
    "get_vocab_node_outputs": [
      "self"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "TrieTokenizerStep": {
    "__post_init__": [
      "self"
    ],
    "fill_vocab": [
      "vocab",
      "indices"
    ],
    "from_rwkv_vocab": [
      "cls",
      "vocab_file_strings"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "WordPieceTokenizationStep": {
    "__post_init__": [
      "self"
    ],
    "vocab_size": [
      "self"
    ],
    "from_hf_json": [
      "cls",
      "tokenizer_json"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "BPETokenizationStep": {
    "finalize": [
      "self"
    ],
    "from_hf_json": [
      "cls",
      "tokenizer_json"
    ],
    "from_tiktoken_encoding": [
      "cls",
      "encoding",
      "reference_vocab"
    ],
    "merges_are_pairs": [
      "self"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "UnigramModelStep": {
    "from_hf_json": [
      "cls",
      "tokenizer_json"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "PostTokenizationStep": {},
  "TruncationStep": {
    "from_hf_json": [
      "cls",
      "tokenizer_json",
      "num_of_added_tokens",
      "max_length"
    ],
    "from_hf_object": [
      "cls",
      "tokenizer",
      "num_of_added_tokens"
    ],
    "validate_inputs": [
      "input_nodes"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "SpecialTokenWithId": {
    "set_token_id": [
      "self",
      "vocab"
    ],
    "token_id": [
      "self",
      "value"
    ]
  },
  "TokenWithTypeId": {},
  "AddToken": {},
  "Sequence": {},
  "CombineSegmentsStep": {
    "__post_init__": [
      "self"
    ],
    "finalize": [
      "self"
    ],
    "set_tokens_ids": [
      "self",
      "vocab"
    ],
    "number_of_added_tokens": [
      "self"
    ],
    "from_hf_json_template_postprocessor": [
      "cls",
      "post_processor_dict",
      "number_of_inputs",
      "add_special_tokens"
    ],
    "from_hf_json_bert_postprocessor": [
      "cls",
      "post_processor_dict",
      "number_of_inputs",
      "add_special_tokens"
    ],
    "from_hf_json_roberta_processor": [
      "cls",
      "post_processor_dict",
      "number_of_inputs",
      "add_special_tokens"
    ],
    "validate_inputs": [
      "self",
      "input_nodes"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "PaddingStep": {
    "from_hf_json": [
      "cls",
      "tokenizer_json",
      "pad_to_max_length",
      "max_length",
      "pad_right"
    ],
    "validate_inputs": [
      "input_nodes"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "DecodingStep": {},
  "VocabDecoderStep": {
    "finalize": [
      "self"
    ],
    "add_special_tokens_to_vocab": [
      "vocab",
      "added_tokens"
    ],
    "from_hf_json": [
      "cls",
      "tokenizer_json",
      "pipeline_vocab",
      "added_tokens",
      "skip_tokens",
      "do_skip_tokens",
      "is_byte_level"
    ],
    "get_vocab_node_outputs": [
      "self"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "CharsToBytesStep": {
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "FuseStep": {
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "UTF8ValidateStep": {
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "ByteFallbackStep": {
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ]
  },
  "RegexDecodingStep": {
    "clean_up_tokenization_spaces": [
      "cls"
    ],
    "parse_replace_dict": [
      "cls",
      "replace_dict"
    ],
    "parse_strip_dict": [
      "cls",
      "replace_dict"
    ],
    "rstrip_space": [
      "cls"
    ],
    "strip_forward_space": [
      "cls"
    ],
    "strip_forward_space_before_not_space": [
      "cls"
    ],
    "replace_end_of_word_suffix": [
      "cls",
      "suffix"
    ],
    "replace_continuing_subword_prefix": [
      "cls",
      "prefix"
    ],
    "get_ov_subgraph": [
      "self",
      "input_nodes"
    ],
    "replace_sp_spaces": [
      "cls"
    ]
  },
  "TokenizerPipeline": {
    "is_byte_level": [
      "self"
    ],
    "get_config": [
      "self"
    ],
    "add_steps": [
      "self",
      "steps"
    ],
    "_": [
      "self",
      "steps"
    ],
    "__getitem__": [
      "self",
      "item"
    ],
    "replace_normalization_step": [
      "step"
    ],
    "merge_normalization_steps": [
      "self"
    ],
    "del_duplicated_split_steps": [
      "self"
    ],
    "merge_regex_split_steps": [
      "self"
    ],
    "update_metaspace_step_with_special_tokens": [
      "self"
    ],
    "finalize": [
      "self"
    ],
    "is_metaspace_prepend_first": [
      "self"
    ],
    "get_tokenizer_ov_subgraph": [
      "self"
    ],
    "normalization_steps": [
      "self"
    ],
    "pre_tokenization_steps": [
      "self"
    ],
    "tokenization_steps": [
      "self"
    ],
    "post_tokenization_steps": [
      "self"
    ],
    "decoding_steps": [
      "self"
    ],
    "add_ragged_dimension": [
      "input_node"
    ],
    "create_decoding_pipeline": [
      "self",
      "input_nodes"
    ],
    "get_detokenizer_ov_subgraph": [
      "self"
    ]
  }
}