{
  "logger": [],
  "REPO_ID": [],
  "punc_norm": [
    "text"
  ],
  "Conditionals": {
    "to": [
      "self",
      "device"
    ],
    "save": [
      "self",
      "fpath"
    ],
    "load": [
      "cls",
      "fpath",
      "map_location"
    ]
  },
  "ChatterboxTurboTTS": {
    "ENC_COND_LEN": [],
    "DEC_COND_LEN": [],
    "__init__": [
      "self",
      "t3",
      "s3gen",
      "ve",
      "tokenizer",
      "device",
      "conds"
    ],
    "from_local": [
      "cls",
      "ckpt_dir",
      "device"
    ],
    "from_pretrained": [
      "cls",
      "device"
    ],
    "norm_loudness": [
      "self",
      "wav",
      "sr",
      "target_lufs"
    ],
    "prepare_conditionals": [
      "self",
      "wav_fpath",
      "exaggeration",
      "norm_loudness"
    ],
    "generate": [
      "self",
      "text",
      "repetition_penalty",
      "min_p",
      "top_p",
      "audio_prompt_path",
      "exaggeration",
      "cfg_weight",
      "temperature",
      "top_k",
      "norm_loudness"
    ]
  },
  "ChatterboxTTS": {
    "ENC_COND_LEN": [],
    "DEC_COND_LEN": [],
    "__init__": [
      "self",
      "t3",
      "s3gen",
      "ve",
      "tokenizer",
      "device",
      "conds"
    ],
    "from_local": [
      "cls",
      "ckpt_dir",
      "device"
    ],
    "from_pretrained": [
      "cls",
      "device"
    ],
    "prepare_conditionals": [
      "self",
      "wav_fpath",
      "exaggeration"
    ],
    "generate": [
      "self",
      "text",
      "repetition_penalty",
      "min_p",
      "top_p",
      "audio_prompt_path",
      "exaggeration",
      "cfg_weight",
      "temperature"
    ]
  },
  "ChatterboxVC": {
    "ENC_COND_LEN": [],
    "DEC_COND_LEN": [],
    "__init__": [
      "self",
      "s3gen",
      "device",
      "ref_dict"
    ],
    "from_local": [
      "cls",
      "ckpt_dir",
      "device"
    ],
    "from_pretrained": [
      "cls",
      "device"
    ],
    "set_target_voice": [
      "self",
      "wav_fpath"
    ],
    "generate": [
      "self",
      "audio",
      "target_voice_path"
    ]
  },
  "SUPPORTED_LANGUAGES": [],
  "ChatterboxMultilingualTTS": {
    "ENC_COND_LEN": [],
    "DEC_COND_LEN": [],
    "__init__": [
      "self",
      "t3",
      "s3gen",
      "ve",
      "tokenizer",
      "device",
      "conds"
    ],
    "get_supported_languages": [
      "cls"
    ],
    "from_local": [
      "cls",
      "ckpt_dir",
      "device"
    ],
    "from_pretrained": [
      "cls",
      "device"
    ],
    "prepare_conditionals": [
      "self",
      "wav_fpath",
      "exaggeration"
    ],
    "generate": [
      "self",
      "text",
      "language_id",
      "audio_prompt_path",
      "exaggeration",
      "cfg_weight",
      "temperature",
      "repetition_penalty",
      "min_p",
      "top_p"
    ]
  },
  "__version__": [],
  "AttrDict": {
    "__init__": [
      "self"
    ]
  },
  "SOS": [],
  "EOS": [],
  "drop_invalid_tokens": [
    "x"
  ],
  "S3_SR": [],
  "S3_HOP": [],
  "S3_TOKEN_HOP": [],
  "S3_TOKEN_RATE": [],
  "SPEECH_VOCAB_SIZE": [],
  "S3Tokenizer": {
    "ignore_state_dict_missing": [],
    "__init__": [
      "self",
      "name",
      "config"
    ],
    "pad": [
      "self",
      "wavs",
      "sr"
    ],
    "_prepare_audio": [
      "self",
      "wavs"
    ],
    "forward": [
      "self",
      "wavs",
      "accelerator",
      "max_len"
    ],
    "log_mel_spectrogram": [
      "self",
      "audio",
      "padding"
    ]
  },
  "LLAMA_520M_CONFIG_DICT": [],
  "GPT2_MEDIUM_CONFIG": [],
  "LLAMA_CONFIGS": [],
  "_ensure_BOT_EOT": [
    "text_tokens",
    "hp"
  ],
  "T3": {
    "__init__": [
      "self",
      "hp"
    ],
    "device": [
      "self"
    ],
    "prepare_conditioning": [
      "self",
      "t3_cond"
    ],
    "prepare_input_embeds": [
      "self"
    ],
    "forward": [
      "self"
    ],
    "loss": [
      "self"
    ],
    "inference": [
      "self"
    ],
    "inference_turbo": [
      "self",
      "t3_cond",
      "text_tokens",
      "temperature",
      "top_k",
      "top_p",
      "repetition_penalty",
      "max_gen_len"
    ]
  },
  "T3HuggingfaceBackend": {
    "__init__": [
      "self",
      "config",
      "llama"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "decoder_cond",
      "use_cache",
      "past_key_values",
      "cache_position"
    ],
    "forward": [
      "self",
      "inputs_embeds",
      "past_key_values",
      "use_cache",
      "output_attentions",
      "output_hidden_states",
      "return_dict"
    ]
  },
  "LLAMA_ALIGNED_HEADS": [],
  "AlignmentAnalysisResult": {},
  "AlignmentStreamAnalyzer": {
    "__init__": [
      "self",
      "tfmr",
      "queue",
      "text_tokens_slice",
      "alignment_layer_idx",
      "eos_idx"
    ],
    "_add_attention_spy": [
      "self",
      "tfmr",
      "buffer_idx",
      "layer_idx",
      "head_idx"
    ],
    "step": [
      "self",
      "logits",
      "next_token"
    ]
  },
  "T3Cond": {
    "to": [
      "self"
    ],
    "save": [
      "self",
      "fpath"
    ],
    "load": [
      "fpath",
      "map_location"
    ]
  },
  "T3CondEnc": {
    "__init__": [
      "self",
      "hp"
    ],
    "forward": [
      "self",
      "cond"
    ]
  },
  "T3Config": {
    "__init__": [
      "self",
      "text_tokens_dict_size"
    ],
    "n_channels": [
      "self"
    ],
    "is_multilingual": [
      "self"
    ],
    "english_only": [
      "cls"
    ],
    "multilingual": [
      "cls"
    ]
  },
  "RelativePositionBias": {
    "__init__": [
      "self",
      "scale",
      "causal",
      "num_buckets",
      "max_distance",
      "heads"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "causal",
      "num_buckets",
      "max_distance"
    ],
    "forward": [
      "self",
      "qk_dots"
    ]
  },
  "AttentionQKV": {
    "__init__": [
      "self",
      "n_heads",
      "head_dim",
      "dropout_rate",
      "scale",
      "flash"
    ],
    "setup_flash_config": [
      "self"
    ],
    "forward": [
      "self",
      "q",
      "k",
      "v",
      "mask"
    ],
    "scaled_dot_product_attention": [
      "self",
      "q",
      "k",
      "v",
      "mask"
    ],
    "flash_attention": [
      "self",
      "q",
      "k",
      "v",
      "mask"
    ],
    "split_heads": [
      "self",
      "x"
    ],
    "combine_heads": [
      "self",
      "x"
    ]
  },
  "AttentionBlock2": {
    "__init__": [
      "self",
      "channels",
      "num_heads",
      "num_head_channels",
      "relative_pos_embeddings",
      "flash_attention",
      "dropout_rate",
      "scale"
    ],
    "forward": [
      "self",
      "x1",
      "x2",
      "mask"
    ]
  },
  "Perceiver": {
    "__init__": [
      "self",
      "pre_attention_query_token",
      "pre_attention_query_size",
      "embedding_dim",
      "num_attn_heads"
    ],
    "forward": [
      "self",
      "h"
    ]
  },
  "LearnedPositionEmbeddings": {
    "__init__": [
      "self",
      "seq_len",
      "model_dim",
      "init"
    ],
    "forward": [
      "self",
      "x"
    ],
    "get_fixed_embedding": [
      "self",
      "idx"
    ]
  },
  "SOT": [],
  "EOT": [],
  "UNK": [],
  "SPACE": [],
  "SPECIAL_TOKENS": [],
  "EnTokenizer": {
    "__init__": [
      "self",
      "vocab_file_path"
    ],
    "check_vocabset_sot_eot": [
      "self"
    ],
    "text_to_tokens": [
      "self",
      "text"
    ],
    "encode": [
      "self",
      "txt"
    ],
    "decode": [
      "self",
      "seq"
    ]
  },
  "_kakasi": [],
  "_dicta": [],
  "_russian_stresser": [],
  "is_kanji": [
    "c"
  ],
  "is_katakana": [
    "c"
  ],
  "hiragana_normalize": [
    "text"
  ],
  "add_hebrew_diacritics": [
    "text"
  ],
  "korean_normalize": [
    "text"
  ],
  "ChineseCangjieConverter": {
    "__init__": [
      "self",
      "model_dir"
    ],
    "_load_cangjie_mapping": [
      "self",
      "model_dir"
    ],
    "_init_segmenter": [
      "self"
    ],
    "_cangjie_encode": [
      "self",
      "glyph"
    ],
    "__call__": [
      "self",
      "text"
    ]
  },
  "add_russian_stress": [
    "text"
  ],
  "MTLTokenizer": {
    "__init__": [
      "self",
      "vocab_file_path"
    ],
    "check_vocabset_sot_eot": [
      "self"
    ],
    "preprocess_text": [
      "self",
      "raw_text",
      "language_id",
      "lowercase",
      "nfkd_normalize"
    ],
    "text_to_tokens": [
      "self",
      "text",
      "language_id",
      "lowercase",
      "nfkd_normalize"
    ],
    "encode": [
      "self",
      "txt",
      "language_id",
      "lowercase",
      "nfkd_normalize"
    ],
    "decode": [
      "self",
      "seq"
    ]
  },
  "mel_basis": [
    "hp"
  ],
  "preemphasis": [
    "wav",
    "hp"
  ],
  "melspectrogram": [
    "wav",
    "hp",
    "pad"
  ],
  "_stft": [
    "y",
    "hp",
    "pad"
  ],
  "_amp_to_db": [
    "x",
    "hp"
  ],
  "_db_to_amp": [
    "x"
  ],
  "_normalize": [
    "s",
    "hp",
    "headroom_db"
  ],
  "VoiceEncConfig": {
    "num_mels": [],
    "sample_rate": [],
    "speaker_embed_size": [],
    "ve_hidden_size": [],
    "flatten_lstm_params": [],
    "n_fft": [],
    "hop_size": [],
    "win_size": [],
    "fmax": [],
    "fmin": [],
    "preemphasis": [],
    "mel_power": [],
    "mel_type": [],
    "normalized_mels": [],
    "ve_partial_frames": [],
    "ve_final_relu": [],
    "stft_magnitude_min": []
  },
  "pack": [
    "arrays",
    "seq_len",
    "pad_value"
  ],
  "get_num_wins": [
    "n_frames",
    "step",
    "min_coverage",
    "hp"
  ],
  "get_frame_step": [
    "overlap",
    "rate",
    "hp"
  ],
  "stride_as_partials": [
    "mel",
    "hp",
    "overlap",
    "rate",
    "min_coverage"
  ],
  "VoiceEncoder": {
    "__init__": [
      "self",
      "hp"
    ],
    "device": [
      "self"
    ],
    "forward": [
      "self",
      "mels"
    ],
    "inference": [
      "self",
      "mels",
      "mel_lens",
      "overlap",
      "rate",
      "min_coverage",
      "batch_size"
    ],
    "utt_to_spk_embed": [
      "utt_embeds"
    ],
    "voice_similarity": [
      "embeds_x",
      "embeds_y"
    ],
    "embeds_from_mels": [
      "self",
      "mels",
      "mel_lens",
      "as_spk",
      "batch_size"
    ],
    "embeds_from_wavs": [
      "self",
      "wavs",
      "sample_rate",
      "as_spk",
      "batch_size",
      "trim_top_db"
    ]
  },
  "ConvRNNF0Predictor": {
    "__init__": [
      "self",
      "num_class",
      "in_channels",
      "cond_channels"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Snake": {
    "__init__": [
      "self",
      "in_features",
      "alpha",
      "alpha_trainable",
      "alpha_logscale"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "get_padding": [
    "kernel_size",
    "dilation"
  ],
  "init_weights": [
    "m",
    "mean",
    "std"
  ],
  "ResBlock": {
    "__init__": [
      "self",
      "channels",
      "kernel_size",
      "dilations"
    ],
    "forward": [
      "self",
      "x"
    ],
    "remove_weight_norm": [
      "self"
    ]
  },
  "SineGen": {
    "__init__": [
      "self",
      "samp_rate",
      "harmonic_num",
      "sine_amp",
      "noise_std",
      "voiced_threshold"
    ],
    "_f02uv": [
      "self",
      "f0"
    ],
    "forward": [
      "self",
      "f0"
    ]
  },
  "SourceModuleHnNSF": {
    "__init__": [
      "self",
      "sampling_rate",
      "upsample_scale",
      "harmonic_num",
      "sine_amp",
      "add_noise_std",
      "voiced_threshod"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "HiFTGenerator": {
    "__init__": [
      "self",
      "in_channels",
      "base_channels",
      "nb_harmonics",
      "sampling_rate",
      "nsf_alpha",
      "nsf_sigma",
      "nsf_voiced_threshold",
      "upsample_rates",
      "upsample_kernel_sizes",
      "istft_params",
      "resblock_kernel_sizes",
      "resblock_dilation_sizes",
      "source_resblock_kernel_sizes",
      "source_resblock_dilation_sizes",
      "lrelu_slope",
      "audio_limit",
      "f0_predictor"
    ],
    "remove_weight_norm": [
      "self"
    ],
    "_stft": [
      "self",
      "x"
    ],
    "_istft": [
      "self",
      "magnitude",
      "phase"
    ],
    "decode": [
      "self",
      "x",
      "s"
    ],
    "forward": [
      "self",
      "batch",
      "device"
    ],
    "inference": [
      "self",
      "speech_feat",
      "cache_source"
    ]
  },
  "pad_list": [
    "xs",
    "pad_value"
  ],
  "extract_feature": [
    "audio"
  ],
  "BasicResBlock": {
    "expansion": [],
    "__init__": [
      "self",
      "in_planes",
      "planes",
      "stride"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FCM": {
    "__init__": [
      "self",
      "block",
      "num_blocks",
      "m_channels",
      "feat_dim"
    ],
    "_make_layer": [
      "self",
      "block",
      "planes",
      "num_blocks",
      "stride"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "get_nonlinear": [
    "config_str",
    "channels"
  ],
  "statistics_pooling": [
    "x",
    "dim",
    "keepdim",
    "unbiased",
    "eps"
  ],
  "StatsPool": {
    "forward": [
      "self",
      "x"
    ]
  },
  "TDNNLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "bias",
      "config_str"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CAMLayer": {
    "__init__": [
      "self",
      "bn_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "bias",
      "reduction"
    ],
    "forward": [
      "self",
      "x"
    ],
    "seg_pooling": [
      "self",
      "x",
      "seg_len",
      "stype"
    ]
  },
  "CAMDenseTDNNLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "bn_channels",
      "kernel_size",
      "stride",
      "dilation",
      "bias",
      "config_str",
      "memory_efficient"
    ],
    "bn_function": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CAMDenseTDNNBlock": {
    "__init__": [
      "self",
      "num_layers",
      "in_channels",
      "out_channels",
      "bn_channels",
      "kernel_size",
      "stride",
      "dilation",
      "bias",
      "config_str",
      "memory_efficient"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "TransitLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "bias",
      "config_str"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "DenseLayer": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "bias",
      "config_str"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CAMPPlus": {
    "__init__": [
      "self",
      "feat_dim",
      "embedding_size",
      "growth_rate",
      "bn_size",
      "init_channels",
      "config_str",
      "memory_efficient",
      "output_level"
    ],
    "forward": [
      "self",
      "x"
    ],
    "inference": [
      "self",
      "audio_list"
    ]
  },
  "CFM_PARAMS": [],
  "mask_to_bias": [
    "mask",
    "dtype"
  ],
  "Transpose": {
    "__init__": [
      "self",
      "dim0",
      "dim1"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CausalBlock1D": {
    "__init__": [
      "self",
      "dim",
      "dim_out"
    ],
    "forward": [
      "self",
      "x",
      "mask"
    ]
  },
  "CausalResnetBlock1D": {
    "__init__": [
      "self",
      "dim",
      "dim_out",
      "time_emb_dim",
      "groups"
    ]
  },
  "CausalConv1d": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "dilation",
      "groups",
      "bias",
      "padding_mode",
      "device",
      "dtype"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConditionalDecoder": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "causal",
      "channels",
      "dropout",
      "attention_head_dim",
      "n_blocks",
      "num_mid_blocks",
      "num_heads",
      "act_fn",
      "meanflow"
    ],
    "dtype": [
      "self"
    ],
    "initialize_weights": [
      "self"
    ],
    "forward": [
      "self",
      "x",
      "mask",
      "mu",
      "t",
      "spks",
      "cond",
      "r"
    ]
  },
  "S3GEN_SR": [],
  "S3GEN_SIL": [],
  "cast_all": [],
  "ConditionalCFM": {
    "__init__": [
      "self",
      "in_channels",
      "cfm_params",
      "n_spks",
      "spk_emb_dim",
      "estimator"
    ],
    "forward": [
      "self",
      "mu",
      "mask",
      "n_timesteps",
      "temperature",
      "spks",
      "cond",
      "prompt_len",
      "flow_cache"
    ],
    "solve_euler": [
      "self",
      "x",
      "t_span",
      "mu",
      "mask",
      "spks",
      "cond",
      "meanflow"
    ],
    "compute_loss": [
      "self",
      "x1",
      "mask",
      "mu",
      "spks",
      "cond"
    ]
  },
  "CausalConditionalCFM": {
    "__init__": [
      "self",
      "in_channels",
      "cfm_params",
      "n_spks",
      "spk_emb_dim",
      "estimator"
    ],
    "forward": [
      "self",
      "mu",
      "mask",
      "n_timesteps",
      "temperature",
      "spks",
      "cond",
      "noised_mels",
      "meanflow"
    ],
    "basic_euler": [
      "self",
      "x",
      "t_span",
      "mu",
      "mask",
      "spks",
      "cond"
    ]
  },
  "_repeat_batch_dim": [
    "tnsr",
    "B",
    "ndim"
  ],
  "CausalMaskedDiffWithXvec": {
    "__init__": [
      "self",
      "input_size",
      "output_size",
      "spk_embed_dim",
      "output_type",
      "vocab_size",
      "input_frame_rate",
      "only_mask_loss",
      "token_mel_ratio",
      "pre_lookahead_len",
      "encoder",
      "decoder",
      "decoder_conf",
      "mel_feat_conf"
    ],
    "compute_loss": [
      "self",
      "batch",
      "device"
    ],
    "inference": [
      "self",
      "token",
      "token_len",
      "prompt_token",
      "prompt_token_len",
      "prompt_feat",
      "prompt_feat_len",
      "embedding",
      "finalize",
      "n_timesteps",
      "noised_mels",
      "meanflow"
    ]
  },
  "get_resampler": [
    "src_sr",
    "dst_sr",
    "device"
  ],
  "S3Token2Mel": {
    "__init__": [
      "self",
      "meanflow"
    ],
    "device": [
      "self"
    ],
    "dtype": [
      "self"
    ],
    "embed_ref": [
      "self",
      "ref_wav",
      "ref_sr",
      "device",
      "ref_fade_out"
    ],
    "forward": [
      "self",
      "speech_tokens",
      "ref_wav",
      "ref_sr",
      "ref_dict",
      "n_cfm_timesteps",
      "finalize",
      "speech_token_lens",
      "noised_mels"
    ]
  },
  "S3Token2Wav": {
    "ignore_state_dict_missing": [],
    "__init__": [
      "self",
      "meanflow"
    ],
    "forward": [
      "self",
      "speech_tokens",
      "ref_wav",
      "ref_sr",
      "ref_dict",
      "finalize",
      "speech_token_lens",
      "skip_vocoder",
      "n_cfm_timesteps",
      "noised_mels"
    ],
    "flow_inference": [
      "self",
      "speech_tokens",
      "ref_wav",
      "ref_sr",
      "ref_dict",
      "n_cfm_timesteps",
      "finalize",
      "speech_token_lens"
    ],
    "hift_inference": [
      "self",
      "speech_feat",
      "cache_source"
    ],
    "inference": [
      "self",
      "speech_tokens",
      "ref_wav",
      "ref_sr",
      "ref_dict",
      "drop_invalid_tokens",
      "n_cfm_timesteps",
      "speech_token_lens"
    ]
  },
  "get_intmeanflow_time_mixer": [
    "dims"
  ],
  "subsequent_chunk_mask": [
    "size",
    "chunk_size",
    "num_left_chunks",
    "device"
  ],
  "add_optional_chunk_mask": [
    "xs",
    "masks",
    "use_dynamic_chunk",
    "use_dynamic_left_chunk",
    "decoding_chunk_size",
    "static_chunk_size",
    "num_decoding_left_chunks",
    "enable_full_context"
  ],
  "make_pad_mask": [
    "lengths",
    "max_len"
  ],
  "COSYVOICE_ACTIVATION_CLASSES": [],
  "COSYVOICE_SUBSAMPLE_CLASSES": [],
  "COSYVOICE_EMB_CLASSES": [],
  "COSYVOICE_ATTENTION_CLASSES": [],
  "hann_window": [],
  "dynamic_range_compression_torch": [
    "x",
    "C",
    "clip_val"
  ],
  "spectral_normalize_torch": [
    "magnitudes"
  ],
  "mel_spectrogram": [
    "y",
    "n_fft",
    "num_mels",
    "sampling_rate",
    "hop_size",
    "win_size",
    "fmin",
    "fmax",
    "center"
  ],
  "SinusoidalPosEmb": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "x",
      "scale"
    ]
  },
  "Block1D": {
    "__init__": [
      "self",
      "dim",
      "dim_out",
      "groups"
    ],
    "forward": [
      "self",
      "x",
      "mask"
    ]
  },
  "ResnetBlock1D": {
    "__init__": [
      "self",
      "dim",
      "dim_out",
      "time_emb_dim",
      "groups"
    ],
    "forward": [
      "self",
      "x",
      "mask",
      "time_emb"
    ]
  },
  "Downsample1D": {
    "__init__": [
      "self",
      "dim"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "TimestepEmbedding": {
    "__init__": [
      "self",
      "in_channels",
      "time_embed_dim",
      "act_fn",
      "out_dim",
      "post_act_fn",
      "cond_proj_dim"
    ],
    "forward": [
      "self",
      "sample",
      "condition"
    ]
  },
  "Upsample1D": {
    "__init__": [
      "self",
      "channels",
      "use_conv",
      "use_conv_transpose",
      "out_channels",
      "name"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "ConformerWrapper": {
    "__init__": [
      "self"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "timestep"
    ]
  },
  "Decoder": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "channels",
      "dropout",
      "attention_head_dim",
      "n_blocks",
      "num_mid_blocks",
      "num_heads",
      "act_fn",
      "down_block_type",
      "mid_block_type",
      "up_block_type"
    ],
    "get_block": [
      "block_type",
      "dim",
      "attention_head_dim",
      "num_heads",
      "dropout",
      "act_fn"
    ],
    "initialize_weights": [
      "self"
    ],
    "forward": [
      "self",
      "x",
      "mask",
      "mu",
      "t",
      "spks",
      "cond"
    ]
  },
  "SnakeBeta": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "alpha",
      "alpha_trainable",
      "alpha_logscale"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FeedForward": {
    "__init__": [
      "self",
      "dim",
      "dim_out",
      "mult",
      "dropout",
      "activation_fn",
      "final_dropout"
    ],
    "forward": [
      "self",
      "hidden_states"
    ]
  },
  "BasicTransformerBlock": {
    "__init__": [
      "self",
      "dim",
      "num_attention_heads",
      "attention_head_dim",
      "dropout",
      "cross_attention_dim",
      "activation_fn",
      "num_embeds_ada_norm",
      "attention_bias",
      "only_cross_attention",
      "double_self_attention",
      "upcast_attention",
      "norm_elementwise_affine",
      "norm_type",
      "final_dropout"
    ],
    "set_chunk_feed_forward": [
      "self",
      "chunk_size",
      "dim"
    ],
    "forward": [
      "self",
      "hidden_states",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "timestep",
      "cross_attention_kwargs",
      "class_labels"
    ]
  },
  "sequence_mask": [
    "length",
    "max_length"
  ],
  "LayerNorm": {
    "__init__": [
      "self",
      "channels",
      "eps"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "ConvReluNorm": {
    "__init__": [
      "self",
      "in_channels",
      "hidden_channels",
      "out_channels",
      "kernel_size",
      "n_layers",
      "p_dropout"
    ],
    "forward": [
      "self",
      "x",
      "x_mask"
    ]
  },
  "DurationPredictor": {
    "__init__": [
      "self",
      "in_channels",
      "filter_channels",
      "kernel_size",
      "p_dropout"
    ],
    "forward": [
      "self",
      "x",
      "x_mask"
    ]
  },
  "RotaryPositionalEmbeddings": {
    "__init__": [
      "self",
      "d",
      "base"
    ],
    "_build_cache": [
      "self",
      "x"
    ],
    "_neg_half": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MultiHeadAttention": {
    "__init__": [
      "self",
      "channels",
      "out_channels",
      "n_heads",
      "heads_share",
      "p_dropout",
      "proximal_bias",
      "proximal_init"
    ],
    "forward": [
      "self",
      "x",
      "c",
      "attn_mask"
    ],
    "attention": [
      "self",
      "query",
      "key",
      "value",
      "mask"
    ],
    "_attention_bias_proximal": [
      "length"
    ]
  },
  "FFN": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "filter_channels",
      "kernel_size",
      "p_dropout"
    ],
    "forward": [
      "self",
      "x",
      "x_mask"
    ]
  },
  "Encoder": {
    "__init__": [
      "self",
      "hidden_channels",
      "filter_channels",
      "n_heads",
      "n_layers",
      "kernel_size",
      "p_dropout"
    ],
    "forward": [
      "self",
      "x",
      "x_mask"
    ]
  },
  "TextEncoder": {
    "__init__": [
      "self",
      "encoder_type",
      "encoder_params",
      "duration_predictor_params",
      "n_vocab",
      "n_spks",
      "spk_emb_dim"
    ],
    "forward": [
      "self",
      "x",
      "x_lengths",
      "spks"
    ]
  },
  "BASECFM": {
    "__init__": [
      "self",
      "n_feats",
      "cfm_params",
      "n_spks",
      "spk_emb_dim"
    ],
    "forward": [
      "self",
      "mu",
      "mask",
      "n_timesteps",
      "temperature",
      "spks",
      "cond"
    ],
    "solve_euler": [
      "self",
      "x",
      "t_span",
      "mu",
      "mask",
      "spks",
      "cond"
    ],
    "compute_loss": [
      "self",
      "x1",
      "mask",
      "mu",
      "spks",
      "cond"
    ]
  },
  "CFM": {
    "__init__": [
      "self",
      "in_channels",
      "out_channel",
      "cfm_params",
      "decoder_params",
      "n_spks",
      "spk_emb_dim"
    ]
  },
  "BaseSubsampling": {
    "__init__": [
      "self"
    ],
    "position_encoding": [
      "self",
      "offset",
      "size"
    ]
  },
  "EmbedinigNoSubsampling": {
    "__init__": [
      "self",
      "idim",
      "odim",
      "dropout_rate",
      "pos_enc_class"
    ],
    "forward": [
      "self",
      "x",
      "x_mask",
      "offset"
    ]
  },
  "LinearNoSubsampling": {
    "__init__": [
      "self",
      "idim",
      "odim",
      "dropout_rate",
      "pos_enc_class"
    ],
    "forward": [
      "self",
      "x",
      "x_mask",
      "offset"
    ]
  },
  "Conv1dSubsampling2": {
    "__init__": [
      "self",
      "idim",
      "odim",
      "dropout_rate",
      "pos_enc_class"
    ],
    "forward": [
      "self",
      "x",
      "x_mask",
      "offset"
    ]
  },
  "Conv2dSubsampling4": {
    "__init__": [
      "self",
      "idim",
      "odim",
      "dropout_rate",
      "pos_enc_class"
    ],
    "forward": [
      "self",
      "x",
      "x_mask",
      "offset"
    ]
  },
  "Conv2dSubsampling6": {
    "__init__": [
      "self",
      "idim",
      "odim",
      "dropout_rate",
      "pos_enc_class"
    ],
    "forward": [
      "self",
      "x",
      "x_mask",
      "offset"
    ]
  },
  "Conv2dSubsampling8": {
    "__init__": [
      "self",
      "idim",
      "odim",
      "dropout_rate",
      "pos_enc_class"
    ],
    "forward": [
      "self",
      "x",
      "x_mask",
      "offset"
    ]
  },
  "LegacyLinearNoSubsampling": {
    "__init__": [
      "self",
      "idim",
      "odim",
      "dropout_rate",
      "pos_enc_class"
    ],
    "forward": [
      "self",
      "x",
      "x_mask",
      "offset"
    ]
  },
  "PositionalEncoding": {
    "__init__": [
      "self",
      "d_model",
      "dropout_rate",
      "max_len",
      "reverse"
    ],
    "forward": [
      "self",
      "x",
      "offset"
    ],
    "position_encoding": [
      "self",
      "offset",
      "size",
      "apply_dropout"
    ]
  },
  "RelPositionalEncoding": {
    "__init__": [
      "self",
      "d_model",
      "dropout_rate",
      "max_len"
    ],
    "forward": [
      "self",
      "x",
      "offset"
    ]
  },
  "WhisperPositionalEncoding": {
    "__init__": [
      "self",
      "d_model",
      "dropout_rate",
      "max_len"
    ]
  },
  "LearnablePositionalEncoding": {
    "__init__": [
      "self",
      "d_model",
      "dropout_rate",
      "max_len"
    ]
  },
  "NoPositionalEncoding": {
    "__init__": [
      "self",
      "d_model",
      "dropout_rate"
    ],
    "forward": [
      "self",
      "x",
      "offset"
    ],
    "position_encoding": [
      "self",
      "offset",
      "size"
    ]
  },
  "EspnetRelPositionalEncoding": {
    "__init__": [
      "self",
      "d_model",
      "dropout_rate",
      "max_len"
    ],
    "extend_pe": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "x",
      "offset"
    ],
    "position_encoding": [
      "self",
      "offset",
      "size"
    ]
  },
  "Swish": {
    "forward": [
      "self",
      "x"
    ]
  },
  "MultiHeadedAttention": {
    "__init__": [
      "self",
      "n_head",
      "n_feat",
      "dropout_rate",
      "key_bias"
    ],
    "forward_qkv": [
      "self",
      "query",
      "key",
      "value"
    ],
    "forward_attention": [
      "self",
      "value",
      "scores",
      "mask"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "mask",
      "pos_emb",
      "cache"
    ]
  },
  "RelPositionMultiHeadedAttention": {
    "__init__": [
      "self",
      "n_head",
      "n_feat",
      "dropout_rate",
      "key_bias"
    ],
    "rel_shift": [
      "self",
      "x"
    ],
    "forward": [
      "self",
      "query",
      "key",
      "value",
      "mask",
      "pos_emb",
      "cache"
    ]
  },
  "PreLookaheadLayer": {
    "__init__": [
      "self",
      "channels",
      "pre_lookahead_len"
    ],
    "forward": [
      "self",
      "inputs"
    ]
  },
  "UpsampleConformerEncoder": {
    "__init__": [
      "self",
      "input_size",
      "output_size",
      "attention_heads",
      "linear_units",
      "num_blocks",
      "dropout_rate",
      "positional_dropout_rate",
      "attention_dropout_rate",
      "input_layer",
      "pos_enc_layer_type",
      "normalize_before",
      "static_chunk_size",
      "use_dynamic_chunk",
      "global_cmvn",
      "use_dynamic_left_chunk",
      "positionwise_conv_kernel_size",
      "macaron_style",
      "selfattention_layer_type",
      "activation_type",
      "use_cnn_module",
      "cnn_module_kernel",
      "causal",
      "cnn_module_norm",
      "key_bias",
      "gradient_checkpointing"
    ],
    "output_size": [
      "self"
    ],
    "forward": [
      "self",
      "xs",
      "xs_lens",
      "decoding_chunk_size",
      "num_decoding_left_chunks"
    ],
    "forward_layers": [
      "self",
      "xs",
      "chunk_masks",
      "pos_emb",
      "mask_pad"
    ],
    "forward_up_layers": [
      "self",
      "xs",
      "chunk_masks",
      "pos_emb",
      "mask_pad"
    ]
  },
  "TransformerEncoderLayer": {
    "__init__": [
      "self",
      "size",
      "self_attn",
      "feed_forward",
      "dropout_rate",
      "normalize_before"
    ],
    "forward": [
      "self",
      "x",
      "mask",
      "pos_emb",
      "mask_pad",
      "att_cache",
      "cnn_cache"
    ]
  },
  "ConformerEncoderLayer": {
    "__init__": [
      "self",
      "size",
      "self_attn",
      "feed_forward",
      "feed_forward_macaron",
      "conv_module",
      "dropout_rate",
      "normalize_before"
    ],
    "forward": [
      "self",
      "x",
      "mask",
      "pos_emb",
      "mask_pad",
      "att_cache",
      "cnn_cache"
    ]
  },
  "ConvolutionModule": {
    "__init__": [
      "self",
      "channels",
      "kernel_size",
      "activation",
      "norm",
      "causal",
      "bias"
    ],
    "forward": [
      "self",
      "x",
      "mask_pad",
      "cache"
    ]
  },
  "PositionwiseFeedForward": {
    "__init__": [
      "self",
      "idim",
      "hidden_units",
      "dropout_rate",
      "activation"
    ],
    "forward": [
      "self",
      "xs"
    ]
  },
  "MoEFFNLayer": {
    "__init__": [
      "self",
      "n_expert",
      "n_expert_per_token",
      "idim",
      "hidden_units",
      "dropout_rate",
      "activation"
    ],
    "forward": [
      "self",
      "xs"
    ]
  }
}