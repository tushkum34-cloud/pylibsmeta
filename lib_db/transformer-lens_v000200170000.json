{
  "HookedTransformerConfig": {
    "__post_init__": [
      "self"
    ],
    "unwrap": [
      "cls",
      "config"
    ],
    "from_dict": [
      "cls",
      "config_dict"
    ],
    "to_dict": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "set_seed_everywhere": [
      "self",
      "seed"
    ],
    "is_layer_norm_activation": [
      "self"
    ]
  },
  "OUTPUT_EMBEDDING": [],
  "VECTOR_TYPES": [],
  "SVDInterpreter": {
    "__init__": [
      "self",
      "model"
    ],
    "get_singular_vectors": [
      "self",
      "vector_type",
      "layer_index",
      "num_vectors",
      "head_index"
    ],
    "_get_singular_vectors_from_matrix": [
      "self",
      "V",
      "embedding",
      "num_vectors"
    ],
    "_get_OV_matrix": [
      "self",
      "layer_index",
      "head_index"
    ],
    "_get_w_in_matrix": [
      "self",
      "layer_index"
    ],
    "_get_w_out_matrix": [
      "self",
      "layer_index"
    ]
  },
  "SingleLoss": [],
  "LossPerToken": [],
  "Loss": [],
  "DTYPE_FROM_STRING": [],
  "T": [],
  "Output": {},
  "HookedTransformer": {
    "__init__": [
      "self",
      "cfg",
      "tokenizer",
      "move_to_device",
      "default_padding_side"
    ],
    "check_hooks_to_add": [
      "self",
      "hook_point",
      "hook_point_name",
      "hook",
      "dir",
      "is_permanent",
      "prepend"
    ],
    "get_pos_offset": [
      "self",
      "past_kv_cache",
      "batch_size"
    ],
    "get_residual": [
      "self",
      "embed",
      "pos_offset",
      "prepend_bos",
      "attention_mask",
      "tokens",
      "return_shortformer_pos_embed",
      "device"
    ],
    "input_to_embed": [
      "self",
      "input",
      "prepend_bos",
      "padding_side",
      "attention_mask",
      "past_kv_cache"
    ],
    "forward": [
      "self",
      "input",
      "return_type",
      "loss_per_token",
      "prepend_bos",
      "padding_side",
      "start_at_layer",
      "tokens",
      "shortformer_pos_embed",
      "attention_mask",
      "stop_at_layer",
      "past_kv_cache"
    ],
    "loss_fn": [
      "self",
      "logits",
      "tokens",
      "attention_mask",
      "per_token"
    ],
    "run_with_cache": [
      "self"
    ],
    "set_tokenizer": [
      "self",
      "tokenizer",
      "default_padding_side"
    ],
    "to_tokens": [
      "self",
      "input",
      "prepend_bos",
      "padding_side",
      "move_to_device",
      "truncate"
    ],
    "to_string": [
      "self",
      "tokens"
    ],
    "to_str_tokens": [
      "self",
      "input",
      "prepend_bos",
      "padding_side"
    ],
    "to_single_token": [
      "self",
      "string"
    ],
    "to_single_str_token": [
      "self",
      "int_token"
    ],
    "get_token_position": [
      "self",
      "single_token",
      "input",
      "mode",
      "prepend_bos",
      "padding_side"
    ],
    "tokens_to_residual_directions": [
      "self",
      "tokens"
    ],
    "to": [
      "self",
      "device_or_dtype",
      "print_details"
    ],
    "cuda": [
      "self",
      "device"
    ],
    "cpu": [
      "self"
    ],
    "mps": [
      "self"
    ],
    "move_model_modules_to_device": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "model_name",
      "fold_ln",
      "center_writing_weights",
      "center_unembed",
      "refactor_factored_attn_matrices",
      "checkpoint_index",
      "checkpoint_value",
      "hf_model",
      "device",
      "n_devices",
      "tokenizer",
      "move_to_device",
      "fold_value_biases",
      "default_prepend_bos",
      "default_padding_side",
      "dtype",
      "first_n_layers"
    ],
    "from_pretrained_no_processing": [
      "cls",
      "model_name",
      "fold_ln",
      "center_writing_weights",
      "center_unembed",
      "refactor_factored_attn_matrices",
      "fold_value_biases",
      "dtype",
      "default_prepend_bos",
      "default_padding_side"
    ],
    "init_weights": [
      "self"
    ],
    "_init_weights_gpt2": [
      "self"
    ],
    "_init_weights_xavier": [
      "self",
      "dist_type"
    ],
    "_init_weights_kaiming": [
      "self",
      "dist_type"
    ],
    "_init_weights_muP": [
      "self",
      "dist_type"
    ],
    "load_and_process_state_dict": [
      "self",
      "state_dict",
      "fold_ln",
      "center_writing_weights",
      "center_unembed",
      "fold_value_biases",
      "refactor_factored_attn_matrices"
    ],
    "fill_missing_keys": [
      "self",
      "state_dict"
    ],
    "fold_layer_norm": [
      "self",
      "state_dict",
      "fold_biases",
      "center_weights"
    ],
    "center_writing_weights": [
      "self",
      "state_dict"
    ],
    "center_unembed": [
      "self",
      "state_dict"
    ],
    "fold_value_biases": [
      "self",
      "state_dict"
    ],
    "refactor_factored_attn_matrices": [
      "self",
      "state_dict"
    ],
    "set_use_attn_result": [
      "self",
      "use_attn_result"
    ],
    "set_use_split_qkv_input": [
      "self",
      "use_split_qkv_input"
    ],
    "set_use_hook_mlp_in": [
      "self",
      "use_hook_mlp_in"
    ],
    "set_use_attn_in": [
      "self",
      "use_attn_in"
    ],
    "set_ungroup_grouped_query_attention": [
      "self",
      "ungroup_grouped_query_attention"
    ],
    "process_weights_": [
      "self",
      "fold_ln",
      "center_writing_weights",
      "center_unembed",
      "refactor_factored_attn_matrices"
    ],
    "generate": [
      "self",
      "input",
      "max_new_tokens",
      "stop_at_eos",
      "eos_token_id",
      "do_sample",
      "top_k",
      "top_p",
      "temperature",
      "freq_penalty",
      "use_past_kv_cache",
      "prepend_bos",
      "padding_side",
      "return_type",
      "verbose"
    ],
    "W_U": [
      "self"
    ],
    "b_U": [
      "self"
    ],
    "W_E": [
      "self"
    ],
    "W_pos": [
      "self"
    ],
    "W_E_pos": [
      "self"
    ],
    "W_K": [
      "self"
    ],
    "W_Q": [
      "self"
    ],
    "W_V": [
      "self"
    ],
    "W_O": [
      "self"
    ],
    "W_in": [
      "self"
    ],
    "W_gate": [
      "self"
    ],
    "W_out": [
      "self"
    ],
    "b_K": [
      "self"
    ],
    "b_Q": [
      "self"
    ],
    "b_V": [
      "self"
    ],
    "b_O": [
      "self"
    ],
    "b_in": [
      "self"
    ],
    "b_out": [
      "self"
    ],
    "QK": [
      "self"
    ],
    "OV": [
      "self"
    ],
    "accumulated_bias": [
      "self",
      "layer",
      "mlp_input",
      "include_mlp_biases"
    ],
    "all_composition_scores": [
      "self",
      "mode"
    ],
    "all_head_labels": [
      "self"
    ],
    "load_sample_training_dataset": [
      "self"
    ],
    "sample_datapoint": [
      "self",
      "tokenize",
      "prepend_bos",
      "padding_side"
    ]
  },
  "HeadName": [],
  "HEAD_NAMES": [],
  "ErrorMeasure": [],
  "LayerHeadTuple": [],
  "LayerToHead": [],
  "INVALID_HEAD_NAME_ERR": [],
  "SEQ_LEN_ERR": [],
  "DET_PAT_NOT_SQUARE_ERR": [],
  "detect_head": [
    "model",
    "seq",
    "detection_pattern",
    "heads",
    "cache"
  ],
  "get_previous_token_head_detection_pattern": [
    "tokens"
  ],
  "get_duplicate_token_head_detection_pattern": [
    "tokens"
  ],
  "get_induction_head_detection_pattern": [
    "tokens"
  ],
  "get_supported_heads": [],
  "compute_head_attention_similarity_score": [
    "attention_pattern",
    "detection_pattern"
  ],
  "CACHE_DIR": [],
  "USE_DEFAULT_VALUE": [],
  "is_library_available": [
    "name"
  ],
  "select_compatible_kwargs": [
    "kwargs_dict",
    "callable"
  ],
  "download_file_from_hf": [
    "repo_name",
    "file_name",
    "subfolder",
    "cache_dir",
    "force_is_torch"
  ],
  "clear_huggingface_cache": [],
  "print_gpu_mem": [
    "step_name"
  ],
  "get_corner": [
    "tensor",
    "n"
  ],
  "to_numpy": [
    "tensor"
  ],
  "lm_cross_entropy_loss": [
    "logits",
    "tokens",
    "attention_mask",
    "per_token"
  ],
  "lm_accuracy": [
    "logits",
    "tokens",
    "per_token"
  ],
  "gelu_new": [
    "input"
  ],
  "gelu_fast": [
    "input"
  ],
  "gelu_pytorch_tanh": [
    "input"
  ],
  "solu": [
    "input"
  ],
  "ACTIVATION_FN_DICT": [],
  "calc_fan_in_and_fan_out": [
    "tensor"
  ],
  "init_xavier_uniform_": [
    "param",
    "gain"
  ],
  "init_xavier_normal_": [
    "param",
    "gain"
  ],
  "init_kaiming_uniform_": [
    "param",
    "a",
    "nonlinearity",
    "gain",
    "mode"
  ],
  "init_kaiming_normal_": [
    "param",
    "a",
    "nonlinearity",
    "gain",
    "mode"
  ],
  "keep_single_column": [
    "dataset",
    "col_name"
  ],
  "tokenize_and_concatenate": [
    "dataset",
    "tokenizer",
    "streaming",
    "max_length",
    "column_name",
    "add_bos_token",
    "num_proc"
  ],
  "sample_logits": [
    "final_logits",
    "top_k",
    "top_p",
    "temperature",
    "freq_penalty",
    "tokens"
  ],
  "SliceInput": [],
  "Slice": {
    "__init__": [
      "self",
      "input_slice"
    ],
    "apply": [
      "self",
      "tensor",
      "dim"
    ],
    "indices": [
      "self",
      "max_ctx"
    ],
    "__repr__": [
      "self"
    ],
    "unwrap": [
      "cls",
      "slice_input"
    ]
  },
  "get_act_name": [
    "name",
    "layer",
    "layer_type"
  ],
  "remove_batch_dim": [
    "tensor"
  ],
  "test_prompt": [
    "prompt",
    "answer",
    "model",
    "prepend_space_to_answer",
    "print_details",
    "prepend_bos",
    "top_k"
  ],
  "transpose": [
    "tensor"
  ],
  "composition_scores": [
    "left",
    "right",
    "broadcast_dims"
  ],
  "get_dataset": [
    "dataset_name"
  ],
  "is_square": [
    "x"
  ],
  "is_lower_triangular": [
    "x"
  ],
  "check_structure": [
    "t1",
    "t2"
  ],
  "get_device": [],
  "override_or_use_default_value": [
    "default_flag",
    "override"
  ],
  "get_offset_position_ids": [
    "past_kv_pos_offset",
    "attention_mask"
  ],
  "get_cumsum_along_dim": [
    "tensor",
    "dim",
    "reverse"
  ],
  "get_attention_mask": [
    "tokenizer",
    "tokens",
    "prepend_bos"
  ],
  "repeat_along_head_dimension": [
    "tensor",
    "n_heads",
    "clone_tensor"
  ],
  "get_nested_attr": [
    "obj",
    "attr_str"
  ],
  "set_nested_attr": [
    "obj",
    "attr_str",
    "value"
  ],
  "LocallyOverridenDefaults": {
    "__init__": [
      "self",
      "model"
    ],
    "__enter__": [
      "self"
    ],
    "__exit__": [
      "self",
      "exc_type",
      "exc_val",
      "exc_tb"
    ]
  },
  "get_tokenizer_with_bos": [
    "tokenizer"
  ],
  "get_input_with_manually_prepended_bos": [
    "tokenizer",
    "input"
  ],
  "get_tokens_with_bos_removed": [
    "tokenizer",
    "tokens"
  ],
  "sanity_check": [
    "model"
  ],
  "make_wiki_data_loader": [
    "tokenizer",
    "batch_size"
  ],
  "make_owt_data_loader": [
    "tokenizer",
    "batch_size"
  ],
  "make_pile_data_loader": [
    "tokenizer",
    "batch_size"
  ],
  "make_code_data_loader": [
    "tokenizer",
    "batch_size"
  ],
  "DATASET_NAMES": [],
  "DATASET_LOADERS": [],
  "evaluate_on_dataset": [
    "model",
    "data_loader",
    "truncate",
    "device"
  ],
  "induction_loss": [
    "model",
    "tokenizer",
    "batch_size",
    "subseq_len",
    "prepend_bos",
    "device"
  ],
  "evaluate": [
    "model",
    "truncate",
    "batch_size",
    "tokenizer"
  ],
  "IOIDataset": {
    "__init__": [
      "self",
      "tokenizer",
      "templates",
      "names",
      "nouns",
      "num_samples",
      "symmetric",
      "prepend_bos"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "get_sample": [
      "self",
      "symmetric"
    ],
    "get_default_names": [],
    "get_default_templates": [],
    "get_default_nouns": []
  },
  "ioi_eval": [
    "model",
    "dataset",
    "batch_size",
    "num_samples",
    "tokenizer",
    "symmetric"
  ],
  "LensHandle": {},
  "NamesFilter": [],
  "_HookFunctionProtocol": {
    "__call__": [
      "self",
      "tensor"
    ]
  },
  "HookFunction": [],
  "DeviceType": [],
  "_grad_t": [],
  "HookPoint": {
    "__init__": [
      "self"
    ],
    "add_perma_hook": [
      "self",
      "hook",
      "dir"
    ],
    "add_hook": [
      "self",
      "hook",
      "dir",
      "is_permanent",
      "level",
      "prepend"
    ],
    "remove_hooks": [
      "self",
      "dir",
      "including_permanent",
      "level"
    ],
    "clear_context": [
      "self"
    ],
    "forward": [
      "self",
      "x"
    ],
    "layer": [
      "self"
    ]
  },
  "HookedRootModule": {
    "__init__": [
      "self"
    ],
    "setup": [
      "self"
    ],
    "hook_points": [
      "self"
    ],
    "remove_all_hook_fns": [
      "self",
      "direction",
      "including_permanent",
      "level"
    ],
    "clear_contexts": [
      "self"
    ],
    "reset_hooks": [
      "self",
      "clear_contexts",
      "direction",
      "including_permanent",
      "level"
    ],
    "check_and_add_hook": [
      "self",
      "hook_point",
      "hook_point_name",
      "hook",
      "dir",
      "is_permanent",
      "level",
      "prepend"
    ],
    "check_hooks_to_add": [
      "self",
      "hook_point",
      "hook_point_name",
      "hook",
      "dir",
      "is_permanent",
      "prepend"
    ],
    "add_hook": [
      "self",
      "name",
      "hook",
      "dir",
      "is_permanent",
      "level",
      "prepend"
    ],
    "add_perma_hook": [
      "self",
      "name",
      "hook",
      "dir"
    ],
    "_enable_hook_with_name": [
      "self",
      "name",
      "hook",
      "dir"
    ],
    "_enable_hooks_for_points": [
      "self",
      "hook_points",
      "enabled",
      "hook",
      "dir"
    ],
    "_enable_hook": [
      "self",
      "name",
      "hook",
      "dir"
    ],
    "hooks": [
      "self",
      "fwd_hooks",
      "bwd_hooks",
      "reset_hooks_end",
      "clear_contexts"
    ],
    "run_with_hooks": [
      "self"
    ],
    "add_caching_hooks": [
      "self",
      "names_filter",
      "incl_bwd",
      "device",
      "remove_batch_dim",
      "cache"
    ],
    "run_with_cache": [
      "self"
    ],
    "get_caching_hooks": [
      "self",
      "names_filter",
      "incl_bwd",
      "device",
      "remove_batch_dim",
      "cache",
      "pos_slice"
    ],
    "cache_all": [
      "self",
      "cache",
      "incl_bwd",
      "device",
      "remove_batch_dim"
    ],
    "cache_some": [
      "self",
      "cache",
      "names",
      "incl_bwd",
      "device",
      "remove_batch_dim"
    ]
  },
  "HookedTransformerKeyValueCacheEntry": {
    "init_cache_entry": [
      "cls",
      "cfg",
      "device",
      "batch_size"
    ],
    "append": [
      "self",
      "new_keys",
      "new_values"
    ]
  },
  "HookedTransformerKeyValueCache": {
    "init_cache": [
      "cls",
      "cfg",
      "device",
      "batch_size"
    ],
    "freeze": [
      "self"
    ],
    "unfreeze": [
      "self"
    ],
    "append_attention_mask": [
      "self",
      "attention_mask"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "OFFICIAL_MODEL_NAMES": [],
  "MODEL_ALIASES": [],
  "NON_HF_HOSTED_MODEL_NAMES": [],
  "DEFAULT_MODEL_ALIASES": [],
  "NEED_REMOTE_CODE_MODELS": [],
  "make_model_alias_map": [],
  "get_official_model_name": [
    "model_name"
  ],
  "convert_hf_model_config": [
    "model_name"
  ],
  "convert_neel_model_config": [
    "official_model_name"
  ],
  "get_pretrained_model_config": [
    "model_name",
    "hf_cfg",
    "checkpoint_index",
    "checkpoint_value",
    "fold_ln",
    "device",
    "n_devices",
    "default_prepend_bos",
    "dtype",
    "first_n_layers",
    "n_ctx"
  ],
  "get_num_params_of_pretrained": [
    "model_name"
  ],
  "STANFORD_CRFM_CHECKPOINTS": [],
  "PYTHIA_CHECKPOINTS": [],
  "PYTHIA_V0_CHECKPOINTS": [],
  "get_checkpoint_labels": [
    "model_name"
  ],
  "get_pretrained_state_dict": [
    "official_model_name",
    "cfg",
    "hf_model",
    "dtype"
  ],
  "fill_missing_keys": [
    "model",
    "state_dict"
  ],
  "Config": {},
  "get_basic_config": [
    "model_name"
  ],
  "HookedTransformerTrainConfig": {},
  "train": [
    "model",
    "config",
    "dataset"
  ],
  "Logits": [],
  "AxisNames": [],
  "make_df_from_ranges": [
    "column_max_ranges",
    "column_names"
  ],
  "CorruptedActivation": [],
  "PatchedActivation": [],
  "generic_activation_patch": [
    "model",
    "corrupted_tokens",
    "clean_cache",
    "patching_metric",
    "patch_setter",
    "activation_name",
    "index_axis_names",
    "index_df",
    "return_index_df"
  ],
  "layer_pos_patch_setter": [
    "corrupted_activation",
    "index",
    "clean_activation"
  ],
  "layer_pos_head_vector_patch_setter": [
    "corrupted_activation",
    "index",
    "clean_activation"
  ],
  "layer_head_vector_patch_setter": [
    "corrupted_activation",
    "index",
    "clean_activation"
  ],
  "layer_head_pattern_patch_setter": [
    "corrupted_activation",
    "index",
    "clean_activation"
  ],
  "layer_head_pos_pattern_patch_setter": [
    "corrupted_activation",
    "index",
    "clean_activation"
  ],
  "layer_head_dest_src_pos_pattern_patch_setter": [
    "corrupted_activation",
    "index",
    "clean_activation"
  ],
  "get_act_patch_resid_pre": [],
  "get_act_patch_resid_mid": [],
  "get_act_patch_attn_out": [],
  "get_act_patch_mlp_out": [],
  "get_act_patch_attn_head_out_by_pos": [],
  "get_act_patch_attn_head_q_by_pos": [],
  "get_act_patch_attn_head_k_by_pos": [],
  "get_act_patch_attn_head_v_by_pos": [],
  "get_act_patch_attn_head_pattern_by_pos": [],
  "get_act_patch_attn_head_pattern_dest_src_pos": [],
  "get_act_patch_attn_head_out_all_pos": [],
  "get_act_patch_attn_head_q_all_pos": [],
  "get_act_patch_attn_head_k_all_pos": [],
  "get_act_patch_attn_head_v_all_pos": [],
  "get_act_patch_attn_head_pattern_all_pos": [],
  "get_act_patch_attn_head_all_pos_every": [
    "model",
    "corrupted_tokens",
    "clean_cache",
    "metric"
  ],
  "get_act_patch_attn_head_by_pos_every": [
    "model",
    "corrupted_tokens",
    "clean_cache",
    "metric"
  ],
  "get_act_patch_block_every": [
    "model",
    "corrupted_tokens",
    "clean_cache",
    "metric"
  ],
  "FactoredMatrix": {
    "__init__": [
      "self",
      "A",
      "B"
    ],
    "__matmul__": [
      "self",
      "other"
    ],
    "__rmatmul__": [
      "self",
      "other"
    ],
    "__mul__": [
      "self",
      "scalar"
    ],
    "__rmul__": [
      "self",
      "scalar"
    ],
    "AB": [
      "self"
    ],
    "BA": [
      "self"
    ],
    "T": [
      "self"
    ],
    "svd": [
      "self"
    ],
    "U": [
      "self"
    ],
    "S": [
      "self"
    ],
    "Vh": [
      "self"
    ],
    "eigenvalues": [
      "self"
    ],
    "_convert_to_slice": [
      "self",
      "sequence",
      "idx"
    ],
    "__getitem__": [
      "self",
      "idx"
    ],
    "norm": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "make_even": [
      "self"
    ],
    "get_corner": [
      "self",
      "k"
    ],
    "ndim": [
      "self"
    ],
    "collapse_l": [
      "self"
    ],
    "collapse_r": [
      "self"
    ],
    "unsqueeze": [
      "self",
      "k"
    ],
    "pair": [
      "self"
    ]
  },
  "HookedEncoder": {
    "__init__": [
      "self",
      "cfg",
      "tokenizer",
      "move_to_device"
    ],
    "to_tokens": [
      "self",
      "input",
      "move_to_device",
      "truncate"
    ],
    "encoder_output": [
      "self",
      "tokens",
      "token_type_ids",
      "one_zero_attention_mask"
    ],
    "forward": [
      "self",
      "input",
      "return_type",
      "token_type_ids",
      "one_zero_attention_mask"
    ],
    "run_with_cache": [
      "self"
    ],
    "to": [
      "self",
      "device_or_dtype",
      "print_details"
    ],
    "cuda": [
      "self",
      "device"
    ],
    "cpu": [
      "self"
    ],
    "mps": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "model_name",
      "checkpoint_index",
      "checkpoint_value",
      "hf_model",
      "device",
      "tokenizer",
      "move_to_device",
      "dtype"
    ],
    "W_U": [
      "self"
    ],
    "b_U": [
      "self"
    ],
    "W_E": [
      "self"
    ],
    "W_pos": [
      "self"
    ],
    "W_E_pos": [
      "self"
    ],
    "W_K": [
      "self"
    ],
    "W_Q": [
      "self"
    ],
    "W_V": [
      "self"
    ],
    "W_O": [
      "self"
    ],
    "W_in": [
      "self"
    ],
    "W_out": [
      "self"
    ],
    "b_K": [
      "self"
    ],
    "b_Q": [
      "self"
    ],
    "b_V": [
      "self"
    ],
    "b_O": [
      "self"
    ],
    "b_in": [
      "self"
    ],
    "b_out": [
      "self"
    ],
    "QK": [
      "self"
    ],
    "OV": [
      "self"
    ],
    "all_head_labels": [
      "self"
    ]
  },
  "HookedEncoderDecoder": {
    "__init__": [
      "self",
      "cfg",
      "tokenizer",
      "move_to_device"
    ],
    "to_tokens": [
      "self",
      "input",
      "move_to_device",
      "truncate"
    ],
    "forward": [
      "self",
      "input",
      "decoder_input",
      "return_type",
      "one_zero_attention_mask"
    ],
    "generate": [
      "self",
      "input",
      "one_zero_attention_mask",
      "max_new_tokens",
      "stop_at_eos",
      "eos_token_id",
      "do_sample",
      "top_k",
      "top_p",
      "temperature",
      "freq_penalty",
      "return_type",
      "verbose"
    ],
    "run_with_cache": [
      "self"
    ],
    "to": [
      "self"
    ],
    "cuda": [
      "self",
      "device"
    ],
    "cpu": [
      "self"
    ],
    "mps": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "model_name",
      "checkpoint_index",
      "checkpoint_value",
      "hf_model",
      "device",
      "tokenizer",
      "move_to_device",
      "dtype"
    ],
    "W_U": [
      "self"
    ],
    "b_U": [
      "self"
    ],
    "W_E": [
      "self"
    ],
    "W_pos": [
      "self"
    ],
    "W_K": [
      "self"
    ],
    "W_Q": [
      "self"
    ],
    "W_V": [
      "self"
    ],
    "W_O": [
      "self"
    ],
    "W_in": [
      "self"
    ],
    "W_out": [
      "self"
    ],
    "b_K": [
      "self"
    ],
    "b_Q": [
      "self"
    ],
    "b_V": [
      "self"
    ],
    "b_O": [
      "self"
    ],
    "b_in": [
      "self"
    ],
    "b_out": [
      "self"
    ],
    "QK": [
      "self"
    ],
    "OV": [
      "self"
    ],
    "all_head_labels": [
      "self"
    ]
  },
  "ActivationCache": {
    "__init__": [
      "self",
      "cache_dict",
      "model",
      "has_batch_dim"
    ],
    "remove_batch_dim": [
      "self"
    ],
    "__repr__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "key"
    ],
    "__len__": [
      "self"
    ],
    "to": [
      "self",
      "device",
      "move_model"
    ],
    "toggle_autodiff": [
      "self",
      "mode"
    ],
    "keys": [
      "self"
    ],
    "values": [
      "self"
    ],
    "items": [
      "self"
    ],
    "__iter__": [
      "self"
    ],
    "apply_slice_to_batch_dim": [
      "self",
      "batch_slice"
    ],
    "accumulated_resid": [
      "self",
      "layer",
      "incl_mid",
      "apply_ln",
      "pos_slice",
      "mlp_input",
      "return_labels"
    ],
    "logit_attrs": [
      "self",
      "residual_stack",
      "tokens",
      "incorrect_tokens",
      "pos_slice",
      "batch_slice",
      "has_batch_dim"
    ],
    "decompose_resid": [
      "self",
      "layer",
      "mlp_input",
      "mode",
      "apply_ln",
      "pos_slice",
      "incl_embeds",
      "return_labels"
    ],
    "compute_head_results": [
      "self"
    ],
    "stack_head_results": [
      "self",
      "layer",
      "return_labels",
      "incl_remainder",
      "pos_slice",
      "apply_ln"
    ],
    "stack_activation": [
      "self",
      "activation_name",
      "layer",
      "sublayer_type"
    ],
    "get_neuron_results": [
      "self",
      "layer",
      "neuron_slice",
      "pos_slice"
    ],
    "stack_neuron_results": [
      "self",
      "layer",
      "pos_slice",
      "neuron_slice",
      "return_labels",
      "incl_remainder",
      "apply_ln"
    ],
    "apply_ln_to_stack": [
      "self",
      "residual_stack",
      "layer",
      "mlp_input",
      "pos_slice",
      "batch_slice",
      "has_batch_dim"
    ],
    "get_full_resid_decomposition": [
      "self",
      "layer",
      "mlp_input",
      "expand_neurons",
      "apply_ln",
      "pos_slice",
      "return_labels"
    ]
  },
  "BertNextSentencePrediction": {
    "__init__": [
      "self",
      "model"
    ],
    "__call__": [
      "self",
      "input",
      "return_type",
      "token_type_ids",
      "one_zero_attention_mask"
    ],
    "to_tokens": [
      "self",
      "input",
      "move_to_device",
      "truncate"
    ],
    "forward": [
      "self",
      "input",
      "return_type",
      "token_type_ids",
      "one_zero_attention_mask"
    ],
    "run_with_cache": [
      "self"
    ]
  },
  "TokenTypeEmbed": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "token_type_ids"
    ]
  },
  "AbstractAttention": {
    "__init__": [
      "self",
      "cfg",
      "attn_type",
      "layer_id"
    ],
    "OV": [
      "self"
    ],
    "QK": [
      "self"
    ],
    "forward": [
      "self",
      "query_input",
      "key_input",
      "value_input",
      "past_kv_cache_entry",
      "additive_attention_mask",
      "attention_mask",
      "position_bias"
    ],
    "_apply_qk_norm": [
      "self",
      "x",
      "norm_module"
    ],
    "calculate_qkv_matrices": [
      "self",
      "query_input",
      "key_input",
      "value_input"
    ],
    "calculate_attention_scores": [
      "self",
      "q",
      "k"
    ],
    "calculate_z_scores": [
      "self",
      "v",
      "pattern"
    ],
    "apply_causal_mask": [
      "self",
      "attn_scores",
      "past_kv_pos_offset",
      "attention_mask"
    ],
    "calculate_sin_cos_rotary": [
      "self",
      "rotary_dim",
      "n_ctx",
      "base",
      "dtype"
    ],
    "rotate_every_two": [
      "self",
      "x"
    ],
    "apply_rotary": [
      "self",
      "x",
      "past_kv_pos_offset",
      "attention_mask"
    ],
    "_extend_rotary_embeddings": [
      "self",
      "new_size"
    ],
    "_extend_mask": [
      "self",
      "new_size"
    ],
    "create_alibi_slope": [
      "n_ctx",
      "device"
    ],
    "create_alibi_multipliers": [
      "n_heads",
      "device"
    ],
    "create_alibi_bias": [
      "n_heads",
      "n_ctx",
      "device"
    ]
  },
  "GroupedQueryAttention": {
    "__init__": [
      "self",
      "cfg",
      "attn_type",
      "layer_id"
    ],
    "W_K": [
      "self",
      "value"
    ],
    "W_V": [
      "self",
      "value"
    ],
    "b_K": [
      "self",
      "value"
    ],
    "b_V": [
      "self",
      "value"
    ],
    "calculate_qkv_matrices": [
      "self",
      "query_input",
      "key_input",
      "value_input"
    ],
    "calculate_attention_scores": [
      "self",
      "q",
      "k"
    ],
    "calculate_z_scores": [
      "self",
      "v",
      "pattern"
    ],
    "_apply_qk_norm": [
      "self",
      "x",
      "norm_module"
    ]
  },
  "BertPooler": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "resid"
    ]
  },
  "BertEmbed": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "input_ids",
      "token_type_ids"
    ]
  },
  "T5Attention": {
    "__init__": [
      "self",
      "cfg",
      "has_relative_attention_bias",
      "attn_type",
      "layer_id"
    ],
    "_relative_position_bucket": [
      "relative_position",
      "bidirectional",
      "num_buckets",
      "max_distance"
    ],
    "compute_relative_attention_bias": [
      "self",
      "query_length",
      "key_length",
      "device"
    ]
  },
  "PosEmbed": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "tokens",
      "past_kv_pos_offset",
      "attention_mask"
    ]
  },
  "BertBlock": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "resid_pre",
      "additive_attention_mask"
    ]
  },
  "T5Block": {
    "__init__": [
      "self",
      "cfg",
      "block_index",
      "is_decoder"
    ],
    "forward": [
      "self",
      "resid_pre",
      "additive_attention_mask",
      "encoder_additive_attention_mask",
      "position_bias",
      "encoder_hidden_states",
      "past_kv_cache_entry"
    ]
  },
  "LayerNorm": {
    "__init__": [
      "self",
      "cfg",
      "length"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Attention": {
    "__init__": [
      "self",
      "cfg",
      "attn_type",
      "layer_id"
    ]
  },
  "BertMLMHead": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "resid"
    ]
  },
  "RMSNorm": {
    "__init__": [
      "self",
      "cfg",
      "length"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Embed": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "tokens"
    ]
  },
  "LayerNormPre": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "Unembed": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "residual"
    ]
  },
  "RMSNormPre": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "BertNSPHead": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "resid"
    ]
  },
  "TransformerBlock": {
    "__init__": [
      "self",
      "cfg",
      "block_index"
    ],
    "forward": [
      "self",
      "resid_pre",
      "shortformer_pos_embed",
      "past_kv_cache_entry",
      "attention_mask"
    ],
    "apply_mlp": [
      "self",
      "normalized_resid"
    ]
  },
  "GatedMLP4Bit": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "CanBeUsedAsMLP": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ],
    "select_activation_function": [
      "self"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "GatedMLP": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MoEGatedMLP": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "MoE": {
    "__init__": [
      "self",
      "cfg"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "simple_attn_linear": [
    "input",
    "w",
    "b"
  ],
  "complex_attn_linear": [
    "input",
    "w",
    "b"
  ],
  "vanilla_addmm": [
    "input",
    "mat1",
    "mat2"
  ],
  "batch_addmm": [
    "bias",
    "weight",
    "x"
  ],
  "AvailableDeviceMemory": [],
  "calculate_available_device_cuda_memory": [
    "i"
  ],
  "determine_available_memory_for_available_devices": [
    "max_devices"
  ],
  "sort_devices_based_on_available_memory": [
    "devices"
  ],
  "get_best_available_cuda_device": [
    "max_devices"
  ],
  "get_best_available_device": [
    "cfg"
  ],
  "get_device_for_block_index": [
    "index",
    "cfg",
    "device"
  ],
  "move_to_and_update_config": [
    "model",
    "device_or_dtype",
    "print_details"
  ],
  "ActivationFunction": [],
  "convert_qwen2_weights": [
    "qwen",
    "cfg"
  ],
  "convert_mistral_weights": [
    "mistral",
    "cfg"
  ],
  "convert_gemma_weights": [
    "gemma",
    "cfg"
  ],
  "convert_opt_weights": [
    "opt",
    "cfg"
  ],
  "convert_gptj_weights": [
    "gptj",
    "cfg"
  ],
  "convert_phi_weights": [
    "phi",
    "cfg"
  ],
  "convert_mixtral_weights": [
    "mixtral",
    "cfg"
  ],
  "convert_bert_weights": [
    "bert",
    "cfg"
  ],
  "convert_mingpt_weights": [
    "old_state_dict",
    "cfg"
  ],
  "convert_gpt2_weights": [
    "gpt2",
    "cfg"
  ],
  "convert_qwen_weights": [
    "qwen",
    "cfg"
  ],
  "convert_nanogpt_weights": [
    "old_state_dict",
    "cfg"
  ],
  "convert_phi3_weights": [
    "phi",
    "cfg"
  ],
  "convert_llama_weights": [
    "llama",
    "cfg"
  ],
  "convert_neel_solu_old_weights": [
    "state_dict",
    "cfg"
  ],
  "convert_bloom_weights": [
    "bloom",
    "cfg"
  ],
  "convert_qwen3_weights": [
    "qwen",
    "cfg"
  ],
  "convert_neo_weights": [
    "neo",
    "cfg"
  ],
  "convert_neox_weights": [
    "neox",
    "cfg"
  ],
  "convert_coder_weights": [
    "model",
    "cfg"
  ],
  "convert_t5_weights": [
    "t5",
    "cfg"
  ],
  "MLPFactory": {
    "create_mlp": [
      "cfg"
    ]
  },
  "ActivationFunctionFactory": {
    "pick_activation_function": [
      "cfg"
    ]
  }
}