{
  "__path__": [],
  "FluentLexer": {
    "name": [],
    "aliases": [],
    "filenames": [],
    "get_tokens_unprocessed": [
      "self",
      "text"
    ]
  },
  "ATOMIC": [],
  "Tokenizer": {
    "__init__": [
      "self",
      "text"
    ],
    "tokenize": [
      "self",
      "node"
    ],
    "tokenize_node": [
      "self",
      "node"
    ],
    "generic_tokenize": [
      "self",
      "node"
    ],
    "tokenize_Variant": [
      "self",
      "node"
    ],
    "_token": [
      "self",
      "node",
      "token"
    ]
  },
  "main": []
}