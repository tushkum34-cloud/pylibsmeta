{
  "ProductKeyMemory": [],
  "exists": [
    "val"
  ],
  "default": [
    "val",
    "d"
  ],
  "eval_decorator": [
    "fn"
  ],
  "top_k": [
    "logits",
    "thres"
  ],
  "Attention": {
    "__init__": [
      "self",
      "dim",
      "dim_head",
      "heads"
    ],
    "forward": [
      "self",
      "x"
    ]
  },
  "FeedForward": [
    "dim",
    "mult"
  ],
  "Transformer": {
    "__init__": [
      "self"
    ],
    "generate": [
      "self",
      "prompt",
      "seq_len",
      "temperature",
      "filter_thres"
    ],
    "forward": [
      "self",
      "x",
      "return_loss"
    ]
  },
  "log": [
    "t",
    "eps"
  ],
  "gumbel_noise": [
    "t"
  ],
  "init_": [
    "t",
    "dim"
  ],
  "list_subtract": [
    "l",
    "r"
  ],
  "fetch_pkm_value_parameters": [
    "module"
  ],
  "fetch_optimizer_parameters": [
    "module",
    "pkm_learning_rate"
  ],
  "MaskedBatchNorm1D": {
    "__init__": [
      "self",
      "fn"
    ],
    "forward": [
      "self",
      "x",
      "mask"
    ]
  },
  "PKM": {
    "__init__": [
      "self",
      "dim",
      "heads",
      "num_keys",
      "topk",
      "dim_head",
      "input_dropout",
      "query_dropout",
      "value_dropout",
      "attn_dropout",
      "use_layernorm",
      "pre_layernorm",
      "differentiable_topk",
      "concat_values_and_combine",
      "norm_output",
      "non_competitive_gates"
    ],
    "forward": [
      "self",
      "x",
      "input_mask",
      "mem_trainable_mask",
      "gumbel_noise_scale"
    ]
  }
}