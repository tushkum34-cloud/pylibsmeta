{
  "__version__": [],
  "REPLACEMENT_CHAR": [],
  "_remove_space": [
    "x"
  ],
  "StreamingDetokenizer": {
    "__slots__": [],
    "reset": [
      "self"
    ],
    "add_token": [
      "self",
      "token",
      "skip_special_token_ids"
    ],
    "finalize": [
      "self"
    ],
    "last_segment": [
      "self"
    ]
  },
  "NaiveStreamingDetokenizer": {
    "__init__": [
      "self",
      "tokenizer"
    ],
    "reset": [
      "self"
    ],
    "add_token": [
      "self",
      "token",
      "skip_special_token_ids"
    ],
    "finalize": [
      "self"
    ],
    "text": [
      "self"
    ],
    "tokens": [
      "self"
    ]
  },
  "SPMStreamingDetokenizer": {
    "__init__": [
      "self",
      "tokenizer",
      "trim_space"
    ],
    "reset": [
      "self"
    ],
    "_flush_bytes": [
      "self"
    ],
    "add_token": [
      "self",
      "token",
      "skip_special_token_ids"
    ],
    "finalize": [
      "self"
    ]
  },
  "BPEStreamingDetokenizer": {
    "_byte_decoder": [],
    "__init__": [
      "self",
      "tokenizer",
      "trim_space"
    ],
    "reset": [
      "self"
    ],
    "add_token": [
      "self",
      "token",
      "skip_special_token_ids"
    ],
    "finalize": [
      "self"
    ],
    "make_byte_decoder": [
      "cls"
    ]
  },
  "TokenizerWrapper": {
    "__init__": [
      "self",
      "tokenizer",
      "detokenizer_class"
    ],
    "__getattr__": [
      "self",
      "attr"
    ]
  },
  "_match": [
    "a",
    "b"
  ],
  "_is_spm_decoder": [
    "decoder"
  ],
  "_is_spm_decoder_no_space": [
    "decoder"
  ],
  "_is_bpe_decoder": [
    "decoder"
  ],
  "load_tokenizer": [
    "model_path",
    "return_tokenizer",
    "tokenizer_config_extra"
  ],
  "DEFAULT_MODEL_PATH": [],
  "DEFAULT_IMAGE": [],
  "DEFAULT_AUDIO": [],
  "DEFAULT_PROMPT": [],
  "DEFAULT_MAX_TOKENS": [],
  "DEFAULT_TEMPERATURE": [],
  "DEFAULT_TOP_P": [],
  "DEFAULT_SEED": [],
  "DEFAULT_QUANTIZED_KV_START": [],
  "parse_arguments": [],
  "generation_stream": [],
  "wired_limit": [
    "model",
    "streams"
  ],
  "GenerationResult": {},
  "generate_step": [
    "input_ids",
    "model",
    "pixel_values",
    "mask"
  ],
  "stream_generate": [
    "model",
    "processor",
    "prompt",
    "image",
    "audio"
  ],
  "generate": [
    "model",
    "processor",
    "prompt",
    "image",
    "audio",
    "verbose"
  ],
  "BatchGenerationResult": {},
  "_left_pad_prompts": [
    "prompts",
    "max_length"
  ],
  "_make_cache": [
    "model",
    "left_padding"
  ],
  "BatchStats": {},
  "BatchResponse": {},
  "Batch": {
    "__len__": [
      "self"
    ],
    "filter": [
      "self",
      "keep_idx"
    ],
    "extend": [
      "self",
      "other"
    ]
  },
  "BatchGenerator": {
    "__init__": [
      "self",
      "model",
      "processor",
      "max_tokens",
      "stop_tokens",
      "sampler",
      "completion_batch_size",
      "prefill_batch_size",
      "prefill_step_size",
      "prompt_cache"
    ],
    "insert": [
      "self",
      "prompts",
      "max_tokens"
    ],
    "_process_prompts": [
      "self",
      "prompts"
    ],
    "_step": [
      "self",
      "input_tokens",
      "prompt_cache"
    ],
    "stats": [
      "self"
    ],
    "_next": [
      "self"
    ],
    "next": [
      "self"
    ]
  },
  "batch_generate": [
    "model",
    "processor",
    "images",
    "audios",
    "prompts",
    "max_tokens",
    "verbose",
    "group_by_shape",
    "track_image_sizes"
  ],
  "_generate_batch": [
    "model",
    "processor",
    "prompts",
    "images",
    "max_tokens",
    "verbose"
  ],
  "main": [],
  "logger": [],
  "custom_print": [],
  "ModelState": {
    "__init__": [
      "self"
    ],
    "load": [
      "self",
      "model_name"
    ]
  },
  "state": [],
  "args": [],
  "chatbot_height": [],
  "stop_generation": [],
  "get_cached_vlm_models": [],
  "load_model_by_name": [
    "model_name",
    "progress"
  ],
  "refresh_model_list": [],
  "extract_image_from_message": [
    "message"
  ],
  "extract_text_from_message": [
    "message"
  ],
  "chat": [
    "message",
    "history",
    "temperature",
    "max_tokens",
    "top_p",
    "repetition_penalty",
    "system_prompt"
  ],
  "stop_generating": [],
  "theme": [],
  "initial_models": [],
  "dark_mode_js": [],
  "toggle_dark_js": [],
  "save_model_js": [],
  "load_model_js": [],
  "deprecate": [
    "remove_version",
    "message",
    "instead",
    "since"
  ],
  "top_p_sampling": [
    "logits",
    "top_p",
    "temperature"
  ],
  "ACTIVATION_QUANTIZATION_MODES": [],
  "MODEL_REMAPPING": [],
  "MAX_FILE_SIZE_GB": [],
  "MODEL_CONVERSION_DTYPES": [],
  "quantize_activations": [
    "model"
  ],
  "skip_multimodal_module": [
    "path"
  ],
  "get_model_and_args": [
    "config"
  ],
  "get_model_path": [
    "path_or_hf_repo",
    "revision",
    "force_download"
  ],
  "load_model": [
    "model_path",
    "lazy"
  ],
  "sanitize_weights": [
    "model_obj",
    "weights",
    "config"
  ],
  "update_module_configs": [
    "model_config",
    "model_class",
    "config",
    "modules"
  ],
  "load": [
    "path_or_hf_repo",
    "adapter_path",
    "lazy",
    "revision"
  ],
  "load_config": [
    "model_path"
  ],
  "load_image_processor": [
    "model_path"
  ],
  "load_processor": [
    "model_path",
    "add_detokenizer",
    "eos_token_ids"
  ],
  "fetch_from_hub": [
    "model_path",
    "lazy"
  ],
  "make_shards": [
    "weights",
    "max_file_size_gb"
  ],
  "upload_to_hub": [
    "path",
    "upload_repo",
    "hf_path"
  ],
  "apply_repetition_penalty": [
    "logits",
    "generated_tokens",
    "penalty"
  ],
  "save_weights": [
    "save_path",
    "model"
  ],
  "save_config": [
    "config",
    "config_path"
  ],
  "load_image": [
    "image_source",
    "timeout"
  ],
  "resize_image": [
    "img",
    "max_size"
  ],
  "process_image": [
    "img",
    "resize_shape",
    "image_processor"
  ],
  "resample_audio": [
    "audio",
    "orig_sr",
    "target_sr"
  ],
  "load_audio": [
    "file",
    "sr",
    "timeout"
  ],
  "normalize_audio_features": [
    "features"
  ],
  "process_inputs": [
    "processor",
    "prompts",
    "images",
    "audio",
    "add_special_tokens",
    "padding",
    "padding_side",
    "return_tensors"
  ],
  "process_inputs_with_fallback": [
    "processor",
    "prompts",
    "images",
    "audio",
    "add_special_tokens",
    "return_tensors"
  ],
  "prepare_inputs": [
    "processor",
    "images",
    "audio",
    "prompts",
    "image_token_index",
    "resize_shape",
    "add_special_tokens",
    "padding",
    "padding_side",
    "pad_to_uniform_size"
  ],
  "group_images_by_shape": [
    "images",
    "disable_grouping"
  ],
  "StoppingCriteria": {
    "__init__": [
      "self",
      "eos_token_ids",
      "tokenizer"
    ],
    "add_eos_token_ids": [
      "self",
      "new_eos_token_ids"
    ],
    "reset": [
      "self",
      "eos_token_ids"
    ],
    "__call__": [
      "self",
      "input_ids"
    ]
  },
  "print_array_report": [
    "t",
    "label"
  ],
  "QUANT_RECIPES": [],
  "mixed_quant_predicate_builder": [
    "recipe",
    "model"
  ],
  "convert": [
    "hf_path",
    "mlx_path",
    "quantize",
    "q_group_size",
    "q_bits",
    "q_mode",
    "dtype",
    "upload_repo",
    "revision",
    "dequantize",
    "trust_remote_code",
    "quant_predicate"
  ],
  "configure_parser": [],
  "MessageFormat": {
    "LIST_WITH_IMAGE": [],
    "LIST_WITH_IMAGE_FIRST": [],
    "LIST_WITH_IMAGE_URL_FIRST": [],
    "LIST_WITH_IMAGE_TYPE": [],
    "LIST_WITH_IMAGE_TYPE_TEXT": [],
    "LIST_WITH_IMAGE_TYPE_TEXT_IMAGE_LAST": [],
    "IMAGE_TOKEN": [],
    "IMAGE_TOKEN_PIPE": [],
    "START_IMAGE_TOKEN": [],
    "IMAGE_TOKEN_NEWLINE": [],
    "NUMBERED_IMAGE_TOKENS": [],
    "PROMPT_ONLY": [],
    "PROMPT_WITH_IMAGE_TOKEN": [],
    "PROMPT_WITH_START_IMAGE_TOKEN": [],
    "VIDEO_WITH_TEXT": []
  },
  "MODEL_CONFIG": [],
  "SINGLE_IMAGE_ONLY_MODELS": [],
  "extract_text_from_content": [
    "content"
  ],
  "MessageBuilder": {
    "text_message": [
      "text"
    ],
    "content_message": [
      "content"
    ],
    "image_message": [],
    "image_url_message": [],
    "audio_message": [],
    "video_message": [
      "video_path",
      "max_pixels",
      "fps"
    ]
  },
  "MessageFormatter": {
    "__init__": [
      "self",
      "model_name"
    ],
    "format_message": [
      "self",
      "prompt",
      "role",
      "skip_image_token",
      "skip_audio_token",
      "num_images",
      "num_audios"
    ],
    "_format_list_with_image": [
      "self",
      "prompt",
      "role",
      "skip_image_token",
      "skip_audio_token",
      "num_images",
      "num_audios",
      "image_first",
      "use_image_url"
    ],
    "_format_list_with_image_type": [
      "self",
      "prompt",
      "role",
      "skip_image_token",
      "skip_audio_token",
      "num_images",
      "num_audios",
      "message_type",
      "image_first"
    ],
    "_format_with_token": [
      "self",
      "prompt",
      "role",
      "skip_image_token",
      "skip_audio_token",
      "num_images",
      "num_audios",
      "token",
      "image_first"
    ],
    "_format_numbered_tokens": [
      "self",
      "prompt",
      "role",
      "skip_image_token",
      "skip_audio_token",
      "num_images",
      "num_audios"
    ],
    "_format_video_message": [
      "self",
      "prompt",
      "role",
      "skip_image_token",
      "skip_audio_token",
      "num_images",
      "num_audios"
    ]
  },
  "get_message_json": [
    "model_name",
    "prompt",
    "role",
    "skip_image_token",
    "skip_audio_token",
    "num_images",
    "num_audios"
  ],
  "get_chat_template": [
    "processor",
    "messages",
    "add_generation_prompt",
    "tokenize"
  ],
  "apply_chat_template": [
    "processor",
    "config",
    "prompt",
    "add_generation_prompt",
    "return_messages",
    "num_images",
    "num_audios"
  ],
  "app": [],
  "MAX_IMAGES": [],
  "model_cache": [],
  "FlexibleBaseModel": {
    "model_config": []
  },
  "load_model_resources": [
    "model_path",
    "adapter_path"
  ],
  "get_cached_model": [
    "model_path",
    "adapter_path"
  ],
  "unload_model_sync": [],
  "ResponseInputTextParam": {},
  "ResponseInputImageParam": {},
  "InputAudio": {},
  "ResponseInputAudioParam": {},
  "ImageUrl": {},
  "ResponseImageUrlParam": {},
  "ResponseOutputText": {},
  "ChatMessage": {},
  "OpenAIRequest": {},
  "OpenAIUsage": {},
  "OpenAIErrorObject": {},
  "OpenAIResponse": {},
  "BaseStreamEvent": {},
  "ContentPartOutputText": {},
  "MessageItem": {},
  "ResponseCreatedEvent": {},
  "ResponseInProgressEvent": {},
  "ResponseOutputItemAddedEvent": {},
  "ResponseContentPartAddedEvent": {},
  "ResponseOutputTextDeltaEvent": {},
  "ResponseOutputTextDoneEvent": {},
  "ResponseContentPartDoneEvent": {},
  "ResponseOutputItemDoneEvent": {},
  "ResponseCompletedEvent": {},
  "StreamEvent": [],
  "VLMRequest": {},
  "GenerationRequest": {},
  "UsageStats": {},
  "ChatRequest": {},
  "ChatChoice": {},
  "ChatResponse": {},
  "ChatStreamChoice": {},
  "ChatStreamChunk": {},
  "ModelInfo": {},
  "ModelsResponse": {},
  "responses_endpoint": [
    "request"
  ],
  "chat_completions_endpoint": [
    "request"
  ],
  "models_endpoint": [],
  "health_check": [],
  "unload_model_endpoint": [],
  "IMAGE_FACTOR": [],
  "MIN_PIXELS": [],
  "MAX_PIXELS": [],
  "MAX_RATIO": [],
  "VIDEO_MIN_PIXELS": [],
  "VIDEO_MAX_PIXELS": [],
  "FRAME_FACTOR": [],
  "FPS": [],
  "FPS_MIN_FRAMES": [],
  "FPS_MAX_FRAMES": [],
  "VIDEO_TOTAL_PIXELS": [],
  "round_by_factor": [
    "number",
    "factor"
  ],
  "ceil_by_factor": [
    "number",
    "factor"
  ],
  "floor_by_factor": [
    "number",
    "factor"
  ],
  "smart_resize": [
    "height",
    "width",
    "factor",
    "min_pixels",
    "max_pixels"
  ],
  "to_rgb": [
    "pil_image"
  ],
  "fetch_image": [
    "ele",
    "size_factor"
  ],
  "smart_nframes": [
    "ele",
    "total_frames",
    "video_fps"
  ],
  "load_video": [
    "ele"
  ],
  "fetch_video": [
    "ele",
    "image_factor",
    "return_video_sample_fps"
  ],
  "extract_vision_info": [
    "conversations"
  ],
  "process_vision_info": [
    "conversations",
    "return_video_kwargs"
  ],
  "VideoFrameExtractor": {
    "__init__": [
      "self",
      "max_frames"
    ],
    "resize_and_center_crop": [
      "self",
      "image",
      "target_size"
    ],
    "extract_frames": [
      "self",
      "video_path"
    ]
  },
  "is_video_model": [
    "model"
  ],
  "is_video_file": [
    "video_path"
  ],
  "MLXVisionChat": {
    "__init__": [
      "self",
      "model_path",
      "temperature",
      "max_tokens",
      "verbose"
    ],
    "print_help": [
      "self"
    ],
    "process_image": [
      "self",
      "image_path"
    ],
    "add_to_history": [
      "self",
      "role",
      "text"
    ],
    "generate_response": [
      "self"
    ],
    "handle_command": [
      "self",
      "command",
      "args"
    ],
    "chat_loop": [
      "self"
    ]
  },
  "process_question": [
    "sample"
  ],
  "normalize_answer": [
    "response",
    "problem"
  ],
  "evaluate_answer": [
    "prediction",
    "ground_truth"
  ],
  "parse_args": [],
  "inference": [
    "model",
    "processor",
    "question",
    "image",
    "max_tokens",
    "temperature",
    "resize_shape",
    "verbose"
  ],
  "OCRBench_val": [
    "results_list",
    "args",
    "model_name",
    "dataset"
  ],
  "create_sampler": [
    "temperature",
    "top_p"
  ],
  "process_batch": [
    "model",
    "processor",
    "batch_samples",
    "args"
  ],
  "extract_answer": [
    "predict",
    "answer"
  ],
  "MMStar_eval": [
    "data",
    "eval_file"
  ],
  "MMMU_SUBJECTS": [],
  "MMMU_PRO_SUBJECTS": [],
  "normalize_number": [
    "s"
  ],
  "MMMU_eval": [
    "data",
    "eval_file"
  ],
  "get_images": [
    "example"
  ],
  "list_subjects": [],
  "LoRaLayer": {
    "__init__": [
      "self",
      "linear",
      "rank",
      "alpha",
      "dropout"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "replace_lora_with_linear": [
    "model"
  ],
  "get_prompt": [
    "model_type",
    "processor",
    "conversation"
  ],
  "Dataset": {
    "__init__": [
      "self",
      "hf_dataset",
      "config",
      "processor",
      "image_processor",
      "take",
      "split",
      "image_resize_shape"
    ],
    "__len__": [
      "self"
    ],
    "__getitem__": [
      "self",
      "idx"
    ]
  },
  "grad_checkpoint": [
    "layer"
  ],
  "TrainingArgs": {},
  "default_loss": [
    "model",
    "inputs",
    "targets",
    "lengths"
  ],
  "Trainer": {
    "__init__": [
      "self",
      "model",
      "optimizer",
      "train_on_completions",
      "assistant_id",
      "clip_gradients"
    ],
    "loss_fn": [
      "self",
      "model",
      "batch"
    ],
    "train_step": [
      "self",
      "batch"
    ],
    "train_epoch": [
      "self",
      "dataloader"
    ]
  },
  "save_adapter": [
    "model",
    "adapter_file"
  ],
  "get_module_by_name": [
    "model",
    "name"
  ],
  "set_module_by_name": [
    "model",
    "name",
    "new_module"
  ],
  "get_peft_model": [
    "model",
    "linear_layers",
    "rank",
    "alpha",
    "dropout",
    "freeze",
    "verbose"
  ],
  "freeze_model": [
    "model"
  ],
  "find_all_linear_names": [
    "model"
  ],
  "count_parameters": [
    "model"
  ],
  "print_trainable_parameters": [
    "model"
  ],
  "apply_lora_layers": [
    "model",
    "adapter_path"
  ],
  "gaussian_blur_axis": [
    "image",
    "sigma",
    "axis"
  ],
  "bilinear_interpolate": [
    "image",
    "new_height",
    "new_width",
    "align_corners"
  ],
  "resize_bilinear": [
    "image",
    "new_size",
    "align_corners",
    "antialias"
  ],
  "LanguageModelOutput": {},
  "InputEmbeddingsFeatures": {
    "to_dict": [
      "self"
    ]
  },
  "BaseModelConfig": {
    "from_dict": [
      "cls",
      "params"
    ],
    "to_dict": [
      "self"
    ]
  },
  "BaseImageProcessor": {
    "__init__": [
      "self",
      "image_mean",
      "image_std",
      "size",
      "crop_size",
      "resample",
      "rescale_factor",
      "data_format"
    ],
    "rescale": [
      "self",
      "image",
      "scale",
      "input_data_format"
    ],
    "normalize": [
      "self",
      "image",
      "mean",
      "std",
      "input_data_format"
    ],
    "preprocess": [
      "self",
      "images"
    ]
  },
  "expand2square": [
    "pil_img",
    "background_color"
  ],
  "check_array_shape": [
    "arr"
  ],
  "check_activation_stats": [
    "name",
    "tensor"
  ],
  "pixel_shuffle": [
    "input_tensor",
    "shuffle_ratio"
  ],
  "interpolate": [
    "pos_embed",
    "size",
    "mode",
    "align_corners"
  ],
  "chunked_attention": [
    "queries",
    "keys",
    "values",
    "scale",
    "chunk_size"
  ],
  "install_auto_processor_patch": [
    "target_model_types",
    "processor_cls"
  ],
  "make_prompt_cache": [
    "model",
    "max_kv_size"
  ],
  "SimpleKVCache": {
    "__init__": [
      "self"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "fetch": [
      "self"
    ],
    "update": [
      "self",
      "keys",
      "values"
    ]
  },
  "SlidingWindowCache": {
    "__init__": [
      "self",
      "max_size",
      "step"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "state": [
      "self",
      "v"
    ],
    "get_max_cache_shape": [
      "self"
    ],
    "meta_state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ]
  },
  "StaticKVCache": {
    "__init__": [
      "self",
      "max_size",
      "step"
    ],
    "update_and_fetch": [
      "self",
      "keys",
      "values"
    ],
    "state": [
      "self",
      "v"
    ],
    "meta_state": [
      "self",
      "v"
    ],
    "is_trimmable": [
      "self"
    ],
    "trim": [
      "self",
      "n"
    ]
  },
  "nearest_interpolate": [
    "x",
    "size",
    "scale_factor"
  ],
  "bicubic_interpolate": [
    "x",
    "size",
    "scale_factor",
    "align_corners",
    "antialias"
  ],
  "grid_sample": [
    "x",
    "grid"
  ],
  "get_optimal_threadgroup": [
    "out_w",
    "out_h"
  ],
  "shift_tokens_right": [
    "input_ids",
    "pad_token_id",
    "decoder_start_token_id"
  ],
  "LearnedPositionEmbedding2D": {
    "__init__": [
      "self",
      "embedding_dim",
      "num_pos"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "PositionalEmbeddingCosine1D": {
    "__init__": [
      "self",
      "embed_dim",
      "max_seq_len"
    ],
    "__call__": [
      "self",
      "seq_embeds"
    ]
  },
  "Model": {
    "__init__": [
      "self",
      "config"
    ],
    "_encode_image": [
      "self",
      "pixel_values",
      "extract_features"
    ],
    "_merge_input_ids_with_image_features": [
      "self",
      "image_features",
      "inputs_embeds"
    ],
    "layers": [
      "self"
    ],
    "make_cache": [
      "self"
    ],
    "get_input_embeddings": [
      "self",
      "input_ids",
      "pixel_values"
    ],
    "__call__": [
      "self",
      "input_ids",
      "pixel_values",
      "cache",
      "decoder_input_ids",
      "decoder_attention_mask",
      "labels"
    ],
    "sanitize": [
      "weights"
    ]
  },
  "VisionConfig": {},
  "TextConfig": {},
  "ModelConfig": {},
  "MlpFC": {
    "__init__": [
      "self",
      "in_features",
      "hidden_features",
      "out_features"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Mlp": {
    "__init__": [
      "self",
      "in_features",
      "hidden_features",
      "out_features"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "DepthWiseConv2d": {
    "__init__": [
      "self",
      "dim_in",
      "kernel_size",
      "padding",
      "stride",
      "bias"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "ConvEmbed": {
    "__init__": [
      "self",
      "patch_size",
      "in_chans",
      "embed_dim",
      "stride",
      "padding",
      "norm_layer",
      "pre_norm"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "ChannelAttention": {
    "__init__": [
      "self",
      "dim",
      "groups",
      "qkv_bias"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "window_partition": [
    "x",
    "window_size"
  ],
  "window_reverse": [
    "windows",
    "batch_size",
    "window_size",
    "H",
    "W"
  ],
  "WindowAttention": {
    "__init__": [
      "self",
      "dim",
      "num_heads",
      "window_size",
      "qkv_bias"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "PreNorm": {
    "__init__": [
      "self",
      "norm",
      "fn",
      "drop_path"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "DropPath": {
    "__init__": [
      "self",
      "drop_prob"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "SpatialBlock": {
    "__init__": [
      "self",
      "dim",
      "num_heads",
      "window_size",
      "mlp_ratio",
      "qkv_bias",
      "drop_path_rate",
      "conv_at_attn",
      "conv_at_ffn"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "ChannelBlock": {
    "__init__": [
      "self",
      "dim",
      "groups",
      "mlp_ratio",
      "qkv_bias",
      "drop_path_rate",
      "conv_at_attn",
      "conv_at_ffn"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "Block": {
    "__init__": [
      "self",
      "dim",
      "num_heads",
      "num_groups",
      "window_size",
      "mlp_ratio",
      "qkv_bias",
      "drop_path_rate",
      "conv_at_attn",
      "conv_at_ffn"
    ],
    "__call__": [
      "self",
      "x",
      "size"
    ]
  },
  "VisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ],
    "sanitize": [
      "weights"
    ]
  },
  "_ORIGINAL_INIT": [],
  "_BOOTSTRAP_PREFIX_TOKENS": [],
  "_BOOTSTRAP_LOC_TOKENS": [],
  "_BOOTSTRAP_SUFFIX_TOKENS": [],
  "_IMAGE_PLACEHOLDER_TOKEN": [],
  "_added_token": [
    "text"
  ],
  "_ensure_florence_tokens": [
    "tokenizer"
  ],
  "_patched_init": [
    "self",
    "image_processor",
    "tokenizer"
  ],
  "Florence2Attention": {
    "__init__": [
      "self",
      "config",
      "is_decoder",
      "is_causal"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "key_value_states",
      "cache",
      "attention_mask",
      "layer_head_mask"
    ]
  },
  "Florence2EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "Florence2DecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "encoder_hidden_states",
      "attention_mask",
      "encoder_attention_mask",
      "cache"
    ]
  },
  "Florence2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "inputs_embeds",
      "attention_mask"
    ]
  },
  "Florence2Decoder": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "attention_mask",
      "encoder_hidden_states",
      "encoder_attention_mask",
      "head_mask",
      "cross_attn_head_mask",
      "inputs_embeds",
      "cache"
    ]
  },
  "Florence2LanguageModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "inputs_embeds",
      "decoder_input_ids",
      "decoder_inputs_embeds",
      "attention_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "cache"
    ]
  },
  "LanguageModel": {
    "__init__": [
      "self",
      "config"
    ],
    "_to_decoder_step_ids": [
      "inputs"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "decoder_input_ids",
      "decoder_inputs_embeds",
      "attention_mask",
      "decoder_attention_mask",
      "encoder_outputs",
      "cache"
    ],
    "layers": [
      "self"
    ],
    "head_dim": [
      "self"
    ],
    "n_kv_heads": [
      "self"
    ],
    "make_cache": [
      "self"
    ]
  },
  "Gemma3MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "masked_scatter": [
    "final_embedding",
    "image_mask_expanded",
    "scaled_image_features"
  ],
  "Attention": {
    "__init__": [
      "self",
      "dims",
      "num_heads",
      "query_input_dims",
      "key_input_dims",
      "value_input_dims",
      "value_dims",
      "value_output_dims",
      "bias"
    ],
    "__call__": [
      "self",
      "x",
      "mask"
    ]
  },
  "MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "EncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask"
    ]
  },
  "Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states",
      "mask"
    ]
  },
  "VisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "SigLipVisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states"
    ]
  },
  "RMSNorm": {
    "__init__": [
      "self",
      "dims",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "clip_residual": [
    "x",
    "y"
  ],
  "TransformerBlock": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Gemma3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "find_closest_aspect_ratio": [
    "aspect_ratio",
    "target_ratios",
    "width",
    "height",
    "image_size"
  ],
  "dynamic_preprocess": [
    "image",
    "min_num",
    "max_num",
    "image_size",
    "use_thumbnail"
  ],
  "DictOutput": {
    "keys": [
      "self"
    ],
    "__getitem__": [
      "self",
      "item"
    ],
    "__setitem__": [
      "self",
      "key",
      "value"
    ]
  },
  "VLChatProcessorOutput": {
    "__len__": [
      "self"
    ]
  },
  "BatchCollateOutput": {},
  "ImageTransform": {
    "__init__": [
      "self",
      "mean",
      "std",
      "normalize"
    ],
    "__call__": [
      "self",
      "pil_img"
    ]
  },
  "DeepseekOCR2Processor": {
    "tokenizer_class": [],
    "attributes": [],
    "__init__": [
      "self",
      "tokenizer",
      "candidate_resolutions",
      "patch_size",
      "downsample_ratio",
      "image_mean",
      "image_std",
      "normalize",
      "image_token",
      "pad_token",
      "add_special_token",
      "sft_format",
      "mask_prompt",
      "ignore_id"
    ],
    "default_chat_template": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "pad_id": [
      "self"
    ],
    "encode": [
      "self",
      "text",
      "bos",
      "eos"
    ],
    "decode": [
      "self",
      "t"
    ],
    "process_one": [
      "self",
      "prompt",
      "images",
      "inference_mode",
      "base_size",
      "image_size",
      "cropping",
      "min_patches",
      "max_patches"
    ],
    "pad_sequence": [
      "self",
      "sequences",
      "padding_value"
    ],
    "tokenize_with_images": [
      "self",
      "conversation",
      "images",
      "base_size",
      "image_size",
      "cropping",
      "min_patches",
      "max_patches"
    ],
    "__call__": [
      "self"
    ],
    "_collate_batch": [
      "self",
      "batch_results",
      "padding"
    ]
  },
  "MlpProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen2EncoderConfig": {},
  "MLPConfig": {},
  "ProjectorConfig": {},
  "SAMViTConfig": {},
  "Conversation": {},
  "Qwen2RMSNorm": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "Qwen2RotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "rotate_half": [
    "x"
  ],
  "apply_rotary_pos_emb": [
    "q",
    "k",
    "cos",
    "sin"
  ],
  "Qwen2MLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen2Attention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids"
    ]
  },
  "Qwen2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids"
    ]
  },
  "Qwen2Decoder2Encoder": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "sam_features"
    ]
  },
  "OPENAI_CLIP_MEAN": [],
  "OPENAI_CLIP_STD": [],
  "pad_to_bounding_box": [
    "image",
    "offset_height",
    "offset_width",
    "target_height",
    "target_width",
    "value"
  ],
  "normalize_image": [
    "image",
    "offset",
    "scale"
  ],
  "resize_and_pad": [
    "image",
    "desired_output_size",
    "pad_value",
    "normalize",
    "image_mean",
    "image_std"
  ],
  "select_tiling": [
    "h",
    "w",
    "patch_size",
    "max_num_patches"
  ],
  "rearrange_patches": [
    "patches",
    "dh",
    "dw",
    "h",
    "w"
  ],
  "rearrange_mask": [
    "mask",
    "dh",
    "dw",
    "h",
    "w"
  ],
  "rearrange_global": [
    "image",
    "dh",
    "dw",
    "h",
    "w"
  ],
  "MolmoImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "max_crops",
      "overlap_margins",
      "base_image_input_size",
      "image_token_length_w",
      "image_token_length_h",
      "image_patch_size",
      "image_padding_mask",
      "do_normalize",
      "image_mean",
      "image_std"
    ],
    "image_to_patches_and_tokens": [
      "self",
      "image",
      "image_patch_token_id",
      "image_col_token_id",
      "image_start_token_id",
      "image_end_token_id"
    ],
    "build_image_input_idx": [
      "self",
      "image_tokens",
      "patch_order",
      "image_patch_token_id"
    ],
    "preprocess": [
      "self",
      "image",
      "image_patch_token_id",
      "image_col_token_id",
      "image_start_token_id",
      "image_end_token_id"
    ]
  },
  "MolmoProcessor": {
    "attributes": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template",
      "add_generation_prompt",
      "tokenize"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "_original_auto_processor_from_pretrained": [],
  "_patched_auto_processor_from_pretrained": [
    "cls",
    "pretrained_model_name_or_path"
  ],
  "ViTMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MultiHeadDotProductAttention": {
    "__init__": [
      "self",
      "config",
      "is_vit_layer"
    ],
    "_split_heads": [
      "self",
      "hidden_states",
      "num_heads"
    ],
    "_merge_heads": [
      "self",
      "hidden_states"
    ],
    "__call__": [
      "self",
      "x",
      "kv"
    ]
  },
  "ResidualAttentionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ResidualAttentionBlocks": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "_expand_token": [
    "token",
    "batch_size"
  ],
  "pad_to_multiple": [
    "x",
    "target_size",
    "pad_mode",
    "pad_value"
  ],
  "VisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "add_pos_emb": [
      "self",
      "x",
      "patch_num"
    ],
    "__call__": [
      "self",
      "x",
      "patch_num"
    ]
  },
  "SwiGLU": {
    "__call__": [
      "self",
      "x"
    ]
  },
  "MolmoBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "Embedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "num_new_embeddings",
      "features",
      "initializer_range",
      "new_embed_initializer_range"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Molmo": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "select_best_resolution": [
    "image_size",
    "candidate_resolutions"
  ],
  "DeepseekVLV2Processor": {
    "tokenizer_class": [],
    "attributes": [],
    "__init__": [
      "self",
      "tokenizer",
      "candidate_resolutions",
      "patch_size",
      "downsample_ratio",
      "image_mean",
      "image_std",
      "normalize",
      "image_token",
      "pad_token",
      "add_special_token",
      "sft_format",
      "mask_prompt",
      "ignore_id"
    ],
    "default_chat_template": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "pad_id": [
      "self"
    ],
    "encode": [
      "self",
      "text",
      "bos",
      "eos"
    ],
    "decode": [
      "self",
      "t"
    ],
    "process_one": [
      "self",
      "prompt",
      "conversations",
      "images",
      "apply_sft_format",
      "inference_mode",
      "system_prompt"
    ],
    "pad_sequence": [
      "self",
      "sequences",
      "padding_value"
    ],
    "tokenize_with_images": [
      "self",
      "conversation",
      "images",
      "bos",
      "eos",
      "cropping"
    ],
    "batchify": [
      "self",
      "sample_list",
      "padding"
    ],
    "__call__": [
      "self"
    ]
  },
  "AttentionPoolLatent": {
    "__init__": [
      "self",
      "in_features",
      "out_features",
      "embed_dim",
      "num_heads",
      "mlp_ratio",
      "qkv_bias",
      "qk_norm",
      "latent_len",
      "latent_dim",
      "pos_embed",
      "pool_type",
      "norm_layer",
      "drop"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "SeparatorStyle": {
    "DeepSeek": [],
    "DeepSeekV2": [],
    "PLAIN": [],
    "ALIGNMENT": []
  },
  "register_conv_template": [
    "template",
    "override"
  ],
  "get_conv_template": [
    "name"
  ],
  "yarn_find_correction_dim": [
    "num_rotations",
    "dim",
    "base",
    "max_position_embeddings"
  ],
  "yarn_find_correction_range": [
    "low_rot",
    "high_rot",
    "dim",
    "base",
    "max_position_embeddings"
  ],
  "yarn_get_mscale": [
    "scale",
    "mscale"
  ],
  "yarn_linear_ramp_mask": [
    "min_val",
    "max_val",
    "dim"
  ],
  "DeepseekV2YarnRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "scaling_factor",
      "original_max_position_embeddings",
      "beta_fast",
      "beta_slow",
      "mscale",
      "mscale_all_dim"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "DeepseekV2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "LlamaAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV2MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MoEGate": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV2MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV2DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV2Model": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "inputs_embeds",
      "cache"
    ]
  },
  "MllamaVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "MllamaVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "MllamaVisionEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "is_gated"
    ],
    "__call__": [
      "self",
      "hidden_state",
      "attention_mask"
    ]
  },
  "MllamaVisionEncoder": {
    "__init__": [
      "self",
      "config",
      "num_layers",
      "is_gated"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "attention_mask"
    ]
  },
  "MllamaPrecomputedAspectRatioEmbedding": {
    "__init__": [
      "self",
      "config",
      "is_gated"
    ],
    "__call__": [
      "self",
      "hidden_state",
      "aspect_ratio_ids"
    ]
  },
  "MllamaPrecomputedPositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_state",
      "aspect_ratio_ids"
    ]
  },
  "_prepare_aspect_ratio_attention_mask": [
    "aspect_ratio_mask",
    "num_patches",
    "target_length"
  ],
  "MllamaTextCrossAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cross_attention_states",
      "attention_mask",
      "cache"
    ]
  },
  "MllamaTextSelfAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "MllamaTextMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MllamaSelfAttentionDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "mask",
      "cache"
    ]
  },
  "MllamaCrossAttentionDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cross_attention_states",
      "attention_mask",
      "full_text_row_masked_out_mask",
      "cache"
    ]
  },
  "MllamaTextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "mask",
      "position_ids",
      "cross_attention_states",
      "cross_attention_mask",
      "full_text_row_masked_out_mask",
      "inputs_embeds",
      "cache"
    ]
  },
  "_validate_images_text_input_order": [
    "images",
    "text"
  ],
  "_ensure_gpt2_bytes_to_unicode": [],
  "OPENAI_DATASET_MEAN": [],
  "OPENAI_DATASET_STD": [],
  "KimiVLImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "patch_size",
      "pad_input",
      "image_mean",
      "image_std",
      "in_token_limit",
      "merge_kernel_size"
    ],
    "rescale": [
      "self",
      "image",
      "merge_kernel_size"
    ],
    "to_mlx": [
      "self",
      "image"
    ],
    "normalize": [
      "self",
      "image"
    ],
    "patchify": [
      "self",
      "image"
    ],
    "_preprocess": [
      "self",
      "image"
    ],
    "preprocess": [
      "self",
      "images",
      "return_tensors"
    ],
    "__call__": [
      "self",
      "images",
      "return_tensors"
    ]
  },
  "KimiVLProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template",
      "add_generation_prompt",
      "tokenize"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "_as_hw_shapes": [
    "grid_hws"
  ],
  "make_block_attention_mask": [
    "cu_seqlens",
    "seq_length"
  ],
  "apply_rotary_pos_emb_vision": [
    "tensor",
    "freqs"
  ],
  "VisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "__call__": [
      "self",
      "seqlen"
    ]
  },
  "Learnable2DInterpPosEmb": {
    "__init__": [
      "self",
      "height",
      "width",
      "dim",
      "interpolation_mode"
    ],
    "_get_pos_emb": [
      "self",
      "shape"
    ],
    "__call__": [
      "self",
      "x",
      "grid_hws"
    ]
  },
  "PatchEmbed": {
    "__init__": [
      "self",
      "patch_size",
      "num_channels",
      "embed_dim",
      "init_pos_emb_height"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "_apply_rope_input_validation": [
    "x",
    "freqs_cis"
  ],
  "view_as_complex": [
    "x"
  ],
  "view_as_real": [
    "x"
  ],
  "apply_rope": [
    "q",
    "k",
    "freqs_cis"
  ],
  "Qwen2VLVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb",
      "attention_mask"
    ]
  },
  "Rope2DPosEmb": {
    "__init__": [
      "self",
      "dim",
      "max_height",
      "max_width",
      "theta_base"
    ],
    "extra_repr": [
      "self"
    ],
    "_precompute_freqs_cis": [
      "self"
    ],
    "get_freqs_cis": [
      "self",
      "grid_hws"
    ]
  },
  "patch_merger": [
    "x",
    "grid_hws",
    "merge_kernel_size"
  ],
  "KimiVLMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "image_features"
    ]
  },
  "DeepseekV3YarnRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "scaling_factor",
      "original_max_position_embeddings",
      "beta_fast",
      "beta_slow",
      "mscale",
      "mscale_all_dim"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "clipped_silu": [
    "x",
    "gate"
  ],
  "DeepseekV3Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV3MLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "group_expert_select": [
    "gates",
    "e_score_correction_bias",
    "top_k",
    "n_group",
    "topk_group",
    "routed_scaling_factor",
    "norm_topk_prob"
  ],
  "DeepseekV3MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DeepseekV3DecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "DeepseekV3Model": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "inputs_embeds",
      "cache",
      "mask"
    ]
  },
  "PatchMerger": {
    "__init__": [
      "self",
      "config",
      "use_postshuffle_norm"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3VLMoEVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "Qwen3VLRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "rope_scaling"
    ],
    "apply_interleaved_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "apply_multimodal_rotary_pos_emb": [
    "q",
    "k",
    "cos",
    "sin",
    "unqueeze_dim"
  ],
  "Qwen3VLDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Qwen3VLModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache",
      "position_ids",
      "visual_pos_masks",
      "deepstack_visual_embeds"
    ],
    "_deepstack_process": [
      "self",
      "hidden_states",
      "visual_pos_masks",
      "visual_embeds"
    ]
  },
  "convert_torch_to_mlx_pad_width": [
    "padding",
    "input_shape"
  ],
  "Gemma3nAudioRelativePositionEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "_get_timing_signal_1d_pos": [
      "self",
      "position",
      "dtype"
    ],
    "_relative_shift": [
      "self",
      "term_bd_before_shift",
      "batch_size",
      "num_heads",
      "num_query_blocks",
      "query_block_size",
      "key_context_size",
      "max_span_plus_1"
    ],
    "__call__": [
      "self",
      "queries",
      "keys"
    ]
  },
  "Gemma3nAudioAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "_pad_dim1": [
      "self",
      "x",
      "dim10_val",
      "dim11_val"
    ],
    "_convert_to_block": [
      "self",
      "x",
      "padding_val"
    ],
    "unfold_mlx": [
      "self",
      "x",
      "dimension",
      "size",
      "step"
    ],
    "_extract_block_context": [
      "self",
      "x"
    ],
    "__call__": [
      "self",
      "x",
      "mask"
    ]
  },
  "Gemma3nCumulativeGroupNorm": {
    "__init__": [
      "self",
      "num_channels",
      "feature_dims",
      "eps",
      "use_scale",
      "use_bias"
    ],
    "__call__": [
      "self",
      "x",
      "mask"
    ]
  },
  "Gemma3nAudioSSCPConvBlock": {
    "__init__": [
      "self",
      "idx",
      "input_freq_dim",
      "config",
      "manual_padding"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Gemma3nAudioSubSampleConvProjection": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Gemma3nAudioConformerAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask"
    ]
  },
  "Gemma3nAudioConformerFeedForward": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Gemma3nAudioConformerLightConv1d": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "audio_encodings"
    ]
  },
  "Gemma3nAudioConformerBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "audio_encodings",
      "audio_mel_mask"
    ]
  },
  "AudioModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "audio_mel",
      "audio_mel_mask"
    ],
    "sanitize": [
      "self",
      "weights"
    ]
  },
  "Gemma3nMultimodalEmbedder": {
    "__init__": [
      "self",
      "multimodal_config",
      "text_config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "inputs_embeds"
    ]
  },
  "AudioConfig": {},
  "MultiQueryAttentionBlockConfig": {},
  "UniversalInvertedResidualConfig": {},
  "EdgeResidualConfig": {},
  "MobileNetV5MultiScaleFusionAdapter": {
    "__init__": [
      "self",
      "in_chs",
      "out_chs",
      "output_resolution",
      "expansion_ratio",
      "interpolation_mode",
      "use_layer_scale",
      "layer_scale_init_value",
      "noskip"
    ],
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "LayerScale2d": {
    "__init__": [
      "self",
      "dim",
      "init_values",
      "inplace"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "rms_norm2d": [
    "x",
    "normalized_shape",
    "weight",
    "eps"
  ],
  "RMSNormAct2d": {
    "__init__": [
      "self",
      "num_channels",
      "eps",
      "apply_act"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "UniversalInvertedResidual": {
    "__init__": [
      "self",
      "in_chs",
      "out_chs",
      "dw_kernel_size_start",
      "dw_kernel_size_mid",
      "dw_kernel_size_end",
      "stride",
      "dilation",
      "group_size",
      "pad_type",
      "noskip",
      "exp_ratio",
      "norm_layer",
      "conv_kwargs",
      "drop_path_rate",
      "layer_scale_init_value"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ConvNormAct": {
    "__init__": [
      "self",
      "conv_cls",
      "in_chs",
      "out_chs",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "groups",
      "bias",
      "apply_act",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "pad_same": [
    "x",
    "kernel_size",
    "stride",
    "dilation",
    "value"
  ],
  "get_padding_value": [
    "padding",
    "kernel_size"
  ],
  "get_same_padding": [
    "input_size",
    "kernel_size",
    "stride",
    "dilation"
  ],
  "get_padding": [
    "kernel_size",
    "stride",
    "dilation"
  ],
  "is_static_pad": [
    "kernel_size",
    "stride",
    "dilation"
  ],
  "Conv2dSame": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "EdgeResidual": {
    "__init__": [
      "self",
      "in_chs",
      "out_chs",
      "exp_kernel_size",
      "stride",
      "dilation",
      "group_size",
      "pad_type",
      "force_in_chs",
      "noskip",
      "expand_ratio",
      "pw_kernel_size",
      "norm_layer"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MobileAttention": {
    "__init__": [
      "self",
      "in_chs",
      "out_chs",
      "stride",
      "dw_kernel_size",
      "dilation",
      "group_size",
      "pad_type",
      "num_heads",
      "key_dim",
      "value_dim",
      "use_multi_query",
      "query_strides",
      "kv_stride",
      "cpe_dw_kernel_size",
      "noskip",
      "act_layer",
      "aa_layer",
      "drop_path_rate",
      "attn_drop",
      "proj_drop",
      "layer_scale_init_value",
      "use_bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "create_conv2d": [
    "in_channels",
    "out_channels",
    "kernel_size",
    "stride",
    "dilation",
    "depthwise",
    "bias"
  ],
  "to_2tuple": [
    "x"
  ],
  "NamedSequential": {
    "__init__": [
      "self"
    ],
    "add_module": [
      "self",
      "name",
      "module"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MultiQueryAttention2d": {
    "__init__": [
      "self",
      "dim",
      "dim_out",
      "num_heads",
      "key_dim",
      "value_dim",
      "query_strides",
      "kv_stride",
      "dilation",
      "padding",
      "dw_kernel_size",
      "attn_drop",
      "proj_drop"
    ],
    "_reshape_input": [
      "self",
      "t"
    ],
    "_reshape_projected_query": [
      "self",
      "t",
      "num_heads",
      "key_dim"
    ],
    "_reshape_output": [
      "self",
      "t",
      "num_heads",
      "h_px",
      "w_px"
    ],
    "__call__": [
      "self",
      "x",
      "attn_mask"
    ]
  },
  "num_groups": [
    "group_size",
    "channels"
  ],
  "make_divisible": [
    "v",
    "divisor",
    "min_value",
    "round_limit"
  ],
  "_er": [
    "kernel_size",
    "filters",
    "strides",
    "expand_ratio",
    "is_multiscale"
  ],
  "_uir": [
    "start_dw_kernel_size",
    "mid_dw_kernel_size",
    "filters",
    "strides",
    "expand_ratio",
    "is_multiscale"
  ],
  "_mmqa": [
    "num_heads",
    "kv_dim",
    "kv_strides",
    "mmqa_avg_pool_kv",
    "is_multiscale"
  ],
  "gemma3n_mobilenet_def": [],
  "VisionTower": {
    "__init__": [
      "self",
      "config"
    ],
    "build": [
      "self"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states"
    ]
  },
  "Gemma3nRMSNorm": {
    "__init__": [
      "self",
      "dim",
      "eps",
      "scale_shift",
      "with_scale"
    ],
    "_norm": [
      "self",
      "x"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "RMSNoScale": {
    "__init__": [
      "self",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Gemma3nLaurelBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Gemma3nAttention": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_kv_shared_layer"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache"
    ]
  },
  "gelu_topk": [
    "inputs",
    "std_multiplier"
  ],
  "Gemma3nAltUp": {
    "__init__": [
      "self",
      "config"
    ],
    "compute_router_modalities": [
      "self",
      "x"
    ],
    "predict": [
      "self",
      "x"
    ],
    "correct": [
      "self",
      "predictions",
      "activated"
    ]
  },
  "Gemma3nDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx",
      "is_kv_shared_layer"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "per_layer_input"
    ]
  },
  "logit_softcap": [
    "softcap",
    "x"
  ],
  "ImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "min_pixels",
      "max_pixels",
      "patch_size",
      "temporal_patch_size",
      "merge_size"
    ],
    "preprocess": [
      "self",
      "images",
      "do_resize",
      "size",
      "min_pixels",
      "max_pixels",
      "resample",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "do_convert_rgb",
      "return_tensors"
    ]
  },
  "PaddleOCRVLProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "__all__": [],
  "PaddleOCRVisionEmbeddings": {
    "__init__": [
      "self",
      "patch_size",
      "image_size",
      "in_channels",
      "embed_dim"
    ],
    "interpolate_pos_encoding": [
      "self",
      "height",
      "width"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "grid_thw"
    ]
  },
  "PaddleOCRProjector": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "spatial_merge_size"
    ],
    "__call__": [
      "self",
      "x",
      "grid_thw"
    ]
  },
  "PaddleOCRVisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "PaddleOCRRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PaddleOCRDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_embeddings"
    ]
  },
  "PaddleOCRModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "LlavaMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ClipVisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states"
    ]
  },
  "Llama": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "CrossAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "query",
      "key_value"
    ]
  },
  "ConnectorMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "VisionLanguageConnector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "image_features",
      "image_masks"
    ]
  },
  "PatchEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "VisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "VisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "JinaVLMProcessor": {
    "tokenizer_class": [],
    "attributes": [],
    "__init__": [
      "self",
      "tokenizer",
      "image_token",
      "chat_template"
    ],
    "chat_template": [
      "self",
      "value"
    ],
    "pad_token": [
      "self"
    ],
    "pad_token_id": [
      "self"
    ],
    "eos_token": [
      "self"
    ],
    "eos_token_id": [
      "self"
    ],
    "bos_token": [
      "self"
    ],
    "bos_token_id": [
      "self"
    ],
    "encode": [
      "self",
      "text",
      "add_special_tokens"
    ],
    "decode": [
      "self",
      "token_ids"
    ],
    "batch_decode": [
      "self",
      "token_ids"
    ],
    "process_one": [
      "self",
      "prompt",
      "images",
      "inference_mode"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "inference_mode",
      "return_tensors"
    ],
    "_collate_batch": [
      "self",
      "batch_results"
    ]
  },
  "CLIP_MEAN": [],
  "CLIP_STD": [],
  "DEFAULT_PATCH_TOKEN_ID": [],
  "DEFAULT_START_TOKEN_ID": [],
  "DEFAULT_END_TOKEN_ID": [],
  "DEFAULT_COLUMN_TOKEN_ID": [],
  "patchify": [
    "array",
    "patch_size",
    "batched"
  ],
  "RoPE": {
    "__init__": [
      "self",
      "dims",
      "theta"
    ],
    "__call__": [
      "self",
      "x",
      "offset"
    ]
  },
  "ExtendedEmbedding": {
    "__init__": [
      "self",
      "vocab_size",
      "additional_size",
      "dims"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "TextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "PerceiverConfig": {},
  "Idefics2PerceiverAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "kv",
      "mask",
      "cache"
    ]
  },
  "Idefics2PerceiverLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "hidden_states",
      "mask"
    ]
  },
  "Idefics2PerceiverResampler": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask"
    ]
  },
  "Idefics2Connector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask"
    ]
  },
  "_num_image_tokens_from_patch_grid": [
    "rows",
    "cols",
    "downsample_factor"
  ],
  "_original_init": [],
  "_compute_image_grid_info": [
    "pixel_values",
    "patch_size"
  ],
  "_original_call": [],
  "_ensure_slow_processor": [
    "processor_instance"
  ],
  "_patched_call": [
    "self",
    "images",
    "text"
  ],
  "Lfm2VlMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "PixelUnshuffleBlock": {
    "__init__": [
      "self",
      "factor"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Llama4MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "image_features"
    ]
  },
  "Llama4VisionPixelShuffleMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "encoded_patches"
    ]
  },
  "reshape_for_broadcast": [
    "freqs_ci",
    "query"
  ],
  "vision_apply_rotary_emb": [
    "query",
    "key",
    "freqs_ci"
  ],
  "Llama4VisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "freqs_ci",
      "mask",
      "cache"
    ]
  },
  "Llama4VisionMLP": {
    "__init__": [
      "self",
      "config",
      "bias",
      "is_projector"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4VisionEncoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_state",
      "freqs_ci",
      "mask"
    ]
  },
  "Llama4VisionEncoder": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "freqs_ci",
      "mask"
    ]
  },
  "Llama4UnfoldConvolution": {
    "__init__": [
      "self",
      "config"
    ],
    "_pair": [
      "self",
      "x"
    ],
    "unfold": [
      "self",
      "input_tensor"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "Llama4VisionRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "MoE": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LlamaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "create_chunked_attention_mask": [
      "self",
      "seq_len",
      "attention_chunk_size",
      "start",
      "offset"
    ],
    "__call__": [
      "self",
      "input_ids",
      "input_embeds",
      "mask",
      "cache"
    ]
  },
  "HunYuanVLImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "min_pixels",
      "max_pixels",
      "patch_size",
      "temporal_patch_size",
      "merge_size",
      "image_mean",
      "image_std",
      "do_resize",
      "do_normalize",
      "do_convert_rgb",
      "config"
    ],
    "_preprocess_single": [
      "self",
      "image"
    ],
    "preprocess": [
      "self",
      "images"
    ],
    "__call__": [
      "self",
      "images"
    ],
    "get_number_of_image_patches": [
      "self",
      "height",
      "width"
    ],
    "from_dict": [
      "cls",
      "config_dict"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "HunYuanVLProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "IMAGE_TOKEN_ID": [],
    "IM_START_TOKEN_ID": [],
    "IM_END_TOKEN_ID": [],
    "PAD_TOKEN_ID": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self"
    ],
    "get_imgs_pos": [
      "self",
      "doc_ids"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "split_image_into_patch_blocks": [
    "pixel_values",
    "patch_size",
    "adaptor_patch_div"
  ],
  "VisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "HunyuanRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "_update_cache": [
      "self",
      "seq_len",
      "dtype"
    ],
    "__call__": [
      "self",
      "x",
      "seq_len"
    ]
  },
  "apply_rotary_pos_emb_xdrope": [
    "q",
    "k",
    "cos",
    "sin",
    "position_ids",
    "xdrope_section",
    "output_size"
  ],
  "DecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "HunyuanModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "inputs_embeds",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "_pair": [
    "x"
  ],
  "unfold": [
    "input",
    "kernel_size",
    "dilation",
    "padding",
    "stride"
  ],
  "Mistral3PatchMerger": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "image_features",
      "image_sizes"
    ]
  },
  "Mistral3MultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "image_sizes"
    ]
  },
  "_get_llama_4_attn_scale": [
    "start",
    "stop",
    "beta",
    "max_position_embeddings"
  ],
  "Ministral3": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "cache",
      "inputs_embeds"
    ]
  },
  "Qwen2VLDecoderLayer": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Qwen2Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "MODEL_TYPE": [],
  "TokenType": {
    "text": [],
    "image": [],
    "video": []
  },
  "VariableResolutionResamplerModel": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim",
      "spatial_conv_size",
      "temporal_conv_size",
      "config"
    ],
    "spatial_conv_reshape": [
      "self",
      "x"
    ],
    "__call__": [
      "self",
      "x",
      "grid_thw"
    ]
  },
  "Ernie4_5_VLTokenizer": {
    "vocab_files_names": [],
    "model_input_names": [],
    "padding_side": [],
    "__init__": [
      "self",
      "vocab_file",
      "bos_token",
      "cls_token",
      "eos_token",
      "mask_token",
      "pad_token",
      "sep_token",
      "unk_token",
      "additional_special_tokens",
      "chat_template"
    ],
    "vocab_size": [
      "self"
    ],
    "space_token_id": [
      "self"
    ],
    "gend_token_id": [
      "self"
    ],
    "get_vocab": [
      "self"
    ],
    "_tokenize": [
      "self",
      "text"
    ],
    "_convert_token_to_id": [
      "self",
      "token"
    ],
    "_convert_id_to_token": [
      "self",
      "id"
    ],
    "build_inputs_with_special_tokens": [
      "self",
      "token_ids_0",
      "token_ids_1"
    ],
    "convert_tokens_to_string": [
      "self",
      "tokens"
    ],
    "save_vocabulary": [
      "self",
      "save_directory",
      "filename_prefix"
    ],
    "_decode": [
      "self"
    ]
  },
  "Ernie4_5_VLProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "IMG_START": [],
    "IMG_END": [],
    "VID_START": [],
    "VID_END": [],
    "IMAGE_PLACEHOLDER": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template",
      "spatial_conv_size",
      "temporal_conv_size"
    ],
    "pad_token": [
      "self"
    ],
    "pad_token_id": [
      "self"
    ],
    "eos_token": [
      "self"
    ],
    "eos_token_id": [
      "self"
    ],
    "bos_token": [
      "self"
    ],
    "bos_token_id": [
      "self"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template",
      "add_generation_prompt",
      "tokenize"
    ],
    "from_pretrained": [
      "pretrained_model_name_or_path"
    ]
  },
  "DFNRopeVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "Ernie4_5RotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "base",
      "mrope_section"
    ],
    "_recomposition_to_3d": [
      "self",
      "freq"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "rotate_half_interleaved": [
    "x"
  ],
  "Ernie4_5_MLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim",
      "use_bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Ernie4_5_MoeMLP": {
    "__init__": [
      "self",
      "args"
    ],
    "_route_experts": [
      "self",
      "x",
      "gate",
      "e_score_correction_bias"
    ],
    "__call__": [
      "self",
      "x",
      "token_type_ids"
    ]
  },
  "Ernie4_5VLDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids",
      "token_type_ids"
    ]
  },
  "Ernie4_5Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache",
      "position_ids",
      "token_type_ids"
    ]
  },
  "IMAGENET_MEAN": [],
  "IMAGENET_STD": [],
  "chat_template": [],
  "IMG_START_TOKEN": [],
  "IMG_END_TOKEN": [],
  "IMG_CONTEXT_TOKEN": [],
  "build_transform": [
    "input_size"
  ],
  "InternVLImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "do_resize",
      "size",
      "resample",
      "do_center_crop",
      "crop_size",
      "do_rescale",
      "rescale_factor",
      "do_normalize",
      "image_mean",
      "image_std",
      "do_dynamic_preprocess",
      "dynamic_min_num",
      "dynamic_max_num",
      "dynamic_use_thumbnail"
    ],
    "preprocess": [
      "self",
      "images",
      "do_dynamic_preprocess",
      "size",
      "return_tensors"
    ]
  },
  "InternVLChatProcessor": {
    "attributes": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "padding",
      "truncation",
      "max_length",
      "return_tensors"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "save_pretrained": [
      "self",
      "save_directory"
    ],
    "from_pretrained": [
      "pretrained_model_name_or_path"
    ]
  },
  "Glm46VProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "Glm4vVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "__call__": [
      "self",
      "seqlen"
    ]
  },
  "Glm4vVisionEmbeddings": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "embeddings",
      "lengths",
      "image_shapes",
      "h_coords",
      "w_coords"
    ]
  },
  "Glm4vVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4vVisionPatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4vVisionAttention": {
    "__init__": [
      "self",
      "dim",
      "num_heads"
    ],
    "__call__": [
      "self",
      "x",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "Glm4vVisionMLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4vVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "_compute_default_rope_parameters": [
    "config"
  ],
  "GLM4VRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "rotate_half_llm": [
    "x"
  ],
  "Glm4vAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_embeddings"
    ]
  },
  "Glm4vMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4vDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_embeddings"
    ]
  },
  "GLM4VModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "cache",
      "mask",
      "position_ids"
    ]
  },
  "IMAGE_TOKEN_INDEX": [],
  "DEFAULT_IMAGE_TOKEN": [],
  "expand_to_square": [
    "image",
    "background_color"
  ],
  "FastVLMImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "size",
      "crop_size",
      "resample",
      "do_resize",
      "do_center_crop",
      "do_rescale",
      "do_normalize",
      "do_convert_rgb",
      "rescale_factor",
      "image_mean",
      "image_std"
    ],
    "preprocess": [
      "self",
      "images",
      "size",
      "crop_size",
      "resample",
      "do_resize",
      "do_center_crop",
      "do_rescale",
      "do_normalize",
      "do_convert_rgb",
      "image_mean",
      "image_std",
      "return_tensors"
    ]
  },
  "FastVLMProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "return_tensors"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "build_vision_projector": [
    "config"
  ],
  "CallableModuleList": {
    "__call__": [
      "self",
      "x"
    ]
  },
  "MHSA": {
    "__init__": [
      "self",
      "dim",
      "head_dim",
      "qkv_bias",
      "attn_drop",
      "proj_drop"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ConvFFN": {
    "__init__": [
      "self",
      "in_channels",
      "hidden_channels",
      "out_channels",
      "act_layer"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LayerNormChannel": {
    "__init__": [
      "self",
      "num_features",
      "eps"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "AttentionBlock": {
    "__init__": [
      "self",
      "dim",
      "mlp_ratio",
      "act_layer",
      "norm_layer"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "RepCPE": {
    "__init__": [
      "self",
      "in_channels",
      "embed_dim",
      "spatial_shape"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ReparamLargeKernelConv": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "groups",
      "activation"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "RepMixer": {
    "__init__": [
      "self",
      "dim",
      "kernel_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "RepMixerBlock": {
    "__init__": [
      "self",
      "dim",
      "kernel_size",
      "mlp_ratio",
      "act_layer"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "basic_blocks": [
    "dim",
    "block_index",
    "num_blocks",
    "token_mixer_type",
    "kernel_size",
    "mlp_ratio",
    "act_layer",
    "norm_layer"
  ],
  "build_fast_vit_network": [
    "config"
  ],
  "SEBlock": {
    "__init__": [
      "self",
      "in_channels",
      "rd_ratio"
    ],
    "__call__": [
      "self",
      "inputs"
    ]
  },
  "MobileOneBlock": {
    "__init__": [
      "self",
      "in_channels",
      "out_channels",
      "kernel_size",
      "stride",
      "padding",
      "dilation",
      "groups",
      "use_se"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "ConvolutionalStem": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "FastViTHDModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states"
    ]
  },
  "GlobalPool2D": {
    "__init__": [
      "self",
      "in_dim",
      "out_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "MLPBlock": {
    "__init__": [
      "self",
      "embedding_dim",
      "mlp_dim",
      "act"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "SAMEncoder": {
    "__init__": [
      "self",
      "img_size",
      "patch_size",
      "in_chans",
      "embed_dim",
      "depth",
      "num_heads",
      "mlp_ratio",
      "out_chans",
      "qkv_bias",
      "norm_layer",
      "act_layer",
      "use_abs_pos",
      "use_rel_pos",
      "rel_pos_zero_init",
      "window_size",
      "global_attn_indexes",
      "downsample_channels"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "window_unpartition": [
    "windows",
    "window_size",
    "pad_hw",
    "hw"
  ],
  "get_rel_pos": [
    "q_size",
    "k_size",
    "rel_pos"
  ],
  "add_decomposed_rel_pos": [
    "attn",
    "q",
    "rel_pos_h",
    "rel_pos_w",
    "q_size",
    "k_size"
  ],
  "SAMViTCfg": {},
  "FastGELUActivation": {
    "__call__": [
      "self",
      "input"
    ]
  },
  "HybridVisionModel": {
    "__init__": [
      "self",
      "config",
      "resolution",
      "ignore_head"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "IMAGE_PATCH_TOKEN": [],
  "IMAGE_LOW_RES_TOKEN": [],
  "IM_START_TOKEN": [],
  "LOW_RES_IMAGE_START_TOKEN": [],
  "FRAME_START_TOKEN": [],
  "IM_END_TOKEN": [],
  "FRAME_END_TOKEN": [],
  "IM_COL_TOKEN": [],
  "IMAGE_PROMPT": [],
  "resize_image_pil": [
    "image",
    "desired_output_size",
    "resample"
  ],
  "build_resized_image": [
    "image",
    "base_image_input_size",
    "resample",
    "image_mean",
    "image_std",
    "image_patch_size"
  ],
  "build_overlapping_crops": [
    "image",
    "max_crops",
    "overlap_margins",
    "base_image_input_size",
    "resample",
    "image_mean",
    "image_std",
    "image_patch_size"
  ],
  "batch_pixels_to_patches": [
    "crops",
    "patch_size"
  ],
  "arange_for_pooling": [
    "idx_arr",
    "pool_h",
    "pool_w"
  ],
  "image_to_patches_and_grids": [
    "image",
    "max_crops",
    "overlap_margins",
    "base_image_input_size",
    "resample",
    "image_mean",
    "image_std",
    "image_patch_size",
    "image_pooling_w",
    "image_pooling_h"
  ],
  "Molmo2ImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "size",
      "resample",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "max_crops",
      "overlap_margins",
      "patch_size",
      "pooling_size"
    ],
    "preprocess": [
      "self",
      "images",
      "size",
      "resample",
      "image_mean",
      "image_std",
      "do_convert_rgb",
      "max_crops",
      "overlap_margins",
      "patch_size",
      "pooling_size",
      "return_tensors"
    ]
  },
  "Molmo2Processor": {
    "attributes": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "image_use_col_tokens",
      "use_single_crop_col_tokens",
      "use_single_crop_start_token"
    ],
    "get_image_tokens": [
      "self",
      "image_grid"
    ],
    "__call__": [
      "self",
      "text",
      "images",
      "padding",
      "truncation",
      "max_length",
      "return_tensors"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template",
      "add_generation_prompt",
      "tokenize"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "_original_auto_processor_from_pretrained_molmo2": [],
  "_patched_auto_processor_from_pretrained_molmo2": [
    "cls",
    "pretrained_model_name_or_path"
  ],
  "VitConfig": {
    "from_dict": [
      "cls",
      "params"
    ],
    "image_num_patch": [
      "self"
    ]
  },
  "AdapterConfig": {},
  "_gelu_from_name": [
    "name"
  ],
  "ViTMultiHeadDotProductAttention": {
    "__init__": [
      "self"
    ],
    "__call__": [
      "self",
      "inputs_q",
      "inputs_kv",
      "attn_mask"
    ]
  },
  "Molmo2VisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Molmo2VisionTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "add_pos_emb": [
      "self",
      "x",
      "patch_num"
    ],
    "__call__": [
      "self",
      "x",
      "patch_num"
    ]
  },
  "ImageProjectorMLP": {
    "__init__": [
      "self",
      "input_dim",
      "hidden_dim",
      "output_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Molmo2Embedding": {
    "__init__": [
      "self",
      "num_embeddings",
      "num_new_embeddings",
      "features"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "LanguageModelMLP": {
    "__init__": [
      "self",
      "input_dim",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Molmo2Attention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "mask",
      "cache"
    ]
  },
  "Molmo2DecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "mask",
      "cache"
    ]
  },
  "Molmo2Transformer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "AyaVisionMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "image_features"
    ],
    "pixel_shuffle": [
      "self",
      "image_features"
    ]
  },
  "CohereModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "DotsVLProcessor": {
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ]
  },
  "DotsSwiGLUFFN": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "DotsPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "grid_thw"
    ]
  },
  "DotsViTPreprocessor": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "grid_thw"
    ]
  },
  "DotsVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "PaliGemmaMultiModalProjector": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "_activation_from_config": [
    "config"
  ],
  "GemmaModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "GlmOcrProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "GlmOcrVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "__call__": [
      "self",
      "seqlen"
    ]
  },
  "GlmOcrVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "GlmOcrVisionPatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "hidden_act",
      "bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GlmOcrVisionAttention": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "position_embeddings"
    ]
  },
  "GlmOcrVisionMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GlmOcrVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "position_embeddings"
    ]
  },
  "GlmOcrRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "apply_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "repeat_interleave": [
    "x",
    "repeats",
    "axis"
  ],
  "GlmOcrAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_embeddings"
    ]
  },
  "GlmOcrMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "GlmOcrDecoderLayer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_embeddings"
    ]
  },
  "GlmOcrTextModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "cache",
      "mask",
      "position_ids"
    ]
  },
  "Qwen3VLMoERotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "rope_scaling"
    ],
    "apply_interleaved_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Qwen3MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3VLMoEDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Qwen3VLMoEModel": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache",
      "position_ids",
      "visual_pos_masks",
      "deepstack_visual_embeds"
    ],
    "_deepstack_process": [
      "self",
      "hidden_states",
      "visual_pos_masks",
      "visual_embeds"
    ]
  },
  "Glm46VMoEProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "__call__": [
      "self",
      "images",
      "text",
      "videos"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "Glm4VMoEProcessor": [],
  "Glm4vMoeVisionRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "theta"
    ],
    "__call__": [
      "self",
      "seqlen"
    ]
  },
  "Glm4vMoeVisionPatchEmbed": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "Glm4vMoeVisionPatchMerger": {
    "__init__": [
      "self",
      "dim",
      "context_dim",
      "bias"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4vMoeVisionAttention": {
    "__init__": [
      "self",
      "dim",
      "num_heads"
    ],
    "__call__": [
      "self",
      "x",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "Glm4vMoeVisionMLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4vMoeVisionBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "rotary_pos_emb"
    ]
  },
  "Glm4vMoeAttention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_embeddings"
    ]
  },
  "Glm4vMoeMLP": {
    "__init__": [
      "self",
      "config",
      "hidden_size",
      "intermediate_size"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Glm4vMoeDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_embeddings"
    ]
  },
  "Phi3V": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  },
  "ClipModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states"
    ]
  },
  "ClipVModel": {
    "__init__": [
      "self",
      "config"
    ]
  },
  "_calc_padded_size": [
    "width",
    "height",
    "padding_unit"
  ],
  "_calc_hd_transform_size": [
    "width",
    "height",
    "hd_num"
  ],
  "_hd_transform": [
    "img",
    "hd_num"
  ],
  "_pad_to_336": [
    "img"
  ],
  "Phi3VImageProcessor": {
    "model_input_names": [],
    "__init__": [
      "self",
      "image_mean",
      "image_std",
      "num_crops",
      "num_img_tokens"
    ],
    "calc_num_image_tokens": [
      "self",
      "image"
    ],
    "_process_single_image": [
      "self",
      "image"
    ],
    "preprocess": [
      "self",
      "images",
      "return_tensors"
    ],
    "__call__": [
      "self",
      "images",
      "return_tensors"
    ]
  },
  "Phi3VProcessor": {
    "attributes": [],
    "valid_kwargs": [],
    "image_processor_class": [],
    "tokenizer_class": [],
    "__init__": [
      "self",
      "image_processor",
      "tokenizer",
      "chat_template"
    ],
    "_convert_images_texts_to_inputs": [
      "self",
      "images",
      "texts",
      "padding",
      "truncation",
      "max_length"
    ],
    "__call__": [
      "self",
      "images",
      "text"
    ],
    "batch_decode": [
      "self"
    ],
    "decode": [
      "self"
    ],
    "apply_chat_template": [
      "self",
      "conversation",
      "chat_template",
      "add_generation_prompt",
      "tokenize"
    ],
    "model_input_names": [
      "self"
    ],
    "from_pretrained": [
      "cls",
      "pretrained_model_name_or_path"
    ]
  },
  "CodePredictorRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "rotate_half_code": [
    "x"
  ],
  "apply_rotary_pos_emb_code": [
    "q",
    "k",
    "cos",
    "sin"
  ],
  "CodePredictorMLP": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "CodePredictorAttention": {
    "__init__": [
      "self",
      "config",
      "idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "CodePredictorDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position",
      "position_embeddings"
    ]
  },
  "CodePredictorModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "generation_steps"
    ]
  },
  "CodePredictor": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "labels",
      "use_cache",
      "cache_position",
      "generation_steps"
    ]
  },
  "TalkerResizeMlp": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "TalkerTextMlp": {
    "__init__": [
      "self",
      "config",
      "intermediate_sz"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "TalkerSparseMoeBlock": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "TalkerModelDecoderLayer": {
    "__init__": [
      "self",
      "config",
      "idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "cache_position"
    ]
  },
  "TalkerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "visual_pos_masks",
      "deepstack_visual_embeds"
    ],
    "_deepstack_process": [
      "self",
      "hidden_states",
      "visual_pos_masks",
      "visual_embeds"
    ]
  },
  "Talker": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "input_ids",
      "attention_mask",
      "position_ids",
      "past_key_values",
      "inputs_embeds",
      "use_cache",
      "cache_position",
      "visual_pos_masks",
      "deepstack_visual_embeds",
      "generation_steps",
      "residual_codes",
      "trailing_text_hidden"
    ],
    "prepare_inputs_for_generation": [
      "self",
      "input_ids",
      "past_hidden",
      "trailing_text_hidden",
      "tts_pad_embed",
      "generation_step",
      "temperature",
      "top_p"
    ],
    "generate": [
      "self",
      "inputs_embeds",
      "trailing_text_hidden",
      "tts_pad_embed",
      "talker_input_ids",
      "max_new_tokens",
      "temperature",
      "top_p"
    ],
    "generate_stream": [
      "self",
      "inputs_embeds",
      "trailing_text_hidden",
      "tts_pad_embed",
      "talker_input_ids",
      "max_new_tokens",
      "temperature",
      "top_p"
    ],
    "sanitize": [
      "self",
      "weights"
    ]
  },
  "process_multimodal_info": [
    "conversation",
    "use_audio_in_video"
  ],
  "prepare_omni_inputs": [
    "processor",
    "conversation",
    "use_audio_in_video"
  ],
  "_get_feat_extract_output_lengths": [
    "input_lengths"
  ],
  "AudioEncoderLayer": {
    "__init__": [
      "self",
      "config",
      "idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "cu_seqlens",
      "attention_mask"
    ]
  },
  "SinusoidsPositionEmbedding": {
    "__init__": [
      "self",
      "length",
      "channels",
      "max_timescale"
    ],
    "__call__": [
      "self",
      "seqlen"
    ]
  },
  "SnakeBeta": {
    "__init__": [
      "self",
      "in_features",
      "alpha"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "LayerScale": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "CausalConvNet": {
    "__init__": [
      "self",
      "in_chn",
      "out_chn",
      "kernel_sz",
      "dilation",
      "stride",
      "groups"
    ],
    "_get_extra_padding_for_conv1d": [
      "self",
      "length"
    ],
    "__call__": [
      "self",
      "hidden_state"
    ]
  },
  "CausalTransConvNet": {
    "__init__": [
      "self",
      "in_chn",
      "out_chn",
      "kernel_sz",
      "stride"
    ],
    "__call__": [
      "self",
      "hidden_state"
    ]
  },
  "ConvNeXtBlock": {
    "__init__": [
      "self",
      "dim"
    ],
    "__call__": [
      "self",
      "hidden_states"
    ]
  },
  "Code2WavDecoderResUnit": {
    "__init__": [
      "self",
      "dim",
      "dilation"
    ],
    "__call__": [
      "self",
      "hidden_state"
    ]
  },
  "Code2WavDecoderBlock": {
    "__init__": [
      "self",
      "config",
      "idx"
    ],
    "__call__": [
      "self",
      "hidden"
    ]
  },
  "Code2WavAttention": {
    "__init__": [
      "self",
      "config",
      "idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "position_embeddings",
      "attention_mask",
      "position_ids"
    ]
  },
  "Code2WavMlp": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Code2WavTransformerLayer": {
    "__init__": [
      "self",
      "config",
      "idx"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "attention_mask",
      "position_ids",
      "position_embeddings"
    ]
  },
  "Code2WavTransformerModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs_embeds",
      "attention_mask",
      "position_ids"
    ]
  },
  "Code2WavModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "codes",
      "input_embeds"
    ],
    "chunked_decode": [
      "self",
      "codes",
      "chunk_size",
      "left_context_size"
    ],
    "stream_decode": [
      "self",
      "codes_buffer",
      "chunk_size",
      "left_context_size",
      "decoded_len"
    ],
    "flush_decode": [
      "self",
      "codes_buffer",
      "left_context_size",
      "decoded_len"
    ],
    "sanitize": [
      "self",
      "weights"
    ]
  },
  "Code2WavConfig": {},
  "CodePredictorConfig": {},
  "ThinkerConfig": {
    "from_dict": [
      "cls",
      "params"
    ]
  },
  "TalkerConfig": {
    "from_dict": [
      "cls",
      "params"
    ]
  },
  "Thinker": {
    "__init__": [
      "self",
      "config"
    ],
    "get_audio_features": [
      "self",
      "input_features",
      "feature_attention_mask",
      "audio_feature_lengths"
    ],
    "get_placeholder_mask": [
      "self",
      "input_ids",
      "inputs_embeds",
      "image_features",
      "video_features",
      "audio_features"
    ],
    "get_input_embeddings": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_values_videos",
      "image_grid_thw",
      "video_grid_thw",
      "input_features",
      "feature_attention_mask",
      "audio_feature_lengths"
    ],
    "merge_input_ids_with_image_features": [
      "image_features",
      "inputs_embeds",
      "input_ids",
      "image_token_index",
      "video_token_index"
    ],
    "layers": [
      "self"
    ],
    "__call__": [
      "self",
      "input_ids",
      "pixel_values",
      "pixel_values_videos",
      "mask",
      "cache"
    ],
    "sanitize": [
      "self",
      "weights"
    ]
  },
  "Qwen3OmniMoeThinkerTextRotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "rope_scaling"
    ],
    "apply_interleaved_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Qwen3OmniMoeThinkerTextSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3OmniMoEThinkerTextDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Qwen3_5RotaryEmbedding": {
    "__init__": [
      "self",
      "dim",
      "max_position_embeddings",
      "base",
      "mrope_section"
    ],
    "apply_interleaved_mrope": [
      "self",
      "freqs",
      "mrope_section"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "Qwen3_5RMSNormGated": {
    "__init__": [
      "self",
      "hidden_size",
      "eps"
    ],
    "__call__": [
      "self",
      "hidden_states",
      "gate"
    ]
  },
  "Qwen3_5Attention": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Qwen3_5MLP": {
    "__init__": [
      "self",
      "dim",
      "hidden_dim"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3_5GatedDeltaNet": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "mask",
      "cache"
    ]
  },
  "Qwen3_5DecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Qwen3_5Model": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Idefics3Connector": {
    "__init__": [
      "self",
      "config"
    ],
    "pixel_shuffle": [
      "self",
      "x",
      "scale_factor"
    ],
    "__call__": [
      "self",
      "image_hidden_states"
    ]
  },
  "Qwen3_5MoeSparseMoeBlock": {
    "__init__": [
      "self",
      "args"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "Qwen3_5MoeDecoderLayer": {
    "__init__": [
      "self",
      "args",
      "layer_idx"
    ],
    "__call__": [
      "self",
      "x",
      "mask",
      "cache",
      "position_ids"
    ]
  },
  "Qwen3_5MoeModel": {
    "__init__": [
      "self",
      "args"
    ]
  },
  "SigLipVisionTower": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states"
    ]
  },
  "MHA": {
    "__init__": [
      "self",
      "dims",
      "num_heads",
      "bias"
    ],
    "__call__": [
      "self",
      "queries",
      "kv",
      "mask"
    ]
  },
  "SigLipMultiheadAttentionPoolingHead": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "get_abs_pos_sam": [
    "abs_pos",
    "tgt_size"
  ],
  "DeepseekOCRProcessor": {
    "tokenizer_class": [],
    "attributes": [],
    "__init__": [
      "self",
      "tokenizer",
      "candidate_resolutions",
      "patch_size",
      "downsample_ratio",
      "image_mean",
      "image_std",
      "normalize",
      "image_token",
      "pad_token",
      "add_special_token",
      "sft_format",
      "mask_prompt",
      "ignore_id"
    ],
    "default_chat_template": [
      "self"
    ],
    "bos_id": [
      "self"
    ],
    "eos_id": [
      "self"
    ],
    "pad_id": [
      "self"
    ],
    "encode": [
      "self",
      "text",
      "bos",
      "eos"
    ],
    "decode": [
      "self",
      "t"
    ],
    "process_one": [
      "self",
      "prompt",
      "images",
      "inference_mode",
      "base_size",
      "image_size",
      "cropping"
    ],
    "pad_sequence": [
      "self",
      "sequences",
      "padding_value"
    ],
    "tokenize_with_images": [
      "self",
      "conversation",
      "images",
      "base_size",
      "image_size",
      "cropping"
    ],
    "__call__": [
      "self"
    ],
    "_collate_batch": [
      "self",
      "batch_results",
      "padding"
    ]
  },
  "NoTPTransformer": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x"
    ]
  },
  "position_ids_in_meshgrid": [
    "patch_embeds_list",
    "max_width"
  ],
  "generate_block_attention_mask": [
    "patch_embeds_list",
    "tensor"
  ],
  "PixtralRotaryEmbedding": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "position_ids"
    ]
  },
  "PixtralVisionModel": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "x",
      "output_hidden_states",
      "image_sizes"
    ]
  },
  "Mistral": {
    "__init__": [
      "self",
      "config"
    ],
    "__call__": [
      "self",
      "inputs",
      "inputs_embeds",
      "mask",
      "cache"
    ]
  }
}